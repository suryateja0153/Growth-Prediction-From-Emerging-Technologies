 you [Music] it's my real pleasure this morning to introduce Chris Manning chris is the Siebel professor of machine learning at Stanford University where he has a joint appointment in computer science and and linguistics reflecting his broad and multidisciplinary background he's also the director of the Stanford AI lab and is one of the folks on the Advisory Board of a new human centered AI initiative at Stanford so you can see the breadth of his his influence at Stanford and be well beyond that he's very very well known for his work on statistical natural language processing ranging from more linguistic perspectives on parsing and grammar induction to more machine learning perspectives including his recent work on glove and and many other techniques that leverage large scale data to solve very interesting problems chris is amazingly well known in many different communities he has over a hundred thousand citations he you can figure out how many that is per waking hours since he publishes his first paper it's probably more than one but another part of his influence I think goes above and beyond his his research that spans both spans language artificial intelligence machine learning information retrieval he's a fellow of all three of the major societies in computer science linguistics and AI but he's also contributed to the field in many other ways he's the author a co-author of two really important books on one on the foundations of statistical NLP and another on in modern information retrieval but he's also with his students develop many tools that are shared how many have used the stanford parser at their sanford NLP tools he shared code of all kinds he shared representations things that arose out of his work on glove and so chris i think is a real scholar in many senses of that that word what going to do today is talk to us about how we can go right now if you look at language and representations many of them focus on understanding the relationships among words similarity structures and in words but he's going to talk much more broadly about how we can bring notions of memory attention compositionality and reasoning to develop systems that have much more intelligence than many of our current systems so Chris without further ado thanks ok good morning and thank you soon for that very kind introduction I hope I can live up to it and takes to everyone for coming to the talk yeah so today what I want to do is talk about some of our recent work on trying to build neural networks that can reason and this is joint work with my student drew Hudson so our current neural network machine learning systems truly excel on a variety of tasks such as speech recognition and computer vision and interestingly they've even been pushed with surprising success to other tasks which you'd think involve higher-level reasoning and understanding a notable success has been the development of neural machine translation which has worked exceedingly well but in some sense even neural machine translation has approached things as a stimulus-response task that your provider here's the input sentence produce the output as a kind of a statistical Association task so these kind of tasks don't require deliberate thinking or reasoning the kind of processes that Daniel Kahneman has referred to as thinking slow so what can we do about reasoning reasoning is central to figuring out a good approach when humans are faced with a new problem in developing longer-term plans for higher-level objective can we use new networks to do the kind of model libera conscious multistep thought so that might be things like middle school style reading comprehension problems which aren't really like what most of the current reading comprehension question-answering datasets are like but much more understanding a broad understanding of the characters and what they're doing in a novel common sense reasoning and problem solving working out plans playing a strategic game but tast like this we really need a better notions of having knowledge reasoning and inference so what then is reasoning how can reasoning which has traditionally been thought of in connection with logics and hand-built knowledge representations be learned by the kind of distributed representation computation units of deep learning so one inspiring viewpoint on this was provided by Leon Batu in 2011 who suggested that the heart of reasoning is having the means for algebraically manipulating previously acquired knowledge in order to answer new questions so but to suggested that we could seek to enhance deep learning systems with reasoning capabilities by from the ground up that reasoning is not necessarily achieved by making logical inferences that he saw a kind of continuity between algebraic algebraically rich inferences and connecting together trainable learning components and in particularly emphasized that central to reasoning is composition rules to guide the combinations of modules to address new tasks and so one of the things I'm want to think about today is how can we start to build composition rules for multi-step reasoning in neural networks and so underlying this is some kind of a conception as to where things might head in deep learning so in some sense the dominant viewpoint in the deep world community is to seek a learning device that is as empty as possible some kind of tabula rasa and so what's happened in a lot of recent work is that the emptiness of the machine learning device is compensated for by providing extra information in the input so a very successful technique in recent years has been data augmentation so effectively you're providing lots of extra inputs on a vision system you rotate the images a little and shave off a few pixels and change the coloration and things like this where you're saying all of that your model you want to build should be invariant to all of those things however I believe that we shouldn't be afraid of good inductive biases of thinking about how we could design our neural network systems so that they're built so that they have the ability to learn quickly and well and that probably having that kind of architectures some architectural constraints is vital to how human beings are able to learn so quickly and well indeed I think the biggest breakthroughs and deep learning have come from building the right kind of inductive biases sort of structural priors into models now of course you can fail if you build models with too rigid structure but we succeed by finding appropriate or flexible structural priors so these successes include convolutions which have been central to work envisioned the notion of attention which has been very central to a lot of the recent advances in natural language processing in other areas also the kind of gating that you find in LST m's or highway networks is again building in a good structural prior into your models in my early deep learning work I was a strong proponent of the idea of tree structured compositional models I believe that that gave the right inductive bias for many human language tasks and also some other some in other domains so the model was that you would take pairs of vectors which would initially be vector representations of words and then you'd build from that a vector representation of a phrase like very good here and then you could continue applying the same pairwise composition operation recursively and you could build up representations of phrases and sentences and we were able to apply this model with some success to various tasks including sentiment analysis so here the model could start reading the sentence there are slow and repetitive parts and start to build up a composed structure where it says well that's a negative impression but it has just enough spice to keep it interesting and it builds up a representation where that part of the symptoms is positive and the positivity wins out so that overall this is a positive thing to say about a movie and these models aren't only applicable to language you can actually apply the same ideas and vision so that visual scenes also commonly have a compositional structure and so in these experiments that we did on the Stanford background data said that it's building up a compositional representation of a church from the pieces of the building and is using that to understand this visual scene interestingly what botter suggests in 2011 as a model for reasoning was essentially exactly the same kind of tree structure and recursive neural network that we started employing for natural language he proposed that the path to universal composition was that you built an association module a in this picture from his paper that maps to representations taken from memory into a new representation of the same sort and that that new representation can be scored by module R in this picture a guy in France and simultaneously the new representation can be put back into the short memory where it can be used recursively to build up a proof tree however for reasons of partly computational efficiency and partly limitations and flexibility it turns out that tree structured composition hasn't really won out in neural network research in the last five years but there are alternatives so one that I already mentioned was this idea of attention I mean if you look at it you can think of attention as almost trees by another name because we're effectively putting a sort of a soft tree structure over the previous nodes to generate a representation of the next node and we can apply that recursively as we work building up along the sequence but we now have sort of soft weights rather than a rigid tree structure you cannot a second alternative for building up something like the two's function that takes multiple arguments is to think that maybe our neural networks could do what logicians call carrying so rather than having multi-argument functions like f that's taking three arguments X Y Z we could instead build intermediate compositions so we can take one argument at a time like X and build an intermediate representation or function which can then take the next argument Y and proceed to build up an overall representation and it seems reasonable to assume that neural sequence models could do this kind of computation so in this current line of work the hope is that we could start exploring your network architectures so that rather than having big fairly unstructured neural networks that act as Association engines or correlation engines that look just for any kind of patterning in the import we could use a model that is more structured with the prior that encourages compositional and and spirit multi-step reasoning however we'd like to do this in models that are still practically usable and so for the models that we've built we've focused on models they're still into n differentiable there's a ton of work now on building reinforcement learning models in which this is not true but I still see it I still see the simplicity of entering differential models as so much easier to train and work with and also it provides us in a space where it's easy to build models that are still scalable to reasonably large problems so in the work today I'm going to concentrate on showing results from the area of visual question-answering so here we've shown a picture and we asked questions about it and the suggestion is that asking questions is a good way to assess understanding this was a viewpoint that was put forward very early on so long ago there used to be the Yale AI school who here is old enough to have heard of the Yale AI school yeah that's about three people and so what members of the Yale AI school was Wendy Leonard who worked on question answering and she writes when a person understands the story they can demonstrate their understanding by answering questions about the story since questions can be devised to query any aspect of text comprehension the ability to answer questions is the strongest possible demonstration of understanding the same is true for visual scenes and what I've been interested in visual question answering is because it's just seemed like a better proving ground for compositional reasoning although there has been a ton of work which I've also been involved in in doing textural question answering systems in general the textual question answering work has still been dominated by lexical semantics matching and there just hasn't been very much opportunity to multi-step iterative reasoning and that domain where it seemed like visual question answering gave a kind of a nice place ground to be looking at multi-step compositional reasoning ok so that's my intro I'm trying to go from Xin learning to machine reading and so for the rest of the talk today I first of all want to tell you about our initial work on Mac networks on the clever task and tell you a little bit about a new dataset gqa that we've developed more recently the visual question-answering and then tell you about more fresh off the press Newell state machine model and they're doing visual question-answering ok so let's just start off with the clever data said so the clever data set came out of considerations of visual question-answering and so the belief of some of the people working in visual question-answering that that the the most used visual question-answering tasks which came from Bartra and per weeks group at georgia tech that it had sort of led to some kind of research on language and vision but it hadn't really been a very good testing ground for actually doing scene understanding and compositional reasoning and so justin johnson and colleagues at face book AI research decided that they should try and come up with a diagnostic data set that especially focused on compositional language and visual reasoning about scenes and so they went back to this old classic of AI blocks world and so they synthesized block world scenes in blender and then they asked long compositional questions about these scenes so in this one there is a purple cube there are some purple cubes but this one is behind a metal object there's a metal object so that seems it has to be this purple cube that is left to a large ball well there's a large ball and it's left of it that seems hopeful what material is the cube and if you haven't seen this data set before you'd probably say no idea but it turns out in this data set all things are made out of two materials they do that metal or rubber so if they're not shiny the right answer is rubber okay and so a number of people have worked on systems to approach this data set but as well as the scene and the question and answer another attribute of this data set which was there if reflection as to how it was constructed that on the right hand side there's a sort of informal representation of a functional program so the way that this data set was constructed was that they were building functional programs that they could run on these data on the visual scenes in an abstracted form and get out of them the answer and then that functional program was converted into natural language and so one of the questions on using this data set then will come up again later is are you just building a system over just textual questions and answers and images or are you making use of these this functional program as added supervision so one of the example of a piece of work that has made use of the functional program has been a line of work by Jakob andreas and then by Justin Johnson himself that has explored Newell module networks so these are partially differentiable models that try to approach the problem composition way but they rely on the strong supervision of the functional program to translate queries into a tree structured functional program so that the first part of the model is LST M encoder/decoder model that reads the textual question and juices the functional program and then the second part of that is another neural network which interprets this functional program so that neural network is built out of custom building blocks for these different semantic operations so there's counting neural network and the comparing neural network and a filtering neural network and they're plugged together in a compositional way and they interpret this functional program and then aim to give the answer from it okay but what I want to do first today is to introduce our version of approaching this problem which is the Mac Network which stood for memory attention composition as a neural model for problem solving and reasoning tasks so we want to have this idea of building a network with structure that encourages it to do multi-step reasoning so it should decompose a problem into a sequence of explicit reasoning steps and each of them as corresponds to one Mac cell in our network but on the other hand it didn't seem right to us the kind of neural module Network approach that I just showed on the previous slide that it seemed much too bespoke by the time you were custom designing individual Network units for comparison filtering counting and things like that that didn't seem sufficiently generic as a model of intelligence so what we wanted is one universal Mac cell that could be used to be for everything and it's versatile enough that it can learn to do different things depending on the context in which it's applied and what we're building is a recurrent neural network but the recurrent neural network is able to have attention backwards throughout a sequence of reasoning steps and so through attention it can a soft way represent a complex reasoning graph and but in a model that still has end to end differentiability and so each Maxell is sort of one step of a reasoning system and so the design we had is of a more articulated design than most standard recurrent neural networks since it the model retains two recurrent states so it has a control state which is used to describe the reasoning operation of the network and the control state is an attention based average of a given query which in our case is just the textual question and then there's a memory state which is based on information which in general we'd say is being collected from a knowledge base that in the particular application here the knowledge base is simply the image that the system is looking at and so the memory is going to be represented as an attention based average over our image or over our knowledge base so there are a couple of things that are worth at least noting here one is that in our model we're not going to make use of the strong supervision of the functional programs at all we're simply working from the question and the picture and attempting to get the answer the other one is this design choice that we're representing thinking in terms of attention so attention based averages over given query and attention based averages over the image the second model I show later does things a bit differently and has more abstraction we'll get to that later but it also represents everything as attention based averages and I can't prove anything here this is just a bet but it seemed to us that the use of attention based models of this sort has proved to be very successful and an immediate good property of these kind of models is that attention gives the kind of easy interpret ability you can say what words is the model looking at and what part of the picture is the model looking at but beyond that it seems that by grounding our models in the space of attention Everage --is that that's appears to be a a useful way to somehow constrain and direct the models and get them to learn more effectively than if we just have unconstrained hidden states okay and slightly more detail so here is our Mac cell so on the top part of the Mac cell we have the control unit which computes a control state so it takes in the question text and the previous control state and we'll generate a new control state by focusing on some aspect of the query and then down in the bottom we have the memory so this part takes in a preceding memory hidden state and our so-called knowledge base here the image and based on the control information and the previous memory State it reads some information out of the knowledge base or image and then that generates a new candidate memory which is combined with preceding memories to generate a new MI memory unit which is then written into the next memory State merging old and new information and so the bet here is at the time we first started this work there had been some quite prominent work from deep mind on neural Turing machines and then differentiable neural computers and it seemed that although you know in theory those models were very powerful it seemed like in practice they were very difficult to control and indeed I think to this day they haven't been demonstrated on any larger scale problems and part of the different he comes from the sort of Turing machine style arbitrariness if you can read and write anywhere and that makes it very difficult to learn and control these models whereas this model has a very simple or ionization since your sequentially laying down memories but each next memory can be done based on attention over previous memories let's see time goes by fast so I probably should do this quickly so I get on to the later ones okay very quickly more details in the paper okay so the control part takes the previous control straight and the query it computes a representation it then uses attention on to the words of the well actually I should explain that for the words of the question there run through a bi-directional lsdm so they're sort of contextual words so it then uses what it's computed up here to put attention over the words of the query in the standard attention distribution way to produce a weighted average over the words of the queries and normally focusing on certain words and that gives the next time steps control signal for the read unit that's taking in the previous memory state and the knowledge base the picture and then it is wanting to get something out of the image and the early part of that allows the previous memory to interact with the image and get stuff out of it in an associative way and then it feeds in the control state and does the second round of using that to put attention over the image and again we have a weighted weighted based on attention retrieval from the image and that creates the new memory state and so then with also the new candidate memory state so for the new candidate memory state that then goes into the light unit and so the new candidate rent memory stayed then itself is combined with past memory States using the control stayed so the control is used to feed which past memory states to pay attention to in a kind of a key-value attention mechanism and so that is then used to calculate a sort of a weighted distribution over previous memories their new memory and that that then gives you the new memory state and the hope is that we can simulate in that way doing a kind of a soft but arbitrary dag of reasoning of successively writing new memory states so that's one Maxell and then we build a Mac Network by building a recurrent sequence model that runs through a bunch of those cells and that gives us a model that's efficient easy to deploy and still fully differentiable but it has the capacity to represent arbitrarily complex reasoning via directed acyclic graphs in a soft way through attention okay so let me provide present the results of this in terms of the initially the clever data set so it had 700,000 training examples 150,000 test examples the space of answers is very small there's metal rubber cube sphere few colors very small numbers 0 1 2 3 4 5 so the baseline is quite high so if you answer the most frequent answer by question type you're already at almost 42 percent but what they'd wanted to show is that this kind of architectures that had been used for visual question answering sort of CN n stacks plus L STM's didn't really work on this data because they didn't do the necessary reasoning so kind of a state-of-the-art bqa system at this time a couple of years ago only got 52 percent compared to that neural module networks Jacob and Reza's work did a ton better God 83% which was not too far off the 92.6% that was reported as human accuracy um but you know actually these problems were synthesized right so there are synthesized images and synthesized answers based on the functional programs so there's absolutely no reason why system shouldn't be able to get a hundred percent on this data and there's a sort of actually a fairly artificial reason why human performance was depressed from a hundred percent not just human laziness and so actually what's happened in more recent work was sort of all of the action moved to the ninety five to a hundred percent space and so initially deep mind came up with a relation network model that got to 95.4% then Justin Johnson did a new generation of neural module networks that got to almost 97 percent people and Montreal with the film network got to ninety seven point seven percent and so relation networks and film are essentially both large CN n stacks interleaved with specialized layers so relation Nets had a relation net layer where every pair of pixels has relationships assessed between it to understand binary relations film inserts these conditional linear normalization layers that tilt the activations based on the question is how'd it get insured of sense about that model does but anyway our mac network works super well it gives 98.9% which essentially halves the remaining error but you might be starting to wonder how meaningful this is and thinking so close to a hundred percent so something that i think is interesting is the following that if we look at learning curves based on how much training data you give to the models and if use instead of giving the model 700,000 examples you only give them 70,000 training examples then what you do is find that other models such as the film model or the new module network model of Justin Johnson they can't actually learn very much from 70,000 examples at all even if 70,000 was a lot right this isn't a very big space you think you shouldn't have to learn something from you know 70 here 100 and examples that they sort of don't get that far above baseline performance and most common classes whereas our magnet model is already getting 86 percent accuracy over 70,000 examples so I think that's actually an interesting proof that the design of the model has the right kind of priors to be solving these kind of multi-step inference problems now the subsequent work Justin Johnson also built a clever humans dataset where he collected 18,000 who were your questions from humans through crowdsourcing where they're told to write questions hard for a smart robot to answer and so these questions have a more diverse vocabulary different kinds of reasonings skills which might not be in the original data set and there's a small training set for fine-tuning so if you run with the original data zero shot on this clever humans data set you get these results when none of the systems work great but magnet does do a little bit better if you fine-tune on the training set you get these results where again the mac net is able to get more value from the the fine-tuning set than any of the other models again I think reflecting its ability to generalize well from small amounts of data and starts to get quite good performance now here's sort of a couple of examples that show how we get the benefits of having interpretable reasoning from having the attention over both the sentence and our image so what color is the map thing to the right of the sphere front of the teeny blue block so this is a very short example where there are these three reasoning steps so on the first reasoning step it focuses on teeny blue block in the language and looks at the teeny blue block in the second reasoning step it's the sphere in front and it's then focusing in the image on the sphere in front and then third reasoning step is color is the map thing and it's going to the map thing and asking its color and correctly answers purple you get better results for longer sequence of reasoning so here's it using six reasoning steps on this example how many objects are either small objects behind that tiny metal cylinder or metallic cubes in front of the large green metal object and often what you see here is in the first couple of time steps in more sort of looks at the macro structure of what it's being asked before it starts to focus visually so initially it's looking at the how many the question type and well either small I'm not sure what it's doing there but then the second step is realizing it's sort of a disjunction between two things and then in the third time step it starts focusing on particular things so large green metal is looking at the large green metal metallic cubes and it's looking at the metallic cubes teeny metal cylinder and looking at that small objects behind things at any rate it's not quite clear how it does the counting but it does in this example correctly if the answer of four so I guess it's managed to somehow get that out of the what's going on okay so that was an initial neuro compositional reasoning engine and so I hope I've shown that it seems like we've gone some good value from having a constrained sequence model with good priors separated out control and memory and exploiting attention but let me move on and get to the newer stuff okay so one problem is that although there were these early using datasets including clever they seem kind of limited there's artificial images and/or language there's a very small space of possible objects and attributes and so although in theory you're doing compositional reasoning the feeling is that because the amount of data is so large and the space is so small that the suspicion is that these models actually just memorize molecules so they just learn what a red metal sphere is and that they therefore they have a lot less compositionality in them than you'd hope they have on the other hand the main visual question-answering benchmarks have also seemed somewhat problematic so there are strong what are often referred to as language biases but i think it's mainly actually more real-world biases and models guess based on priors so snow covers the ground the grass is green and things like that there's also visual biases to overly say land salient objects and it's hard to tell when systems are going wrong what exactly is causing it and really the questions are too simple the questions are often simply what color is the grass so little reasoning or compositionality is required and so and we've worked to produce this new data set for compositional question answering over real world in images called gqa and in some sense this is clever done on a larger scale so we've generated we Tet start with real images which come from the same MS Coco sources and everybody uses in the vision world and then generate questions involving sort of compositional questions a bit like clever and so we generate 10 million compositional questions overall and then generate a balance smaller set of questions with closely controlled answer distributions and the way we do that is that we make use of and provide a scene graph with each image that represents its semantics and then the questions like in clever come with a functional program grounded in the scene graph that shows that semantics perhaps surprisingly the questions we build are generated using a traditional rule-based question engine but you know this means that it's sort of just so the precise semantics that a rule-based system can give you that we're just sort of turning the scene graph representation in a very controlled manner into question natural language questions are still a controlled language but we tried to give much more in the way of linguistic diversity and a large vocabulary and since we totally can understand the scenes and we can have metrics that can assess the consistency and model the answers and various other metrics okay so in slightly more detail our starting off point is the visual genome data set that my Stanford colleagues are on Jai Krishna and Michael Bernstein Fei Fei Li and others developed where you're staying off at the MS koko pictures and then they're putting over this a scene graph representations shown at rise where there are identified objects which are the pink things like helmet watch and man these objects can have attributes silver helmet black watch and then they can be connected together by relationship so the man is wearing a helmet or the cow is kneeling on the grass we produce the sort of improved visual genome so rather than just bounding boxes we made use of the last few years and computer vision technology and now have maths over images but more importantly the original scene graphs are completely unrestrained in their natural language labeling of things so we move to a clear ontology of concepts by resolving synonyms discarding some very unclear or rare things so our ontology has 1,700 objects 600 attributes and 330 relations which are grouped into 60 categories and sub-categories we also augment the graphs with some additional information we put in positional relation information some kind of comparative information of same or different color and some other global information that seems useful but intend to be in the scene graphs thinks about the weather and things like that ok so then once we have these scene graphs we generate using our rule-based engine natural language questions and you know this is a conventional rule-based natural language generation system with probabilistic rules so we can have sort of common and uncommon patterns with a sort of standard kind of context free style grammar that can build up descriptions there's then kind of quite a lot of work behind the scenes which is then trying to make these questions be good questions so we want to make sure they're answerable they're uniquely answer a ball and that they seem reasonably natural so that you know they're sort of actually then a lot of large somewhat induced somewhat hand-built lexico that underlie what kind of words and natural words to use to describe various kinds of relationships then lightly more abstractly expressed in the scene graph and this gives you an idea of the differences between DQ a and G QA so visual question answering has sort of crowdsource questions and crowd sourced answers in unrestricted natural language so you know the V QA questions are real human questions whereas g QA the questions are more artificial because we're generating them from our grammar on the other hand the g QA questions we believe her a better testbed for exploring c understanding and reasoning because the vqa questions are sort of sort of random does this man need a haircut what is different about the man's shirt that shows this is for a special occasion and rely on a lot of sort of world knowledge that you can't really answer things from the question our our questions are sort of more straightforward seeing understanding questions which are perhaps more artificial sounding is there a necktie in the picture there's not red but we're at least we're thinking that this is probably a sort of a better testbed for exploring visual understanding and compositional reasoning than vqa has provided and you know this isn't a one is better this is sort of better for a certain purpose kind of an answer here are the baseline accuracies that we got with this new data set so global prior is if you're just saying if you're asking a color question you give the most common color answer you get whatever about 17% this is a vision only model so there's been a tradition vqa of seeing how far you can get with a vision only model or a language only model the local prior is prior prior for a particular kind of object so this is if you're asking what color is the Apple you do rather better than the global prior the El STM is the language only model and this is the result has always been shown in current vqa systems that you can do not terribly by ignoring the picture altogether and just answering based on the language that you're given LST m plus CNN is sort of the standard baseline bqa system bottom-up is the recent kind of winning the QA systems of Peter Anderson and colleagues that use bottom-up and top-down attention if you look really closely you'll notice that our Mac model was better than any of these other models but somewhat disappointingly the Mac model doesn't actually work very well this data set is trying to prove the point a more abstract reasoning and it's way below humans that are coming in at about 90% accuracy so in the last bit of the talk I then want to tell you a bit about a new model that we've been exploring that tries to do somewhat better than this and so now the thing that we've been exploring this new model is so for visual question answering well question and answers are clearly about language but maybe we could actually make more progress on visual question answering rather than having these systems which have vision on one side and language on the other side if we instead represented what we're doing for visual question answering is doing everything in terms of an internal language of thought so what we'd like to do is have a common conceptual representation which we can represent both language which we use to represent both language expressions and visual scenes in and then we'll be able to reason using that and we'll be able to do better ok so I am no expert on human visual perception but nevertheless my superficial understanding of human visual perception is the idea that brains have a photo of in their brain that they're using isn't supported by most of what's in human visual perception instead that you know eyes are making momentary fixations and what they're getting out of each fixation is some kind of very high-level seen gist right so you do a fixation on the man and you see there's a man who looks like a cyclist with neo glasses helmet gloves watch and you're sort of aware of the fact there's a grassland scene with a cow somewhere off to the right but you know you don't actually get a lot of detail out of that and then you'll sort of Sicard make another fixation and maybe you look at the cow and then you Motors has horns and a bell on the front or something like that but it's really that so they're fairly abstracted seen just as what's actually in the brain as you do this visual processing so in the same way our hope is that we can use concepts to organize our visual sensory experience and we can build from those a kind of an abstraction a world model to represent what we're seeing in our environment and our world models will essentially be the scene graphs that I just showed you and that those we'll be able to use those to draw inferences and so this is sort of fits in with the general deep learning story so the hope of deep learning models is that we could use the depth to learn higher level abstractions and the reason why that should be useful is that for surface signals like visual signals there's all sorts of complications and variations but if we can build a higher level abstractions we should be able to disentangle our representations and that will allow us to improve generalization and so in particular the way we want to do that is by building a model that does content-based attention over concepts so attention is our central operation again that we're using here so we're going to have a large set of concepts and we're going to put attention over a few concepts and crucially what we're going to explore here is previously in the most other vision systems you're putting attention directly over the image over pixel space but here it's arguing that maybe if instead we can put our visual attention over concept space we'll be able to do rather better where concept space is our language of thought so this is sort of related to what yoshua bengio has proposed as is so called consciousness prior I'm not sure I like that name but I think the idea is basically right so the general research program is to say that we should try and learn deep representations that disentangle explanatory factors and then his suggestion is that conscious state is a very low dimensional vector which is sort of an attention mechanism over the disentangled deep representation the way we're doing things is somewhat different what he proposes but in some sense it's a sort of same ideas of disentangle concept space putting attention over it and so this is a model of the neural state machine so we have a vocabulary of embedded concepts which are our atomic semantic units and that is now cleaned up visual genome ontology that I told you about earlier and then but this time we're going to translate both questions and images into these concepts so that they both speak the same language so everything is going to be represented as a tension over concept vocabulary and so we hope that this will give us sort of a model of concept learning and use somewhat similar to what humans might do okay so in your state machine is a differentiable graph based model which simulates a state machine and so in some sense we're hoping to see it combining some of the strengths of both neural and traditional symbolic approaches and so this is what we do we've got two stages of construction and in France so in the construction stage we are going to take image and turn it into a scene graph and so the way we're doing this is following other work that's done image two scene graph and again including work by Ron Jai Krishna and others at Stanford we're using similar methods to generate scene graphs from images we do object recognition and then have for the components that infer relationship like attributes of objects and relations between objects and simultaneously we run encoder/decoder style morale over the natural language question and turn into a sequence of instructions which are at tension over concepts in our abstracted concept space and then we're going to do inference which is going to be a state machine style computation over our graph to compute an answer so you know formally we have something that looks like a finite state machine with States and edges then there are kind of a sequence of instructions that are going to be sort of like an input but we're going to do things in probabilistic manner so we start with the probability distribution over initial States and then we have a neural state transition function which computes probabilities of next States based on the current state distribution and what the instruction is okay so this is it all pictorially so from an image we construct a scene graph which happens to also look like a state machine where the states correspond objects' and the transitions correspond to relations and the states have these properties and all of this is represented in the soft way as attention over concepts in our concept ontology and so effectively these attentions over concepts give us a disentangled representation in terms of concepts and their properties and relations between them so it's all factorized using our concept vocabulary the question is then also translated into a series of instructions and so each instruction is again an attention distribution over concepts but if we just go with the Arg max of it what is the red fruit inside of the bowl to the right of the coffeemaker gets interpreted to instruction sequence of coffeemaker right ball inside red and so then we can run these instructions on our state machine and say well start with the coffeemaker you want to look to the right there should be a bowl there you want to look inside that and there's something red and the network will then be able to say oh yeah that's an apple and will then be able to answer the question correctly now here's one more example of that so what is the tall object the left of the bed made of so the model will form the instruction sequence bed left tall made and so at all first of all the looking at the bed looking to the left of the bed confirming this something tall there then asking what it's made of and will turn the answer red but you know I'm just sort of doing this as sort of one hot one hot language and describing it really at each stage this is done as sort of soft attention distributions and at all times there's a soft a soft activation level over the entire of the scene graph um so here's our new model so this is over gqa again so these were all the models I already talked about and Mac and so we are actually getting a nice lift there from the neural state machine model which is now working quite a bit better um fine point you'll notice um max doing a bit better than before as well yeah it turns out if you turn these models for longer outweighs them do a bit better nevertheless in neural state machine is doing kind of well better than that and I hope that that's sort of showing that you know working having reduced things both to the language of concepts gives you some power here a couple more demonstrations so there's another interesting data set that was done good to know I shower go ow and colleagues from Georgia Tech where they wanted to sort of give a better test of visual question-answering than standard vqa data set so they produced this vq acp data set where the liberally the distribution was changed between the training set and the test set so for example the training set for what sport is at questions the two most common answers were tennis and baseball that they changed the distributions on the test set the most common sports showing the skiing then baseball soccer I'm skateboarding etc so you have a different distribution and so if you've actually an understanding concepts you should be able to still get the answer right but if you're just guessing based on world priors you'll get the answers badly wrong and so what they showed was most current models you can see here here are the neural module networks that we talked about before again and various other models at that point it seemed like well no actually they weren't understanding concepts and so many of these models their performance essentially halfed when you went to the vq acp data set okay so sector tension networks perform terribly on this data set so I'll go out without produce their own model that worked better and I'm interesting this was actually also a model that mapped both images and language to concepts so they sort of had that idea as well then here are all the 2019 models on archive which are doing rather better than their model but again with the neural state machine model I think we're getting additional power by having this state machine then we can simulate reasoning on I have one more example of that but time is going by so maybe I should skip do you want I don't have many slides left so I could do this example then okay I will so we can do the same kind of thing with gqa that since we do have scene graphs and functional programs behind all of gqa we can also do the same trick that they did of changing distributions so we can check we can have distinctions instructor between training and testing so we can have the training only using some linguistic constructs for the notion of covering or what something is made of and at testing doing different kinds of questions or we can have different content so at training we can not have any questions that refer to types of foods or animals and a testing time we do have questions of that sort and the fine point here is I mean it's not the thing we don't have you know completely novel object so it's not that we're sort of asking it test time what's this animal and there's a red panda it never saw a red panda at training time so far objects in isolation we've got our trained object detector which has seen all of the objects that map onto our concept vocabulary in the training data but that's only sort of learning in isolation so this is sort of saying can it compose together that knowledge to be able to answer new kinds of questions that it hadn't been asked about at training time and so here are our results here and again you know the Mac Network will argue as a bit along the right lines but the neuro state machine is much more effectively being able to generalize these new types of questions where some of the sort of more standard bqa models you know it's not that they don't work at all but perform fairly poorly at generalizing to new types of things like this so in conclusion yeah so so the QA is a problem has always been considerably about language but I think it might be interesting to explore whether you can think in terms of a conceptual space or a language of thought where you use their common representation for different modalities for reasoning about and that my actually give additional power and I've outlined a model that did that I mean in general I think there's this sort of exciting question how can we take our neural network models have been so successful for sensory perception tasks and work out whether we can also use them for thinking slow tasks involving understanding and multi-step compositional reasoning then so the overall goal is can we build neural networks that think and I think there's actually a reasonable hope that we might be able to do that and I've suggested that at least one promising direction has been to do this by trying to build models which use attention over abstracted disentangled concepts and then do multi-step reasoning by having an iterative attention process over different time steps thank you my experience fabulous and provoking talk why don't we spend the next 10 or 15 minutes answering questions and then give you a little bit of a break before you head off tune in all these compositional reasoning intersect one thing that's clearly missing is hierarchy to any show the picture of all these people neck tall woman short boy and man came up to me with the family but family was never mentioned in the scene actually family so do you think yes I mean I guess yeah I mean so I think that's a completely valid observation and outside the question was these models are doing some kind of multi-step reasoning yes that they're not representing hierarchy or perhaps not even really representing compositionality and it seems like they should I mean that actually goes back to this early picture I showed right there right I think there's no doubt at all that there's lots of need to represent composition hierarchy because so a lot of the time yes we're sort of thinking in terms of these hierarchical models where we're sort of seeing bigger composite holes and I agree we haven't really been addressing that but I I think we should we sort of did in this old work in 2011 maybe we'll get back to one of these days the testing methodology so at the end you should have you know if the distribution on the training and the testing was different you'd get you know a worse result is there any effort to like systematically test by every possible you know object of the scene or you know it seems like you could be since you're generating these queries and these scenes you know automatically you just systematically generate all the possible inputs right I mean so so yeah I mean that's sort of not something I talked about but I mean that's actually one of the things that we aim I guess oh swears down here not very explained very well on that last point I mean that was something that we saw as one of the big advantages of this sort of having the controlled questions of gqa that we can generate lots of questions which are effectively answering the same or related things you know so we can our serve what color is the go-kart is the go-kart red is the go-kart green and we could assess automatically whether a system is giving consistent answers and it turns out that a lot of the time the QA systems don't give consistent answers and you know if you do that for relational ones as well we could take what is to the left of the go card and maybe there's a person then we can say what are some right of the person and ask backwards and so on so and so better assess actual understanding rather than random guessing and so we actually have some metrics that we define that look at consistency of answering and have gotten some results on that yeah so I have a question about state and your state marshal and currently I think you have 117 or 1070 kind of object and the day of the week on the mission on such a night for cavalry of judges I think performance should be very bad so I I think it's the I question about the detection is even lower than accuracy of the vicar's comes a final bqa answer i queasy so how can you really get such a high IQ z to answer the questions but based on a very poor performance of object detection so I think maybe there are two answers to that I mean I think one answer that is it turns out that a lot of it's sort of a distribution of data question it turns out that a lot of the stuff that's in the scene graphs that are ultimately come from visual Geno and though we've normalized them is fairly simple stuff right that there's a lot of things that's talking about people and fruits and cars and airplanes and so there's a even though we do some things to rebalance the data that I'll there are sort of core concepts basic categories if you will and psychological terms that are used a lot and a much fewer in number and so that helps I think the other part of the ants so is we don't have to get the answer exactly right because of each point we're putting an attention distribution across the space of concepts and so to the extent that the model roughly thinks that oh it's probably one of these four things and I don't know which the model can place non-trivial attention over all of those four things and reason forward from there even though you know if it was just guessing at random between them then it'd only be getting 25% accuracy we have some problems as we get sound run object detection and then the models a rizzoli model nurse for this run scene so - something wrong with them impacts final answer is correct just like running running and design culture right so we do not learn anything kind of we do not learn that use for with the meaning so there's plenty a chance of getting things wrong but you know there's also a chance that the sort of multi-step reasoning could actually be more human-like because I mean if you sort of say a go-kart to the right of a person well even if your go-cart detector isn't that good if you know it's to the right of a person well you could say how that thing over there must be a go-kart right and so the actual relational reasoning can actually help you do better and you want person behind there so it seems that there are two main approaches to Montes the reason one is you know explicit modular discrete constructions versus you know having a universal kind of continuous actions and it seems that you have tried both of them in put in two of your works one is the maxim which is the universal cell as opposed to your recent work which is more like a discrete instructions so from your experience which one of these approaches you think would be more foo-foo Brochu um yeah so I guess I'm right at the moment I'll say the neuro state machine because look it seems to be working there but I think right so there's clearly a wide open space of approaches and it's not really clear that I or anyone know the best approach but right so in some sense the neural state machine approach is sort of more specific with us different things compared to the Mac architecture but it's still not as specific and custom as something like the neural modular networks right that it's still being done as sort of there aren't special units to do anything like spatial reasoning or counting or anything like that right that it's still doing a more general attention based computation and so in some sense I still see it as similar to the Mac family I don't know yeah two singers no object detector no relation particular just a gone through synapse and use your architecture right so the question is you're using a vision system and there's an answer that you should be able to get where you just say here's the gold scene graph for this scene run it on that and then you should be really you're back in that state then like you should be able to get a hundred percent off of the question and the gold scene graph no I don't have those answers and you know that would be a interesting thing to do I mean I think clearly it had been much much higher number right here there's sort of two parts of this question and one of which is what is the difficulty of the vision problem of recognizing things and the other one is how successful in you actually reason across these scene graphs questions from the NLP team and Emma sorry I am okay Learning Team and I'm uh sorry I because you're gonna be with Chris over the next few are so there any non MS or so if we combine computer vision and healthy it's me QA or gq8 and my background is yes art so my question is really how can a SAR be combined with life or do you think one day we will be working on something like combining all these three areas yes ours to eat healthier yes so yeah so I haven't done that so I mean in you know I have nothing against speech recognition of course in some sense of speech recognition hasn't seen the right place to play for trying to do this higher-level reasoning I mean you know on the one hand there's clearly a connection right when humans are doing speech recognition they have a much higher level understanding of where they are and what's being talked about and they use that to help with their speech recognition and you should be able to do the same and so yes we should be able to combine all of these modalities together and do much more I mean in practice that's tended not to be what happens in speech systems and the speech is recognized to the word level before anything else happens and to the extent that that's true if you're sort of more interested on sort of higher-level reasoning incorporating in speech hasn't seemed the easy ways to sort of make the problem more interesting where I'm doing multimodal work with vision has seen much more approachable anyone on this side of a question yes so you talked about how the clever data set was basically a very large amount of data but like a very small space and so you basically make a bigger version of that in the form of gqa so what's the intuition on how complex you want to make this data sets so that these high capacity models like don't actually memorize it versus like actually learn to reason on them yeah I mean I can so say my thoughts but you know they they sort of you know the usual kind of half and form thoughts yeah so I mean I guess and research somehow you want to find the right sweet spot where there things aren't too difficult but it presents the right kind of challenges and questions for what you want to pursue and you know I guess my feeling coming into this was that if you compared textual question-answering first visual question answering that even though there had been progress every year and visual question answering there's a way in which the problem seemed sort of too hard and too diverse so that where is the textual question answering there's been a really good ramp in fact in some sense it might seem like it's almost too easy with all of these results on textual question answer claiming better than human results on squad and the data sets for visual question answering it sort of seemed like the space of questions and knowledge and so on was sort of two variegated and hard for the current systems but on the other hand the blocks world system seemed like they were too small and too easy and so yeah we were looking for something that was sort of in the middle which presented reasoning challenges over real vision problems but is still sort of more constrained than just saying Turco's come up with some question to ask and you know at that point you know it's a bit effectively that you're betting that the this might be a sort of an interesting level of difficulty for people to explore for working on scene understanding and you know it can certainly be criticized because our questions are still sort of add artificial when it comes down to it but we hope that they're sort of from a broad enough space that it's actually an interesting good challenge for developing visual scene understanding reasoning systems for a few years okay that's not X keen yeah so in your opinion what do you think is a role of logical inferences it seems to me that it might make sense for these models to know that if something is black it could not be read it is to the left of the entities to the left of whatever the transitive of left Ness and so on do you think there is a role for not just a logical infinity so certainly there is a role for having more understanding of domains and being able to make use of that understanding so I mean there's kept slightly more of that than I sort of fully made clear right so that we do sort of so by construction so when I said there was a space of concepts that we put into taxonomy so we actually are exploiting that texana me and this is sort of how we have a kind of a hand done and middlee hand done disentangled representation so that we have sort of taxonomic concepts like colors and so then for color for the property of color you'll have a tension distribution over different colors so in a sort of a soft way it is representing that the choice of colors are complimentary from each other so we are making some use of that kind of information but there's certainly a lot of other reasoning information like sort of liver to the left of and to the right of being reverse its we aren't capturing so yeah I think there is absolutely a role to be doing more inference of a logical character and future models [Applause] 