 Hello everyone. Today we have a talk by Chen Song. Chen Song is a fourth year PhD student at the Embedding Sensing and Computing Lab at University of Buffalo. His PhD research focuses on emerging cyber-physical systems such as smart manufacturing, mobile healthcare, and human biometrics. Specifically, he's interested in using machine learning and sensor fusion technologies to address system-level challenges in security, efficiency, and reliability. Today he's actually this is some comments on this talk as this is ongoing work, and this is like a checkpointing talk, in fact, I would say because the work is not complete. But he's going to talk about the system that he built to do motion estimation in VR, specifically using sensor fusion and machine learning. With that, please welcome Chen.  Thank you, everyone. Thanks for the introduction of my mentors Shuayb. Next, I'm going to introduce some project idea here. The sensor fusion for learning based motion estimation in VR. So, motion tracking in VR is important. During a time series, we always want to know where the controller is and where is the headset is. The most important information we want to know is a 3D orientation and it's spatial position. By this kind of information, we can properly render the information in the VR space and provide better gaming experience for example. The isotropic accuracy is very important. The traditional way to track those devices is based on the computer vision simply using the front camera located at headset. We built a system based upon for the mixed reality. The way we get the position tracking data is to utilize the steam VR SDK which allows the developer to walk with mixed reality VR in the Unity environment. So, by exploring the steam VR SDK, we were able to find particular multiple classes that can giving us the information we're interested. For example, the motion controller information class can give us the position and rotation information from the hazard and the controller. More importantly, there's another class called the XR.input tracking class will record the tracking status of the mixed reality. There is three index for those tracking status; high, approximate, and none. The high index means that at that moment the mixed reality is currently tracking the controller in the space. While the approximate it means that the controller is going out of the field of view of the camera and the system doesn't know where it is. None, the index zero means that the system didn't find the device at all in the space.  They're not connecting or what do you mean?  Yeah, like they're not either not connecting or that there is soundtracking issues by the system. Now, the current mixed reality system is able to track the device accurately in the high tracking status, high accuracy tracking status. But the stage we want to emphasize is the approximate stage, when the controller going out of the field of view we can categorize them into two different cases. The first one is the controller is moving at the boundary of the field of view. Which means that it will moving back very quickly and the system will being able to capture it again. The other one is a completely moving out, and after two seconds it will going back. So, we redefine the tracking status by counting the consecutive states of the approximate. If it's one, then it's the approximate. If there's four consecutive one, then we define it as out of field of view. So, the performance of mixed reality tracking is like this. For example, the red trajectory is the ground truth. We see that when its moving out of the field of view, the system cannot track at all, and the mixed reality prediction of the controller position just get stuck at this point. When it's moving back, the system was able to find it again. Then there is a big jump here without the continuous tracking. Because of this the tracking accuracy for different state is different. In a high tracking stage the mixed reality can have tracking accuracy arrow within four centimeters. In the approximate stage it's around 10 centimeters, but when it's going out of field of view in this particular stage it can be as high, the arrow can go as high as 50 centimeters which doesn't provide any useful information at all. So, our proposed approach to track the controller's position where its going out or field view can break into three steps. The first one is we utilized the lightweight sensor, ultrasound sensor. The second one we generate a learning-based model. Then eventually we will establish a data fusion system to integrate all the information we have. Though this is the dataflow we have. We have the mixed reality sensor data here provides us the position stream, and we have the biosensor system and a learning-based module. Then we fuse the data and generate a better position tracking information. So then I will introduce each module one by one. The first is the lightweight sensor for ultrasound ranging. The principle is to utilize a time of flight of the ultrasound to calculate the distance. Now assume we have the hazard and the controller, we have both the ultrasound located on each of the device. During at T1 we transmit the ultrasound signal, and then T2 we receive it. Then we can calculate the distance based on the difference between T1 and T2. One thing important to note here is the time synchronization between these two devices will be very important.  What are the units for the temperature here?  The temperature is Celsius degree. This is the entire for the VE sound.  Velocity.  This is the for the velocity. This is the architecture of the ultrasound sensing system we developed. We have a wireless synchronization layer, and we have ultrasound ranging layer. Then we recalculated the real-time distance in this layer and transmit all the data through the UDP via this data transmission layer. This is the system diagram we built. We have this synchronization processors, we have one master for the synchronization beacon generator, and we have on the left controller and right controller we have correspondingly a slave processor to receive synchronization become. For the transmitter, we have one on the handset and two located on the controller also. Then we use the synchronization processor to generate operating commands for those ultrasound transmitter to sending the ultrasound periodically.  Are these blue and orange do they mean different channels?  Yeah. They are in different channels. Once the receiver receive the ultrasound data from this transmitter, it will calculate the distance and send out those real-time distance through the Wi-Fi channel to the PC side. There's one more synchronization slave here on the PC side, we read the data through the serial port on the PC and use this particular processor to link the PC clock with the cycle time. This is the way how we can connect the bottom with the ground PC clock.  Chen, so what is the difference between sling slave one and the receiver one?  The synchronization processor does one job, this is periodically sending out the synchronization beacon, and these three will receive the beacon. Once they receive it, they will know what to do based on our design of catalog.  This claim is just use from time synchronization on the Wi-Fi.  Okay.  Yeah, because on this receiver, we use the Wi-Fi module to send out the UDP, so we don't want any interference, that's why we put into two processors. This is a list of hardware information we use in the system design, and we use a oscilloscope, four-channel oscilloscope to do the hardware debug. This is one of the photo when we're doing the experiment, we have a transmitter and we have a receiver. Then because of the physical properties in the ultrasound sensor itself, we design a mechanism to properly avoid the randomly self-generated ultrasound noise and create a clean window for the data detection, and this time difference here should be matched with the physical distance here. That's how we capture the time differences and calculate the real physical distance. So, eventually, we integrate the entire system upon the mixed reality devices. We put four sensors on the handset, and one receiving sensor on the controller. So, in this way, the ultrasound receiver on the controller will know how far it is away from the headset.  So, technically, you just added a small CPU with it so on Wi-Fi and the microphone connected to do the distance estimation in the controller?  Yeah.  You added to one small CPU with all those receivers in parallel to do the basic, the pinging to coordinate the synchronization?  Yeah.  Okay.  Then we take a look at the synchronization performance because it's very, very important. One millisecond difference can generate the error of 30 centimeters because the ultrasound travels really fast.  Sir, can ask one question? You had one receiver and four transmitters?  Yeah. The four transmitters is just used to cover the entire space. It will generate the ultrasound sequentially.  But why, oh, I see. So, they all ping at the same time? Okay.  Just the radiation pattern of those switches is very narrow because its diaphragm with the size of the wavelength.  Okay.  Because we have three synchronization slave here, each of them has different arriving delay. So, we used the statistic method to collect a large chunk of data and calculate it the standard deviation for each of them and we utilized the delay information and compensate it when we're calculating the distance. Just compensate that amount of arriving difference in time.  So, what is the difference between the three?  The difference is the standard deviation will slightly increase because we stack three synchronization processors in there.  One synchronization unit, and that is when two of them are transmitting in the middle one? Three of them are trying to synchronize on the same channel. So, there's interference when you have more than one guide trying to synchronize.  Can I ask more question about the four transmitters? So, now you have only extracting one distance estimate, right?  Yeah.  Have you considered, I know that with their narrow beams and there's destruction in all that, but have you considered aiming them in sequence and getting multiple distance estimates?  Of course we have, I think but the problem is limited field of view. I mean, the transmission angle of the ultrasound.  The uni.  So, you can get nothing like from the ones that are not,  No.  Okay.  So, if there is an error. You said, if there is a delay of one millisecond, that could cause some 30 centimeter.  Yeah, tracking error.  Tracking error, and that would be consistent. So, consistently, you will be off by 30 centimeters or that can be in any direction?  The receiving delay has no relation to the corresponding position, but it has a variance in the standard deviation. Sometimes it arrives early, sometimes it arrives late. That's why we use the statistic method to calculate it in standard deviation to compensate.  So, technically, a couple of millisecond is 15 centimeters, 12 millisecond is 24 centimeters. You have these added as an error with this type of synchrolization?  Yeah. That is standard deviation.  So, that's the standard deviation.  It can be busy also.  Meaning, the delay would be zero also. The standard deviation is up to 55.  Okay, and you do this 10 times per second?  Yeah. I do it multiple times. Yeah.  Okay, good.  This is the result of the ultrasound tracking accuracy. We evaluated its performance with regard to the distance and angle. The average distance error is around nine centimeters and a standard deviation is around 10.  So, this includes the clock synchronization and,  Yeah.  The measurement of the ultrasound?  Yeah, this is the entire sensing system. The result of entire sensing system. Now, the second module is the Learning-based model for tracking. Remember, we want to estimate the position of the controller when it's moving out of field of view. When it's moving out, then what we have is only the accurate mixed reality prediction information. When it's inside of the field of view, then we need some mechanism to utilize this kind of information and predict the following trajectory. So, that's why we take a look into the autoregressive time series forecasting model. Because this model can utilize the previous information to generate the next one and these hyperparameters, A, can be also learned by the machine learning. Particularly, we used LSTM to track the time series data. The LSTM includes three modules. The forget gate is the one that it use to forget learning part of the pattern in the previous window and forget some of them. In the input gate, it learns the pattern in the current learning window and to merge the current knowledge with the partial previous knowledge. Then it generates the output in the output gate. So, just giving an example how the LSTM work is, here the train signal is the sine wave and by training the model, the corresponding LST model can accurately learns the periodical movement and generate the trajectory in orange, exactly fitting the original one. Applying to our mixed reality data, we train this model with the mixed reality trajectory when it's inside of field of view and we predict the data on the testing set. As we can see here, the blue one, the predicted distance fits very well to the ground truth.  Can we go a slide back? So, what the neural network predicts? The coordinate or the coefficients in the regression? Go on. One more.  Coordinates.  Coordinates, okay. You don't try to predict the coefficient?  No.  Okay.  Then, once we get the predicted distance, then we use spherical projection method to update the estimated position, to update the position information. Assuming we have this d estimated, then we have the current measurement of the position, then we can correct this position by projecting it to a sphere that has the radians of d estimate. Yeah, right now it's [inaudible]  Where did the other data come from?  We will cover that later.  Okay.  Yeah. So, how do we get the distance here? There's actually two sources. One is the data ultrasound ranging system and the second one is the either the LSTM prediction model or the mixed reality prediction, when it's inside of field of view. Then, we can use a Kalman filter to fuse the distance from these two sources and generate a filtered distance. Now, here we use the Kalman filter to learn a Kalman filter gain and also update the covariance matrix here in each step based on the previous trajectory information.  So, you track the movement of the controller visually, to the moment it gets out of site, and then use just a prediction using a Kalman filter under the constraints of the distance, which you measure constantly?  Actually, the moment when it's moving outside of view, we use the learning-based neuronetwork the, LSTM to predict its next position, but this prediction can be inaccurate, and we fuse this information together with the ultrasound distance.  So, technically, you do more than just a Kalman filtering to predict the trajectory. There is implicitly the ability and the constraints of the movement of the human body. On top of that, you add one more constraint which is constantly measuring the distance to the controller.  Yeah.  Okay.  Now, I'll go through the pipeline of the entire system. Assuming the mixed reality prediction coming every 20 millisecond and accordingly, in each prediction it will have the corresponding tracking status. As we said, we use a sliding window using the previous mixed reality predicted trajectory to predict the next one. We will keep that LSTM, the neuronet prediction in a separate array, which also has that.  So, this MXR prediction is what they have in the mixed reality system. They're all super duper [inaudible] system.  Using that we make a future prediction.  Okay.  Now, we have another data source, the ultrasound distance source that has a different, sampling frequency of 60 milliseconds. To fuse the information on the left with ultrasound distance, we first need to do the down-sampling, from 20-minute, from 50 hertz to every 16 milliseconds. Then, we have, after we get the down-sampled data, we first do a selection.  Sorry, can I ask you a question. Why is it down-sampling the ultrasound distance, rather than up-sampling?  Oh because the-  Sorry, rather than down-sampling it?  -because based on what we're experiment, observation, the ultrasound because the WiFi synchronization delay, the ultrasound is kind of noisy compared to the mixed reality data. That's why we don't want to up-sample air to create more, to generate more, introduce more noise.  But, you do measure over 60 milliseconds.  Yeah, yeah, yeah. Here then, we have two data sources here. We have one down-sample mixed reality prediction and the ultrasound distance at each 60 milliseconds. Then, in order to utilize this Kalman filter, we select these two data sources, based on its tracking status. If it is two, then it means that the system is tracking the controller position accurately. Then, we trust more in this mixed reality prediction. Otherwise, when it goes to one, then it means that the controller is moving out of field of view and the mixed reality data is invalid. Then, we will utilize the ultrasound distance information. Then, this will be the first distance source. Also, during the tracking, we also keep a list of the LSTM, the neuronet prediction. We will use that as the second data source and we fuse it to generate a fuse prediction at each time stamp. I mean, every 60 milliseconds.  Jack, do you know if the headset has any sort of predictions, when you're moving close to being out of field of view?  Sorry?  So, when you're moving out of field of view, does the headset have any kind of predictions?  No, the headset only giving us its own position in the 3D space. That's it. That's the only information we can get so far.  Okay.  Then, by knowing the fused distance prediction, then we can do a spherical projection we just talked about and project the position estimation from the neuro-network to the correct sphere.  So, what is the input for the LSTM prediction?  The input of the LSTM prediction is the previous window of the tracking results.  Of the MXR tracking?  Initially it is, then it will just, using the iterative prediction. Initially, we start with the tracking position status high and we utilize for example, 200 mixer.  So, it's an extrapolator?  Yeah, yeah. Then, we predict the next and we put this predictive LSTM prediction, put it back to the stack and generate the prediction by itself. So, we no longer rely on the mixed reality later. So, this is the overall. Overall eventually, we will have the tracking status, the mixed reality prediction, the neuro-network prediction, and the fused prediction. We will select it accordingly. When the tracking status is high, then we use the mixed reality data, when the tracking status is bad, then we check if the ultrasound measurement has arrived. It is not arrived there, we trust the neuro-network prediction, or we will rely on the fused prediction, which is calculated based on the ultrasound distance. Oops. Further, we can apply a offset compensate, if the tracking status is low in this duration. Then, we can further correct the neuro-network prediction by adding the offset between the ultrasound prediction and the LSTM prediction during this window. So, this is the entire block diagram. For the data collection setup we-  Can you go back on that?  -okay. Oh, the prosper is means the down-sampled data.  Okay, okay.  So, MXR estimation is the same here and there, or some sample? Okay, go ahead.  In the data collection we use eight-camera OptiTrack system to do the motion capture, and we also collect the mixed reality tracking data from the Unity, for the in-field and out-field field of view tracking. Also, we have the additional information of the ultrasound sensor and the LSTM model, the prediction generated by the LSTM model. This is the framework of the collection. We collect the ultrasound data, we will do some preprocessing to smooth it. We talked there are some noises of the raw data, and we applied the LSTM network upon it, do the data fusion, and we will have the estimated position. Then, we will compare the estimated position at each time stamp with the ground truths we collected by the OptiTrack system.  Powerful millimeter?  Oh yeah the-  Really?  Yes the sub-millimeter accuracy of the track system.  Wow!  So, you will try and put times [inaudible] the OptiTrack system with the other things, right?  Oh, yeah, the time synchronization is based on the world clock.  PC clock.  Yeah, the PC clock. This is the data we collected. Basically, we have two hours of random trajectory in the 3D space. Okay, and we just do some movement that incorporating the in-field of view tracking and out of field of view tracking. So, this is the result of the ultrasound distance tracking. As we can see here, the blue line is the mixed reality tracking result and the red line is the ground truth. The green one is the distance we measure by the ultrasound. So, when it's going out of field of view, the mixed reality just gets stuck where it is at this point, and only the ultrasound signal can go along with the ground truth. Then, we have sound arrow. We have sound tracking arrow summarized here. For the in-field of view, the mixed reality. Oops. Has a lower tracking error than ultrasound, but when it's going out of field of view, it just go high. This number is actually depends on the movement of your hand.  What the two numbers mean? 3.48 plus 3.00. What these means? The first line. Yes.  This one?  What's those things? What are the two numbers?  Oh, it's the average and the standard deviation.  Okay.  Sorry. It's the average and standard deviation. Here is also another result of the tracking distance errors summary, but for here we apply the common filter upon the two data source and it still generates the error the lower than the mixed reality prediction itself when going out of field of view. The LSTM tracking in field of view is pretty good.  In the [inaudible] there is no LSTM, there are just ultrasound and-  Here, we're talking about the LSTM tracking performance. Though it works pretty well on the left side, you can see the blue one is the LSTM prediction result, and the pink one is the ground truth. Though it always follows the ground truth, and when the tracking status is high in the field of view, the LSTM actually can generate a higher accuracy than the mixed reality itself. It shows the potential that we can use this method to further correct the tracking performance for the mixed reality. But when it's going out of field of view, we trend the LSTM. Next, we try to evaluate how LSTM can work to predict the trajectory when it's going out of field of view. We did the experiment with different window size and when the window size is low, it doesn't really learn a lot here. This is the point where the controller going out. If you increase the window size up to like 200 samples. Then initially, at the very beginning, it learns the trajectory pretty well. Then, afterwards because we're stacking more and more predicted results into the learning window, then because of the accumulative error adjust in reasonable at some point up to 200 milliseconds. So, within the when is going outside of view in the next 200 milliseconds, we were able to predict this correct tracking, but after that then the neural network just failed. So again, the way we train the model at the current stage is just a iterative training mechanism. We're putting the prediction result back to the previous window.  Well, and circumstances is'it short-term and long-term prediction difference between the two things right?  Yeah.  So, this does well in the short-term not in the long-term.  Not in the long term.  So, for long-term, do you want to trust in the Fusion Model, do you want to trust in ultrasound more and correct again, give it another chance to do another short-term than long-term fusion model.  Yeah. So, I think that's why I think this is unfinished work. It's [inaudible]  Yeah. So, the way we looked at the results so far is the first way we realized that the prediction train compared to the actual waveform here, which cannot be clearly visualized here, but you can guess on concept with the previous figure. Here, the predicted result is somehow very low frequency. So, that's why we're saying we need to apply some de-trending masses to de-trend the original data moving the non-stationary tenancy in the data, so that we can let the LSTM better learn the local high-frequency moving feature instead of the low frequency one. Also, we can try the larger models to increase the models capability of learning the previous information to predict future one. There's also two promising ways we are rethinking that can be applied in the next step. The first is the direct h-step ahead forecasting and MIMO model. Remember so far we are using a learning window size and always to predict the next one and because of the iterative prediction mechanism, the prediction just going from good to bad. So, but we can apply this model in a parallel way and generate a direct H-step prediction model which means that in the first parallel model we use it to predict the next m plus t plus one and we stack with the second model parallely, landing that to predict the t minus two and just up to t minus n. So, in this way this parallel model architecture will be able to learn in the trajectory from t plus one to t plus m as the entire trunk. So, the advantage of this model here compared to the one we use is it there's no error accumulation here, but the disadvantage is because of the parallel stacking, it has very high computation cost. Also, we can try the MIMO models, which mean in the training we just fill a trunk the training data with the fixed window size and directly let it to predict the next trunk. So, compared to the previous one, this one you only need to train one model instead of the parallel m models. Also, it has no error accumulation, but the disadvantage is that you have a fixed prediction lens which will not be very flexible in the real practice. Maybe you want a long tracking window or a short tracking window, but this mass can only give you a fixed one. So, it's flexibility here. So, the conclusion is so far we build an ultrasound ranging system which can give us the distance accuracy of of around eight centimeters and we developed an Iterative time series prediction model based on the LSTM. The good prediction can go up to 200 milliseconds and certainly we establish the entire sensor fusion system to fuse the available data for the position information. Again, for last I thank my mentor Shuayb Zarar, for all the help he provides during my intern and also the entire Audio and Research Group and also Microsoft Research for giving me the opportunity to doing research here. Thank you. 