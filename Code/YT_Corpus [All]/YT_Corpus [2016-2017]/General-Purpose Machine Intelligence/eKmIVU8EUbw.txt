 hey Siri Any songs you recommend? The Blacker the berry by Kendrick Lamar Good pick. Why that one? Because f*ck the police Hello World welcome to Sirajology! In this episode, we’re going to build a movie recommender system. Recommender systems are all around us, almost every single major service on the internet uses them to show you things that you’d like based on your interests.  Sometimes its so subtle you don’t even notice. They help personalize content on the Web, which makes users happy, which makes companies money. The two most ubiquitous types of recommender systems are content based and collaborative. Content based systems focus on each user individually. It looks at the items you’ve already expressed interest in (via ‘likes’ and ‘ratings’) and records their keywords, attributes, and tags. Then your profile is gradually built with these attributes. Once your profile is built, the system will start recommending items you’d like with similar attributes to the ones you’ve already expressed interest in. So if you’re on an e-commerce site and you buy a bunch of Nickelback t-shirts, those items will have tags like ‘#worstbandever’ and ’stupid’. Based on that content, the system will suggest similarly tagged items for you, like ‘creed t shirts’ and ‘wal-mart brand guitars’. Then there are collaborative systems. These are the most ubiquitous type of recommender system. A collaborative system recommends you items based on what other similar users have expressed interest in. It collaborates with many users preferences to generate you a recommendation. So if you really like base jumping and constantly buy base jumping gear online, the system will find users who have similar purchase history and ratings for those same products. Once its found those similar users, it can recommend other items that they’ve bought that you haven’t. It’s likely that you’ll be into those products as well. So we’re going to build an app that can recommend movies you’d like in 10 lines of C++ using Amazon’s newly released machine learning library called Deep Scalable Sparse Tensor Network Engine, or Destiny. Sigh. We’ve got to think of a better name guys. Our model will be a neural network because lets be real. The more i learn about machine learning the more i realize just how much they outperform every other model almost every time. Then we’re going to train it in the cloud using AWS because i aint got time to train this on my macbook. DSSTNE is what Amazon built for production-use specifically to recommend products to customers that they might like. It’s optimized for sparse data and multi-GPU computation. Data is sparse if it contains a lot of zeroes, as in not a whole lot of valuable information. Recommendations usually operate on sparse data, not everything is connected, but you can manage to find some valuable links between people and items. Most ML libraries implement data-parallel training, as in it splits the training data across multiple GPUs. This works, but there’s definitely a tradeoff between speed and accuracy. DSSTNE uses model-parallel training, so instead of splitting the data across multiple GPUs it splits the model across multiple GPUs. So all the layers are spread out across multiple GPUs on the same server automatically for you. Amazon had to do this because the weight matrices it had for recommendations, that is all the mappings of users and attributes, just didn’t fit in the memory of a single GPU. When it comes to ML libraries, DSSTNE isn’t as general purpose as TensorFlow (no recurrent net or LSTM capabilities yet) but it is twice as fast when it comes to dealing with sparse data. So we’ll follow our methodology and collect our dataset, build the model, train the model, and test the model. Lets start off by collecting our dataset. We’ll initialize our helper class object and then call the retrieveDataset method with the parameter as the URL to our downloadable model. In our case, we’re going to use a sample MovieLens dataset which contains user ratings for a lot of different movies and their associated tags. Once we have that, we’ll want to convert it to a format our ML library can read. In this case, its the NetCDF format. NetCDF is designed for efficient serialization of a large array of numbers and its what DSSTNE expects. We’ll generate it for both the input and output layer of our neural network and we’ll use the name of the downloaded dataset as our parameter. Both of these functions generate a NetCDF file , an index file for neurons, and an index file for features. Once we’ve generated our model, it’s time to train our neural network. In DSSTNE, you build your model in a JSON file instead of programmatically. We can see in the config.json file the structure of the neural network. This is where we set our hyper-parameters. The most important takeaway here is that we are creating a 3 layer feedforward neural network, (that means data just flows one way) with one 128 node hidden layer and our activation function at each node is the classic sigmoid (which turns values into probabilities). We can go ahead and run our train function with the batch size and number of epochs as the parameters. We’ll set the batch size or number of examples to 256 and the number of epochs or runs to 10. Once we run this, it’ll create a newly trained model file called gl.nc which we can then use to predict recommendations. Our last step is to predict recommendations so we can just call the predict method and set the number of recommendations parameter to 10. This will place the newly created predictions in the ‘recs’ file. That’s pretty much it for the code! Now that we have our code ready to be compiled and run, we’ll want to upload it to AWS. AWS is Amazon’s cloud computing service and I’m just going to assume you’ve already signed up for an AWS account. To start off we’ll click on the EC2 button which will take us to Amazon’s cloud computing service. Then we’ll want to make sure we’re in the US East (North Virginia) region, since Amazon created a preconfigured image with dependencies like CUDA and OpenMPI already setup for us in that region. We’ll click on AMIs under the images directory of the left sidebar and search for the instance called (ami-d6f2e6bc). It should pop up, and then we’ll click the blue launch button to spin up an instance using that image. Then it’ll show us a list of instance types. Since we want to speed up training time, let’s go ahead and choose the GPU option here. Then we’ll click 'review and launch' and see the final page before we can launch our instance. Everything looks good to go, so let’s click launch. It’ll prompt you to create a new key pair, go ahead and download it so you have it locally. This will help authorize your machine to connect to AWS Now that we’ve successfully launched a GPU instance on AWS. Now we need to upload our code to it and train it. I’m a fan of using FileZilla to upload files so let’s use that. I’ll click the site manager icon, then paste in my host name. I gotta sure I set my protocol to SFTP, then set my login type to normal and the user is called ubuntu. Once we’ve set the fields we can click connect and it’lll show us all the current files in our instance. Let’s go ahead and drag and drop our project into the root folder. Now that our code is in our EC2 instance, we can open up terminal and SSH into it. We can find the ssh snippet for terminal under the instances section once we click the connect button. Perfect, let’s just paste this baby right into terminal. Boom, we’re in. Let’s cd into our directory. Before we run our code we’ll need to do 2 things — add the MPICC and NVCC compilers to our PATH and make the library. We can export MPICC and then run make. Then we’ll export NVCC. Now we can run our script and once that’s done, we’ll have recommendations in our recs folder. Let’s make sure they’re there. Awesome! These values map to movie IDs. That’s pretty much it! You can scale your neural net accordingly depending on the size of your data. Recommendation engines personalize the experience for your users which has proven to enhance their experience and increases their satisfaction. In this age of AI, you can’t do without it. For more info check out the links down below and please subscribe for more machine learning videos, for now I’ve gotta go fix a runtime error so thanks for watching 