 okay so what's worrying Elan musk this most ambitious of men the founder of Tesla SpaceX and Solar City all at the same time he's not revolutionising technology he's building new technology sectors previously was one of the founders of PayPal he had a different successful internet company is he afraid of anything yes of existential risk from artificial general intelligence the inspiration for Iron Man in the movie a lung musk says that artificial intelligence is our biggest existential threat the is isn't supposed to be at the end and he's put his money where his mouth is he donated 10 million dollars to different research institutions in 2013 more than has been given to the sector in human history he also put his mouth where his money is very important often when people give this sort of money they want to stay anonymous so people don't pester them for more or they want to keep it private but he's spoken out about this issue and it's not just him people like Stephen Hawking and Bill Gates have come out publicly and they can't speak out about every issue but they've chosen to speak on those Sam Altman the president of Y Combinator got together with a lone musk and a number of Silicon Valley billionaires for a new institute called open AI and they pledged a billion dollars and it's already started professor Stuart Russell of the earth T of california-berkeley wrote the leading textbook on AI and he's been buttonholing his colleagues and trying to convince them that this topic is serious a word about me yoshua Fuchs Joshua Fox I've been interested in this area for 10 years since back when was just a bunch of weirdos on the internet and I'm one of them but now as you see it's branched out to the people in positions of influence are starting to get interested for a few years I was research associate with the machine intelligence Research Institute and I wrote some peer-reviewed papers and I also published the world's first ever popular science magazine article on the topic in Hebrew in Galileo I'm not a core math researcher but I understand these topics well enough to understand why they're important and to share them with you so you'll see where this is going today I'll talk about existential risk and ask you if the destruction of humanity is a problem spoiler alert yes and specifically I'll talk about artificial general intelligence as a risk I'll describe some very important research directions to try to reduce this risk and very importantly say what you can do because this is not just a story if you're convinced by the argument then you should do something if there's anything you can do that will even slightly reduce the risk existential risk means the destruction of humanity or a severe reduction in human value eternal misery for all humanity all seven point four billion of us you can't understand that number it's called scope and sensitivity the death of one person is a tragedy the death of a million is a statistic but you can shut up and multiply and the expected harm equals the probability of harm times the outcome and that's very very bad and so for the same reason you wear a seatbelt even though you probably won't get in a car crash if you can just slightly reduce the probability of a very bad outcome you can do something important but it's not just the numbers what's worse the death of 7.3 billion people or the last 100 million so the first number is larger but the death of the last human puts an end to human potential I want a future where billions and trillions of our descendants live and learn and love and grow and create I want that to go on for millions and millions of years if humanity ends it's not just today's humans who are gone but any future ones it's so hard to understand existential risk but I grew up in the shadow of the Cold War and I didn't see a way that humanity could survive we did but that's just luck if we had all died I wouldn't be here explaining to the next existential risk the secret launch codes and the American intercontinental ballistic missiles were zero zero zero zero zero zero through the whole Cold War because the honchos were afraid when the time came it might be too hard to shoot the missiles off that's how close we came good book command and control it describes a lot of other mishaps in the American side on the Soviet side we don't know even as much but specifically I want to talk about the risk from artificial superintelligence which i think is the most serious one narrow AI is racing ahead just in the last few years we have self driven cars and just a couple months ago an AI called alphago beat the world's go champion just 10 years ago AI experts were saying that go was such a complicated game that no human know a I would ever beat the champion and if it happened we will be so close to artificial general intelligence that you will make no difference but they're changing their prognostications saying no go is a complicated games not that complicated but we're very close I remember just two years ago I used to joke that in high tech if you said the words artificial intelligence you'd be fired now I don't mean that on the spot they'd fire you I mean that you'd be treated as an astronaut floating unconnected to business realities just in the last year PhDs in deep learning are being snatched up for million-dollar salaries this is racing ahead but I don't know when artificial general intelligence will come there don't believe anyone who gives you an exact date talking about Ray Kurzweil but there have been surveys and experts were asked but what year is there a 50% chance of having artificial general intelligence and the median answer was 2040 so don't get too fixated on that number but think about a probability distribution starting now and heading out 100 years into the future people already care about global warming and that's said to have its maximum impact a few decades into the future yes good so that's roughly human level machine intelligence it's not a precise definition but it's an AI that can deal in a flexibly under a variety of challenges so I don't have an exact definition but what actually matters is not the definition but whether they I can make a tremendous difference in the world for good or for bad so the the number of scientific breakthroughs that remain and the exact definition of what we're going to meet that will be AG I depend on the path that could be a human brain emulation it could be designed roughly on the model of human brain or it could be designed de novo using some principles that computer scientists figure out and it will look very differently in each of those cases but I would say it takes maybe two or three more scientific breakthroughs how long is the scientific breakthrough take they do happen occasionally yes it doesn't include consciousness first of all I don't know what that isn't your does anyone else it's been debated endlessly what really matters is whether the AI can maximize the utility function I'll talk about that more now does it include a self-preservation instinct no it doesn't but it turns out that any intelligence will tend to have a self-preservation instinct because if it's dead it can't achieve whatever it was trying to do now I'll talk about that too when this AI comes what matters is if it has a tremendous effect on our future and I'm not a techno optimist I'm not a techno pessimist I'm a techno volatilized I think that tremendously powerful technologies will either be very good or very bad but nowhere like what we have today so just a brief word on the good side because if we're dead it doesn't really matter how good the good side is we have to stay alive to do anything and humans have done wonderful things they've also done bad things but they've done some good things they've fed the world cured diseases and the roughly human-level AI can do all those things and if it's super human it can do more if it's designed to do that that is the hard part I'd like to see a world that fulfills all the many human values which are very hard to define of living and being happy and an AI can bring us in that direction but to understand what this future AI will do and what it will look like what will be like I have one principle for you and that is don't anthropomorphize don't think data think data don't think this friendly robot is kind of unemotional think maximizing utility function hill climbing machine learning optimization genetic algorithms I don't know what it will be like but it will not necessarily be anything like a human it's not a human eliminate those preconceptions in fact an intelligence in principle can have any goal this is our second Alty thesis from Professor Nick Bostrom I recommend that book it just came out a couple years ago it says that intelligence and final purposes are independent doesn't matter if the purposes are good or harmful they can be any purposes if we look at some narrow ai's now these are narrow not general they do things that are generally aimed at some narrow human purpose humans want to cure diseases they want to make money on the stock market they want to defeat their enemies in war but each one of those is a very specific thing each of us wants much more than that but you could have a narrow AI that does something that humans don't really care about like making piles of sand where each pile has a different prime number of grains two grains three grains five seven now in that way I could probably do that but what if I was a general AI and it could make bigger and bigger piles and it figures out how to make mountains of sand where each one has exactly a prime number of grains with all the cleverness and trickiness and wildness that I barely even have I couldn't do it I don't think I can make a mountain with a prime number of grains the it can have opry or you can have any goal that's defined for it or that develops as a result of a bug however there's an opposite thesis I've told you a minute ago that intelligence can have any goal but they will tend to have shared sub goals because these help it achieve its final purpose for example any intelligence will want to gather resources because those resources help it achieve its goal for example money the cooperation of people and in the limit of very powerful intelligence they'll always want mass and energy to help it achieve its goals any intelligence will want to exist not because of some animal urge to survive but because if it's dead it can't keep optimizing for its goals any intelligence will ultimately fight to preserve its goals because if you change its goals it will not be maximized that utility function if a Marxist hacker wants to break into that Wall Street computer and make it give its money to the poor and if it's generally intelligent which of course today's a eyes aren't and if it can figure out the hackers coming it will do anything to stop it because if it's not making money it's not maximizing for its utility function and critically any intelligence will want to become more intelligent because that helps it achieve its goals we humans have a hard time getting more intelligent maybe we can study but an AI could just by cloud resources or maybe improve its own algorithms after all we're talking about an AI that's roughly human levels and if human engineers could bring it to this level why can't it maybe improve its own algorithms and having done so be more able to achieve its goals and be more able to improve its intelligence and so on and so on in a climbing spiral of intelligence to vastly super-human levels to explain this our favorite thought experiment is called paper clipping so imagine a near-future when a team at Tel Aviv University has developed the world's first artificial general intelligence and they're going to test it and they have to choose a goal of course not curing cancer because that gets too much into biological agents no they're going to give it an innocuous goal gathering paper clips the utility function equals a number of paper clips it's gathered and so they turn it on and if they did a good job it gathers paper clips maybe it finds one in the drawer or it makes money and has some shipped or maybe it makes itself more intelligent and it finds a way to make more money and to buy a paper clip Factory and to buy an iron mine and maybe another thing it will do is make itself even more intelligent and it Minds out the Earth's iron core and converts it to paper clips and then it finds a way to transmute all the matter in the earth to paper clips and then it exhausts the earth and it moves out into the solar system and soon all you have is a ball of paperclips expanding it near the speed of light across the galaxy okay now that sounds ridiculous it sounds unintelligent so forget the word intelligence forget I said it it's a utility function Maximizer it's doing a pretty good job oh is that not what you meant so in hackers jargon dictionary there is describe the DW I am programming language command does your favorite programming language have that DW I am do what I mean because that's what we need here if this isn't what you meant then define what you mean so the danger is not terminator we hate it when journalists always pull out a picture of arnold schwarzenegger when they hear about this because i can't think of anything else the danger is side effects from otherwise innocuous from otherwise innocuous goals and I could have harmful goals or could have helpful goals like curing cancer where the utility function is the negative of the number of cancer cells it's going to little fewer less cancer the better it's doing and it figures out that if it wipes out all life forms there's no cancer cells it won well that's not what you meant well you better tell it now some objections so some people will say why would anyone build such a harmful AI and the answer is that it wasn't built to be harmful it was built to cure cancer or gather paper clips some people will say wouldn't any super intelligence learn to be cooperative and moral don't enter poem or Phi's morality is a human value which I know and love but ask yourself why it would cooperate it would cooperate to get things in trading with humans sometimes some formalities says that you are nice not because you get something in trade but because you expect in general others will be nice but once it's it vastly super-human levels it has no interest just think what gets it more paperclips that's all and some people will say can't good AI stop the harmful ones that depends very much on which AI comes first because whichever AI is there first will act to prevent any others from coming into existence because that keeps it away from resources that it can use to maximize its utility for and unfortunately a team that doesn't care at all about safety can move faster than a team that cares about safety so yes sadly I think the default is that we all die that also depends on how fast the improvement is hypothetically if improve moves very very slow you could have AI self developing roughly similar paces I see no reason why they should be at roughly equal speeds however another objection just pull the plug everybody's a wise guy I'm gonna pull the plug yes for a simple AI although if you try to pull the plug on alphago the go machine doesn't have a plug it's in the cloud somewhere but anyway any sufficiently and you said this powerful intelligence will act to prevent you from unplugging it because that keeps it from making money on the stock market or killing cancer cells or whatever some put it in a box I'll just talk to it over text chat so first of all it's cleverer than you and it will trick its way out it will pretend to have a bugs you're going with the debugger but even if not and this is a very important point that I'm gonna get to in greater depth later the AI is part of the universe it's implemented on computer chips with actual electrons now can it wiggle its electrons back and forth fast enough to do something outside of its Faraday cage I don't know how to but I'm just humanly intelligent but it's important for you to realize that it's all part of that great quantum wave which is a universe and don't think you're you're cleverer than it now there's a fascinating case of a genetic algorithm as was already a few years ago that was going with this was going to design an oscillator circuit so the scientists had it do that and it designed an oscillator circuit and then they realized what it had done it had repurposed the motherboard to pick up oscillations from rate radio waves from fans in the environment this is just some genetic algorithm in some university a few years ago it wasn't even an AI and it broke out broke out of its software model into the hardware environment how much the more so when it's smarter than you some people will say the human eye is too special to reproduce and first of all I think it is hard to copy a human brain and I do think that de novo designed AI will come first the same way that an airplane came before a copy of a bird but don't forget that human brains are made of atoms and there is no law of physics keeping you from making new ones I and my wife made four of them I maybe can take credit for this one as well of course they did the old-fashioned way brains are part of the universe there's no law of physics saying you can't copy them however there's a counterpoint professor Robin Hanson his book just came out two weeks ago buy it and read at age of em's he says our future will have trillions of emulated human beings living in a software environment competing trading working and he could be right I'm not single-minded about my prognostications things are probabilistic risks nothing's a certainty but I do think that that will not happen first of all as I said they novo AI is easier secondly if a human is emulated that human can boost up the super intelligence I use cloud computing resources I'm flesh and blood what if I was plugged in and even if I couldn't use them to boost my virtual brain I could feed into Packard developer an AI are they novo AI but humans are risky creatures they can be kind of dangerous and they go crazy all too easily my brain is a very delicate thing how much the more so when it's in a software substrate and just a few changes can just totally mess it up so another objection says let's just kind of build it and make it safe as we go there is no area of engineering for building bridges that don't fall down there's an area of engineering for building bridges and they're not supposed to fall down the creators of the java virtual machine knew that if they want to network safety they had to bake it into the java virtual machine from the beginning it's not perfect but it's way better than C because they thought of it in advance and you may only have one chance another objection what about unemployment the White House just came out with something on that and I say I don't like that either in the 1920s nuclear weapons were a threat over the horizon and radioactive substances were very dangerous to workers and watch factor because they painted radium onto the dials and they licked their brushes and they got tongue cancer yeah it's awful and it would be good to stop that but if you could see nuclear bombs coming in some people did you shouldn't neglect that just because of the shorter term serious and not as serious risk another objection and this is professor at Andrew Inc of Stanford now of by do machine learning expert I hope I'm not misinterpreting him but he says it's too far off I don't know what to do sure it's dangerous but whatever no if you're convinced that it's a risk do something about it you cannot stick your head in the sand for twenty years playing with your machine learning till we all die in summary you are not in control of the systems you create evolution created us and it's utility function is of course that activity in a given environment it's not a thinking creature but it is a utility function Maximizer and we don't do what it wants at all now we use birth control to get fat we don't cooperate with our creator AI research today doesn't even think about getting the cooperation of a future super intelligence it's done for narrow commercial goals and remember that simple and a trivial genetic algorithm that broke out of its software box think about those two superhumanly powerful entities in the Cold War that used game theory that's why game theory was invented to threaten to destroy the world they weren't in control of anyone you have to define it right from the beginning so now I'm going to talk about research directions what we can do there is a number of efforts none of them is certain but this is the most important math problem in the world one of the more difficult or not not the most difficult and it has two parts defining a correct value system for this future AI one that is good for us and making sure that it retains this value system as itself improves we'll have a great ally in that the AI itself will fight to protect its own goal system as I told you before but for the first few rounds it might go to be a little bit buggy it might go off balance I'm only gonna talk briefly about the first challenge because there's limited time and because this has received less research interest and that is defining the right value system one which gives us the world we want to live in a world which maximizes the complex mix of human values of love life health beauty creativity and challenges we've got to have challenges not today i itself just as the cancer cure computer isn't trying to cure its own cancer but will bring us a world that we want to live in there have been many attempts in this in human history there's that Christian heaven where you play the harp and a marble palace that would be interesting for about one week there's the socialist paradise where everybody is well fed and get lots of fiber but I want an interesting future with life and vigor and energy and I actually don't know what it looks like but one potential direction is called coherent extrapolated volition and it says let's take the desires of all humanity and extrapolate them to a future in which we are wiser think faster and know more and hope that that converges on a coherent extrapolated volition so there are a lot of obvious problems with that but it's one direction that's been touched on another Direction is called inverse reinforcement learning where they eye learns what we want from our behavior and a subcategory of that is where the AI simulates billions of Earth's and tries out different value system and sees how that works out the moral problem there is that if you simulate the world well enough then you have moral agents who can suffer and you don't want that but these are a few efforts in that direction excuse me yeah yeah a perfectly simulated human is indistinguishable from us so you wouldn't want to perfectly simulate a human to suffer and if you're going to try out many value systems you're gonna try it on though the Marxist value system see if we can live in a stall in its paradise and see if people suffer you don't want that because they're people if you simulate them well enough you could try to simulate them almost well enough so if they don't suffer but that's a challenge was the question here yes yeah all right yeah I'll very briefly talk about that and I'll take of course more questions at the end the Chinese room says if I'm summarizing correctly that you put a person in a but in a room with a Chinese dictionary and people give in directions in Chinese and they translate they don't understand a thing flip flip flip through the dictionary translate the instructions and carry out the action so it's a sort of mechanistic system and you ask does this have consciousness or something and to put it very briefly I don't care it's a person in a room with the dictionary translating I care about a lot of things I care about whether a system can do stuff like cure cancer or kill everybody I care about whether persons are suffering or being happy and persons includes humans but it might include something else too so those are things they care about but the Chinese room experiment to me it's pretty obvious that if it does everything everything everything a human does precisely exactly a human touch then it's indistinguishable from a human but it would be so slow because of combinatorics you couldn't do in the lifespan of the universe that's the quick and simple answer of the Chinese room so let's talk about the second math problem now just an aside even if this didn't involve saving the human species it would be brain frying li awesome math and I just love this stuff if you have not read goodell asure buff do so before you die this is maybe the most amazing book ever written in the 70s and it's all about systems that talk about themselves mathematical system music that talks about itself paintings asure you know the paintings that include themselves and if you ever wonder where this can be taken into serious research directions you'll see a lot of those ideas and what I'm about to talk about and remember that the goal here is for the AI to preserve its values as itself improves is it boosts up it doesn't go off the tracks and get different values they're unsafe for Humanity so the pit the first day I we build will make a child or it will just change itself and that child will be smarter and one thing about the parent cannot predict what the child will do because the child is smarter just as I cannot predict what Lisa doll will do in a game of go because he's better than me so the parent area has a problem by the way this is similar to us dealing with the first day I but it's simpler to talk about a eyes themselves so it wants to prove that the child is reliable using formal logic it doesn't know what the child will do but at least that it solves certain that it fulfills certain constraints of safety and this is what it wants to prove it wants to prove that I'm sorry I made a mistake in this slide and I'm going to maybe even correct it now because it's now okay I can't the second very sorry because this is actually formal logic I would prefer to have it right rather than just talking over it yes child if child AI yes okay if a child a I can okay view control f5 okay and yes good sorry for that now the parent AI wants to prove this that it goes like this if child a I can prove P then P in other words the parent wants to prove that the child is reliable that if the child can prove something that you can trust that that thing is true that actually makes sense there's nothing weird about it let's just say it is reliable we just we just want to prove it's reliable but importantly without the parent necessarily being able to prove P itself because remember the parent is weaker than the child you can't prove anything that everything the child can in terms of formal logical systems weaker systems cannot necessarily prove all the statements of heads is a definition the can prove all the statements that the stronger ones can prove so we want to be able to handle also those tough cases it's stronger yeah stronger did I say was weaker let me just say that again if it wasn't clear and precise to edit it the parent wants to prove that if the child there I can prove P then P in general always and that includes the tougher cases that the parent cannot itself prove because a child is smarter okay so it cannot prove the child soundness sadly Libs theorem says no that's impossible a system cannot it goes like this it cannot prove the reliability of a stronger system if parent can I apparently I can prove that if child a I can prove P then P then parent a I can prove P and that's the easy case remember parent a I can prove the easy cases so what all this says is the moment Parenthood I prove that it can trust child on P then people it's the easy one forget that no I want this to be generally true whoops theorem is related to girdles and completeness results you may have heard of Turing's halting problem and there are a lot of these results in mathematics that are about systems that can't prove facts about themselves or in this case about stronger systems so it's a real limitation you cannot prove that this powerful AI is safe no an aside why the formalism we're getting here into the very foundations of formal logic and a lot of people say this is you cannot rebuild mathematics math is ok but let's get coding and the answer is that in formal efforts do have their place for example a machine intelligence Research Institute was doing a simple discussion of what might happen in the future groundworks the morality and then they there's some areas of simpler math that are not foundations of logic but when you're dealing with the system smarter than you one of the few tools you have for dealing with it is formal proof and luckily the AI is logic can in principle be more transparent than humans we are spaghetti code the AI code in principle can be read and analyzed although if it's a neural network or a deep learning or genetic algorithm that might be difficult so this is all part of reflexive decision theory this is a decision theory where the agent treats itself as a part of the world the entity can be caused and it is a cause that AI was caused it was coded it can be edited and its code can be in principle read or predicted now this may seem obvious to you of course the AI is part of the world I mean who is uh who is saying otherwise yes of course the AI is part of the world who is saying otherwise but many decision theories treat the mind decisions as magic by magic I mean they're not part of the model the decision theory will tell you what to decide they'll tell you if you have choice between five and ten shekels take ten but they don't talk about the mind that made it it's something that can be affected and that can have an influence I'm going to talk in one minute about that about why that can be relevant but you've already heard a bit about that which is that the parent AI is trying to predict and analyze and talk about the child AI they're reading each other's minds if all this talk about mind-reading sounds a little bit too much like ESP or psychics to you and fine forget it forget about human brains although you can be predicted for example mostly you're gonna walk out through the door and not try to walk straight through the wall I can predict that probabilistically but forget that for the moment just say a eyes and each one is trying to predict the other using solid math to really understand so that's why we want a reflexive decision theory and to exemplify this I'm going to give you two paradoxes which like most paradoxes are not paradoxes they're just problems that haven't been solved yet first one is Newcomb's problem a super intelligent alien Omega comes to you and offers you some money and it says there's already money under these boxes you can take either box ba or just a box a has a thousand dollars it's transparent box B has either nothing or a million if I predicted you would be greedy and grab both I put nothing in box me nothing if I put if I predicted you would not be greenie I put a million dollars in box B if I predicted you would only take B what should you do causal decision theory is the leading decision theory in academia and it says you should take both do the math if there's nothing in Box B I get an extra thousand from taking both if there's a million in box B I take extra thousand for taking both again Omega already put the money based on what it predicted it's not going to change that and by the way Omega has done this to a thousand people you've seen them do it before and Omega was almost always right you know what Omega was wrong three percent of the time this is you can do probabilistic calculations if you want I say this is wrong you should take only one box because you get richer that way and what cause of decision theory forgets is that your brain is causing Omega to do something does it read your mind again you're an AI your Omega figured out what what you would do get read your code or did some probabilistic calculations of what an AI is likely to do and it your thoughts your algorithm actually has something in the world other than the mere action of course all decision theories that you cause something you reach on you grab the boxes but here your actual mind can cause something Newcomb's problem seems kind of theoretical it's a super-intelligent only in doing predicting you but again predicting people and certainly predicting some AIS is not impossible another problem is the toxoplasmosis problem toxoplasmosis is a real disease that infects hundreds of millions of people and often has nasty psychiatric symptoms but often has no symptoms at all and also this is real when mice get it they are attracted to cats because this parasite has actually been evolved to get the mice into the cats now let's imagine for the purpose of our thought experiment that humans get toxoplasmosis from the environment and that it also attracts them to cats so let's imagine now you want to pet a cat what should you do evidential decision theory says don't pet the cat because we know what happens to people who pet cats more often than pregnant chance they get nasty psychiatric symptoms because they had latent toxoplasmosis I say that's nonsense go ahead pet the cat either you have this parasite in your blood or you don't but either way it's not gonna make it worse enjoy yourself because evidential decision theory forgot the causality of what's causing you to want to pet the cat and again it's a causality that reaches straight into the brain again in this theoretical case if you if you again if you want again teh eyes remember people can change its code in principle or do other things to it so just to briefly review these decision theories with a couple of equations I won't go through all the equations because the number of variables that just get complicated but just to see the key features here causal decision theory says do the action that causes the the best result so there's a symbol here for causality I recommend judea pearls book on causality and it just says Arg max take the best action with the utility given the probabilities of this action causing certain outcomes given the way the world the situation of the world is evidential decision theory says do the action such as the expected value condition on the action is highest so simple Bayesian probability calculation there's no mention of causality here and this neither of these take into account the fact that the entity is itself a part of the world let's talk a bit about update list decision theory this is a new decision theory just in the last few years and it says do the action that is in accord with the highest value algorithm that you could be now if I tell you to be the best algorithm you could be I don't know you think about that so again if you don't like that just think about a is a eyes are nothing but a string of Lisp codes are a string of Haskell code and you have to ask of this string of unicode characters which string produces the best results so updates the decision theory says UDP is of all possible UDP s is the one that causes the best outcome now there's a strange fact about this equation here which is that you you're trying to figure out what you DPS but UDP is also inside the acquire so these special quote marks are saying that UDP is quieting itself so someone here in a trivial t-shirt another can you show us soaked whining is of course when a program you can look at it afterwards that is Python code that prints itself you run that code and prints character for character exactly itself and if you look at that simple Python Quine you'll see that it kind of holds a copy of itself as a string right it's a data type called a string inside itself so the two levels here and which UDP exists the use in the mention the plain old software code and the string which is the software code and it has to achieve a fixed point where that particular string of software code would produce actions this signer says it produces action such that they cause an outcome in the world which has the highest utility algorithm so this is really cool what it's saying is that the world is one big decision tree so as you go through life you see something you take an action and then you see something else and you take a knight of course that's simplifying the world is so complicated that this is intractably difficult to compute but then again so our causal and evidential decision theories that in itself is not a problem so you go through life and there are all these decisions causal decision theory says make the better the best decision update this decision theory says look at this entire tree of the universe look at all the probabilities of things you'll observe under different conditions and the probabilities of what you'll do when you see those observations and make the optimal choice of all possible strings of Perl code just kidding that you could possibly be that is nice and it sounds almost obvious when I say to you I'll be the best algorithm course you gonna choose the best algorithm right but there's actually one theoretical aspect there which is very very difficult and which has not been solved and that is how to predict your own behavior if you are a deterministic algorithm again that string of code or even if your probabilistic the math works out similarly so predicting your own behavior is not in itself weird I can predict that I too will probably go to work tomorrow I too will walk out the door not crash through the wall there's nothing weird about it but it gets difficult when you try to formalize it because it involves counterfactuals if I try to figure out the probabilities of what I will do and won't do then of course I'm betting partially and what in fact in reality I won't do when the time comes I'm betting on myself doing something which I won't do now again is it impossible to bet on a false statement no of course not when you bet on that coin in the air most of the people that in casinos bet on a false statement that's why they lose money when the roulette ball comes up luhan's red instead of black right that in itself is not impossible and probability deals in counterfactuals but the problem occurs when you try to merge probability with formal logic just as the theory of gravitation and quantum mechanics are fairly well understood but the attempt to unify them runs into some real problems so I'm going to ask you a little puzzle what odds will you take that the googles digit of pi is even Google if the search company hasn't completely wiped your memory is ten to the hundredth power so I'm gonna give you just two seconds to even think about this okay the answer is 5050 really you've got to say 5050 otherwise you will lose money I was going to meeting with the colleague and we were good we're betting on who's going to drive so he bets on a 20th digit of pi I hadn't looked it up before if I had that wouldn't be a fair bet but because I didn't look it up before it was a fair bet it's a it's as random as anything but what if I asked you to bet on lay odds in the thousandth digit of pi what about the the third digit of pi is that even oh yeah yeah that's easy right that's that's not a bad you know 3.14 right so something funny is happening here the Google digit of pi you really have to tell me the answer is 5050 but for this third digit of pi you it's 100% you know what it is unless you put a small percentage chance you got totally confused so there's something a little strange going on here and the it has to do with your computational power which is that you are laying probability is tightly coupled to how fast you can compute I can compute although it's a 3.14 okay I can compute the third digit of pi or at least remember H but the question arises can I compute the 20th the Google's and so on and even setting aside computation power you're doing something very problematic here by debt by bailing probabilities on a false fact if I'll a problem if I lay a bet that the third digit of pi is odd I'm gonna lose right well the Google's digit of pi is either odd or even you don't know and if you lay a bet on it you are putting 50% chance on a false statement and your have a half chance of believing a false statement Danger Danger Danger principle of explosion boom and you remember this from elementary logic false implies anything if you believe false then pigs fly and the moon is made of green cheese and I'm going to be a billionaire tomorrow yes indeed yes yes right that's true you are the thing is this first of all it does involve computational power secondly before it's done the calculation it's going to lay on and it's okay to lay the odds and it's okay to be wrong as I said the question arises is what happens when you try to formalize it using the principles of formal logic combined with probability and informal logic and probability you say the probability times the expectation now let me talk about the third digit of pi if the third digit of pi is odd then I will be a billionaire tomorrow that is a true statement because false implies anything now for the third digit of pi it's too easy because we know that that's a false statement and I have to put a hundred percent zero percent chance on that which kind of makes things easier but in the case of the Google vision of PI until I compute it I don't know if it's a true or false even but it is it's absolutely by the rules of logic either fall even or odd and to do the simple expectation calculation on a false statement on a statement which I know one or the other is false I can justify that has a half billion dollar expected value bet but of course I just made up the number a billion when I said it be a billionaire I could have said trillionaire I could have said anything it just falls apart so combining probability with axiomatic logic is a very difficult thing it involves computational power and to just go back to the motivation it has to do with decision theories what I predict what I will do and I try to do this formally now who is doing this research because the most important thing is to do something about this one difference between the kurtzweil alien approach and what I'm saying is that I'm saying this because you have to do something now kurtzweil also believes in doing something which is advancing technology and that's cool he's done a great job of that but the key point here is not to be amazed its to either be convinced or not of course and if you're convinced to go ahead and do something to reduce that probability slightly these are all the people in the world today doing that mathematical research and reflects the decision theory and you can be one of them amazingly there are actual jobs open including at the University of Oxford amazing because it's hard to get an academic job nowadays but it's hard to find people who can fill this sort of job and if you aren't going through the research you can donate in contrast to the usual idea it's just as good to donate as to do like the thousand dollar an hour lawyer who gives out food at a soup kitchen and if he feels good about that we'll go ahead but he could just hire 50 people to give out the food they'd have jobs more food to be given out he would just have to do more boring billion dollar contracts for another client so yes so giving the money can often be more effective than doing yourself of course depending where your comparative advantages and a third thing you can do is advocate which is of course what I'm doing now I think there's little need for that if you believed in finding a cure for cancer you would either devote your life to finding a cancer cure as a researcher or you give money to a Cancer Research Institute going around convincing people the cancer is bad maybe if you caused them to donate but beyond people's being so-called awareness is of little value beyond the scope of donating or maybe going in for some sort of test advocacy has a small place and these are the Institute's that are doing this today there's a machine intelligence Research Institute near the University of california-berkeley led by eliezer yudkowsky and they've been doing this sort of deep math research in recent years just two weeks ago they announced the new program of less obsessively rigorous research working with machine learning community to find principles that might make today's machine learning safer obviously they're doing it to build bridges to the machine learning world and maybe to find some principles that will be applicable to superhuman AI there is the future of humanity it instituted Oxford led by Professor Nick Bostrom I already recommended his book and there they've done some social research in other words I talked about predicting when AI AGI will come and they've they're doing mathematical research and ni control which means any way of getting a superhuman area to cooperate but not necessarily with that sort of fanatic rigor that I talked about earlier and there's some new groups I'm really happy to see these even though none of them has proven itself yet because it shows that the field is growing and there's some prestige behind these names too there's a center for the study of existential risk at Cambridge and it's led by Professor Sir Martin Rees he was formerly the astronomer royal I put his picture because it's good to see senior academics stepping up there's also a new organization focus specifically on intelligence not all existentialists but specifically intelligence and they just got a 10 million pound grant from the leaver hum foundation and I'm really happy because if you've seen Unilever products like soap this is a soap fortune and it's good to see that not just alpha geeks are getting involved so that has just got started there's also open AI which is a new Research Institute with top top researchers that are being pulled away from Google by the opportunity to do their own research unfettered by the commercial world and they're completely aware of safety needs they work closely with all these people I'm still a little concerned that they're not really doing the safety research they're aware but I don't really know if that's what they're focusing on they're backed by a number of Silicon Valley billionaires and it's good to see those names up there and a factor of altruism is a field that says that if you're going to do good in the world to do it well as they say doing good better so this organization is funded by Dustin mousekewitz who is the world's youngest self-made billionaire he was in Facebook and they do research which until recently has said that the best way you can give your money is to get bed nets to prevent malaria in Africa and that's still a very good thing to do you get the most live save per dollar that way but just in the last month they've come around and they're having a major research program to decide whether Aix risk is a good place in fact necessarily a better place to put your money or a worse place and this is good because just a few years ago these people were deeply aware these people talk to each other but they were deeply aware of these needs and they were skeptical which is okay they had their counter arguments they've gradually come around and some big people are going to help in this area just some names young Tollan is a founder of Kazaa and of skype and he's been going around talking about this giving seed donations to a lot of the organizations i talked about including professor max tegmark future of life Institute he's at MIT got to go some people at Harvard and the future of life Institute is mostly just given out alums 10 million but I hope they're planning to do more it's a new organization I mentioned a lung musk and his crew and many many smaller donors don't forget them it's critical because it helps convince the bigger donors that they're not alone and because the smaller amounts do add up so in conclusion the AI does not hate you nor does it love you but you are made out of atoms which you can use for something else it's eliezer yudkowsky that's the risk that we're trying to prevent so join me come save the world in the circles that I run and we take that seriously there's a person who saved a billion lives already Norman Borlaug developed a new form of wheat which radically reduced the extent of mass starvation in places like India and Pakistan and it's been estimated he may have saved more than a billion lives he's not the only one he can be done with the right research sometimes and come join us less wrong Tel Aviv is full of people interested in this area and also happen to be interesting people although it's on Tuesdays it's some different Tuesday's from the generalist engineer which you are strongly encouraged to attend as well and of course I'll take questions now and you're also all encouraged to contact me Joshua Fox comm afterwards so over to you please 