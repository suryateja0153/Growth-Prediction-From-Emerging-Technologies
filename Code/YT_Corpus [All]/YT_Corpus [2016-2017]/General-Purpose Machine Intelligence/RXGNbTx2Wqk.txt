 next I'd like to invite Naveen rau the CEO of nirvana into the discussion naveen's focus is on developing the next generation of infrastructure to support machine intelligence Naveen thanks everyone yeah it was a great talk I'm a big fan as a cattle platform actually one of our head data scientists actually was number two on Kegel and that was this that was his main resume point so and yeah actually so what we're doing at nirvana is really trying to provide a platform to allow companies to take advantage of these sorts of methods many times people come up with very creative solutions on platforms like kaggle and they want to apply them in the real world so real data problem to have real industry value and so we're providing a comprehensive set of tools and infrastructure tied in those tools as highly optimized for these problems so overall our system kind of looks like this so you import data various types are supported we're a Python based platform so you know anything anything works basically but we have optimized it's especially for images video text and speech those the big four that we tend to see I think most in the world probably falls in those categories but some of our customers are actually using things that are tabular genomic data these kinds of inputs as well and we are centered around deep learning so we are a deep learning platform we have optimized the primitives all the way down to the firmware level kind of assembly level even twiddling bits at this point on GPUs and I'll show you some some speed benchmarks in a moment but we're also working on new silicon and that'll give us a new bump in performance an order of magnitude faster and also much larger models will be possible and so one of the main things here is that we provide a platform to allow companies to extract information from their own data so Joaquin from Facebook was talking about earlier training on specific use cases from Facebook is actually very different than training on corpora from from news articles or our manuals and so we actually find this to be true across the board image is the same thing of trying to build a specialized system for one particular purpose you don't want to use a general-purpose train model the same thing is true for humans by the way tomato farmers are better at seeing red than the rest of us so we need to build solutions for specific problems not generic ones I'll very quickly go through what deep warning is I think this crowd already knows this but the crux of it is really that you can discover these features from data automatically so you know building a face detector finding what's important in a face to identifying that face is something we used to do before now we let the algorithm discover it from the data and it turns out that works quite well the reason there's a lot of excitement around deep learning obviously is because it does work and it actually gets better than human performance in many tasks and we're seeing that happen over and over again in different data modalities kind of start off in the image space and that's what image net was all about here's sort of the top five error rate through time and this is where deep learning techniques start being applied to this problem and we saw a very precipitous drop in in performance or increase in performance that drop in error rates happen at in 2012 and that was basically about 10% difference that was kind of unprecedented this in this field and you know we kept iterating year after year and now we're actually down to about three three point five percent or three percent I think and human performance is about five and that's a trained human trying a human that actually went and studied these labels so I personally find that incredible as a computer scientist and a neuroscientist this is really hard human brains are really good at picking out information and this is pretty incredible so deep warning is a service where did that idea come from so we took inspiration actually from companies like Google Facebook also seems to be doing the same thing which is not surprising but I'm great to hear so they basically built a platform internally they have many different use cases I believe I heard that on the Google brain platform there's something like 600 different applications being built so we said well okay companies diverse as Google has many different application types different types of data but there is something centralized about deep learning so maybe we start doing that for the external world there are many companies out there that don't have core competency in this area they don't really know even how to get started sometimes they certainly don't know how to build an optimized infrastructure for deep learning there there are some subtleties to where you actually like configure the system and how you actually get the right software on the right system and get the inputs into that system the right way and doing this properly can really make or break your solution you can have you know two or three orders of magnitude difference in training speed if you don't do it right so this content that's the next Segway is into neon that's our framework that we've built an open source from neon so it is on it is on github so feel free to go and download it there and play with it it is extremely fast and we spent a lot of time making it fast we've made it very easy to upload or to import different sorts of data that most people tend to find useful and this will be the front end to our cloud or is the front end to our cloud currently so you can run this on your CPU or on your GPU based system today if you really want to scale that up to a much larger kind of problems you can actually run on our cloud service which in interfaces directly with AWS so customers can keep their their data on s3 and we can pull it out there sort of seamlessly and actually train a neural network models on large corpora of data it also supports multiple backends so today we have the GPU and we have two variants of that actually we have the latest iteration from Nvidia called the Maxwell architecture and we actually have support for the previous ones as well as CPUs so you can run on a CPU cluster albeit with some constraints because of the CPU itself we're gonna be adding xeon phi soon from intel and also the nirvana engine which is our customized hardware that will show a lot of new capabilities under cloud coming early next year so as I mentioned we are fast and we spent a lot of time on this and so this is actually a third-party benchmark done by a researcher at Facebook he basically has taken the a single system you know a single GPU you know with a specific kind of CPU and run various frameworks for a specific problem like Google and ads and you can see there where we're at the top of the list CUDA antenna and torch torch is an open source framework from Jana Kuhn and now you use heavily by Facebook and others kudi and n is from Nvidia so we actually optimized this more than nvidia optimize that which kind of tweets the model but I think chainer is another open source framework tensorflow of course from Google I think that's actually been updated that might even be faster than torture right now I have to go back and get the latest so a little bit about our hardware so like a GPU did for graphics we're doing something similar for deep warning we're focused on the primitives of deep warning which are actually different from graphics they're not the same graphics chips make a better starting point for deep learning than a CPU does but it's not it's not the best you can do and so a couple of aspects of this I can get into some details but we have a higher level of compute density about 10x higher we can do this because numeric representations don't need to be I Triple E 32-bit floating-point for deep neural network to converge if you're doing something it's that's that's highly general-purpose you're doing weather simulations and other kinds of things you do need sort of that fungible representation but when you focus on deep neural networks they're actually tolerant to quantization noise and other other sources of noise and we could take advantage of that to actually squeeze more parallelism onto the chip we also have a high speed HBM memory this is actually in package you can see there on the top right the memory itself is on an inner poseur all inside of the package so actually makes for a very simple board design in some sense but makes for lower power as well as much more memory bandwidth we also have a lot of on chip memory so this is important for things like neural networks where you have parameters that need to be updated frequently these connect with on the die and we have about seven or eight times more memory than a GPU does and one of the big things we have is also the scalable distributed architecture so we're shooting for what's called model parallelism this is where you actually break a part of model and its parameters across different chips and the big problem with doing that is that you need a lot of i/o between those chips to keep them synchronized and we actually have a proprietary interconnect it's 2.4 terabits per second aggregate i/o between chips that's outside of the PCI bus and we can we can do very interesting things with parallelization like 3d Tauruses and things like that which are theoretical lower bounds for distributed linear algebra and we do support training and inference in our topology today looks something like this we do have bottlenecks here for getting data into the system that's okay because a GPU is kind of close to compute bound anyway the distribution happens through the PCI bus tomorrow like next year when we get to our system it'll look a little bit more like this so the distribution across the chips happens on our own proprietary interconnects the PCI buses are used simply for shoving data into the system and some of the things we can do on our platform today and our customers have access to out of the box are like state-of-the-art image classification video activity detection speech-to-text so here's an example of a video classifier I think again walking was talking about some things like this these models are available for general purpose use so companies that have video problems and they want to figure out a use case can take that and and start working from that to build their own solution that seems to be very powerful because each customer seems to want a little bit of a different solution and they have their own data sets to to Train off of this is a the state of the art speech of text model it's actually deep speech to from from by to each research team we have a optimized implementation in our cloud so we've actually found that that's in in specific instances actually does better than humans tend to do which i think is kind of cool just a few years ago I never would have thought we would have gotten there in previous systems for speech to text for kind of unusable and so it's kind of incredible to me that in a few years we can get to that point where we have something that is really usable for context-dependent problems so Nirvana cloud kind of looks like this we have a bunch of racks sitting in a data center somewhere and there's there's a command line interface as well as a web interface and so once you define a model and neon you can submit it to run on the cloud you can watch your jobs on the on the GUI and once you're happy with it you can click that deploy button and get a REST API and then integrate that into your front-end application so that's kind of a quick and easy way to get going we have other deployment strategies as well for different kinds of embedded systems out there too so with that I would summarize so d4 nning is really the new computational paradigm this is the way we're going to make sense of data and so that's what I think is extremely exciting we have a whole new way of building and thinking about a computer and that's what we're about it at Nirvana we do do learning and inference on the data neon has the state-of-the-art GPU stuff now and will be the front and is the front into the cloud as well as our hardware and you know watch out for that coming early next year sometime thanks for attention 