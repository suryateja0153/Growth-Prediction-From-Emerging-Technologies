 hello and welcome to this session about the custom recognition Intelligence Service or Chris for short Chris is part of Microsoft cognitive services a suite of services and api's that let developers harness the power of machine learning to build more intelligent apps my name is Osama and I'm part of Christine voice is quickly becoming an essential mode of interaction between people and machines but voice interfaces are only as good as the speech recognition systems that power them today I'll show you how to use Chris to create a speech recognition experience tailored to your application before we begin let's briefly talk about how speech recognition systems work speech recognizers try to determine what was said based on the voice signal captured by the microphone and prior knowledge about the language speech recognition systems perform best when two conditions have been met first the system knows the most common queries and vocabulary terms that will be used in an application for example words and phrases used to make grocery lists and a shopping app would be quite different than those used to book an airline ticket in a travel app second the system knows about acoustic environment in which an application will be used for example if an app is designed to be used in a noisy warehouse the system should have previously seen many examples of speech captured in this environment the Microsoft speech the text engine is a world-class speech recognition cloud service that provides state-of-the-art performance across many common usage scenarios such as interacting with Cortana on your smartphone tablet or PC searching the web by voice or texting a friend however if your application contains particular vocabulary terms such as product names or jargon that rarely occur in a typical speech or if your app will be deployed in a typical environment then customizing the speech recognition system will enable you to maximize the system's performance for your application simply put Chris helps you enhance the accuracy of Microsoft's world-class speech recognition system by teaching it about the language and acoustic conditions of your application before we continue I'd like to explain how speech recognition performance is measured the most common metric is called word error rate which is the percentage of words that the speech recognition systems get wrong the lower this number the better the recognition is for example a word error rate of 10% means that the recognition system gets one word wrong out of ten improvements in word error rate are typically measured in relative terms for example if system a has a word error rate of 20 percent and system B has a word error rate of 15 we say that system B made 25 percent fewer errors and system a or that system B reduced the word error rate from system a by 25 percent now let's talk specifically about Chris Chris is a cloud service that enables application developers to build enhanced speech recognition systems using simple web interface these systems can be deployed to cloud services for use in applications Chris greatly simplifies the complex process of building speech recognition you provide the data and Chris handles the rest with a few clicks to better understand how Chris can help your application let's consider a simple example imagine that you're building a travel planning application you'd like users to be able to ask the app about flight information and airline information you initially use a general-purpose cloud-based speech service to perform the speech recognition such as the speech API in Microsoft cognitive services while this works great you like to try to make it even better experience for your users first let's consider the language used while the task does not contain any really unusual terms we expect most speech queries to be about travel and reservations this means terms like city names airline names and airports are expected to occur more often than in most everyday speech the particular subject matter of an app is called a domain and we'd like to teach the speech recognizers about the travel domain let's see how we can do this with Chris this is Chris web UI here in the top left you can see the various operations that you can do with Chris acoustic data is where you manage your acoustic data sets acoustic models is whether you create a customized acoustic models language data is here you manage your text language data language models is where you create new language models customize for your application deployments is where you manage customized speech recognizers that have been deployed to cloud services once I start working with Chris I'd like to be able to measure how well chris is doing learning my application this is done by using an evaluation data set or test set for short in this example our test set consists of recordings of users speaking queries I expect my app to encounter each recording is by the transcription of what was said whenever I work with Chris I can request that Chris evaluate this test set and give me a before-and-after word error rate measurements this way I can clearly see how much improvement I'm getting here is an example of a test recording after creating the test set we can now proceed to creating a custom language model for our travel application to create a custom language model I need to provide text examples to the service that reflects both the style and content of the task the users will be performing with my application in this example I've created a list of 1,000 queries related to flight information and reservations I've uploaded a text document containing these examples to Chris once the data have been uploaded I am ready to customize the language model to do so I will tell Chris to create a custom language model using the data set I've uploaded I will also ask Chris to evaluate the speech recognition performance on my test set before and after adaptation here you can see the results of the adaptation as you can see here before adaptation we got a word error rate of 11.5% with the adaptation the word error rate was reduced to 7.9 percent thus the custom language model made 31 percent fewer errors than the original system so this is great we've improved the user experience of our application by improving the speech recognition accuracy actually our application became so successful that one Airport wants to install it in kiosks in the terminal so that passengers can use it to ask questions regarding flights but now we're faced with a new challenge the acoustic environment in an airport terminal is very noisy background noise from passing passengers shops and intercom announcements may adversely affect the speech recognition accuracy to be sure let's collect some data this will enable us to create test set for this new airport kiosk scenario here is an example of our recorded test data in an airport to show the effect of the noisy environment on our speech recognition system let's run the same language adaptation that we've done before but to use our new airport data to test it as you can see here the word error rate is now twenty point nine and the adapted language model reduces the error rate to fifteen percent well this is a sizeable improvement we can do even better by using Chris to teach the speech recognizer about the noisy airport environment let's see how Chris can be used to teach the speech recognizer about the noisy airport environment first we need to collect speech data unlike the language custom acoustics require recorded speech examples these speech examples should consist of users speaking to the app in an environment so close to the expected deployment environment as possible the amount of speech data can be anywhere between 30 minutes to several hours or more in this example I've created a data set of about three and a half hours of travel related phrases spoken in an airport terminal as you can see here our baseline of 20.8% but after acoustic model adaptation we got 13.8% that's a great improvement now let's see the effect of both language and acoustic models combined here I've repeated the language model adaptation but use the newly adapted acoustic model to perform the testing as you can see here our baseline is 13.8% and our final word error rate is eleven point four this is about 46% reduction in word error rate from our starting point of twenty point nine percent that's more like it now that we have great results from our offline tests we're ready to deploy our custom speech recognizer to a cloud service for use by our application to do so we go to deployments and create new one and specify which models to deploy as you can see here I've already deployed our custom model store and endpoint and we can see here the endpoint address as you can see here there are two end point URLs one for plain HTTP requests and the other one for WebSocket requests for speech SDK all I need to do in order to use the speech recognizer is to copy paste the URL that I want and use the primary or secondary key here I can also from this screen test some audio files I can upload a quick file here and get the transcription just to see how my endpoint is doing here I've created a very simple application using the speech SDK on the left you'll see the recognition returned by the default speech recognizer and on the right you see recognitions returned from our deployed speech recognizer I've also set three test waves from our test set let's try the first one now let's try another wave file between here on the left you see the output from the default recognizer as you can see it missed a lot of the speech while on the right our customized speech recognizer got all the speech right let's try the third one again on the left he can see the default recognizer missed a lot of the speech at the end while on the right our customized recognizer managed to recognize all the text in this session we demonstrated how to use the custom recognition intelligence service to create a custom recognizer tailored to your applications environment user population and vocabulary we also showed how to then deploy a custom speech recognizer to a cloud service for use by your application thank you for watching for more information about Kris please visit the links on the screen 