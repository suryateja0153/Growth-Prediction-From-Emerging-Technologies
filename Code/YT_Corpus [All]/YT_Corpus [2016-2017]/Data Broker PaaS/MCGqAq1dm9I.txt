 welcome everybody to this talk my name is Julian Fisher I'm CEO of any nines we are a cloud cloud foundry consultancy currently focused on class family data services located in Germany so the talk is going to be about building a production trade Postgres cloud foundry service which leads us to the question what does production grade mean so with a variety of customers we've been working in the past we've seen various definitions of of production grade so for this talk we'll actually go with the definition we at any nines have been using in the past because for historical reasons we run a public any nines but a public cloud foundry platform called any nines so back in 2013 we started running cloud foundry as a as a public offering so when using when offering cloud foundry to a public audience you actually onboard various kinds of users so people you don't know people with you know poorly designed apps people with with vicious apps people bad people trying to break your platform so operating a public pass actually turned into a learning environment so the production readiness litmus test you can have is offering a public platform basically so I think it can't get any worse no in case yeah you will spend the public offering you will also be fine in a more private environment so actually with these experience we've made we started to investigate how to create data services because they were not you know back in 2013 nobody had service brokers there were some you know shipping with cloud foundry but they've been deprecated them so we used them and they were well it's say not really production ready so we actually started investigating what doesn't actually mean to have you know a database being deployed on a public on a cloud foundry so we've been through a journey that took us like two years and six people working on naanum on a service framework to doing taking care of data services so this talk will focus on Postgres but basically most of of the things that can be transferred to other data services as well so and as the rest of of the conversation we have will be around leading through the design decisions that are necessary when building a data service sadly 25 minutes or 30 minutes are not enough to cover this this topic I can you can you know come to our booth and push play I will talk like two days straight about this topic so feel free to do so so one of the most important things when building a cloud foundry service is how do you actually implement the service broker API so there are only a few methods to be implemented looks simple but isn't so one of the most important decisions is actually what is your service instance going to be and for Postgres there are a number of decisions you actually have to make and one of which is what you actually want to offer as a service instance so we actually come back to this question as it will require going through some other Postgres decisions one of which is whether using a Postgres server or a single server or a cluster so well offering a platform you will have users you know playing around with a small toy app they won't have a Postgres server that's cheap like five years a month and some some customers with a production create app maybe they move away from physical servers they to go to your platform they want to have a production cray database that's being clustered in order to be stable even when infrastructure incidents happen so the question on whether to deploy a server a cluster cannot be answered as a as a general this is how to do it right answer but it heavily depends on the the context of the customer so let's go into the cluster topic just for a few minutes because once you got the ability to deploy a cluster you can deploy a single instance anyway so the clustering Postgres and making Postgres highly available is a little cumbersome because it inherently has never been designed to do that and with the transaction based sequel database you know reminding of the cap theorem it's not it's not a simple task to do so one of the possibilities to actually make a database to make a database highly available is to introduce replication so when thinking of replication you that you make think of whether using synchronous or asynchronous replication where synchronous replication means that transaction has to be confirmed by a majority of the cluster nodes where a synchronous replication you're right against the master and those changes will be replicated to a slave accepting that there's a certain difference between the master and the slave called the so called replication like so the far away your your slaves are the greater the replication like and in case something goes wrong the greater your data loss is because there is a data loss with the size of the replication lag so in our case we've been looking at my sequel and Postgres we decided to go with Postgres as the first relational database to implement using the service framework so we had in mind that there's my sequel with Galera very nice synchronous database management cluster based on synchronous as replication so we thought looking at Postgres bosphorus has built-in replication since Postgres 9 and it's a synchronous replication we thought wine fair enough having one already BMS with synchronous replication and one with a synchronous replication we stick with s increments for Postgres so that decision been done we figured out that not too hard that the replication facilities within posters are fairly limited and there is a variety of tools you can use and to to actually overcome that problem and it's not really easy to find out which one to use but we actually try to build a Postgres and make this available to our customers which is you know you know easy to automate because we we actually have to take care of the cluster management so we we just looked at the replication built into Postgres i'm seeing that there is actually no cluster management there's replication but don't cluster management so what is last time management mean is you have let's say three databases a master and two slaves what happens if your master database server goes away so you need something that actually recognizes that yes your master is gone that seems easy but it is not because sometimes your network might have trouble so in case of a network partitioning you it could be the case that the communication between the service are disturbed but the masters actually there so you have to find a way to ensure that you're not performing a failover although still you have an so those are the tasks usually solved by cluster managers not worth talking about that too much here so there are solutions to that but we need a component taking care of this so a small summary of the cluster thing is that you want to have three nodes instead of two so that you have always a majority of service so you can clearly judge whether there's well that a majority can decide upon whether there's you and who's it going to be so after looking at several solutions his from historical reasons we've been operating Cloud Foundry and sorry of databases post trust databases for seven years now we've been starting with physical data bases being clustered in such a cluster with Massa and slave we've been using pacemaker to to do the cluster management so the first approach was can we do the automation can we take the automation we have around pacemaker and put it into a Cloud Foundry environment and clearly the answer to this is no you can't well you can but it's not really meaningful because pacemaker is a beast it depends on every single Linux library ever written so in order to pacify that you will actually have to Pasha Phi the universe so that's a bad idea also it's it's not really nice to automate it it's the way pacemaker has been done is it's not really something you want to put into a Bosch release so what we found is that rep manager is a good resolution so it's simple it does the job and it you know it actually based it basically does the job fairly very good so it does also monitor your application performance but most importantly does failure detection and helps performing automated failures there's a little research to that and there are a lot of let's say edge cases and when it comes to plow foundry because you can't just you know take away one server and promote another server in in a cloud foundry environment because there might be IP address changes so how do you actually tell your application that the application should now write to a different database server so that's one of the problems to be solved in our case we added a console cluster to our service framework so to be a little ahead of the talk we we use Bosh underneath to deploy database clusters so we ever there's a change in the cluster we'll tell our consul and we use a DNS alias to in the credentials so that your application will always have a DNS entry resolving to the right master and the cluster manager the rep manager in this case has one of the one of the purposes is that when it promotes a new master we will talk to the console and update the alias pointing to the master so the trigger actually comes from the rep manager but the execution of the actual failover stun using console alright once we actually made a decision that we want to support a clustered Postgres we also have to make the decision what the service instance is going to be is it going to be a single cluster that will be sliced up or is it going to be a cluster per service instance so actually two different strategies come to mind a shared or a dedicated approach with a shared approach what you do is basically you create a single Postgres server or single Postgres cluster and you slice it up into different databases and each database is going to represent a service instance so this is very easy because you need to do you need to create a bas-reliefs you need to deploy one bosh deployment creating your Postgres cluster and your service broker will then access this cluster and return appropriate credentials the drawback of this solution however is that the isolation between the service instances are pretty weak because Postgres has isolation built in you know multi-tenancy capabilities but they are fairly restricted so when exiting a database server one app can drag down the performance of the entire cluster hitting the cache or you know creating disk utilization and CPU utilization so the contract towards the customer will be fairly fuzzy because you never know what you of a database you actually get another major problem with that approach that even with a cluster your cluster represents a single point of failure within your entire architecture so a production Cloud Foundry system you have a runtime and the runtime is you know it's just an awesome piece of technology cloud family is awesome right so you'll deploy a tremendous amount of apps against against the runtime and now you have a load of apps and one database cluster I mean you know that doesn't sound right doesn't appeal right for the obvious reason that whenever your database cluster goes down and I can tell you you have a component in your system it's going to fail whatever it is it's going to break right so we recently melt it down our OpenStack because of a kernel driver issue and because all the hosts we are the same we have the same problem all the hosts so 20 out of 24 holds died within one hour right so availability zones sorry didn't say that what's just all gone all right so let's say we want we really want to ensure that whenever a cluster goes down for some reason the situation won't look like this because a lot of applications will rely on your post press and I can tell you how this feels exactly like that because your phone will keep on ringing and customers will give you a bad time because they are so disappointed because they said newest they expected the platform to work so the problem with the shared cluster in general and that's true for every data service is that once this cluster goes down you all your instances are gone and you'll have a lot of trouble so what's the counter strategy to that obviously it's going to be dedicated clusters for everybody so instead of having a single database blast our single database instance you'll have multiple of them maybe even both so in the ideal world and we've made that happen you can actually create a single instance or a large single instance or a single cluster or a large cluster and you can also migrate between them and with that you'll have a big advantage also when creating a contract towards your customer because now when you create a dedicated cluster you use infrastructure resources for and the infrastructure isolation to create thus multi-tenancy behavior which means that when I provision a Postgres with four gigs of ram you're going to get four gigs of ram CPU and a certain amount of disk and in case your application needs more resources will just scale to a larger database but it's never going to be your neighbor tracking down your cluster because his app actually goes crazy unless you have a let's say unfair amount of over-commitment in your infrastructure which is totally up to you to decide but the solution is actually safe so looking at the same scenario you now have a different ratio between applications and service instances so one of these serves instances go down the problem is pretty much contained and you have only one angry call to answer right saying well I excuse me if things went wrong and we're going to fix it so your problems are going your problem with Postgres failures are going to be contained right so we've been through the question of having a single server or a cluster we've been through the question of being of having a shared or a dedicate our dedicated approach so ideally you have a choice between single or cluster and it's going to be dedicated well the drawback obviously is that it uses more infrastructure resources but then you have a stable contract to the customer it leads to the question when do I actually provision those virtual machines so two strategies again come to mind we could actually pre provision those virtual machines so that can be immediately handed or we'll do this later so with the pre-prohibition strategy you have a service broker and a pool of service instance like you know several of each plan you offer and whenever somebody performs a create service command you'll just assign one of the virtual one of the service instances out of your virtual machine pool same for cluster just that you you know assign a cluster instead the problem with that approach is obvious like you'll have ten of those things on hand and there's a hackathon going on and people start creating service instances like crazy you run out of pre provision instances so also these pre provision instances will consume infrastructure resources even if nobody uses the database so it's actually again a counter strategy strategy that comes to mind which is why don't we provision this these service instances once you know somebody creates service so in order to do that you have to provide some automation and whenever you do CF create this will actually create them a a post press server or a post crest cluster so pre provisioning the benefit is you will have your service instance right away and the on-demand well you have the advantage that you don't use the resources and once you go down that that path you'll be able to serve as many instances as your infrastructure actually has resources so we actually started with the pre provision approach because then we could actually fill the pool being deployed manually you know already giving the customer the appearance of having you know dedicated instances and then we actually did the automation afterwards filling up the pool once the automation is ready automatically and then of course we can actually switch down the pool size because you'll actually have to provide you know instances from each service plan so we can turn our framework into deploying those things entirely on demand a mixture is interesting in cases where you have CI pipeline creating service certain service instances at a high pace so where the provision time does matter to you so it's a kind of drawback and a balance you have to make you have to make those design decisions and you can configure it so we'd like to have a pool small server for for testing purposes on hand and the rest is going to be provisioned on-demand so with that being said you can't on-demand provision something without automation so one of the key questions is how do you actually do this automation and while containers are very modern and fancy and maybe it's going to be the future we had the impression that having a database a database should be close to the metal as close as as it is possible because often you know performance is is an issue and also we would like to have an automation technology we can really really rely on and after cooperating cloud foundry with Bosch for years we really fell in love with wash and that's not very obvious because our team was using chef for six or seven years so for them Bosch was really a challenge to what they already been using but they learned to fall in love with Bosch one of the reason is because Bosch gives you infrastructure independence and we moved infrastructure twice we actually started on VMware moved to OpenStack for cost reasons and recently move to Amazon for stability reasons but that's just because we we can't run OpenStack we are a platform company another infrastructure company also I've not seen many solutions that really inherently do do the orchestration of entire distributed systems so well as bashed us including virtual machine and persistent disk image while being entirely uncoupled from operating system so back in the chef's days you have a cookbook with if else clauses for different operating system also using different package managers gives you a very heterogeneous system in the end so in this hole in the operating system support will will actually go through the the cookbook and it's not very nice so with boss you have a clear contract here also the separation of of a blueprint of a distributed system let's say yeah the blueprint in a bas-reliefs and in contradiction to that the the specific construction is a very interesting approach in Bosch so when it comes to deploying data services the advantage you get is looking at the Postgres cluster example is we have a Bosch release that that deploys a Postgres cluster but the same Bosch release with the different many tasks can just deploy a single machine so you actually cover a variety of data service plans with just a single automation also interesting using Bosch is once you use Bosch to deploy your Postgres cluster you get the monitoring and self-healing capabilities of wash for free so as I said the rep manager will take care of your data of your instance so whenever a database server goes down the rep manager will talk to console and your application will continue to write to one to the new database master but Bosch will recognize that there's a missing virtual machine and will just redirect the virtual machine and the virtual machine comes up it will actually recognize that it's not there's a masked anymore it's a new virtual machine right so we recognize that it's that's now it's a slave and be an integrate into the cluster as a new slave so we had actually have with Bosch a integrated way of recovering from a degraded mode after an incident that's a very nice thing to do that is also topped by the scalability scenario where we also want to be able to take a single service instance let's say I've created a small app and deployed it on the platform but now my app need to grow so what I can do is see if create update and turn this into a large cluster so how is that possible it's possible because the sows framework actually creates a new Bosch deployment and you hand over Bosch the Bosch deployment and Bosch will actually create new virtual machines and you know scale the one that's existing and copy over the data so you you get that behavior fairly at low cost it's not for free you have to do some management around it but it's it's it's so much that's already been done by Bosch that it's fantastic so of course the same strategy applies when you want to create one you scale a small cluster like this fellow on the right side to a large cluster it's just taking down the virtual machines one after the other so your service keeps on running scaling the virtual machines so you scaled your cluster all right so now with that all being said well this fancy thing is like how does it actually look like in in the resulting system the architectural overview is looking like that so we found out that the service broker basically does nothing we released data service specific so we outsource everything that's specific to a data service into a small separate micro service called the Postgres SPI service provider interface comparable to the cloud provider interface of Bosch so what this fellow does is offer the meter data so telling which service plans are offered and also when creating a service binding it issues the credentials for the initial credentials this includes the initial credentials so credential management and everything service specific is going to be in the SPI so these cells broker then triggers the creation of Bosch deployment which will then talk to Bosch and subsequently of course there will be virtual machines being deployed by motion so the service progress has said it implements the Cloud Foundry service broker API is generic for all services we have we have rabid Redis rabbitmq more going to be in Postgres and it can be configured to use the spi as a remote service the spi itself as a certain capsule aids those data service pacific logic and among that the service catalog and the credential management so the deployer is a small abstraction abstracting from bosh deployment so it actually does two things first it manages deployments of course and the second is it manages templates which is can be you know seen as a borscht manifest with placeholders in it so how does it actually look and how those how do these components interact is whenever you can you see that yeah so whenever you call a create service you'll actually hit the service broker who will then talk to the spi because what the service broker has to do in the next step is to to trigger a deployment using the deployer in order to do that it has to hand over the name of the template to to deploy as well as some deployment attributes so one of the information that is required to do that is the the service plan the customer has chosen which maps then to a deployment template so the system what the system actually does it creates a service proko that lets you trigger bosch deployments so actually you could also deploy cloud families with that solution if you create a partial ease for cloud families so yeah after you got those information you can then pick a deployment against the deployer by handing over the template and the attributes who will then generate the deployment manifest and trigger the deployment the cloud controller then keeps on palling whether the deployment is already done and once it's done the the service broker will store some metadata about the deployment because if you want to create them later a service binding you'll have to know that there is a data you know dedicated instance running somewhere so the SPI is able to connect to this database server and you know create a new database user so you have to store some metadata which is again not service specific because it's handled by the SPI in the end so this works like charm we've been using X solution for roughly developing it for two years and using it more than a year and on our platform and yeah it's proven and it works so what can we actually learn from that is first of all designing a data service go with the dedicated service instances anything based on shared cluster is dangerous might be working if your company is small but it's scale would be I would be using it so every every shared data service we've been offering it exploded at some point at time so with that on-demand provisioning is essential so you have to pick an automation tool you are familiar with and you'll you know have to find something preferably that really takes care of the life cycle of a distributed system such as a database so because you you will have to solve the problems of how to update that in the end as well so the biggest challenge was not about the framework was not about bosh was not about everything who actually was about Postgres so finding a Postgres replication and clustering truths that was the things we actually have investigate and how to make this failover happen on on infrastructure but still not being infrastructure specific so we can take the service framework and we've deployed it on vmware we've deployed it on OpenStack we've deployed it on Amazon and we didn't have to have to change a thing despite of cloud configuration of the of the boss releases obviously so let's consider configuration or change of code so we also had to learn a lot of Postgres and iterally you know shape the thing edge cases have been found so you have to do some automation around that but yeah that's about it it works the strategy works so and we also open to conversation on how to to share that with you so just approached me and asked if you're interested in something like that will we help you building data servers if you want to so feel free to ask any question about this questions that they handsome fellow in the blue shirt yes [Applause] [Laughter] you all wait now okay so basically overall very good thought father there good approach Thank You Wayne I have to respectfully disagree a bit about the dedicated versus shared there are times when you want to go share it especially if you like you're a service provider and you've got just massive amounts so the key there is actually investigating whether you can actually have a pet coffee sorry if you whether you can actually have a plan to start somebody on shared like the free tier and then migrate them very easily to a dedicated it's like that's something that could be a way to go to yeah wash with as first Postgres is concerned oftentimes you can actually get a much better performance scenario if you have separate discs so Bosch currently has I'm saying this for the community as a whole Bosch has a really nasty limitation of a one disc policy I'm hoping it's in the roadmap to fix that I'd like everybody to apply pressure for that because that can really help the services stories when to playing with Bosch and we just announced at the Postgres conf in New York City that open sourcing of a similar project called the RDP G it's we did it for GE they allowed us to open source it a lot of the same concepts and approaches were done within it so now that that's open source what I'm I would literally like to see as if you know is this open source and can we merge efforts instead of having two efforts and like what are your thoughts on that well my thoughts on open sourcing that is that we are actually currently investigating opens sourcing it so this solution has also been this developed over time we've been using it at the platform so it could be open so soon but this is a discussion that's currently ongoing so I can answer that - final decree yet but we're currently talking about with partners and how actually open sourcing could look like because we have a development team to fund here and if there's no license money coming in we have to replace that so that is very fair so everybody hire them so that they can open either one of the models we actually could apply is that we'll have sponsorships so that people can influence the backlog of such a solution maybe telling us which data service to make next so we opened such as suggestions here sounds good thanks welcome any other questions No well then yeah there's a plucking system in the framework that allows you to create streamed backups so you can actually read will say right ahead locks and then stream it in chunks to let's say OpenStack Swift or Amazon s3 we can only have only basic strategies implemented like like creating a dump instead of write a headlock logging I think Stockman Wayne has something interesting it could be integrated as well so yes there is something foreseen in the framework but implementation the plugins actually need to make sure a little more it's it's that's the thing currently under development all right thanks so I'll be around 