 so yeah we're going to talk about passing kubernetes today first a little bit about me I'm the creator and CTO of day switch we're obviously gonna gonna show and you guys are gonna learn more about here little-known fact I was actually one of the largest external contributors to the docker project during its formative months so if you ever use things like the volume subsystem or the - PFLAG or bind mounts or looked at a thing called host config those are all things that we added into the docker project in the early days to get early versions of Deus working also as one of the first projects to use core OS and SCD for sort of this platform use case you know we ended up in production with a lot of stuff early and and therefore helped shape a lot of operational best practices around core OS intensity so chase was acquired by Engine Yard in April of 2015 we are a leading open source platform as-a-service we now according to docker hub have 5.7 million downloads of version one of Deus this is the pre kubernetes version and you know sort of generally what we're doing is we're building products for the kubernetes ecosystem we're going to talk about workflow today which is sort of the traditional Heroku style platform as a service layer but also projects like helm which is the package manager for kubernetes and a few other projects in this sort of space really answering the question I have kubernetes now what what we found in the wild is that kubernetes is a cluster manager you know it's got a lot of rough edges and there's there's quite a distance to go from you know being you know having a kubernetes cluster to actually getting work done as a software team and we're trying to fill in those gaps so you know generally the mission for us is to accelerate this transformation to what you know is starting to be called cloud native computing part of this is about empowering developers you know being able to create applications deploy code configure environments you know view vlogs when your app 500s scale out via the process model for for 12 factor apps and more than that you know this is really important to you know this idea of developer self-service is really important so we're trying to empower developers through that but we're also in trying to empower operators and and I find this idea of this cloud native operate model really quite interesting so this is how Google runs operations inside of Google so you know they have some experience with managing distributed systems and platforms at scale what they've done is they've taken the traditional operator job you know someone you know in DevOps and they've actually segmented it into three areas so you have infrastructure ops which is your Hardware racking and stacking gear or that sort of thing which is by the way sort of made obsolete by the whole invention of is right you don't typically need to do this job I'm Google does it because they have custom Hardware other other reasons and then you have cluster ops right which is this idea that you're taking nodes in lots of different racks inside a data center or in public cloud and you're tying those nodes into this concept of a cluster right and that cluster is tolerant of nodes coming into and out of existence failures that sort of thing and there's a group of people who are good at this and their job is to expose a set of cluster api's up to another separate operations function which is application ops and inside of Google known as sre site reliability engineers and these are operations people who are intimately familiar with the type of workloads that they're operating at scale so for example you know dealing with all the monetary and monitoring and telemetry data that come out of apps now that's a very distinct job and it's actually quite different than you know what you need to operate a board cluster or kubernetes cluster at scale those are different skill sets different sort of conceptual problems and wildly different from the type of infrastructure ops you know that most people are dealing with and yet if you think about you know folks today the typical DevOps for all it's everything from provisioning boxes all the way up through glueing them together with some you know tooling and all the way up through loading apps and workloads on them and managing those workloads so you know kind of asking a little too much so trying to move the world a little bit closer inch by inch towards towards this model something that we do so talk quickly about cloud natives so not sure if you found out about this cloud native computing foundation but it's a pretty interesting group I know a lot of people are like oh yeah I know Foundation wonderful right but this one feels a little interesting to me it's it's a you know for you know folks who want a new world for these born in the cloud applications and and interestingly you know Cloud Foundry Foundation is a member a founding member of this as well as folks like Google and you know many of a docker and many of the other big names in the space so what is a cloud native application right in the the CN CF defines it very specifically its container packaged dynamically scheduled micro service based right now inside of the CN CF they're trying to deal with all these different bits and pieces that are required to make distributed systems like this work if you look at this diagram everything in blue is sort of still being decided right winners and losers are being shaken out in sort of the market of open source so a lot of stuff needs to solidify before the CN CF you know is going to be complete and this mission of trying to make this more stable and they're interoperable another goal of CNC if you think sort of forward-thinking is you know where does a micro service architecture end up leading you right there's this idea of composite micro services which are microcircuit are constructed of other micro services and other composite micro services and there's sort of a dependency graph that forms where you know change in one service may actually end up impacting a series of other services and having that modeled starts to become pretty important when you're deploying on these clusters at scale another thing very forward-thinking is the future of publication and deployment our chief architect has this phrase that I like a lot which is modern applications that are cloud native are sufficiently complicated that they need to be flown by the instruments which is to say that really the monitoring and telemetry coming off those applications is really should be informing you of how you're supposed to handle these so things like roll forward roll backs you know this idea of changes in a dependency graph that propagate automatically this is the future that we're all building towards and we're clearly not there yet but having some of these primitives worked out and having the goals in mind is pretty important the CN CF is taking the approach of sort of an incubator model for different technologies and the first one that they chose kubernetes was which was offered up as the seed technology and just a few weeks ago was accepted into the CNC F as sort of a donated technology now and that doesn't mean that CN CF is mandating kubernetes its they're saying this is part of the toolkit for the future of cloud native applications and we expect there to be lots more projects donated in the future so quickly about kubernetes you know how many you here have heard about kubernetes yeah so okay basically every hand alright well that's good the real idea here for kubernetes is about managing applications and non machines it's an API designed for cluster management and otherwise known as cluster manager so the 10,000 foot view is you have users talking to the kubernetes master which is made up of these different components which talk to these couplets that run on different notes but o either platform builders or users need to care about is that there's an API that they talk to either via COO control or via an API client and that talks to this big cluster of containers and that's really all you need to care about now the real question going forward is how leaky is that at that abstraction there right you know do you actually just get to care about running stuff across hosts in any you know running any OS or on any public cloud you know I think there's a fair point that it's the you know the abstraction is still a little bit leaky today but the bet is that over time this API is really good hold that abstractions gonna hold and no matter where you you know what whether you're running a public cloud bare metal private cloud any OS the kubernetes api is going to be the same thing so real world adoption you know this slide is actually pretty old and there's a few big main customers that we're working with that we can't talk about that are actually on this slide this stuff is being adopted like crazy right now so you know if you just take a look at the activity in the github project it's pretty clear that that's that that's happening so why Cooper names for us well it's got this vibrant community which is just crazy if you look at the github project and grow stats the idea of declarative API is everything in kubernetes is I want the cluster to have this desired state and then the cluster reconciles that desired state in a series of control loops and that's our that's the entire architecture of how the platform works it's very beautiful it's very elegant and it's really what you want for large-scale distributed systems so we believe it has these red abstractions though there are many of them and they're complicated but it also promotes loose coupling on the idea of labeling inside kubernetes is very unique you don't see that sort of thing in other cluster managers today and you know to us and to me specifically this really does represent Google's 10 years experience using containers at scale there's a lot of lessons learned that went into the design of kubernetes and frankly a lot of things that they did they did differently than they did it Borg you know they did them differently in kubernetes problem using kubernetes is really complicated specifically for developers for operators and people who have our heads in this you know day in day out it's you know we can get it you know where we live and breathe this stuff for devs you know your average Enterprise Java shop they don't get this they don't want to know what a replication controller is they don't want to understand label selectors you know this is just way out of their depth right so that's why we add tasks choose to build things like Deus workflow which is a private pass running on your own infrastructure and this really provides this simple developer experience it looks a lot like Heroku or something like cloud foundry but powered by kubernetes and using all the primitives in kubernetes and deploying things the right way so that applications in kubernetes managed by day's work full look and feel like any other application in kubernetes and really for us this this tool ends up being a force multiplier for operations teams we don't see developers themselves you know developers go off the shelf will pick up docker right and start building docker images and containers the people who are picking up deus workflow are typically operate operators right there are operations teams who are like I don't want to be in the business of you know trying to help my you know developers get their sandbox apps up and running I just want to give them a space and let them run and then if there's a pipeline that we need to create later we'll worry about that later right and I'll show you a little bit about that so today's workflow conceptually sits atop the kubernetes api we provide source to image builds you know a lot like you know Heroku build packs for example while guy aggregation application released in roll authentication authorization a very nice edge routing system that all sit on top of kubernetes and you know leverage kubernetes primitives where where possible so the workflow for developers is you know get push I'll show you this in a minute get push days PS scale out the web containers it's sort of this imperative you know type in a series of commands do a thing kind of model that made are very successful but this is actually not how you drive kubernetes at all right kubernetes right manifests you submit the manifests to the cluster and that's great for a distributed system and we use that declarative model under the hood the problem is developers don't think that way they don't want to write manifests they want to they want to say do this do that change this configuration right and having to submit a you know a yam will change or adjacent change you know is a little bit much for some folks for operators it's pretty simple we leverage kubernetes primitives right Coop's ETL and a new project called helm which I'll talk about a minute so I'll talk about now helm best way to find share and use software built for kubernetes so it was interesting for us yeah when we started working with kubernetes day in and day out we were like wow this is really hard I got to write these manifests and gonna find docker images that work with different things we wanted to make it really easy to get up and running with common workloads to find new workloads and collaborate with the team on workload definitions in sort of the context of source control and we create a project called helm to do this and you know it was it was a pretty you know folks really agree that it was a problem space that needed to be solved for sure and we ended up joining forces with the folks at Google on their deployment manager project and are in the midst of joining helm and deployment manager into a new project also called helm that lives at kubernetes slash helm and is now part of the CN CF so helm versus workflow right you know what why would you choose one versus the other well workflow is really focused on twelve factor applications we have a really streamlined experience for developers who are comfortable riding within those those boundaries of twelve factor there's also lots of non twelve factor apps and and not just you know the database is queues cashiers things like that but anything that needs to swing a volume around inside a cluster right Deus components itself right we have a data store so guess what we use home to install and manage Deus so I was actually part of the reason that we had for building it so demo time what I'm gonna show you here is we're gonna kind of run this demo in Reverse what I'm gonna do is I'm going to show you the running thing and we'll deploy some apps to it I'll show you the different workflows and then I'll uninstall it and show you guys a little bit more account if we have some time so what are we looking at here right so what this is is a coop CTL get of all the pods services replication controllers in the Deus namespace now this isn't a talk on kubernetes so if you don't know what services pause and replication controllers are you're gonna have to pretend you do for a minute because that talks a lot longer but what you can see here are the running containers essentially for you know our source image builder our controller which is our API surface a database which backs the controller happens to be Postgres Mineo which is a really cool go based object storage system the registry is just pretty much an off-the-shelf docker registry our router which is an engine X router mesh powered by kubernetes annotations and and and some interesting ways and a project called workflow manager which is for helping with the upgrade process and versioning and assistance like that we also show the services and the other internal cluster IPs and sort of the desired and current state of the different bits here so what I'll do is I'm gonna jump into an example app directory so this is a little bigger this is an example go app and what this is gonna print is where's the line here powered by some variable which is going to be essentially an environment variable or the strain days so here's here's how this works for developers if I do a deus Who am I you can see with the CLI I'm currently logged into a deus cluster at deus gabart Evita IO so I can type in Deus create go is the name of my app and it's going to add a git remote and I'm gonna do get push dais master and what this is gonna do is this is gonna send the get contents of the get you know tree into the server side that's going to take the risk it receive pack it's going to unpack it it's gonna run it through a build pack which is going to spit out a slug which is 2.2 megabytes and then it's going to upload that thing to object storage and then it's going to you know basically create a docker container that it can inject you know that's gonna be able to run that slug and then it's going to orchestrate running a single instance of that container which is come to the default behavior now that I will happen pretty fast there I mean I don't know how fast that was but if we curl go gabber T V dot IO which is my cluster name we can see powered by Deus on release v2 now what actually happened there this is a command coop CTL get pause services and replication controllers for the namespace go so what happened was when I hate Deus create go behind the scenes we created a kubernetes namespace for the application because that's best practice it is to have logical separation for your application and by namespace and we also created a service that 10.30 to 1:30 exposing port 84 for this HTTP based service we assume services the point of view dais work for HTTP based stateless 12 factor services so we can make that assumption and you know now that we've deployed this thing we actually have a replication controller which is version 2 of the app we're asking for one of these and you can see up at the top of one running pod with the readiness checks passing so everything's working and this is what sort of it looks like natively inside kubernetes now if I want to scale this out and I'm a developer I think in terms of this Deus scale web equals four I don't have to write a manifest and resubmit the manifests home thank you good now what happened was in four seconds we that command returned and we now have four running versions of this app all of which are passing health checks and everything is fine and dandy so now what happens when you know this is very default example what happens when we want to change some configuration well we use this config set command to pass environment variables which are how we handle configuration inside of twelve factor so what we can see here if we flip back over to this is we're going to start rolling out Version three of this web application and it's not until the new versions are in service that we can actually remove the old version otherwise it's not a zero downtime deployment right so you can see here we are well it happened already so that was four four containers were replaced fairly quickly there if we curl this again because we are reading from that environment variable you can see we're powered by kubernetes if I hit it a couple times you can see that pod IDs are you know changing we're balancing you know across the different across the different of different pods there we're also managing this release ledger right dais releases allow you know as a sort of monotonically incrementing ledger of different changes made to the application so you can know who on your team did what and when and this is extremely useful in the world of you know platforms who you know the fact that I deployed a given Shaw at a certain point is is really nice from a standpoint of a dev environment so I'm going to quickly show you some other examples here so we're gonna do a day's create on a docker file application this particular application has a docker file that looks like this from Alpine Linux powered by de is sort of the same kind of deal so let's get pushed this thing I apologize there's a little bit of a debug output in this this is actually a beta release at the at the moment but what this is going to do is this is going to build the application not using a build pack like the first example but it's going to build it using a docker file and while this is going I want to touch you a little bit about philosophy we have with regard to the build stage of these applications that we like to refer to as crawl walk run so oftentimes developers who are approaching this stuff don't want to learn how to build an application they just want the damn thing to work and so bill packs are great for that because as long as you follow convention about how you lay out the files on your filesystem inside of the repo bill packs just work and you know you could just make everything just fly right now what will happen inevitably is your need Lib xml version something that's not in the base image of you know what the build pack is using or you're gonna need to customize the user land execution environment in in some way that build packs don't don't work with that's the point at which it's probably gonna make sense for you to start writing docker files from scratch right and so you take a look at the stalker file you know this is how you're gonna get that custom version of lib XSLT or whatever it is that you need it into into your application now note that with this model though you're still get pushing into the cluster right if the developer is you know kind of diving on their laptop it's just kind of a Deb in the cloud sort of environment where you view dev on your laptop you get push you know you see the thing running in the cloud attached to all the cloud based services that you need to attach to now a certain point it's gonna make a lot more sense rather than having someone get push this thing into the cluster every single time and have it rebuild an image every single time you're actually going to have CI your your CI system start building some of these images right and that's you know so there's so bill packs are crawl and this is walk the docker image workflow is is what is run so what I'll show you here is we will de is create you know some random app namespace and I read out because I didn't it's not to get directory but Wilder a tabletop and so what I'm gonna do is I'm going to go to the do a days pull Deus slash example go calling latest for the app while they're tabletop and what this is doing is this is actually taking a pre-built docker image could have been built on a CI this one happens to reside on docker hub and what it did is it promoted that directly into the Applications space here so if I curl Wilder tabletop like every TV I know you can see powered by days which is just some docker file and this really makes for a really nice transition for folks who are looking to get up and running in this in this new world but for whatever reason don't want to learn docker they don't want to learn how to do this stuff and and and and really solving this problem for operators of okay I stood up this big cluster now like how do I let my developers use it without having to you know give them four weeks of training like they have to do inside of Google right cool so that's that any questions about well you know what I'll save questions for the end let me just show you quickly how we uninstall and reinstall this platform using how I'm while I'm at the commands terminal so I'm gonna helm uninstall I should assure you a helm search first so by typing in helm search if they come up there's two update charts you know this currently uses the like a brew model backed by github repos I have if I do a helm repo list you can see that I have two different repos the canonical helm charts which is maintained by the community has lots of different stuff in it and then I have another read book called the day a slash arts repo charts are the unit of like you call like a template for kubernetes application or workload if I do a helm search I can just get a list of every single thing you'll notice that the day a slash stuff is the day and things in the days charts repo there's a little bit of name spacing going on there but there's all this stuff available like you know people added spark and storm and different things that run on top of kubernetes quite quite interesting but the day's workflow - beta 2 is is what I'm currently using so if I do a helm uninstall workflow - beta 1 - end day is how you have to specify the namespace you want to put the things into or uninstall from I can uninstall all the objects and we flip back over to the view of the dais namespace what you'll start to see in a minute here is everything pop out of existence we begin by deleting some services so you'll see the services start to slowly disappear but this is also the inverse of this is how we stand the platform up and once everything passes the readiness checks and the role of the liveness probes inside of kubernetes you're up and running so really if you have a kubernetes cluster you can go from kubernetes to full-blown you know days power days workflow powered platform is service in Oh about two minutes which is a pretty compelling story granted standing up the kubernetes cluster might take you a little longer if you're not on something like Google's container engine but you know it's still a breeze compared to a lot of the other ways it takes to set up a platform so great well that was what I want to show on the demo side in terms of the the this is by the way a beta that I was showing you this beta one we just released it on March 25th we have these two-week Sprint's that we're doing operator UX stability and monitoring each are gonna produce a beta out of those and we're gonna have a release candidate targeted for May 25th and stable should come out and on June 8th going forward one of the reasons why we're fairly confident these numbers is the beta that you've seen is actually quite stable in large part due to the fact that kubernetes itself is just such a rock-solid engine that this is probably at this point an order of magnitude more stable than what we had in the in the version one which is powered by Fleet and Etsy directly and also the beta one is feature complete or mostly feature complete so there's very little that we're adding the way new features it's all around sort of stability and operator UX so after that a couple things we're working on one is add a stock tour easy way to submit issues if you are having trouble we found it's you know we're very focused on sort of community engagement and making sure that the open source community and people who want to use this for free can engage with us really well so we want to build some tooling to a lot of people to report issues that's what day it's doctor is we do have a number of very large customers and for those folks getting more granular control over team's permissions authentication bridging that with a enterprise authentication authorization story is extremely important so we've done some work there and a service broker I'm actually a big fan of the Cloud Foundry service broker api's and so what we're doing is we're taking those api's and we're applying them to the world of kubernetes because a lot of people don't want to run everything inside the cluster manager right they don't want their databases necessarily running inside the cluster maybe if you're on Amazon you might want to take use of you know similar or on Google for that matter you might wanna take use of bigquery right which is not something you're going to run inside of kubernetes right most likely so being to bridge not just the services that are non 12 factor that reside in your cluster but more importantly bridge to services that are provided by a cloud provider and connect those two worlds between you know the the workflow powered apps and the ones you know that are provided by a cloud provider extremely important and that's some of the work we're doing with service broker past that private registry support is really important more we can do there and we've gotten a lot of demand for private non-routable applications you can do this kind of behind the scenes today but we can make this experience smoother and then you know tracking the kubernetes api is one of the things that is sort of different about our approach versus some of the other folks who operate in the kubernetes spaces we wait for features to fully baked inside kubernetes before we adopt them and because of that we you know there's always new stuff getting rolled into kubernetes because it's improving so fast so we have to as a you know open source software team we have to put in cycles to like get up to speed with the latest best practices in kubernetes and the way we look at that is we're really doing that on behalf of our users you know are these you know DevOps folks inside of typically large enterprise so with that I want to mention that we do have a open planning process that I'm actually quite proud of it's something we've been doing for many many months we have typically we release the first Tuesday of every month and then the first Thursday of every month we have a community meeting you know online a face-to-face sort of meeting and talk openly about what the roadmap where we're going and you know solicit feedback from the community so we don't take decisions in private where we can avoid it so that's that any questions first person I ask a question gets a gets a t-shirt a designer I feel like you have to have some incentive here for audience that's quiet yeah good so the question was does having a cessation get on the server introduce a security problem you know it it you you know it's I think the bigger security problem is building images on Cluster generally requires access to you know mount overlay file systems and things like that so the containers have to run privileged the actual code for the SSH server is all custom go based SSH you know we don't actually run like you know OpenSSH that sort of thing so it's the the SSH server itself is pretty you know any like any other controlled user land container the mounting of you know the docker socket there's a way you can build in cluster but but our building cluster without having to mount the docker socket but you know it's a little tricky yeah yeah all the stuff runs and containers anyway so it's secure right by default right yes pros and cons of days versus cloud family that's a interesting one so you know I think that our I would say first the first thing I would say is is a difference in philosophical approach so with DES it's what we're trying to do is we're trying to take a cluster manager like kubernetes and add a platform layer to it right or did lots of different things to it Cloud Foundry is a big stack of stuff that does everything from VM management all the way you know up through what what I'm showing you here at the platform layer I actually have a lot of respect for the folks who originally built cloud founder because that's a really difficult thing to do to have all of that overhead you know to have to manage all that everything from the scheduler to you know the routing and your VM management all that stuff we have taken an approach that says you know we're you know as technology matures it tends to mature from the ground up right so what you saw was like the eye as layer starting over time to ossify and get more stable and then there was this other layer called the cluster manager layer which who knew existed so it's things like Amazo sand and kubernetes and things like Diego inside of inside of claw foundry we're starting to see that layer now ossify and so what we're trying to do is try to focus as much as we can on the platform experience and less on the underlying plumbing and let kind of that get solved by this sort of standardization and ossification being spearheaded by the scenes yeah yeah so multi-tenancy is a good question you know it's what we found in the field is that a lot of people have a different definition of multi-tenancy some think of it as like you know why this is a developer self-service right so each developer is their own tenants on the system and so we want isolation between tenants some people think about tenants as divisions inside of a company other people think about tenants as like different untrusted actors like you might find in a public past environment so it depends on your definition of multi-tenancy in terms of the security concerns there they've generally fall into two categories one is at the runtime level so like sort of the docker container and sort of the OS execution layer we feel like with things like set comp you know being added to the to the docker engine and you know Cisco filtering and sort of the general container security stuff that the runtime level is getting pretty buttoned-up image management and what are you running in terms of the code that's a much trickier problem but if you have a controlled build pipeline and a controlled build process you can mitigate that pretty pretty effectively now the other side of this is the networking side and in the networking side is you know pretty complicated at this point there are a ton of vendors in the space I just came here from a meeting with the good folks that we've works where this was basically the entire topic of conversation is how can we as a community better solve what's referred to in the industry is micro segmentation or the ability to separate container networking based on policy lots of other folks in the space tackling this as well the Signet working group SIG's are special interest groups inside of the kubernetes commune that tackle different problems they recently came out with a proposal for how to do use labeling to enforce fine grading network access controls so what I would say is networking is an unsolved problem as of this minute in terms of network security inside of kubernetes cluster so separating on tenants you know either you trust them or you don't it is is the is what I would say going forward I expect that to be solved and I do not expect the runtime component to be a blocker there are other questions no all right great well I'll be around for a little bit so if you have a question and in your little shy I'm feel free to find me afterward otherwise thanks everyone for coming 