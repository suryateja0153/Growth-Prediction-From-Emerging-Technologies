 hello everyone and welcome to our webinar presented by bold radius and Mesa sphere on unlocking your data with Mesa sphere on behalf of both of our teams we would like to thank you for taking the time out of your very busy schedules to join us today you are in listen-only mode so if you have any questions throughout the webinar today please type them into the chat box at the bottom of your webinar control panel and we will answer as many of them as we can at the end of the webinar now I would like to introduce our presenters Aaron Williams head of advocacy for Mesa sphere and Jean Glover trainer and consultant at bowled radius I will turn it over to you both so this is Shawn speaking so welcome to our webinar on on mezzos and mesosphere and the data center operating system I want to give you a quick outline of what we're going to cover in this in this webinar so we're going to give a brief introduction to what Apache Meadows is and and what the company mesosphere does with mezzos and how it enriches the ecosystem of you meadows we're going to talk about how mesosphere can help alleviate the burden of setting up infrastructure for a big data platform adoption will also see how leveraging mesosphere is data center operating system and the nfinity product can help us create a data platform and finally we're gonna wrap up with a demo application that makes makes use of the data center operating system also known as DCOs and it's it's a pretty cool application it's using crime data from Chicago municipality and a whole suite of new technologies to demonstrate the use of DCOs so my name is shawn glover i'm a trainer and consultant here at bold radius I've been working with Scala and the light bend reactant platform for a number of years now I'd like to give Aaron the chance to introduce himself to Aaron yeah hey yeah thanks Sean so I'm Aaron Williams I head up our advocacy efforts at Mesa sphere so part of that is working with developers to drive adoption of both mesosphere and the open source packages that are part of Mesa spheres of mesas etc and part of that also is doing outreach the open source community great so let's move on to the next slide and talk about what what exactly Apache missives is for those of you don't know so I think it's best described by a quote by when the co-creators Ben Heineman where he said we want people to be able to program for the data center just like they would program for their laptop so what does this mean so as companies have embraced clustered technologies there is a growing need to manage for that cluster effectively the good analogy is on your personal computer you take a lot of this for granted your operating system manages how to effectively run processes to make the most of the resources you have available such as CPU cores or or memory so mezzos is a an open source Apache project that enables you to manage your clustered resources more effectively it allows you to allocate all the resources of a cluster as if it were one big server in your data center so all the resources and then you have available in one server CPU cores memory hard disk network ports you can pool all of these together and slice and dice them traditionally enterprises may create one cluster per technology that's pretty common you might have like one the new cluster with with yarn running on it with HDFS you might have a web server cluster you may have a caching cluster that supports your web server cluster and etc so so mezzos allows you to slice and dice all the resources of a cluster to maximize the use of each server and we'll go into an example in the next slide how you can see that mezzos itself is a is a cluster technology and it has been scaled to thousands of nodes it tolerant so if one server goes down another will take its place just like many other clustered technologies you're probably familiar with it's been used by organizations all around the world and just to name a few there's a lot of household brands that use it but just to name a few there's Apple and Netflix as well as CERN laboratory have all made use of of mess-ups and I just want to give hand it over to Aaron quickly to talk about how it's easy to try new technologies on vessels yeah that's that's one of the most interesting things so you know you mentioned how you could take these applications and sort of slice and dice your your data center up based on the different kinds of workloads you have and the reality is data centers are becoming more diverse in terms of the kinds of workloads the kinds of applications that they have running on top of them and so what you really need is an infrastructure that gives you the flexibility and the confidence to be able to try out new technologies to be able to stay on top of whatever the newest trends are without necessarily having to go and read architect things within the data center and at the data center layer and that's one of the real benefits of Mesa else once you have your entire Dennis data center and the the resources of your data center abstracted into one big pool it makes it easy for you to be able to go and try these new technologies and try new things and stay on top and and maybe Shawn the next slide you can talk a little bit about how when you have that diversity of workload how this elastic sharing really makes a big difference sure yeah this this is a good example I think on this on this slide is graphic in particular about how what the difference can looks like between a traditional model and and what mezzos can can offer so like I said before we can traditionally use statically allocated means and that that requires a lot of resources whether they're virtual machines or whole physical servers so mezzos provides fine-grained resource and access control by using Linux containers containers if you're not aware are are kind of like super lightweight virtual machines that run by that run on the platforms offerings said they don't have as much overhead as a as a full virtual machine they're a lot easier they're a lot faster to spin up and and take down and provision than a VM containers Linux containers are what allowed mezzos to isolate the task from other running tasks that happen to be running on the same node you can contain an individual task to run in a limited resource environment so if a resource if a task is is a hog with the the resources on that server you can limit what it can actually use so that there's room for other tasks on your server to make use of those resources too and obviously you can you can provision how many cores how much memory what ports each task has access to and you can do all of that programmatically and finally you can you can run your tasks in a prepackaged filesystem image allowing it to run in a different environment so for example you might have a task that has some kind of system dependency that you don't want other tasks to have access to you can easily create that kind of isolation with a container so if any of that sounds familiar to you you may have heard of it in reference to a really popular technology these days called so darker just like mezzos is a is a container technology so mezzos ships with its own container technology just felt the default mezzos container but you can also use docker to provision actual docker apps and and other images using using that technology as well so what this all brings what this all gives you is biased isolating running tasks in containers it's possible to run multiple tasks on one server so that you can use all the resources for doing that all the resources on that one node for doing more than one thing so actually the actual example that I have on the screen here in the first row each box represents a physical server and if we look at web we can we can say oh my my web servers generally are busier during peak times like the middle of the day there's more what West then and there's fewer in the evening my caching cluster has a pretty consistent look to its performance requirements and it's it only uses a small portion of the physical servers resources but it uses them fairly consistently and then maybe you have something like I do which has and the inverse characteristics usage characteristics to your web server so could you might be doing a lot of batch analysis in the middle of the night whereas what I was doing it serving a class during the day so maybe these two things are a good fit to put together on the same server so on the second row of this graphic we're trying to visualize how you could maybe pick these three services and they wouldn't work they would run very well together as as mezzos tasks and because they only use limited resources of the server you can run them all on on one server so where you had three servers before you could potentially just have the one and then have two more servers that you can either take out of your cluster or use for something else so on the next slide Aaron's gonna discuss a bit more about what mesosphere are into the table yeah sure so so um I think the the conversation about the diversity of these different kinds of applications different kinds of frameworks is really important and what we see with these modern applications is this need or this desire to create microservices put them in containers so that they're easy to manage that creates sort of the functions and logic of your application and then also hook that up to this big data and analytics make it possible for you to do the streaming and search and the time series across that data and connect all this together to make a modern application now of course the the the famous quote is that all companies are becoming software companies or the actual quote is how companies are software companies and they're competing against each other based on the quality of their software based on the success and the the differentiation of these modern applications and so you know one of the things that's really important then is how do you actually take and create a foundation for those applications to run on that is solid that doesn't mean you're spending all of your time managing the resources of the data center and instead you're spending your time focused on building those great modern applications so we've talked a little bit about what ASOS does in terms of as a sort of kernel if you will within your data center how it manages those resources and then what we do what Mesa Square does is we create what we call the data center operating system with Mesa as the kernel but it includes all the other things you need to be able to make those modern apps work so great user interface great security and governance monitoring easy container orchestration with using different kinds of orchestrators so it's not even about picking one or picking another it's about how do you plug those different orchestrators in tomatoes to make your application work and so you know our goal is to be able to make that DC LS a solid foundation and that our customers can run and build these modern apps on top of it and when we do that we also obscure then the infrastructure at the very lowest level so whether you're running this in the cloud or on-premise um sort of a hybrid environment having the DC OS as your as your common foundation means that your apps can run literally anywhere and it also means that we end up with this ecosystem of people building on top of this operating system so you know we use that metaphor operating system very liberally because we think it fits we think that what we've really built here is something that is very similar to the operating system that you have on your laptop or the operating system that you're used to on a server it's about managing the resources of that physical piece of hardware and making it possible then for you to run all kinds of applications on top somebody you want to talk a little bit about where it goes from a data perspective from here yeah so let's let's narrow our focus a bit you didn't talk about data platform here so traditionally data ingestion is batch orientated there's there's lots of proprietary solutions out there that enterprises have used for data ingestion for warehousing data and analyze data unfortunately most of these solutions tend to be more on the batch end of the spectrum so bad batch definitely makes it easy to integrate systems it's it's kind of like the lowest common denominator for data transmission it's the easiest for is it's also easiest for many or large organizations and many data vendors to support batch because they can just support they can just implement a feature to export their data in this manner and then something else can pick up that file it can be transmitted by FTP or whatever and and be consumed by some other data source so batches batches okay but oftentimes you need you need that data faster it's acceptable in some situations but we see more and more companies focus not just on aggregation and analytics of the data but they want to do it as fast as possible sometimes data is time sensitive and the older the data the more stale the insights that we can derive from it become so a couple of examples ways we make use of streaming data are to do things like build a better user experience for our web app many companies rely on real-time data to satisfy these requirements of a rich end user experience so one example is a an ad platform that's analyzing real-time buying habits of users to deliver up-to-date and trending advertisements to even more users that's one example another would be predictive analytics or recommendation systems so if you're a user of Netflix and you've already seen in action you're presented with recommended shows and movies based on your own browsing habits as well as habits of users like you so how do we streamed it how do how do enterprise screen data how do they get this up and running in the last five years there's been a bit of a renaissance in terms of data technologies now available especially in terms of data streaming technologies so one way one way enterprises can can react quicker is to process data using shorter batch intervals or they can evaluate some of these new best-of-breed technologies so I'm thinking of technologies like spark apache spark apache storm flink is a new one that a lot of people talking about these days we can use these new technologies to process data in minutes or seconds or even close to to real time so mezzos like Aaron was saying earlier makes it really really easy to provision new technologies to try them out but also just you know the pain and actually setting up these these clusters is is a lot less because because we can reuse some of our infrastructure we have and specifically DCOs can can make it easier to provision those servers those technologies alright so Aaron's gonna discuss a little bit about what how how the implementing a new data platform can be a bit of a burden for the enterprise yeah and I I suspect most folks that have that are on the call that have have experienced this before can understand why the adoption of these Big Data technologies has has been such a challenge you know we get it we've been there trying to get these big data platforms integrated making sure that you're choosing the correct the correct platform the one that's going to be there for more than six months is going to be hot for more than six months being able to connect that into a broader or set of tools that you need to actually ingest do something interesting with that data being able to visualize that data that all of those pieces are so critical and so it's it's not just about how do you get one technology up and running it's really more about how do you get your entire data center up and running and focused on these big data challenges and that's I think really was so interesting and what you'll see in the demo we get to that point is how easy it is how easy it can be to connect those different pieces together and get them working so that you actually get the results you want relatively quickly the other thing I'll point out here is this goes back to my point before about how important it is to get companies focused on the things that are real differentiators as opposed to the things that require them to build new pieces of their data center this is a real stumbling block for data scientists they know that they want to be able to manage that data and manipulate that data but they don't want to spend all their time getting the things running in the data center that it takes to make that possible and so that's what I think the next big step is for us as an industry is how can we get data scientists all the tools they need so they can be successful while at the same time making sure that the devops guys aren't being run completely ragged getting something new up every every other day and and with that you know let's talk a little bit about sort of where DCOs and this infinity stack come in to play to help solve this big data problem and Shawn I'll just walk through some of these some of the benefits and then we can talk in a little bit more detail about each of these different frameworks so there's really a few a few things here we talked a little bit about DCOs being this kind of foundational piece it's what you need we think to be able to manage a data center at scale in production but then you know running on top of that should be the set of frameworks that are needed to be able to manage this big data and what that means from our perspective are these things at the top these purple circles so how do you get the events that you need turn those in two feeds that you can actually manage how do you get the analytics out of that in real time so you can see what's happening how do you then put that into storage and make it possible for you to have the reactive apps that are actually making those that you can actually make decisions on so it's not just about seeing what's happening but about doing something about it and so what we've done is taken what we think are a handful of the frameworks that are pretty standard now in the industry bundled them together wrap them up on top of DCOs to make it super easy to install super easy to to manage and call that infinity that's our solution for managing Big Data so it includes things like Kafka which is event processing and spark which is the big data processing Cassandra the database acha being able to actually take action so against all of those frameworks then or combining all those frameworks then gives you this complete solution we think for managing data and I know Shawn you wanted to say a little bit more specifically about those frameworks yeah that's right so being Infinity mesosphere infinity stack makes use of some of these best to breed data technologies and if you're somewhat up to date with big data platforms you probably heard most of the most of them already but what some of the things I wanted to call out here as you can probably see by the slide is that all of these services all of all of these technologies actually run on the JVM and as we all know the JVM is a fairly robust execution environment for any kind of program that compiles in the java bytecode including including scale applications and I just wanted to call out that three of these core DCOs technologies are written in Scala and before I continue I should probably take a bit of an aside here and if you're unfamiliar with that logo beside ACTA that's that's type that's light Ben's new logo so live Ben is type safe they just had a rebranding event the morning I believe so so that's what that logo is so I'll try to refer to typesafe as light ban from now on but they might might serve about a few times so these these technologies being JVM technologies a lot of them being scale and in fact make it a natural fit for light Bend formerly known as typesafe old radius mesosphere to partner up so a light Bend endorses spark they they do have engineers actually working on spark codebase and contributing back back to the open source project they provide training and support subscriptions to it as well and so is so is bold radius impact and then akka is part of the light Bend reactive platform and it is very well suited for building application logic for data platforms so let's take a bit of a deep dive into what we can do what what mesosphere affinity can do to help illuminate infrastructure as a burden to Big Data adoption so there's four points we're gonna cover I have them here at a high level and I'm going to drill into each one so one is all of these services are highly available in fault-tolerant and they work well in a clustered environment to a complete set of tools that can be used for interactive data discovery in real time stream processing is is available at your fingertips three it and this is a this is really key it makes makes life easier for ops people it makes provisioning super easy you can scale your services up and down as required DCOs mezzos and marathon they all kind of work together to give us that one cluster to manage rather than separate clusters per technology most most operational concerns are handled for us automatically such as starting up and shutting down services as well as how we react to failures and how we recover from them for we get all of this capability on-premise or using a group platform like AWS I'm going to dive into each one of these points of detail so the first one is highly available in fault-tolerant services so a big advantage of clustered and decentralized platforms is that it makes it easier to update our platform in in-flight and with ever than any downtime so we can add extra processing capability at runtime and we can we can do that by using popular deployment methodologies that we read about in continuous delivery literature and continuous integration literature like Bluegreen deployments they can easily be employed to ensure production availability of your of your infrastructure each each underlying technology in fact in the in the infinity stack has its own high availability and fault tolerant characteristics so so an example a couple examples our CAPA has you know it's master and slave analogy of leaders and followers used to using Cooper doesn't anymore it has its own leader election process but it's it's robust it has per partition and replication settings and and all sorts of other characteristics to spark partitions it's data across different nodes so it processes data on different nodes it also spreads the data across those nodes and lives in the memory of them for quick access it has a feature known as lineage so if one node goes down and it was containing some of that complete data set spark has smarts to recalculate that data set based on the source data and then of course akka has a whole suite of clustering strategies that you can make use of to all of these services can be backed by fault tolerant storage so HDFS for example Hadoop distributed file system can be used and is used by all of these services and you can use HDFS or cell if you have a need to mr. data to it or or for whatever reason you want to use HDFS you can use it as a service as well and this allows for improved safety availability and durability in the event of a failure and finally you can have individual policies on how each of these services we have two failure so you can configure the policies to react to how how you mediate from failed server events to improve your availability the second point was the complete toolkit for data discovery so this this slide shows a deal pipeline using some of the DCOs services at a very high level I want to break in I want to break down in break it down into three components so number one is ingesting data so ingesting data in a manner that's recoverable in the event of a failure is the first step in creating a robust streaming platform and in DCOs terms we're using catkin for that calf has been the de facto leader in terms of handling large data volumes while maintaining core concepts of a cute so it has very good it supports pretty typical to message order principles first-in first-out as well as very strong message delivery semantics so at least once or twice once or or the hypothetical exactly once message semantics Kafka's tagline is is a shock absorber for your internal infrastructure so it's a it's a really good tech to have right at the beginning of your data pipeline because it can handle that load from from just about anything you can throw at it so a good example is Internet of Things right we're seeing we're seeing a lot of companies generating more data than ever especially with Internet of Things so things like your your SmartWatch are throwing off events and at a greater number than ever seen before in Kafka can help absorb some of that load so you don't you know overwhelm other parts of your infrastructure behind it captain was originally developed by engineers at LinkedIn so they could satisfy their page events analytics platform and what that was is basically every every user event that happened on the the page on the LinkedIn page with generate an event so you can imagine with LinkedIn being a massive social website how many interactions they have per per minute or per second so they develop Kafka to respond to this need to have something consume data at that scale so after we ingest data we need to process it somehow so the data gets ingested by Kafka it's seeing that it's in the queue topic there and then we then we asked art to start ingesting that data at a reasonable rate or rate that it's comfortable with and then we can do all sorts of things to do analysis that that were traditionally done in batch in a batch sense so spark was originally developed to improve performance of big data batch analysis things traditionally done by hadoop mapreduce spark has a really nice Scala API and it's it's model afternoon ask Alec collections API so if you familiar with Scalia you would get up and running with spark pretty easily it also has a spark SQL dialect that's proving to be a more popular API for for the technology SPARC can process the data immediately and then put it into what we call materialized views and we can use these for further analytical queries by spark down the data processing pipeline or we can connect workbook products like Zeppelin directly to them and then do ad hoc queries to to learn new insights so a Zeppelin can connect cluster and you can see that maybe your data that you're currently streaming to see how healthy it is you can use Zeppelin to create dashboards and really any kind of on any other operational insight you might you might need you can create through Zeppelin and connect it to spark and then one of the last stages in your pipeline would be probably to persist this somewhere to a fault-tolerant idea store like Cassandra or maybe you'd maybe you'd send it back to Kafka and then Sparkle pick it up again to reprocess and rich that data further or perhaps your your AK application will consume it from from Cassandra on a on as-needed basis so that's that's see that's a toolkit for do discovery that you have at your disposal so next we're gonna I'm going to briefly talk about provisioning so it's super easy to add or remove services with DCOs I'm not going to belabor this point because you can see it in the demo but basically it makes DC us makes it super easy to build a loosely coupled architecture and you can you can adapt and utilize new services and frameworks as they emerge or as they fall out of favor you can try new things really easy you can monitor performance of your data pipeline and scale as required and you can you can remove the burden of managing where your data resides by using persistent storage volumes like HDFS user administration is a is a big op so mesosphere has really good ways to support your data governance requirements by authorizing people who are giving access to people who should have access to certain parts of your infrastructure and finally the last last point that we're going to dive into is you get the same experience on-premises or in cloud so we can run DC OS on single developer laptop to a thousands of machine and we can easily we can easily use a public cloud platform like a sure AWS or or Google or you can you can choose to keep your solution in helps an on-prem you can enjoy the same consistent user experience if you wanna maybe start on Prem and migrate your workloads to the cloud it really doesn't make much of a difference you can even write your data intelligently based on content so if you have fairly sensitive information you may choose and you may may be mandated to process it on-premise with with all your own security requirements so you can do that and you can also leverage the cloud for processing less sensitive data and eCos lets you lets you do both so that wraps up the most of the deck so time for the fun part I'm gonna pass it over to Aaron and he's going to go through a demo of DCOs and we're gonna provision something from scratch it's gonna be amazing that's a high bar but it'll be it'll be fun that's for sure so yeah let's let's let's actually show you how it's done so we've got about 15 minutes or so so I don't know if I'll be able to get through the entire demo but we'll do I think the most interesting pieces so getting a few of these frameworks up and running getting the apps up and running so you can see what that looks like and how DCOs handles them we may or may not actually be able to get to the point where we start a data producer but but let's see how far we go and certainly the important thing is that this demo that we're going to show I'll just pop up the URLs is available for you to try on your own as well so I'm gonna walk you through the starter that can kind of point out a few of the more interesting features but I encourage you to go and try it for yourself and if you have questions or if you're curious about drilling into any piece of it certainly reach out to me these slides as well as the recording of this webinar will be available afterwards as well so you don't have to necessarily feverishly write down these your we'll make them available or you can only send me an email Aaron Williams at mesosphere dot IO and I'll be happy to point you in the right direction okay so back to the demo this demo is based off of a pile of data that we found from the Chicago Police they provide a data dump of all of the crimes that happen in the Chicago area and we took about six months of those of that crime data put it into a flat file and then ran it through a data processing pipeline so we can see what it looked like and so just starting at the left I'm gonna walk you through what these pieces are and then I'll jump in and show you how we're actually doing it so first step is on the far left there the crime day producer that's a piece of software that we wrote that takes the flat file and turns it into basically a stream of events so this is basically giving us the opportunity to mimic real time excuse-me data but you could imagine you know that being replaced by your favorites IOT pipe or whatever data stream that that you have we did it with a flat file just so it was a little bit more predictable what we're gonna do is feed that data into Kafka as Sean said earlier that's really sort of the shock absorber so it does a good job of queuing up all those events and making them available at a at a reasonable pace that'll get fed that'll feed the data then into spark which was which does the actual data processing and spits it out into two directions one is what we're calling the online visualization this will show you that data coming in in real time so you can see those crimes sort of as a function of time temporally you can see how that crime is happening and then on the bottom side we're going to pump spark is going to pump that data out into an offline database where we can we can put that data over top of a map and see the visualization of it in terms of location and so that will give us a really interesting heat map of where the crime is occurring within the city all of this is running on top of the data center operating system DCOs and so this whole demo is basically us showing how easy it is to get this kind of a data processing pipeline up and running with DC LS all right so I finally get to leave slides first things first I've gone ahead and provisioned a cluster inside a sure I use Azure primarily because we've been working with Microsoft a lot they recently announced their Azure container service which is based on DC OS so I'm not using them out of any particular any particular need though the reality is DCOs runs on your favorite cloud or on-premise oh you could run this literally anywhere you can see the summary of the cluster that I've created it has a single master and it has a handful of agent nodes which are running here one of them is a public agent the rest are private and the master also has a public facing piece to it as well if I click into the master node you can see this is the master IP address will note that down because we'll need that to actually access the DCOs GUI and if we go back and look at the public agent the public agent also has a public IP address will note that down as well because that's how we'll access the different applications that we get up and running inside the cluster and that's really all we need from the cluster we got the cluster itself up and running and provisioned we've noted those two IP addresses and now let's jump into the actual DCOs itself what I've done is you know I've pointed my browser to the public IP address for the master that brings up this DCOs GUI using this DCs movie then I can see everything that's happening inside my data center and I can see what maysa and what the technologies are what DCOs is doing to actually get my applications running within that cluster when you first come to DCOs you'll see you get this sort of welcome message and it gives you a command here that you can copy and paste into your terminal to get the line interface installed that's one of the really powerful things about DCOs we offer both interfaces so you can choose to come in and see what's happening in sort of the visual GUI or you can certainly go in and hack through it using the CLI if that's your preference so that's another thing that I've done ahead of time I went ahead and installed the CLI when I type TC LS now that gives me my DCOs commands so that's all installed and running DCOs itself was installed and running you can see right now there's nothing happening inside my clusters so no surprise there there's nothing nothing running no tasks no allocation I can go in and actually see the nodes though I told you this is a seven node cluster so there's my seven nodes plus the one public node I can see that there's nothing happening okay no surprise there that's kind of the that's kind of the good news that's what we expected okay so let's do the first step if I if I think back to the diagram that I showed the first service that we're going to need is Coffman so let's get caught up and running it's really easy it's a single command within the COI it's DCOs package install cockta so what this is gonna do is DCOs is going to confirm that dcs is going to download Kafka from our repository of services it's going to install Kafka into DCOs get it running within the cluster and then make the caca service available for me to actually use you'll see in the the CLI here that it's not just installing the application it's also installing a sub command within the CLI so that I can manage Kafka from within the CLI so that's one of the really interesting things I think about DCOs when you install these services and frameworks you're not just installing the code that actually runs you're actually extending TCOs itself to make it easy to use these different use different services now if I come back to the dashboard look at that I've got one task running you can see now I'm actually starting to use some CPU because caca is up and running if I jump into the services now you'll see Kafka and marathon here I'm gonna note down actually I don't know just yet but I'm going to go take a look at this inside marathon because marathon is manages all of our tasks you'll see I've got Kaka and you'll see the status is running so that's great news and finally I'll just go look at the notes so you can actually see we've got a little bit of CPU usage now thanks to having Kafka up and running okay that was easy let's get some let's go our first broker started though because in order to use Kafka you actually have to have a broker running so the first thing to do is to add a broker again you'll see I'm using the sub command for conf NL within D cos that starts that creates my broker and now I'm gonna start it and that will actually get the broker up and running once it actually gets running it's gonna give me the end point for that broker I'm going to note that down because we're going to use that end point later when we create our crime data producer will point the data producer to actually pump data to that end point so you'll see when it actually starts oops when it starts things it gives me this end point so I'll just note that down real quick and like I said I don't know if we'll actually have time to get to that point but just in case ok great so now if I come back here you'll see I've got Kafka actually up and running it's now posting you from idle because I actually have a broker running for Kafka you'll see it it's now consuming some resources because brokers actually do stuff so that's great news now the next piece of the of the demo is sparked so let's do the same thing we did before package spark permit again this is doing exactly the same thing so now and you know it's downloading the package for spark it's getting it up and running inside DCOs it's installing the sub command for it so that we have the easy ability to manage it so that's great moves I'm gonna come back over here to take a look at these services again and if I go in to marathon and I open up my service UI for marathon now you will see that it's deploying spark within marathon now deploying spark actually takes a couple of minutes so I thought maybe you know Sean you and I can ex provide a little bit of color to what's actually happening here because I know we've both tried in the past to get spark up and running within clusters and the challenge of doing that the challenge of getting spark running is sometimes it can be a real challenge I can be difficult to get it running in a production cluster and you know I think we want to really highlight the fact that with DCOs getting spark running is as simple as a single command line as opposed to all the things you have to do to make it work if you just want to run it in a standalone cluster yeah yeah for sure like my my experience is obviously you have a few different options for setting up a cluster for spark you got the standalone you have you have mezzos obviously and you have yarn and just you know I'm setting up setting up something like yarn for example which is part of the Hadoop stack is is no trivial task and in many cases you don't set up you might use something like Hortonworks or so their image to do that but it was generally have like a ton of extra stuff that that you don't need or even even want really and I think the one one the biggest benefits of DCOs is just just the fact that it's one line so if you did want to set up new servers yourself he could use provisioning tools like chef or cancel but you'd have like these big play books to set up all the system dependencies to run these services and they have a lot of system dependencies so those those like that one DCOs line the setup spark replaces a whole set of playbooks ants will play books or chef recipes now we're doing it at the point well let's give it back but but I think that's a really important point you know with with most most of the times I've gone through that pain it's it's been about going into specific SS aging into specific boxes to get specific services running within those boxes and I think you know one of the real benefits here is this is a hands-off approach to actually getting these services and getting these frameworks up and running you tell DC LS that you want it up and running and you let DCOs do the hard work of figuring out which actual node it should be running on how to combine those different applications across those different nodes to optimize the experience so let's go back to here and yeah it looks like SPARC is run so you just open this back up one more time make sure that we're done deploy yeah so SPARC is running so that's great so there it was one command I don't know what that was about two minutes to get sparked okay so now we have the first two pieces of that demo already already move on so we've got Kafka and we've got spark if I can just jump back real quick to the picture just because the next piece is we we've chosen two applications to run on the online side that's the top side of the demo it's a database called influx DV and it's a visualizer called Griffin ax so what I'm going to do is I'm going to get those two applications running now inside marathon those two are containerized applications that that will run within within DCOs now you know why did we choose these two well it shows sort of a plethora of the different kinds of applications you can run so even while you have that standardized big data stack of Kafka and SPARC then you can choose whichever these specific tools are that your app developers or your big data scientists might be most comfortable with for actually visualizing and manipulating that data so next step I'm going to go into our I'm gonna go into our have a separate repo here with the time series demo so this is this is straight from the github repo so you can feel free to go out and download this and install it yourself within the online within the online folder you'll see we've got two JSON files these two JSON files are basically just the descriptions of those two applications the containerized descriptions of them so that when we pass this off to marathon it knows how to download and run and download and install and run these two particular applications so I'm gonna come back here I'm gonna do DCOs and we'll start with the influx DB so what this is going to do is tell marathon to add this app based on this particular JSON file okay great so now that app is installed I'm gonna come back here to the DC TOS UI and you can see that now in Flex DB is deploying within marathon giving us a instance of that application I'm gonna click through in the specific interface for the influx DB app and you can see let me make it a little bit bigger you can see I have two ports down here at the bottom that makes it worse these two ports one is the public port one is the private API port the public port is the actual public interface GUI for this particular application so if I go to the public agent server's IP address and I use that port number I will be able to see the GUI for in flux DB and the private port is used when I want to connect the DB up to other services that are running within the system so when I connect the influx DB to Tofana for instance to be able to see that that user that particular visualization of it I'll use the private pour so note those two ports down because we'll we'll use them in just a second 165 61 and 165 6-2 great now open up a browser and I'll go to the public agent IP address which was 41 to 82 for 0.189 and the public eye public port number one six six one and there we go there's our influx DB GUI this is being served to us from the app itself I'm gonna set these port to the private port number just so it knows where to expect the API calls once I save that now I've got my database all set up and ready to run I'll quickly create a database we'll call it now you can see if I click down over here you'll see I've got a TS demo database created inside a flux DB so it's ready for the data now from spark basically so there again simple command to do the installation of the app simple amount of configuration using the GUI designed to be something that's easy to get up and run I'm going to go back here and I'm gonna now actually add the other app that I have which is fauna gravano does exactly the same thing so it's going to start a new app inside marathon you'll see it's now deploying inside marathon I'm gonna go in and get the IP address for Griffin ax 6 3 5 4 and then if I go back here again same public agent IP address Newport 6 3 5 4 and I better just make sure it's actually running first ok it's running now you'll see I get the interface for we're fine so Ravana so no surprises there now now this is where I'll actually Cree my my front-end interfaces this is how I'll actually visualize that data then coming from the coming from the influx DB now unfortunately we're coming to the end of our time this this tends to happen because the demo itself is a fairly in-depth demo the good news is you have the ability to go in and as I said play with this for yourself so I think the next two steps are just to connect these the DB and graph on to each other so that the so that Cortana knows which table for instance within the DB it's going to be doing its visualization on and then to get the crime data producer actually up and running so with Big Data through this pipeline those two steps are relatively straightforward but I'm not going to go into them now just because I want to make sure we leave time for at least a couple of questions so what I'm gonna do instead is I'm gonna jump to a quick screenshot of what the visualization looks like as I said getting from where we've stopped to this is is not a big step it's it's literally you know a handful of additional commands and getting those apps sort of connected to each other but what you get then is the really nice visualization of the data coming in in real time you can split it out across different types of date of crimes so in this case we've chosen thefts and assaults and we show those then on a graph in in real time so Griffin ah like a citizen is one example of the kind of tool you can use for doing excuse me visualization of course you could use whatever your favorite visualization tool was if you wanted to replace it with something else that would be that would be very easy to do and then the other side of the demo is taking that same data from spark and pumping it into a visualizer that will overlay the longitude and latitude of that data onto a map and that's also part of that demo that you can go and and see for yourself explicitly how we've done that I think this if you think about it in terms of what we were trying to do with this presentation it was focused mostly on being able to give you kind of the 101 intro these kinds of things this actual data manipulation is maybe a step further but I'm happy to go into it offline if anybody has any questions or if anybody wants to go into it in more detail so with that I'm gonna actually even skip this kind of ramp up slide in the interest of time I'm gonna open it up to questions and at the same time I'm gonna put the resources back up there just so that everybody has them Seana of you have the chat screen open if anyone has any questions source oh yeah there's been a couple it's been a quite a few good questions actually one maybe one a good one that go over with is it mesosphere capable of requesting additional resources from the given infrastructure as a service provider so I'm guessing if you want to expand your cluster on AWS how how would you go about doing that yeah so we have the ability to expand the cluster through the CLI you can expand the cluster through the GUI as well so the ability to expand is is definitely there now I think you know you write in the hit on the point though about the connection between DCOs and the provider the underlying provider to be able to make that possible so it's not the case that we have it for every provider but one of the things that we're excited about as we continue to to roll out more providers is how closely we've been working with those cloud providers to make those kinds of features possible so from the DCMS side and from the meso s-- technology side definitely and then we just had to make sure that it works for yours Pacific providers so with Azure container service for instance that's part of what we baked in with Microsoft right I think we have time for one more this one is with all the automation involved what resources are available if things don't go as planned if something fails and we need to get in and troubleshoot do we still have full control you would have as if you're running you're honest and your next box yeah sure and I'll just say you know that I'll answer that sort of with two things one you still have the ability with DCs to go in and manage things at the meso level so that means you know managing things specifically for the specific servers that are running you can go in and and manage them individually if you really want to you can also still of course SSH into specific boxes and and tweak things the underlying pieces that we've bundled together to make DC OS are still available to you just as if you were running them standalone so that's been an important piece I think of what's made DC OS so successful is you can think of it as kind of a layer on top of these existing frameworks and your power with these existing frameworks is still there so no change there now I think you know most of the customers that we talk to are interested in being able to push that functionality up into the GUI or into the CLI to make it possible to be able to manage those things a little bit more holistically so we're working on that as well so when we find features and and requests like that that we think belong at a slightly higher level where we're working to get them pushed into the DC OS level instead of always having to go down to the meso stuff right I think we're at the hour and do we I think we should probably wrap up there are a couple more questions but Aaron you're you were suggesting that we can maybe address these offline yeah absolutely I'm certainly available so feel free to shoot me an email I can also point you to the community slack channels that we have for DC OSS as a product and as a company mesosphere is really involved and likes to stay involved with our ecosystem through those slack channels so if you should be an email afterwards I'll be happy to hook you up with the right folks that can help drill into the specific piece you want to talk about and if it's something I can help with of course I'm more than happy to do that great what's handed back to Rachel thanks everyone yeah thanks perfect thank you very much guys on behalf of the mesosphere and the bold radius teams we would like to thank you very much for tuning into our webinar today I'm you will receive a copy of the webinar recording and the slides later this week as they were saying if you have any questions that we didn't get to I know there was a few of them so please don't hesitate to email Aaron Shawn or myself and we will get those answered for you thank you and have a great day 