 my name is Alex housing fair my name is Tom boffin lesson we're both senior consultants at noq that's a consulting company here in Germany so last year I was at micro exchange talking about frameworks to build microservices and I had the chance to be hired for a few different projects where I could do some consulting around this and lightly we noticed them yeah there are there are some challenges when you build these micro services that people might not be aware of and that is basically the motivation for the for the talk today that I will go into detail in a few seconds so we structured the talk around the motivation distributed logging distributing metrics and the conclusions we derive from that so when we get hired for consulting gigs usually what people approach us with is we want to break our monolith and if you review such monolithic applications what we tend to do at the first step and you look into these boxes you will likely find that inside these boxes you have like multiple domains or in terms of domain driven design bounded context if you want you if you are then able to like really separate these bounded context and make them separately deployable and independent components you will reach the micro service architecture I think so far most most people around at the conference will likely have experienced these steps so far whom of you is actually at that step and tried out microservices in their organization already yeah most of them I thought so and um yeah let me give you an example of a customer that did that even before we approached him and we call this example a broken mullet so this customer broke down his system it's it's a railway ticket ticket sales and ticket management system and as you can see there is a ticket sales front end but it's here in green that connects to ticket sales back ends you have a pricing engine you have booking offices with back office integration and things like this and what you might also notice is that the bottom right of the diagram you see a legend that tells you about the different programming languages that were used within this as i call it the broken monolid and you see that there is a high diversity of different programming languages and each system within this whole whole application each component was built by a different team and the customer approached us because he had a problem as I said it was a ticket sale system and he noticed that in some circumstances that he couldn't clearly explain so far some of the tickets that would actually be priced around 200 euro euros each were sold 40 euros and that happened multiple times a day and he said I really need your help to investigate what is going on and I heard you're familiar with the micro service approach and please review the the application when we stepped into this project as far as I know they have already been investigating this issue for two months and if you imagine the cost that this buck cost ya was already yeah hi management attention on this and we try to dive down into the distributed system that we saw and found okay it has to be something related to the pricing engine so we focused on this one first and as you can see this is written in haskell whom of you knows how to develop haskell here awesome yeah it's two people and exactly in the project in the whole company there were exactly two people who knew but these were external and as the pricing engine was considered more or less complete these externals were there only on fridays every two weeks so yeah analyzing this this whole thing took us about eight weeks until we noticed okay there is a problem with with some input that the pricing engine gets and that is basically coming from the ticket sales back end so in the end we found that the pricing engine had a back that no one experienced before because there were like i would say in variants that weren't monitored enough so the pricing engine just expected that this value would never reach a value of zero or beyond and yeah wrong assumption so we said okay you took some architectural decisions here and you really found that you're you considered you are the main architecture what we consider as you decided which like domains you have and which blocks into which blocks to separate your application and you certainly took some decisions on the on the microarchitecture level where we would say okay how is your application or each component structured internally like which programming language she choose but you didn't spend too much time on the microarchitecture determining how do your systems actually talk to each other and how do i monitor them and that is what we want to talk about today logging in a distributed system because one of the management people there said with proper logging this wouldn't have happened yep i have my own this is actually the key point we cannot really prove whether this would have avoided that but in our new systems we are pretty sure that it does to draw one conclusion first it's really important to see logging like in like an API that's what we what we are applying here is a you need to define a well-thought login concept I guess most of you are using logging like we are used to to do it like we just add our developer thoughts to system out using a logging framework of course but we we figured that when we aggregate all this information and we we want to have it in a more structured way we need to think about what are the basically logging interfaces what are the ideas and the concepts we want to transmit into a into a aggregated system who actually uses MDC's or thread contexts of you and it's just a few people I think that that that's a good point so let me just explain that a little bit the idea of thread context is basically a way to enrich your lock data with context information like for instance we have an example here this is a lock for j 2 we have a thread context where we can put a login ID to into a it's something like a threat local environment where we put that login ID and that means from that point in time on every lock message is annotated with with that login ID and this can be used I mean login ID is pretty obvious but we can also add request IDs and stuff like that that we want to use to correlate log messages and and we can then define a log lay out basically to add this this context information into the lock message and then the log looks like this in the in the log file at the end but you may figure that if you now want to read this information this is also something we do frequently but we want to avoid is to ssh to a system we then use grep or less and to find things and when then we yeah basically grab for John Doe to see what happened with John Doe and stuff like that what what is way better actually is to define a structured log layout and it's quite obvious that the jason is a good idea so we can just define Jason layout and then we get the lock example here so we have the login login ID set to turn though we can find that here we see the lock messages and we see signs time stems and stuff like this so if we decide to use that structured messages it's more difficult actually to access the data with less but it's a way better to use dedicated towards to access that data then then essence etching to a machine in reading text files basically we will come to that later once we have that we can also think of em defining quality of services for lock messages I mean we all know that from from messaging basically that we can define different quality of services of four different messages some are more important some are less important and basically for log messages we have the same and usually we only distinguish the lock levels like warning or debug or error or fatal stuff like that but actually there can be more for instance we could say this is related to an SQL update this is related to an SQL delete and then we can use markers and filters actually provided by our logging framework I'm yet to have a more fine-grained routing of that message and mean why we also have logging frameworks where we can have messages routed to him to a system that is more reliable and then we can use that for auditing and we can route them to another system and where we only store things that are related to security issues so we really recommend to come up with an logging concept up front where you define what kind of messages we have what are the categories we can define something like a taxonomy of of categories and then we that way we basically build something like in like an API for log messages and a guideline for the teams so to say so if we say concept and we really mean a concept up front yes and this is something like when you're set up your I don't know maven or Gradle or whatever set up or you are continuous delivery pipeline you should also think of those things what do you want to see in your logging a safe system can be filed but can be aggregated systems so basically the the next step then would be at aggregate locks from different formats from different systems into a single single system and then you have to have the possibility to search and correlate them correlation is key when you have like a micro services architecture and you when you enter the system you create a request ID for instance and then you can pass that around using that MDC's or thread context basically to your log messages and can use systems like Tracy and to to have the possibility to bring them across system boundaries and then you can use this correlation ID to trace basically the lock messages through your system that would be like a Pullman's approach and I don't know whom of you was in adrian's talked just before our talk you mentioned Zipkin which is definitely the state-of-the-art way of tracing distributed systems I mean yeah I think there's a difference maybe this is something we should add to that concept and so logging has become cheaper and cheaper so I remember the times where where operation said yeah you can only lock in error everything else is actually not possible because it consumes too much memory too much disk space and it's too slow and all that stuff but meanwhile you have asynchronous logging mechanisms you can basically everything but this is of course not a good idea and you should carefully think of what what you want to trace and in what dimension of you are so you should first think of what is the question you want to ask and then you can see how you can get along to that one addition to that is usually when you have organizational structure where teams just build the system and then hand over to some operations team they don't really think about what they need to operate their system but if there once brought into the position where they have to like yeah handle production problems mostly in real time then they start to think about okay I need this piece of logging data and the more often they are confronted with such situations the better the logging gets so they really start to think about okay I don't need these hundreds of lines of outputs from the spring framework for example but any this specific line that is currently missing from my business logic yeah so let's maybe drill down a bit into the we just took logstash as in prototype typical architecture for that there are a couple of tools they do the same thing and some some of them do it even better but I really like this slide because it shows that log stash is basically built like an ETL tool so you have a couple of inputs you have codecs that can understand the data that comes in in different formats we have then filters that can enrich the data or can drop data or can modify data and we have basically outputs where we can then tailor that information and the idea basically is whether the the cool thing of that is that that you can use either a file input or TCP input to just consume log data from your driver applications or whatever or from your Apache you can then filter them a little bit and enrich them and then and that was the nice idea actually of this architecture not just convert that to CSV or to another file or send an email but just to push that into elasticsearch which was quite a nice idea when they started with that was kind of eye-opening to me because then all the lock data was indexed and we can do queries and we can search and correlate for that so yeah there are other use cases when you when you just want to start build up a logging infrastructure and you would have to for example crossing department boundaries the log stash is a my point of view a good start because you have the different inputs so you can for example read from a file or from Zeus lock then output just to the format that you can for example download from the server and then yeah analyze locally on your system or somewhere else if there is no elasticsearch available to you so that is a my point of view a good starting point definitely and the idea is to get an overall view of your system so that you can basically pipe all the information into that and the idea is to get the means out of that later so a typical architecture of the of this so-called LX tech it's elastic search log stash in cabana which is then visualizing that stuff it's basically that we have multiple multiple log stash instances and that consume the data either from files or get that from from bite by TCP for instance and then we push that data into elasticsearch and then use cubana to visualize that yeah and as you can see in some setups the way from log stash to elasticsearch might actually be a little bit longer and so a recent project of mine we had set up this this infrastructure but what I wasn't faced with before is the customer had a very distributed setup and what I mean with distributed is that the elastic search was look hated in Switzerland and they had their application running enable WS in u.s. west australia and taiwan and you might not know the average latency between Australia and Switzerland was around 500 milliseconds per request and yeah we really faced issues there with pushing the the lock data even though we we adopted batching there and we had to come up with a different setup that is basically the recommendation from elastic which is the company behind the three products to decouple all this via aratus instance so radius is more or less used as a broker or as a queuing system and the different locks dash instances in the world push into a more or less local to them rattus instance and then you have another log stash indexer running in switzerland that pulls from the different raddest instances so you have inputs that you define in the indexer and these are basically the reddest instances that looks pretty nice on this chart but the production experience in this was a bit different so it turned out that our application was lucky enough to have around 300,000 customers in Australia and what basically happened is that they flooded the raddest instance with lock data and we didn't notice because what happened there is that Redis of course is an in-memory database so to say and if you don't monitor this closely enough and with closely enough I mean if the memory usage goes above sixty percent and you not immediately set up a second rattus instance and connect the other lock stashes to the second or third Redis instance then you might actually lose your lock data without noticing because when Redis doesn't accept any more data than log stash will just drop it so that is something to keep in mind an alternative set up that is suggested there is to exchange Redis with haja Kafka ruvim has heard of Apache Kafka so far yeah whom of you has Apache Kafka running okay is it a an experience that you would like to share with the others I mean in the end Apache Kafka requires a zookeeper cluster that you have to set up and it's like your whole infrastructure keeps growing and growing and growing just to have your log data securely transported into elasticsearch you really have to weigh off whether you just want to have like three reddit instances there and set up another one or whether you want to go the journey with Apache Kafka not saying that Apache Kafka is bad I actually like it but you really have to weigh off your operational costs there you should also be aware that the amount of space you need for a log message is is ten times the amount of a single log message when it's indexed by elastic search so this is also something you basically by in when you want to access your lock data in in that way but we definitely think it's worth so once we get the data here we basically want to visualize that and want to have the ability to to drill down into that and this is the thing where Qabbani comes into into the game this is a pretty old screenshot basically but it's good sketches how cubana is intended to work so basically we in the first instance we forget about this upper part so we have here the the message the lock messages or events that we gather in elasticsearch we have here filters and we can when we click on that we can open that and we can drill down into that information and usually we also have something like it looks like a histogram but it isn't it look it's like the you have the time time series and you see how many events you have there and you can use that in order to define it what I'm range you basically wanted to look and then you can define such nice charts and for this time frame you had to find you can see how this chart basically moves so for instance here we see the hits and misses from from a cash for instance and when you're just in the first instance it just looks nice but what is really important about that because it's really a team team in a blur so in my current project it's it's not not that modern actually it's more like an enterprise the project dead it works like this I have to file a request that I need access to a locked eyes out to a log file and then I have to wait three days or so and then I get the message here's the lock data in the worst case I'd only get the information that the log data was already deleted because it was write a note too large or whatever and this is really a thing if you have this in production and you have this on QA and this in the Deaf system and you can you and your team feels more responsible and for the data that is there it also leads to a yeah I think application to of the Boy Scout rule that you take care about your log messages I mean I think we all add debug log messages that don't belong into a production lock and if you see that in a globally available system in all developers steal that then you also feel like more like removing that again yeah one thing to mention about cubana and the whole elk stack it's open source so you can use it and free to use and though you have to of course have the infrastructure ready elasticsearch is considered to be counterproductive to be running with less than 16 gigabytes of memory so that's something you have to consider but as soon as you run this in a more let's say security aware context there are some questions that start to pop up so for example what data are you collecting there is there na user specific data and it's your average developer allowed to see this data are you collecting data about the employees that certainly needs to be filtered out that is something that the acts sec and especially cubana doesn't bring in the free version there is a product that you have to buy from elastic that is called shield that is able to filter your data based on authorizations there is an alternative to cabana and yeah more more or less the Alex tech that's called Greylock and Greylock brings this feature in in the open source version and it additionally has the ability to pre-render and catch some things on the server side so Cabana is more or less completely a single page app so it runs in your browser and directly connects to the elastic search part that has to be available and open for for requests and if you take this for for a spin and try this out you might also want to take a look at greylock which is at least I like the UI better but that's like a matter of taste it is it at least you can use logstash to to anonymize your data and then it's also I think you're right gimana just visualize everything anyways the next step you could do but this is probably still something like a poor man's approach but we we liked it in our scenario amidst used the logging and the log stash stack extech for alerting and we have a similar example for that so next to nagios alert our capacitor you can use log stash to filter this log stream for alerts and we have a simple example here like we check if the message contains critical fatal error shin and then we basically add the tag this is an alarm message then we have something like where we can remove that tag again if this is not as critical as expected and then if we are in the production system and we have alarm in the text then we call pagerduty and basically send that lock message to our admin or the person responsible I guess in the best case the developer and one important thing we can see right here is when we match against the the message this is the thing where I'd like to emphasize that we we need to see lock messages as api's because once we define rules to go against lock messages they break immediately if I change the lock message or if a developer change change and changes the log messages and right now I think in the mindset of a developer log messages are something they are basically private to the developer they change them as they they like they don't expect that they are used as api's so basically we think logging is cool and of course when we have all these log messages we can we can that probably use to to define also or together some some metrics out of that and of course you can do that you can just count how many messages of that type we have then again you have to keep in mind that this serves as an API in that case but we think basically you should watch out when doing that because there's a difference lock messages this is a stream of data where when you want to have metrics you more have a have a view as it's the now stretches of a system that you want to get and you also probably want to see more like like these statues has evolved or it's moving and but it's not really depending on the stream of events in an example for that way it's for instance even difficult to get get that information out of log files is when you want to see the saturation of your database pools of Redwood of course you could log now we added one threat or we removed one threat but then then you have to do the correlation on that log messages so it's it's basically better to have an alternative to directly have the possibility to push metrics into a centralized system to again have a aggregated view on distributed systems and basically we distinguish three kinds of metrics and so the first our business metrics and this is something the c-level executives usually are interested in this is in one project it was like how many mortgage notes have we been processed today or what is our transaction volume today or compared to yesterday in what how long was the the longest approval process stuff like that em but then we have also the view on the application and this would be something like requests per second or how many errors we have and what is the saturation in our thread ports how many garbage collection to do we have in addition on that we want to look into the system matrix and things like cpu load memory disk disk space or even the entropy of your system we usually have have issues with the oracle DB JDBC driver if there's no not enough entropy in your system and usually if you have issues with your application or issues with your business metrics they typically correlate with other kinds of metrics so what we want to recommend is basically to not only focus on the application metrics but also think of exposing business metrics and system metrics and once you have the ability to push that into an to an aggregated system you can combine all those views and once you have an issue and you can overlay all this information and then you can see well yeah probably this business metrics went down because here we had this high CPU spikes and they were probably related because of work costs because because we had some I don't know Mt threadpool it's stuff like that usually when when I approached development teams and tell them you should gather some metrics of your of your system the people ask me why should a developer care about this I mean metrics yeah this some ops thing right I mean CPU usage we don't care about this they have to monitor it and they just have to spin up another instance and yeah then we're just fine yeah we're not operating this so let me ask the question whom of you is gathering metrics from his applications running in production perfect so I probably don't have to tell you that in the end what we are paid for is to achieving the expectations that our customers put in us and they basically measure us by checking metrics so one thing that we could do is think about how we can measure these metrics ourselves in our systems and let me tell you story of what we did at a customer of ours the idea was to set up a deployment pipeline like kind of continuous delivery with some manual steps in between so whenever a developer pushes something to get Jenkins will build this run some unit and integration tests push it onto our deployed on to a development test box then you run some acceptance tests in this case with cucumber and a specific configuration for this development test box of course like adjusted today to the actual capabilities of the box then if these tests were successful the stuff would go forward into the testing shadow box again some acceptance tests and after some manual testing would go to the production shadow box and then again you would have the acceptance tests and I was asked by the by the department leader of the of the development department do we really have to have this whole infrastructure and measure all that stuff and collect it in in some central database what is actually the advantage of this and the answer is clearly if we take metrics from each step that the acceptance tests run it I mean if the except we are always aiming for fast feedback right we check with the acceptance tests on the in the development box whether something goes wrong and then we won't either even bother the testing team with with some software that's not yeah fit for purpose right but if we take the metrics from these these acceptance tests and the results so we don't change the the tests and the boxes then we can see the differences between our different commits in the results of the of the tests that means that a developer will be able to tell whether the committee just made influence the performance of the system negatively or positively but just having gathered these metrics when the upper acceptance tests run and this is basically happening on every an ivory box because the boxes get bigger and bigger you get more you get more tests you get more instances running because in production you likely don't have only one instance of the application but multiple and then you get more feedback before the stuff even goes into production and you can directly tell whether this should go into production or not so if we're thinking of the metrics what is there actually to be to be yeah considered that we need to collect yeah so actually we want to briefly go about the options or our types of metrics we can we can gather we base that basically on the drop visits match library who's who's using that from of you are not so many would be interesting to understand how are the others gather their metrics but maybe we can do that offline so I think that the first and most simple way to gather a matrix is a gauge basically this is an instrument that just measures a value and in this drop visit library so we actually we can highly recommend it and because it provides a decent API am 22 instrument your code nicely and gauges for instance use more like a pull approach where you implement a method that just returns a value you can get that from a database you can get that from I don't know hash map or you can just calculate that and the metrics library is continuously pulling that information calling that method you can even add something like caching if that's too expensive to calculate and collect all that data and can then push it at a certain time and to an upstream system basically or to the console whatever you like the next higher thing basically is the counter and they the API for counter is very simple it's like increment and decrement but this is even a nice nice way for instance if you have something like a thread pool you can just increment and decrement that when when you add and remove things and can push that data the next sophisticated thing our meters and meters are supposed to calculate the rate of an event so the FBI is basically like you say mark mark mark mark and it calculates that occurrence of those mark events over the time and the nice thing of this of those meters in this matrix library m instead it also calculates the mean mean rate and also calculates for you they have the moving averages for one minute five minutes and 15 minutes and when you push that information into a or try to push that into a dashboard and you can compare those values or even on the console you get some idea m how how your system behaves or how it it changes basically and then the most sophisticated thing is basically a histogram I don't know who was in Adrian's talk he talked a lot about histograms and the nice thing of histograms is that in this implementation from from Kota hail in this library and we found a way to have only a few I think thousand thousand twenty eight samples and he uses reservoir sampling m2 to have this set being statistically representative and that way you can also get the information like the min min max but also the standard deviation in all the percentiles um and those person tiles are are important to get information like this search query or no ninety-five percent of all search queries responded faster than 500 milliseconds or stuff like that um which brings us to the next thing which is a timer encode I heads library the timer is basically in histogram over the duration so if you would implement something that answers the question em like you want to see the histogram of all those search queries then this is the thing you would like to use and now once we we had the ability to measure those metrics in our single application in this way in in this example it's a java application and then we basically want to push that into a next stage which is collecting samples for the coda Hale library and we can say that collect and sample also belongs into that library but there are a couple of other libraries where you do the measuring independently of that and then push it into a system that does some further a quick and normalizes that data for instance this could be stats T and where you push UDP packets into that systems and it dis collects the data and then push it basically into a store and such a store could be graphite or or in flux to be in flux tube use the then the cool thing right now and you can use that and then to query your your data and when you can graph that and can query that and then you can bring the graphs on dashboards and with the queries you can do something like complex event processing or anomaly detection in order to react on certain activities in your metrics or you can use that for for alerting um so speaking of dashboards there's kafana which basically looks like cabana and I think this is not just a coincidence and this is basically a dashboard that that works for technicians you can bring all the I think we have the logins in the logins one hour before for instance in in that particular chart so you can define nice dashboards where you can see how your system of systems basically behaves so the idea is to bring all the data from your front service and your back ends and maybe databases all together and then try to correlate that and maybe one important thing we spotted here is we naively took the i think is it said that the person tides the 25 50 75 90 and 95 5th percentile and asked Havana to just visualize that kafana sorry but what we didn't figure basically is that we use the average to combine that this is one thing you should keep in mind those person tights cannot be averaged basically because this is this is just nonsense so it's better to take the max or to take the first or last item in this event stream so this wasn't wasn't lesson lesson learned for us and that we carefully need to think about what data is actually in the system and what are we visualizing there then yeah something that that I learned and in a recent project is that graph Anna is nice for for technicians but it's only one tool so if you're thinking of maybe you have the netflix tech then you have the district's dashboard some of you might have an Operations team that uses nagios with its own UI so you have different you eyes visualizing metrics and you need something that actually aggregates into a complete state of your system especially for like a sea level management manager and one recommendation that we found for this would be dashing that is like a very nice ecchi representation of your metrics and really focuses on the simple things one thing that that we wanted to point out because it's an ongoing discussion in the metrics community at the moment is this discussion about push versus pull and my point of view this is nothing new but anyway so recently with graphite like for many years and in flux DB we had the push approach that basically applications would push their metrics to the target and we listed the some of the advantages and disadvantages I think the most important part of the the most important part about the discussion is basically who initiates the transfer right so who is responsible for starting the communication is it the producer that produces the stuff and pushes it out when he's ready to do this taking into into account whether he's under high load at the moment and cannot just produce the the metrics every 10 seconds all whether it's an approach that you want to pull the data that is what Prometheus system that is built by soundcloud engineers is doing they basically have a target system that is a Prometheus installation and that pulls the metric data for example from some HTTP endpoint as I said I think basically it's about who initiates this there are advantages and disadvantages on both sides if you decide to generally go for pool you anyway have to additionally have to have some push use cases because for short-lived things as Edwin just mentioned for example AWS lambda services that only live for a few seconds or milliseconds even you will not be able to reach them with a directed pool that goes there every I don't know ten seconds right I think we could just discuss that for hours yeah because it's also for instance it's nice if you have a developer machine you could just connect with the pull approach to your production system and see all that matrix and but on your differently defined monitoring system they are pro in contrast and I think we we are not really decided there's no general recommendation it's it's basically depends on on your scenarios so maybe some takeaways or recommendations so consider retention policies so even it's like in the in the logging case gathering metrics is gathering matrix is not expensive but storing matrix is expensive and when you think of retention policies you can basically define how you can yeah aggregate the data and store it at a lower resolution to keep the sense of the data but then it doesn't need as much as memory in your starch system and you should always carefully design your your dash parts because this is important like we figured that we basically measured nonsense and then basically you should also think about non-standard graphs so most people just think in pie charts or bar charts or line charts but there are different graph types that basically fit better to visualize certain scenarios I think you need to skip that right I don't know just to give you a short example of such a sample architecture that we build because I just mentioned that Capcom might be a bit too much that is a sample sample diagram of just the logging and metrics infrastructure that we build up and to give you a hint this is the user requests coming in and all the rest so this engine X and the gah are the i would say business part of the system and all the rest is just for monitoring and metrics we are unfortunately lacking the time but if you want to discuss the decisions for these for these things and where I think in on the conference and downstairs at the inner cube booth like for the rest of the day so feel free to approach us okay conclusions from the from the whole thing you definitely have to think about what you want to monitor and and lock and create some concepts as guidelines for your teams that is in our point of view part of the macro architecture that you have to do and certainly we recommend to collect and aggregate the locks and a metrics to enable your teams to take care of their applications otherwise you will foster some behavior of throwing the thing over the wall and leaving the rest to an Operations team that might not even be able to to estimate the effort that they have to take yet i'm already said dashboards should be tailored for your audience so providing a graph on a dashboard to a sea level manager can have the disadvantage that he really drills down and yeah put some interesting questions on you and definitely you have to correlate your data the right way to make some conscious decisions and something that we just recently saw at a customer think about what you need to lock don't create your very own big data problem we just had a customer that is logging 1.5 terabyte of lock data a day yes and we think logging is very important but it's not the core business right yeah okay so keep in mind logging shows event metrics show static state and don't fly blind thank you thank you okay great thanks for a nice talk we've got three minutes for questions so yeah first ones already there I thank you for the talk and we are right now in the stage of deciding how to aggregate the data we collect and my question will be is there any automatic tool to aggregate the matrix and decrease the resolution of the matrix you Colette and the other question will be in case of logging right now we are just using elastic search curator to create a new snapshot in amazon s3 and delete from the current elasticsearch the old data but how would you go about aggregating logs okay there are two questions yeah I have to say unfortunately there are a lot of tools to aggregate metric data so it's really hard to pick the right one for you in our case we for example picked stats d2 to correlate some things but we even have some background queries in influx to be that aggregate data over time so that you can say the data that is like one month old we don't need this over in a resolution up to 10 seconds but one minute would be okay then we try to to correlate these they are these things and for example if we have histograms there we only keep those with the max values for example yes so in flux to be offers offers course for this I think in general it's a nice idea to see that as several buckets you have this first bucket where you have audit data in high resolution and then you once this bucket is full so to say like you have information for one day then you collapse that into an aggregated value and put that into the next packet which has a higher time range but lower resolution and basically influx to be and graph on and even r rd2 will provide that means so in a different way you just should look at that stuff yeah and the same goes for your log data the question is whether you really need the lock data that was there five months or six months ago but what you might want to have our the maître metrics that you can derive from that like I have seen many people that derive metrics from there Apache access logs you can I mean these are calculated on the fly and you can just like take out the metrics put them into some time series data base and then get rid of the excess logs that would also be possible and this is actually us as a sorry the key difference between metrics and logging so in metrics you can aggregate and collapse them but with logging you can do that you are losing real information that I think one other question and who's first down mmm two of them maybe it's if your answer quick then we can take l will try depends on a question right so thanks for the talk question is we are thinking of how do we actually deploy a service and define the metrics it will provide and then have that ultimate like in an automated fashion be available on whatever lock metric dashboard we we use do you have any like suggestions that we could do that because we currently can't and we always deploy service go to the monitoring tool figure out yeah this provides these some bad metrics like create the center created ash board okay done but it's manual and we want some sort of auto-discovery also adding is there something might know can recommend or it just comment on I think at the end it boils down to that push versus pull discussion whether you're either just push to one central stage or you have a service registry and get the data from there your scenario sounds a bit like that this pool is then nicer for you and then you may look into Prometheus from SoundCloud yeah one thing that I have to mention there for this architecture we had no idea what kind of metrics we would want to have but luckily we were basing or we were standing on the shoulders of giants so to say we were using a spring booth there and that is like automatically from scratch providing some metrics of the of the application and we chose to use stats d that is basically by baked into Telegraph here Challon graph is another tool from the influx DB team and there is that this is a kind of decoupling that I would really advise you to to take to say okay I have my application that pushes to some i would say local aggregator and then you go forward and push this into some central database because this decoupling really enables you to for example even push into multiple databases so you have kind of the same advantage that you have with a pool approach okay thank you i would suggest the other question you can do that directly later because now there's lunch and okay okay great here thanks 