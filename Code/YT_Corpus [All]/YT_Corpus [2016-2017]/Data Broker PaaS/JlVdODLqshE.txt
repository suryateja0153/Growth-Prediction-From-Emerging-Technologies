 we all feeling well rested not wednesday morning I mean I know it's like the crack of 11 and early in the morning I can't see anything up here is everybody been having a good open stacks on it yeah been lots of good content I've only been able to make it to a few few sessions but they've been really good sessions so I know we've got about two minutes before we start but I figured I'd come up here and say hi and banter a little bit so that way you know I feel a little more comfortable in front of you guys you know know anybody find any good music last night I heard that there was a band playing like two blocks I don't know what direction two blocks away from the hilton that was pretty good so this is austin the center of live music right ya know i would i would imagine not yeah I don't know we walked sixth street at one point last night it was very quiet surprisingly there was not a lot of people out it's Wednesday for it was Wednesday alright so just a few seconds so I'll go ahead and start since I don't see a line at the door or anything so first of all thank you everybody for coming out I do appreciate you I don't it's not early yet but I do appreciate you coming out to the second set of sessions for the day so thank you very much my name is Andrew Sullivan I'm a technical marketing engineer with netapp effectively that means because I have both technical and marketing and my title nobody actually takes me seriously on either side of the house so ultimately the analogy I like to use is if anybody saw office space right the guy who you know I deal with the customers i take the requirements to the engineers we kind of do the inverse we go and ask engineers a lot of really stupid questions and then take that and turn it into things that are consumable by the rest of the world so today I'm here to talk about a couple of interesting things right so one or first and foremost is going to be containers right containers are an interesting topic containers are something that a lot of people are really interested in and in particular when we look at the OpenStack environment right containers are sort of a natural transition so containers are not virtualization and we'll talk about that more in a few minutes but in particular the focus of this session is rapid prototyping with your data your data that's on manila or sender and the container echo system as managed by magnum inside of OpenStack so rapid prototyping is something that's I think we all do or should do at some point in time and but I'm not limiting it to just rapid prototyping that was what was on the on the title right but really what we're talking about is test in dev right we want the ability to leverage our actual data or as close to production data as we can get when we're doing any operations that are non production right think about doing tests I go and I implement a new feature I modify something in the database and when I do that how do I know that when I push it to production is going to behave correctly now this is something that we can do right and have been do I would have to assume inside of the OpenStack echo system for some time but how long does it take to push an application when you're using nova instances maybe a few minutes maybe a few hours depending on what you're doing what's all having to happen inside of there will be containers that changes make containers instantiate in seconds we can create an entire application very very quickly and what this means is that when I'm doing that prototyping when I'm doing that testing when I'm doing development work I can much faster iterate over what I'm trying to do so from an agenda standpoint today I talked about a number of different things right first and foremost the components what are we using inside of OpenStack in order to make this easier what components are using i also want to talk about containers right now if anybody's unfamiliar with containers it'll be a very high level overview you're welcome to always ask me questions at any point in time we'll talk about container orchestrators in particular the ones that's magnum is aware of magnum abstracts away for us we'll talk about the application that we used in order to create the demo that's coming in part three right and then we'll actually demo what this looks like what does it look like to go through and clone our data bring it into that environment and then begin redeploying our application in multiple instances and finally a brief summary and conclusion so I would feel remiss if I didn't talk about something that is commonly referred to as continuous integration often abbreviated as CI and usually lump together with continuous delivery or continuous deployment CI is a concept CI is an ocean CI is not a tool chain CI is not a thing that you can buy much like DevOps but the goal here is to rapidly to constantly be building we want to find out as quickly as we can when there's an error and the thing that we're creating and the application in the code so that way we can just as quickly fix it right fail fast is what is often used so we're taking this concept and we're bringing it into our OpenStack environment and more quarterly aware well more importantly to me we're layering it on top of containers as well so continuous integration is something that has become more and more prevalent it has become more and more interesting and more and more popular amongst many organizations because it's becoming easier OpenStack is taking off OpenStack is becoming more and more popular so the tool chains are rounded are becoming easier so the components that we're looking at today I hope that at least none of these first few are a surprise to you right these are things that have existed for a long time we're just using them and maybe a slightly new or slightly different way and the first one I want to talk about is manila manila is the software I'm sorry the shared file systems as a service offering great senders been around it was one of the original projects inside of OpenStack and it provides block storage middle on the other hand provides NFS sifs HDFS whatever type of shared file system we happen to need and this is important for a number of different reasons right sometimes you need to have one file system that is shared amongst many different right many tens or hundreds potentially of clients right nova instances container instances whatever that happens to be additionally one of the things that we like to take into account is the neutron integration right we want to ensure that only our tenant network has access to it but in our particular case we're really concerned about a couple of things notably manila just like sender supports snapshotting and cloning of those datasets and with most modern storage systems most enterprise storage systems and yes I I am wearing a net up shirt I do work for net up I might be a little biased but almost all of the enterprise storage system supports the thin cloning concept right where we're not actually cloning our data we are instead creating a snapshot we are creating a new volume effectively that only contains the changes and the new data so it doesn't matter if I have one gigabyte or one terabyte or one exabyte worth of data having that thin clone feature means that I can rapidly create those clones without consuming additional space so going forward if you don't have a storage system that's capable of doing that evaluate whether or not it's something that's necessary and there's lots of ways that you can work around that as well even if you're using local storage something like a copy on write snapshot leverage through lvm is possible is capable of doing this or very similar type work so the second thing we want to talk about is of course cinder I'm not going to bore you with the details of cinder I hope by now we all know and love sender pretty well right but much like with Manila there's a couple of things that we care about here first as being that same concept of snapshot create a new volume based of that snapshot right we don't want to clone all of the data we don't want to literally copy the data that is slow that is expensive but also importance is what many of the vendors are doing with the extra specs where we're able to pass additional information down to the storage so that we can capitalize on other features now again I'm a netapp employee I'm most familiar with our feature set so we have things like qos I know we're not the only one and qos means that well what if I'm running my production data sets on a set of disks that have you know their performance limited maybe they're 10k drives or 7.2 k drives I don't want to impact my production data set so I can put a qos policy that is second effectively limits the secondary copy the secondary data set and prevents it from interfering with the production data set these are critical things because well we don't want to inhibit the actual production application right we don't want to impact the business we don't want to impact the things that are happening up there so make sure you're familiar with the ex respects that your storage offering has make sure that you intelligently apply those in order to maximize what's happening on your clones data set as well as your production data set so the Third's OpenStack solution that or become sorry components project but we want to look at is magnum a magnum is particularly interesting in that it is not itself a container orchestrator right a container orchestrator is responsible primarily for two things the first one is orchestration right scheduling if I have a thousand containers I went to figure out I want you scheduler orchestrator to figure out where to execute these at think of this is the way that Nova operates I don't choose which host my virtual machine is running on Nova figures it out for me but Magnum is not an Orchestrator Magnum abstracts the creation of those orchestrators for us in particular docker swarm kubra Nettie's and mrs. technically it's marathon on top of me so spit we'll cover that in a minute Magnum is interesting because not only does it abstract the creation of these it also abstract the interaction with them right I can choose to interact directly with two red eddies are swarm I can choose to interact with it through Magnum so it's a very powerful tool but it's the gateway into what we're actually trying to do here now in this instance in the demo that will have in a few minutes what we did was deploy swarm on top of Nova instances interestingly if you look at most of the research that is being done like by companies like data dog who are on the expo floor they publish periodic reports the vast majority of containers are running on virtual machines and the vast majority of them run for less than five minutes I think it's something like 39 or I'm sorry twenty-nine percent run for less than five minutes and forty something percent for less than an hour this tells us that containers are being used for these ephemeral type purposes right now once you test something I want to instantiate it and quickly just destroy it so I also think it's important to talk about containers themselves containers are confusing particularly for those of us that have been longtime infrastructure administrators right where we're familiar with how virtual machines work because it's pretty obvious and I think it's important to realize that a container is not a virtual machine or a virtual machine is exactly that it's a logical abstraction of physical hardware there's a virtual motherboard with virtual BIOS and a virtual network adapter plugged into it and virtual hard drives attached to it and all of those things have overhead all of those things have additional needs but a container is a process we're simply creating a process in the host Colonel just like any other process and then we leverage a couple of features that google introduced a decade ago namespaces and see groups namespaces make that process it fences it isolates that process so that it is by itself and see groups layer resource constraints on top of that but namespaces aren't limited to just the process we can do things with namespaces for the file system for the user space for the network space and this is where docker comes in you see most people use the terms docker and containers interchangeably but in fact docker is not containers Dockers and abstraction for containers they make it so that mortals like me can consume the container at any point in time using a simple command docker run it instantiate seeeeee everything that we need in order to do that and that in that name spacing in particular the file name spacing is what makes us think what makes it look taste smell and feel like we are inside another operating system in that container you see that container process whether it's bash or Java or jboss or whatever you happen to be running that process can have a new file system attached to it that's what the namespace does when you think of a docker image a docker image is nothing more than a gzipped folder that contains all of the files that represents that file system when I instantiate the process it gets namespace knopf we attach a new file system at the root inside of our container and that process bash can now look at root and see maybe it's the Ubuntu file system layout maybe it's synthos maybe it's red hat whatever that happens to be it's not executing Ubuntu or sent us or whatever you're using in that distro it's that process with a familiar file system layout a familiar toolchain a familiar library set that's all a container is so it's not a virtualization technology it's a process isolation technology so containers themselves are just a small portion of this right as I mentioned before right imagine running your your OpenStack environment right dozens hundreds thousands of Nova instances having to choose where each one of those runs in real time that would be terrible and this is what happens with containers as well if I'm just using docker vanilla docker on an individual host well I have to instantiate it manually each one of those has to be I have to choose where to run it and this is where the orchestrators come into play in the orchestrators as I mentioned before bring two primary things one scheduling right they all have some sort of scheduling mechanism that allows me to create a pool of resources and then distribute the containers across them and the other one is additional basically discovery services this can be in the form of overlay networking so that containers on separate house can communicate with each other as though they are on the same network think of this as Neutron they also have things like discovery services so either literally or something analogous to DNS so when I instantiate a container I can give it a name on that container network that overlay Network and now I can talk to that individual container that service just as though or on a real network so the first of the orchestrators that I'll talk about is docker swarm dr. swarm is an acquisition that came that ducker did about a year and a half ago and it is arguably the simplest which is both a good and bad thing right it's good because well that means that we can quickly spend these up and use them and that is exactly what has been happening many of our development teams whether or not you realize it today are probably using docker sworn when I talk to customers I like to ask particularly infrastructure teams right has your development team stopped making so many requests for new virtual machines from you right have they suddenly dropped the number of resources right creating new Nova instances or VMware virtual machines where it suddenly drop the number because what's happening is they request a handful of resources they put some sort of container orchestrator on there and then they deploy on that so instead of constantly churning through virtual machines they do it in container so the important thing to remember about docker swarm is that it is effectively a pass through to dr. itself I can take a number of docker hosts something up to a thousand or so in the current version and I can talk to them I can treat them as though they are a single docker host using the same exact docker CLI as I have when I'm talking to an individual host I can talk to the swarm and say run this container or run these containers and it will do exactly that dr. swarm uses the compose application definition that's something that I should have mentioned earlier each one of these orchestrators has an application definition language if you will where I have to describe what my application looks like it has this many of these containers right this many that are running maybe Apache this many of that are running mysql this many that are running python and here's how they communicate with each other so it uses that to build to create instantiate the resources around it swarm is arguably again the easiest of these to consume now the second one that I'll talk about is Cooper daddies and Cooper Nettie's is a particularly interesting project to me personally because it is built off of Google's Borg now Cooper Nettie's was recently donated given to the CNC f the cloud native computing foundation so it is now underneath their management which is a linux foundation project so google is most definitely playing good corporate citizen here and open sourcing the whole thing but it has a really really good pedigree right Google Borg is what google uses in their own data centers they took all of those lessons learned and they're applying it to Cooper Nettie's so we know that there's a lot of really good things that are happening inside of there now the primary difference well at our level the primary difference is simply how we define applications there's lots of differences at the underlying level with the scheduler and all of that type of stuff but we don't need to go into that here so within Cooper Nettie's instead of using compose in order to define our application Cooper neji's uses the concept of pods and services the pot is one or more containers I say please execute this pod we leverage a replication controller to say make sure that there is at least 10 instant is of that running at all time it does exactly that to think of it as a high-availability mechanism and then we have a service which abstract the access to that right so if I need to access that underlying pod I give it an IP leveraging a service if anybody watch the keynotes with Craig McKee and Alex Cole be on Tuesday they were showing cobranet he's running the OpenStack services right when he was destroying when he was ending those containers it's the replication controller that restarts them inside of that cluster so this is technology that is very much in use today so the third one is a little bit confusing right me sews is a resource framework reso's doesn't actually schedule things it is simply there to say I have this much resources this may CPUs this much RAM what do you want me to do with it somebody tell me what to do with this and what we end up with when we stay inside of the apache foundation is marathon marathon is layered on top of me sews and it acts as that container scheduler it has its own application definition language as well and the two of them work in conjunction with each other in order to execute those containers in order to go through and ensure that swell enough of container a or container type a container type B are working together now interestingly and confusingly Miso's is again a generic resource framework Kuber Nettie's and swarm can both use Miso's as their underlying execution engine there's other companies mesosphere for example who recently open source their data center operating system where you can execute other frameworks other tasks directly against that to me says cluster so things like Cassandra notes I don't necessarily need to put it into an application definition using marathon I can execute it directly against measles or Kafka or a dupe or any number of other things inside of that shared resource cluster so Hadoop tends to be a very large the largest scalability wise there are clusters that are tens of thousands of nodes in size and also the most flexible of the solutions that are available we see very very large customers using this and there are some who have publicly stated that they're using it Apple has come forward and said that they use me sews with Siri when you instantiate Siri when you press that button it stands up a container running on top of me sews somewhere in the apple cloud so for the demo we sat down and started creating an application submit one of my teammates and I we sat down and we created a fictitious application which we fondly referred to as project obstinance and walk through and started to write the code inside of here and create all this other stuff and then it occurred to us that doesn't really matter the application is technically unimportant everybody has a different application every application has a different architecture so instead we cheated we went with the most highly scalable most robust most widely used application we can think of fatha if anybody's looked at the developer documentation on the OpenStack website fofo is the sample application that they distribute that they use and it turns out it's actually a not a bad example use case right we have multiple services running inside of there we can treat them as though they are a service oriented architecture type system if we want to we could break those out into microservices so it makes for a an interesting play and it was also done we didn't have to put a whole lot of development effort into it we did make some minor modifications to make things easier to read easier to use for example but again the point that I'm trying to make here is ultimately the application that we use in this demo is unimportant this applies regardless of the application that you're using regardless of how much data you have we've got a few tens of gigabytes of data inside of here the exact same principle applies if you have a few tens of terabytes of data the concept doesn't change so what's going to happen when we look at this demo right there's a number of interesting things that are going to happen here and the first one is just looking at our application running right hey it really is running it's running inside of Nova instances in the we want to take the underlying data store it doesn't matter if it's manila or cinder in this instance we use manila and we're going to clone in snapshot create a new volume based off of that snapshot once that's done we instantiate a new swarm cluster leveraging madam again that only takes a few minutes we're simply instantiating Nova instances and deploying software on top of them we introduce that manila share and at that point we can deploy any application we want inside of containers and provide access to that data now there's an interesting workflow that evolves that comes out of this right in that snapshots give us that point in time recovery so one of the examples I like to use is a database I'm a developer who's going through and creating the next version and I have to do a database schema update well i can write out the sequel in order to do that and i can execute it in production and cross my fingers or i can go in and I can iteratively develop that so take the concept of okay i created this environments I now have a separate snap shouted isolated copy of my data I do the first set of tasks it worked great okay create a new snapshot right all of this is intrinsic to the technology I can go forward take the next phase on maybe I fail miserably I can roll back so I can now treat my data as an asset during the development phase likewise when I'm going to do my production rollouts leverage those snapshots leverage snapshot so that you can roll back quickly so finally after the data has been reintroduced we simply prove or I'm sorry after the data has been reintroduced and we have it inside of a containerized environment we simply prove that no it really is the exact same application and at this point we can do whatever we need to it from a data point from a storage standpoint again modern enterprise storage systems are only going to save only going to store the deltas so yes if you go in and you accidentally RM on the entire file system you're probably going to create a very large snapshot out of that generally speaking we're not deleting terabytes of data during an application update so it's a very small change it consumes a very small amount of space they're very quick and there is no reason why we should not be using these so let's hope that I can get our video here to work there we go so during this and I'm not going to narrate exactly what's happening but during this phase what we're looking at is that we do have Nova instances and we have a manila share that has been provisioned in this instance I used awk in order to limit the fields that are returned because it is a gigantic very difficult to read a very wide output once we have our share or in this instance we're proving that we have our instances on our shares all i'm doing is jumping into our application services nove instance in my deployment this is running the amqp server as well as the database server and we're going in and we're just showing that there is indeed a real database inside of here one that has a hundred and fifty thousand rows as we'll see in just a second we created a lot of fractals at one point I spun up several dozen instances of the worker threads in order to create all of those so our MySQL database is a 15 or so gigabytes in size or a pretty good size database not one that we typically are fairly large for a web application fairly small for an enterprise type application typically we'll see here there are 75 hundred pages of fractals that's after i modified it to display 50 at a time instead of five at a time so lots and lots of data that's going on inside of here now at this point what we want to do is deploy our magnum bay our swarm bay i did this first because it does take a couple of minutes right instantiating the nova instances and all that other stuff i will note that all of this is done in real time i'm not using any video magic i'm not speeding anything up here i did go through and edit out where I made typos so I'm not a perfect typer for the record but all of it aside from that is done in real time so we can see creating these nodes creating a magnum Bay it's pretty simple just uses heat and because it's just using heat we can go in and we can actually see what it's doing when I go through instantiates this we can see all of those resources that are associated with it we can see what's happening with each one of them if there's errors inside of any of those and it's very very easy to destroy and Riaan Stan she ate these the concept behind containers is that the application is portable wherever i instantiate that application it's going to execute the same so we see here at the end Bay that I deployed has one master and to know swarm nodes what I'm getting here is the floating IP associated with our swarm master because the next step is to actually jump into that particular master node now one thing I also want to note is I'm this is a demo so I'm showing all of this manually because it would be really really boring for you to see me write a Python script or execute a Python script that does all of this that's just not not fun right so all of this can absolutely be automated in this instance again because we want it to show for demo purposes this is what swarm looks like you see we have two nodes running inside of there you'll notice I'm using the standard docker interface to run with it in this instance what I'm doing is pulling my internal git repository and showing that there is a docker composed file that defines our default application so we instantiate that application going through and creating our services I browse to that particular container or I'm sorry container host that's running inside of there we see that we have a instance of the application that does not have our data at this points right we have not mapped that Manila share into the magnum environment so let's bring down that application which does take a second and after we get through this phase what will go to is we'll do is we'll go and actually create the snapshot of our Manila share and this is something that takes very very little time you remember before I was referencing a the very long outputs so there's our very long output of the manila list we create a snapshot after that's done we will go through and we will create a new volume a new share based off of that snapshot and then the last phase is to map it over into the network for our particular application now Manila shares are interesting in that in this instance we're using NFS which means that we have to enable write a sub bring a subset or an IP range to be able to access it because this is a lab I went with the highly secure everything but it is one of those things to be aware of particularly if you're using application networks you want to make sure that your tenant network your application network is the one that has been enabled for access to that after that that little listing there was simply showing that we really did create a second chair and now we're jumping into our Kubra Nettie's host again I'm sorry our swarm host again now the interesting thing here is that we're using a docker volume plugin to create our volume in particular this is the netapp docker volume plugin although there are many other storage companies that have the same same thing emc in particular with rex ray so what we're doing is because we have the name of an existing share we can map that in to docker and now it can become managed just like any other doc or volume so what we see here we created that volume using the same name of the share we see it twice in that output because each of the two nodes has it mounted and now we can go in and all I'm doing here is showing that yeah that data really is there i created a temporary container mounted it in there and showed the database tables so at this point we're going back into the application folder and showing a second dr. compose file which leverages all of that data we're creating new containers that are taking advantage of our data share and now we create the application again so the point here is that at this point we have now recreated our entire application inside of containers with the exact same data and there is no tie between the production data and this application based data right this test data whatever I do inside of here will not affect the production data set so there's nothing that I can do to harm it I can do all of the testing I can mess up I can remove the database I can do all of those things worryfree so as a developer and in particular as we're going through the QA or the acceptance phases it's really good to have production data right now this is also interesting in that we could also do something like test migrating from a nova based appointment to a container based deployment all without having to worry about affecting our data I can reset I can rapidly iterate over that application inside of containers again you saw how quickly it stood up much much faster than the Nova instances which on average took about a minute 45 when I was doing all of the testing and set up for this so containers took about 15 to 20 seconds to instantiate to the exact same thing so big time savings so a quick summary of what we've seen here right some of the key takeaways that we want to think about so first and foremost applications themselves aren't getting any smaller netapp's very own Val Bercovici gives a presentation and I meant to grab the slide from him before setting this up that shows applications are growing in number of lines of code as well as in data that's being generated if we look at the Apollo mission right the apollo lunar landers there is a couple dozen thousand lines of code that we're running inside of there if we look at this watch on my wrist which is a SmartWatch there's a couple million lines of code inside of here there's dozens of millions of lives and code in modern cars I'm not even counting Tesla applications are getting more and more complex and we also have an affliction or an affection depending on your perspective for data with the concept of big data the concept of I want to save everything because I don't know what I'm going to need but that also introduces risk what happens if I screw it up I've worked with customers before who are afraid of automation they will not automate their infrastructure because well if I have one guy who's doing it and he messes something up he just broke three servers if I'm using automation I just broke three thousand servers so we want to be careful we want to de-risk the process as much as we possibly can using the tools and technologies that we have at hand and this is getting easier and easier and easier as time goes on and this talks to the second bullet right testing and validation with real data is critically important particularly for business-critical applications I've had a personal mission for the two-plus years that I've worked at netapp to remind people that I t does not exist for the sake of i.t i.t exists for the business IT exists to help the business do whatever it's doing making money through selling goods or helping people do whatever it is that they happen to need helping with or whatever your business is doing we're in the business of selling things I t exists to help the business make sure you're protecting the business the other thing that I like to remind people of is data is money data is the life of your business it's what you use to make decisions it's what you use in order to well go through and in our case sell things make better things so losing data is not an option and finally containers are a tool they're not the answer to everything they are not a panacea they don't fix all of the problems of the data center but they are a tool that can be leveraged to improve overall operations just leveraging containers does not automatically mean that you are DevOps or that you have deployed microservices but they're a tool that can be used in order to help that testing that validation process in conjunction with things like this cloning concept and reintroducing it to the containerized environment so I'd like to thank you for your time I am happy to take questions whether in the group if you want to come up to me afterwards I'll be in the booth today I think the booths are open until one thirty or so I noticed our booth manager schedule themselves for duty from 130 to 230 that's awful convenient so anyways I'm happy to take questions at any point time and finally I would very much like to thank you for your time yet again i hope everybody has had a great OpenStack summit what kind of strategies would you utilize for doing dev tests this is this whole idea of moving production data into dev test but you know a lot of times there's private confidential secure information in our production databases for scrubbing that before we do that that transition yes so I have done this before when I was a customer we used to do this and it was through an automated system either we gave our developers the option of using a nightly snapshot right so every night at I think it was 1am we would create a snapshot and we had a task that would go through and sanitize the data it took about five minutes alternatively depending on the project in the quantity of data they could in real-time request a clone of that data right it was through a storage as a service portal rather than other ways now inside of the OpenStack echo system it's something that you'll have to address with the developers directly right because in this in this demo in particular right we natively used manila or you can natively you sender to do the same thing so unfortunately if that's if they have enough access to do that if that's what they want to do at least from from my awareness I can't prevent them from doing that again if they have the permissions I can't you can clone it but you can't access the data doesn't make sense so maybe a storage as a service portal or something like that where you can request or they can request those services to happen maybe the best way I have had success personally with that before any other questions it's hard to see up here all right thank you again oh yes sir so the question is the question is what type of access controls security controls are we putting in place in netapp products for containers versus anything else I got you okay so let me just to be sure I'm clear right so containers introduce another layer into the the paradigm right and what you're asking is how do we secure at the container layer versus at the dock or host layer for example so right now we do that through the docker volume plugin and the authentication mechanisms that are inside of there and by that I mean so if i have my daugher volume instance plugin instance right I have given been given a username that i authenticate with against cluster e8 on top for example right you have yours I cannot see your volumes because we are using different users now from an export rule standpoint and things of that nature it's still being done at the host level and this is where things like Magnum are really really useful right because if we're separate tenants your Cobra Nettie's swarm mesos cluster right are going to be instantiated on your Nova instances associated with you mine will be with mine right I don't know that they exist separately so it's not one of those the products themselves we're not changing anything with how that's authentication mechanism works it's more relying on that layer above and simple regular authentication authorization at the user layer with storage virtual machines or we have the same concept in SolidFire e-series is slightly different but not significantly alright well thank you very much everybody and have a great rest of the day 