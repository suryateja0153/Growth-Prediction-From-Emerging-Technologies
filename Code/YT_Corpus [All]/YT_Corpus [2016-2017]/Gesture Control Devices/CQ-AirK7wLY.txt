 my name is Vikram Iyer from the University of Washington and today I'm gonna be presenting finger.i oh this is joint work with Raja Lakshmi Nanda Kumar advised by Shawn kala Kota and Dez neat and so I'd like to start off by oh that's not good let's go back so I'd like to start off with a question there we go so I'd like to start off by asking a question can we achieve finger tracking for near device interaction with absolutely no instrumentation of the finger and without any need for line of sight to it if we could do this we could think of all kinds of exciting applications that we would enable on smart phones and smart watches so one example that we actually built her a few examples that we built are you could imagine putting your smartphone on a table and then using it to track your finger motion even when it's not on the screen of the device and without any additional hardware you can turn anything into a drawing surface imagine this would be even more useful on a device that has a really tiny screen because now instead of trying to draw something on your SmartWatch you could make gestures on your skin or perform some kind of gesture in midair to control the device and lastly if you don't need any line-of-sight then you could even perform interactions in the presence of occlusions so let's say I could put my phone in my pocket and then do some simple gesture to change the volume so this is a problem that's interested the HCI community for some time and recently people have been exploring radar solutions to this problem one of the notable examples of this is Google's project soli which uses the Doppler shifts in very high frequency signals to to identify certain gestures but from a technical perspective while this is very interesting it has two key flaws it prevents it from being adopted on mobile devices the first is that these radio waves are traveling at the speed of light and so in order to get finger or centimeter level finger tracking or any kind of centimeter referent level excuse me centimeter level resolution you need gigahertz abend with that you need a process on some small mobile device and this just adds a lot of complexity and isn't really practical the other problem is that you actually need this radio that's capable of transmitting your high frequency signals and so that would mean adding yet another radio to your mobile device so chardee has bluetooth Wi-Fi whatever else and i would consume additional power area etc and that's something that all device manufacturers really want to avoid so with finger IO we take a different approach and we demonstrate actual finger tracking not just classification of certain gestures using no instrumentation of the user and without the need for line of sight to the finger we do this by introducing novel algorithms and techniques to transform an ordinary mobile phone or SmartWatch into an active sonar system without the need for custom additional hardware we demonstrate this system is capable of achieving an accuracy of 0.8 centimeters when we tested on a samsung galaxy s4 and 1.2 centimeters on a smartwatch prototype when building this system we faced two key challenges the first was how do we even transform an ordinary mobile phone or something into an active sonar system and the second is using that system how can we get sub centimeter level finger tracking I'm going to talk first about this first challenge of how we can build a sonar system using a mobile phone so at a high level the way that sonar works is by transmitting out pulses of sound waves and then measuring reflections of these sound waves off of various objects in the environment so in our case we use the speakers that are built into a phone to transmit these these audio signals and then we measure the reflections of these off of the moving finger on the microphones that are also built into the device so when the system is running the speaker is periodically emitting these pulses of sound and then what we leverage is the fact that as the finger moves the amount of time that it takes for these pulses to reflect off of the finger and arrived back at the microphone is going to change and so we also know that even though there are all kinds of things in the environment that are going be reflecting sound the finger is actually moving the and so over time the amount of the arrival time of these echoes is actually going to be changing and that is what we're measuring so the question here is how accurately can we resolve the arrival time of these echoes because sound travels much slower than the speed of light by sampling at 48 kilohertz which is available on most mobile phones we can achieve a timing resolution of one sample that corresponds to a distance resolution of 0.7 centimeters now because we have two microphones on a mobile phone we can combine these two measurements to achieve 2d finger tracking with sub centimeter resolution so now that I have explained at a high level how sonar actually works the question is can you achieve sub centimeter accuracy in practice I'm going to talk a little bit about this now and some of the challenges that we faced here so the key here is we need to accurately identify when an echo arrives at the microphone and so a first-order solution to this would be to transmit a signal like a chirp that has a very high autocorrelation then we can correlate the receive signal with the transmitted signal and use that to get the time that it arrives so here's an example of what that might look like when we do this correlation operation mathematically each of these Peaks corresponds to an echo arriving at the microphone as I mentioned before we can see there are many of these Peaks in the signal and those correspond to all the things that are reflecting sound at the environment but again we are interested in measuring a moving finger and so the key assumption that we make is that the closest moving echo is what corresponds to finger and so now we can look at an example of what might happen as we're actually tracking a finger here we can see that this echo is clearly moving closer to the phone and that's what that's the finger movement that we're actually tracking so I tried implementing this this correlation technique but with various different signals that we tried we were only able to achieve an accuracy of two to three samples with which we could resolve the survival time and that translates to an accuracy of about three centimeters and while that might sound really great and is better than what others have achieved with these systems the problem is that 3 centimeters is still about twice the width of your finger and so we needed to figure out a way to get the exact arrival time of the neck of to get sub centimeter level resolution to solve this problem we look to wireless networks for inspiration specifically if we look at Wi-Fi you can see that you have a transmitter and a receiver that aren't synchronized in any way they don't share any kind of common clock but somehow the the phone or whatever is receiving Wi-Fi packets has to figure out exactly when a message starts in order to successfully decoded the way that Wi-Fi solves this problem is using a technique called orthogonal frequency division multiplexing or OFDM for short I'm going to explain a little bit about how OFDM actually worse and how it achieves good timing accuracy so for OFDM you start out with data and the frequency domain in our case we don't really care about something data but we can just take random values and we can take an inverse Fourier transform of these frequency values to generate a time domain signal that we could actually transmit from our speaker now the key to leveraging OFDM for achieving timing accuracy is to take the first part of that signal the first few samples and simply append it to the end so intuitively we now have a signal that over this time period looks somewhat periodic and if you think back to any kind of basic signal processing you've taken remember that periodic signals have all kinds of useful properties that we can exploit in the frequency domain specifically we note that timing errors are going to correspond to phase offsets when we take a Fourier transform let me walk you through exactly how this works so if we start out with the case when we know exactly when a signal arrives we take a Fourier transform starting at exactly the beginning of our OFDM symbol then if we look at the phase it's going to be perfectly flat and constant over the different frequencies but then if we have just a one sample error then we're going to see a slope in the phase it continues across the different frequencies which is a well-defined self of two pi in the case of a one sample error now if we increase this error to two samples then we suddenly have a slope of 4 pi so there's a linear change in the slope of the phase whenever we have a time offset and so using this we can correct any kind of inaccuracy we have in estimating the beginning and get the exact time of arrival so now to put this all together as I mentioned before we're going to be continuously transmitting these pulses in our sonar system and in this case we choose our pulses to be OFDM symbols we designed these to be in the range of 80 20 kilohertz which is above the range of adult human hearing and we transmit them we space them apart by five point nine two milliseconds we chose this number because that's the amount of time that it takes for an echo to go from the phone up to a distance of one meter and back and that's the maximum range that we were interested in looking at so the second thing that we do is to get a course estimate of the timing we can we can use correlation as I mentioned before and that'll get us to within two to three samples of the actual arrival time and then lastly we can leverage this phase property that I just talked about to get the exact time of arrival and get our sub centimeter level accuracy so now I'm going to talk a little bit about the evaluation that we did to see how well this system performs in practice the first question that we really had was how accurate is finger IO if we look we did a user study with 10 different users and we asked each of them to draw a pattern of their choice on the screen of a smartphone and at the same time that they were drawing this so we had our reference we ran finger i/o on a separate smartphone those placed 25 centimeters away we compared finger Aya's estimates with our reference drawing on the smartphone touchscreen and saw that we had an agreement of 0.8 centimeters this is a median value and the the maximum error that we saw across all these experiments was one point two centimeters if anyone's interested in the exact distribution we have this in the paper we also did these measurements at different points in 50 by 100 centimeter grid around the phone and we found that these results were pretty consistent I'd like to show you now some of the examples of things that users actually drew we can see here that the in in black we have the reference drawing that was taken by the smartphone puffs screen and then in green we have finger IOT's estimates I'd like to note here that the x and y axes are actually in centimeters and so these are very small drawings where if you didn't have centimeter level accuracy you wouldn't be able to get anything that's close to reconstructing the drawing we also wanted to test how our system would perform on a smaller form factor device like a SmartWatch unfortunately there aren't any smartwatches on the market that have two microphones so we built our own SmartWatch prototype to see how this would perform in our case this is just a simple plastic case that had two microphones in it as well as the speaker and some recording electronics we did the same kind of experiment where in this case we had users where our SmartWatch prototype and while we did this we had them draw a shape on a smartphone touchscreen so we had reference when we compared these measurements we got an accuracy of one point two centimeters over an area of 25 by 50 centimeters we note that the accuracy if the SmartWatch wasn't quite as great as a smartphone this is most likely just because of our recording electronics and some we didn't really optimize a system for a noise performance or anything like that we're confident that a future solution could achieve better accuracy similar to a smartphone so the last question that we had one of the practical challenges that we have to address in the system is how do we figure out when a user is making the motion that we want to measure if it's going to be constantly tracking a finger to do to address this we design a specific gesture that could be used to start and stop this system so here the gesture that we chose just a simple swipe to the right and then a movement to the left that occurs very close at a distance of less than five centimeters from the phone we're able to identify this with greater than ninety percent accuracy so then our question became how many false positives do we have when we compared this gesture to just random a finger of motion we had our 10 users each draw just random patterns for a period of 1 minute each and then we analyze that data in both the case of the SmartWatch and the smartphone just see if we ever falsely identified this start and stop gesture in the case of the watch we had zero false positives and in the case of the phone we had two that we're both from the same user which suggests that there isn't any kind of systematic error here so I'd like to conclude by summarizing finger.i owes contributions we demonstrate tracking a finger with absolutely no instrumentation of the user and no line of sight to the finger we do this by introducing novel algorithms of techniques to transform ordinary mobile phones and smartwatches into active sonar systems without the need for additional hardware we hope that finger i/o enables all kinds of exciting new directions for research in this space and that others can or that we as a community really can build on these techniques to achieve even more exciting things like tracking multiple fingers or complex 3d gestures in the future thank you okay so we have time for questions do we have any questions yes hi my name is Evans Jessica from Stanford University um thank you so much this was awesome I was curious if you ever value ated or did any evaluation of the system for a user in motion it seems like you're a primary assumption that the finger is the closest thing moving might break down at that point right so at this point we haven't really tested a finger in motion but we're pretty confident that using some some additional techniques by leveraging other sensors on the phone like an accelerometer you may actually even be able to get better performance because you could think of then if you had measurements of exactly where your phone was you're essentially creating an antenna array type of thing where you can combine measurements from multiple different points in space to possibly get better thank you hi Gregory about I'm confused here because I don't understand your line-of-sight point because if you had the phone sitting here and you had another phone like that and you were moving your finger like this there's no way this would work right so what what we're talking about for a line of sight is really line-of-sight through anything that sound can pass through and so that's going to be dependent on you know the material in our case we were really testing for things like fabric if you had a very loud speaker for example you might still be able to get that goes okay yes you just need to use a different term than line of sight sure okay Daniel ashbrook Rochester Institute of Technology I'm curious you restricted the range in which you could operate this but I mean wondering actually how far away you could be and still successfully use this yes so the range choice that we made was it was specific to our implementation in principle you could use these same techniques would say a louder speaker or something at a greater distance we chose the 1 meter distance as a nice trade-off between where we could detect our minimum signal level and you know as a way to ignore any any kind of thank you I Drive from CMU so in your experiments where the motions of the fingers always moving or the users stop at any point were there any differences in accuracy when the finger stops just from my understanding the the tracking is a dependent on motion so so what we were tracking was continuous patterns if the user stops then our system doesn't actually track that because we're looking at moving echoes like I mentioned so you can we at what we do is we actually threshold our signal to figure out when you know we actually have a moving echo right so when a user stops we would just keep reporting that same point the other problem is that we because we don't have three microphones if you take your finger off the screen if that's what you're referring to we can't resolve this in a future system with an additional microphone so this is there a drop in accuracy when user stops no no so we would just continue reporting that same point because really at that point you you know you have a drawing that just stops at one point right I see so I guess have you done any tests where the usual the finger stops and then there's a report of any like change in the signal that's being measured well the users because you know it's some gestures required to stop or some registers preparing to move but in my from what I understand there is a requirement of motion right so again the the way this system would work is that when a user does stop we just continue reporting the same point and so if you're interested in like you know a specific gesture that stops for a period of time then you could look at you know when when this point stays constant over time and then the user would start moving again then you would start tracking different points does that answer your question sure thank you just about though as I use a solar so is this finger i/o sensitive to environment noise if so you do we have figure out this problem right so we actually did some evaluation of other noise in the environment in the paper and as I mentioned before the the way the system works is we do there are going to be changes in the environment but we're specifically looking at a moving finger so some change that's happening over time in these echoes that were measuring and specifically we look at the closest changing echo and that's what we identifies hi Cory Pittman in Central Florida how well were you able to isolate the fingers movements from the rest of the hand so when you were running a user studied you have to tell people don't move your hand wired like dope I'm sorry focus I'm moving your finger and not moving your entire hand because that would be tracked as well right so for for the applications of we were looking at where you're actually drawing something because you know your fingers really connected to your hand the dominant motion that we would be measuring would still follow the same pattern so okay I could still be an issue in some cases that people wanted to just draw it small so I can't do something like tracking the exact hand pose with the limitations that we have right now but future work could definitely expand okay thank you 