 an underground student from tsinghua university and this work is dumb during my internship at Carnegie Mellon HCI ways the you become group and with my cursors hooli and any and day and so I'm glad to be here today to present our work serendipity a finger gesture using recognizing system using the off-the-shelf SmartWatch so interacting with wearable devices are still very frustrating today due to this relatively small screen size and also the fact that finger problem so researchers has been working on trying to expand the injection space beyond limited screen size and the traditional touch interaction so for example in the figures with so Chris assonet or they try to use a magnetic sensor and so that users can perform those touch gestures about the touch screen instead of attaching actually touching on the screen and also so researchers has used other sensors like a distance sensor so they can send some twist twist gestures on the watch instead of touching on the screen and I believe that in the first talked in our session Sherman gives us a very pervasive understanding of this work on how people expand the interaction space and also previous always available interfaces have targeting 5 Franklin gestures using specialized sensors and this increases the cost and decrease the wearability of using them for detecting fine grand gestures and disadvantages most motivate us to use a approach that really available and easy to install so motion sensors such as the accelerometer has been a research in previous work and they the researchers use them to recognize noticeable gestures and gross hand motions such as a gesture shown in the figure and these gestures are very obtrusive and cannot be performed without drawing attention to oneself so in our work we leverage motion sensor that are rarely available on the SmartWatch for example the excellent accelerometer and gyroscope to sense fine green gestures we posit that the muscle activity differs when performing these finger gestures and the causing different type of vibration and motions on the waist that could be captured by our motion sensors on the SmartWatch so this video shows the gestures we are trying to recognize hope so so the first one pinch and tap finger Rob fingers squeeze and wave fingers so we used an off-the-shelf samsung get gear watch and direct organized these five gestures we target this gesture because we want to support really short and short and easy interactions on the SmartWatch so this gesture should be performing really quick and easy to perform with symbolic meaning that can be mapped to certain functions to control our UI on the SmartWatch for example the tap finger can be mapped to clicking a button and the rock fingers can be mapped to scrolling a page also fine grain gestures recalls the list motion and to perform this gesture so that they will cost really a little physical or mental effort to the users so in a pilot study we will recorded data from all the motion sensors available through Android API the accelerometer gyroscope gravity and rotation sensors so we continuously click data for a length of 10 seconds for each gesture with a sampling rate of 50 Hertz and we use the one second sliding window to perform statistic and identity feature extraction we did not observe any distinct pattern from this from the rotation sensor and the gravity sensor which are on the second row second row of the figure and but we did observe them from the accelerometer and gyroscope so in the final design of our system we use the data correctly from the three sensors in the first row of this figure so next hour introduced how our system would avoid false positives and classify our gestures so the first step is pre-processing we collected rodentia data for the three axis the XYZ of the three sensors and then with to compress in four different orientations wish I were introduced in a few slides later and we calculate the magnitude of the combined sensors for example the the X the square root of x squared plus y squared so we so we refer to it as X Y and similarly we do this for Y Z X Z and X Y Z so altogether we collected seven statistic feature from the one second window and then we perform both fft transfer fast Fourier transformation transform on the out on the window and produce 25 power bands and then we keep the lower 10 bands because this beverage should be sufficient enough to cover the frequency feature of our gestures and so for more details of the pre-processing process please refer to our paper so then we try to reduce the number of false positives without requiring an activation gesture so we implemented an algorithm based on dynamic term whopping and k-nearest neighbor so to distinguish the noise from the gesture we can calculate the dtw distance and then select the nearest sample as the candidate gesture so if the distance is within a threshold we would infer that a gesture is performed and then we will use our classifiers to classify this gesture and otherwise if the distance is beyond these threshold we were defined that this is a noise so the casper will not react to this data so then we use a supervised machine learning approach to recognize the gestures we test performance across a few basic classifier for example the support vector machine and the native base and also a logistic regression overall the SVM the SVM achieved the best result across all users in our pilot study and during the real world usage we our system can try all the classifiers and then automatically select the best classifier for and the features for each user so we conduct a ten participant lap study for two to evaluate the effectiveness of our surrendered system so participant finish a two-session data collections study on several days and we did not require the position of the device to be exactly the same on this to session and across all the participants also the tightness of the band is not a was adjusted according to their own preference so that this would mimic the real world use of the small watchers and in this study the participants perform the gesture in different orientations that we try to capture variation of these gestures as much as possible so we choose these three orientations that exemplifies really common use cases as shown in the figure also we collected two more extra data to two-minute extra data for a false positive detection on both sessions so in this two-minute the participant would perfect with wearing their watch while they are typing on the laptop or talking to other people as long as they not perform the gestures so then we use this data for our false positive detection and the result is shown here as this graph show that the performance across our ten participants so we use the f1 score for measurement and the mean f1 score across all participants is eighty-seven percent we notice that the performance of participant 575 nine and ten is lower than the others because this participant they perform certain gestures differently in the two session and so this indicates that our participant may need more practice before they can perform this gesture constantly and this is the coefficient matrix of the accuracy of the gestures shown in our experiment we find it that rub finger output from all the other gestures as it is very distinct in the frequency to perform this gesture also pinch and tap were confused with each other most often because the movement involved in these two fingers are really relatively similar so by introducing also by introducing a rub finger for three times as an activation gesture we achieved 0.38 false positive rate and five five percent false negative rate so after the data collection session we several of the participant give us some informal feedback of the gestures so some of the some of them indicates that they prefer gestures of larger scale like the squeezing instead of the pinch because this kind of gesture gave them a sense of control and and not much effort to perform this gesture and also some pointed out that they would rather have a relative it's more a small gesture set as long as they are there are some gesture that their favorite and they function robustly so in conclusion we demonstrate that our system can detect five finger gesture with an average f1 school of eighty-seven percent so besides prefer improving the classification accuracy and expanding the range of gesture we can recognize we will discuss the ideas that so how how we can expand our work for the finger gesture recognition so the first one would be going mobile so in this work we explore the feasibility of using the motion sensor to recognize gestures in three orientations so it'll be really interesting to know whether we can put this to mobile so whether we can also recognize them when the user is in the move and also we seek other several uploading applications for example we see a large application area for our work in the monitoring health and well-being in particular we would like to explore whether our techniques could be used for detecting finger and hand motions to detect some diseases like Parkinson's and also we would believe our gesture will be used for in the cross device situation for example we can apply this gesture to improve interaction with our mobile phones or head mounted devices for example if a phone call comes in we would simply the kind of phone call by waving our fingers and moreover we envision that our finger based gesture interaction technique would work well too to cooperate with the head mounted advice like Google glass and so maybe whole and even and this provides a less obtrusive approach for interaction with this device in public areas ok thank you for listening and I'm happy to take questions right so so what we use in this experiment is 50 rows of sample rate which is the wrong stable so actually you can get a fast as 100 hairs or even 200 so so you were but you need to do a further processing on that data so but 30 hers is the stable recently we can get okay thank you 