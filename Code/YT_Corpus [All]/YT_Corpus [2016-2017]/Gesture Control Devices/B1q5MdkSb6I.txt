 so my name is June I'm from University Glasgow and like you just said you couldn't talk about do that there so when users want to use matter gestures to interact to something one of the first things they have to do is find where to perform their gestures if they don't Jack you're in a place where the sensors can see them then that actions are going to have no effect on the device so they merge their show is an example of how that may happen the user wants to gesture towards their phone on the coffee table but your hand doesn't defend the sensor space for that device unlike other types of input like touch screen and port or keyboard and put the Madeira gesture sensing has a bet for more vague and ambiguous boundary between when users are interacting with the system and not interacting with the system also uncertainty about where the sensors are located and what you can see can add to this problem know that more devices and technologies are able to sense gestures and movement there's also the problem of having many devices within the same space and sensing and put it same thing and the situation shown by them is there there's also the chance that the users gestures will be sensed by more than just the device that they want to interact with and also and there's a risk of ordinary movements being treated as gestures unintentionally it's a for example if the person in their binge there was to reach forward and pick up the phone there's a chance that their TV for example might think that that was a gesture so these are problems are carving user suggests m'dear gesture systems a draftsman a gesture system is when you initiate interaction with it and it's the first step of interaction which happens before users actually do anything to control the system as mentioned in the previous slides for the usability problems that might occur at this stage are the user gesturing in the wrong place gesture is affecting the wrong system and ordinary movements have an unintended effects on a device those last two are more commonly known as the major steps problem so one way to overcome these problems is by giving users feedback a common way of doing this is to visualize what the sensors can see on screen like shown there so the user might see something like a celery or hand or their assembly after body which they can use to adapt every gesture so for example if they weren't gesture and in a very good place then they might move their position in front of the sensor so that their movements are more likely to be picked up but what about devices that are unable to give users that same level of feedback rich sensing capabilities are being added to increasingly smaller devices devices like mobile phones and small household objects and they have small spins or no screen at all on which to give users feedback so in the case of mobile phones for example we can interact with them using gestures but we already struggle to fit good interfaces on those small spins information there is already quite cluttered so providing even more is just going to add to that problem also those small screens might be that good to see from quite far away how so with interfaces and devices may also have small screens or no screen on its to get feedback so we need to thank you for not already to do that so be investigated interactions for address and gesture systems focusing on devices like these with limits its display capabilities but the interaction techniques could also be used by pretty much any matter gesture system using TVs or large displays for example so we considered the use of interactive light feedback this uses simple light sources to display information these could be small lights embedded and devices like the mobile phone shown on the left there or they could be separate light sources like the lamp shown on the right this approach allows devices to illuminate the area surrounding them kathan ad hoc displays for given feedback in case of mobile phones this keeps the screen free for a human contact feedback can be sure and over a much larger daily esterdome than the device for simple objects we know display at all with SLO special feedback to be given when that would otherwise be impossible we also considered using tactile feedback so tactile feedback during madera gestures can be challenging because the user doesn't actually contact the device that their gesture or not so we use viber tactile feedback from wearable devices devices like smart watches or activity trackers which have biscuit a theory which we could use to prevent information during other interactions finally we also used audio feedback because it's an easy modalities to provide in terms of technical challenges so we developed an interaction technique called do that there which can overcome some of the usability problems which I spoke about earlier and it uses the three types of output which I just mentioned this interaction has three parts do that which tells users how to detect that input towards the system and there which helps them find where to perform gestures are now talked about each part separately starting over there and then bring them both together later on so the data interaction helps users find where to perform gestures so that their input can be sensed more reliably it does this by giving users feedback mm how well they can be seen by the sensors so for camera based systems this might be better when the user is closer to the center of the field of view and also an appropriate distance from the sensor let's type of feedback also lets the user know that they can be sensed and the system is tracking them and pay attention to them so we use brightness of light to show users how well they can be seen with the light becoming brighter as a gesture than a better position or do you in tactile feedback use that Geiger counter metaphor with repeated tones becoming more frequent as they approach the better position for perform and input these designs map just one dimension of information for feedback so it doesn't give explicit instruction or spatial cues like move left or move closer so the ideas that the users have to explore the space themselves and form their own understanding of how to have more successful interactions the paper has more details about her be allowed to these designs so our first study evaluated this feedback to see how effective it was for gauging users we did this use now targeting tasks where participants had to find target points positioned over a mobile phone LEDs around the edge of the phone gave light feedback with tactile cues being delivered through an actuator worn on the wrist audio feedback came from a loudspeaker so look to each of those feedback type separately as well as offer combinations the target task is shown on the video there was the users had to explore this space over the four news an extended input finger using the feedback to guide them towards the targets and when afforded find that the use that her hand to press a button tend to task so we found that users could locate points with 51 millimeters accuracy find that feedback modality had an impact on the accuracy with better performance and conditions included audio feedback at the same time these conditions were also slower than those of five audio for light feedback was given accuracy was not as high as if I do oh but then factions were quicker and performance was best when these types of feedback were combined so some participants explained that they would use the different types of feedback for different things using their light to initially position your hand before Horning and using the audio or tactile feedback so next week they do that part of interaction this tells users how to directed and put towards a particular system or device this helps to overcome the Midas touch problem because the user shows which system they want to interact with and also shows an intention to enter that as an example why that's necessary the image there shows how multiple devices might be sensing input from the same space potentially all formats and upon any movements that a user makes by direct an input towards one system in particular the others know that any actions that follow are not intended for them so if s interaction and users to that that input using something which we call rhythmic gestures rhythmic gestures are gestures are repeated and time fed of them sure when using an animation those animations might be sure and using something like the LED display shown on the previous slides or they could also be shown on-screen if that was available by combining different hand movements with different movement speeds we can create quite a large design space just from a few simple gestures so we chose five hand movements for the gestures shown on the screen there that was waved in the hand from side to side moving up and down forwards and backwards and moving in a circular path we also selected four speeds for those gestures which gives a total of 20 gestures so by assigning different movements and speeds to different devices users could detect that input towards one by performing the appropriate gesture for example if they were to direct them towards the mobile phone there did move your hand from side to side and tell me the animation is it moves across the table effectively users are using this Tameka selection these are examples of what the animations might look like if we were using a simple circular LED display around the dial basically the movement of the light shows users a movement that they need to make as well as the speed at which they need to replicate that moving on so a second study evaluated hair where users could actually perform the sniper gesture we wanted to see if they'd be able to move in time with the animations and at different speeds and we were also interested in what role feedback adds on their gesture performance so we get four types of feedback about gestures one just used the LED animations to show them how to move the other free gave additional audio and tactile feedback which was basically given a short torn or a sound at the end of each part of that individual gesture the details about heavy came to that's in the paper as well so the task in this experiment was to march out of MIT gesture which means to perform the cadet gesture and time with the animation for a minimum period of time that was approximately three full movements and sync with the animation we give users a limited amount of time in which to complete each task and we measured the success rate of that how long it took them and we also gotten forgive difficulty Nathan's for that as well unlike in the previous experiment we used a wall mounted circular LED with the Kinect for gesture sensing so users performed almost all different gestures successfully the success rate was 93% success was varying across gesture movement and temple Razak paper being presented tomorrow by another group called passing which presents a similar interaction technique and they got very similar success rates as well which I thought was quite interesting in our study the two circular gestures performed poorly especially the fastest movement speeds which are highlighted in red and contrast the safe to say than up-and-down gestures were perform the best if almost 100% completion rate and the path length paper they also found that circular movements were not performing as well as more rectangular trajectories so there might be a interesting reason for them generally different gestures with the store movement speeds were more successful with the success rate the kissing is the gestures got faster results showed that feedback had no effect on gesture success so participants didn't perform any differently even given non visual feedback about their movements so users took just over two seconds to match each gesture with timings their normalized in the 500 millisecond interval feedback had no effect on time but as the graph shows interval and movement dead so gestures with shorter intervals are faster movement speeds took longer to complete with the most noticeable difference with the shorter interpreter the circular ones also highlighted in red took longer to complete the Netafim movements so finally we combine the two interactions into one technique called do that they're finding where to gesture and directing and seeing a direct an input towards a system can happen at the same time and the interaction would be quite cumbersome and time-consuming of users had to do these separately more experienced users are also more likely to know where to perform gestures for a system so they could just skip this step and go straight to directing the benefit instead so if the conveying technique users gesture to directed input while also using the feedback from the system to adapt where their gesture do so that they can be more accurately sensed as a video shows their the gesture animation changes in brightness based on where the users moved in their hands so when when the animations at its brightest is there a gesture and in a good position so each of the entered actions has its own the peasants own feedback which could be combined in different ways one option is to just give both types of feedback in our modalities at the same time so if that's the gesture animations would be brighter when users gesture in a good space and more difficult to see vaner gesture and in a bad place the audio and tactile feedback could be delivered at same time as well an alternative to this is to use different modalities for different types of information so since light is needed to show the rhythmic gesture animations for direct and input we could use audio or haptic use instead to help users find 30 after using the feedback which we used in the first experiment so we looked at less the same problem in the final study here itself got onto in a second another design question when designing these techniques is how much feedback the users actually need to tell them where to gesture so once users have actually started performing gestures and they've started to interact with the system they might not benefit from happiness extra feedback which tells them where to gesture instead that might be sufficient to stop giving that feedback once a gestures and progress so these questions in mind we came up with five feedback designs they're explained in more detail in the paper save time these designs were dollars to investigate two questions and our final study the first is how should we present two types of feedback using multiple modalities so should we combine that across them or divide the information between the two and do users benefit from feedback telling them where to gesture all of the times you're an interaction or able to be sufficient to stop that once you start to perform them so efforts that they investigate that our users could use this interaction technique as well as investigating the effect of the different feedback designs mentioned before this was basically a combination of the past two studies so users had to match it up make gesture by also performing as close to a target point as possible there are two factors in this study which are a feedback and gesture so which was the two FS gesturing the last study which we're moving the hand from side to side and moving the hand up and then and we had those performed that there's 700 millisecond movement speeds because that that quite well before so in this experiment users max 99 at 99.9% of gestures which is similar to the completion rate for those gestures in the previous study they also gestured with an 80 millimeters of the target point it took from just under four seconds to find the target point and complete a gesture and that four seconds includes just over two seconds spent matching the gesture animation interaction times were the losses when the feedback is combined across modalities this might have been because the visual feedback directly affected the visibility of the gesture animations so that made a further more noticeable effect when users were trying to follow that so for example that means if your hand was in the bad position the animation would be more difficult to see so it compares users to find that space before they actually start to provide the input they also found that users did not necessarily benefit from getting feedback mm where to perform yes to resolve the time so they highlighted results they're sure the designs that where the feedback stopped once a gesture started there wasn't that much different to the other designs right it's given throughout the entire interaction so when somebody are do that their interaction seems to be successful users completed almost all of the gestures and the dead soul with an 80 millimeters of the target points and that Freud study that's quite good considering they were gesturing with quite large arm movements from the opposite side of the room several meters away interaction time is good as well users found where to gesture and match the Kinect gesture offend about four seconds that is they were able to address the system within four seconds in terms of feedback design we found that it's better to combine feedback across all three modalities rather than getting different types of information from different ones there seems to come down to the more noticeable effect of the light feedback when the animations as I mentioned before we also found that we could stop giving feedback telling users were to perform gestures once they had started prevailing through sorts of concludes users need to address my dear gesture systems so that their input can be sensed and so that their gestures only affect the system they intend to interact with and this paper represent do that there which is a technique for addressing med their gesture systems this can be used by a variety system form factors without needing the screen for feedback this means that it can be used by typical gesture systems for large displays as well as emergent gesture systems like mobile phones small objects or encar interfaces devices which are more limited than our ability to provide feedback but which increasingly have richer sense and capabilities thank you any questions hi thank you for the informative talk this is Kai from UMBC I noticed between study 1 and study 2 you went from a single finger gesture to an arm or a whole hand yeah did you see any reduction in accuracy from the single finger which required you to go to a larger area No so the reason we looked at those different types of gesture was to covered a broader class of devices so not just focusing on mobile phones but also small objects which you may interact with from the other side of the room like like switches music players thermostats that sort of thing so in the first study we found that users performed with I think it's 51 millimeters accuracy for those small finger movements and the accuracy with a much larger tower movements and farther away is 80 millimeters so there wasn't that great a difference between the two there's there was any reason we made that switch that's just part of the design to cover those but yeah they're actually similar yeah so give me in terms of their participants yeah yes so the one thing we unnecessary acting with the one device so you know something I'd like to do in future is actually have multiple devices they are external animations at the same time and see any of those competing animations would be distracting and affect the input research from psychomotor studies on sensitive motor synchronization which is moving in times with basically led animations like this suggests that distractors have a small but not not much for noticeable effect on that so I would guess there wouldn't be a problem but I think that research dough needs to be done 