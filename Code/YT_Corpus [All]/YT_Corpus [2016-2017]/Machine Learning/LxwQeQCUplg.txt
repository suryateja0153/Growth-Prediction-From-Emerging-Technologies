 good afternoon my name is Cory Alston and the CTO of financial services for Google cloud platform we're very lucky this afternoon to have Matt Taylor with us Matt's the CTO of kensho now Matt's going to do a proper introduction to himself and to Ken show in a few minutes but I just wanted to start off by saying that Ken shows an extraordinary company that's pioneering real-time statistics and analytics machine learning for financial services now Matt and I this afternoon are going to talk about Ken show on Google cloud platform machine learning in your browser and we've got a real treat because Matt's come up with a custom tensorflow machine learning model based on Ken shows data that he's going to demonstrate for us now we're gonna split this presentation into two portions I'm going to lead off relatively quickly and I'll tell you about why Google cloud platforms the best place for you to do your analytics and machine learning not just a good option or a good place but the best place for you to do those activities Matt's then going to take over and he's going to talk about Ken show and their machine learning but I just realized I made a mistake when I was preparing for this I didn't sort of take into account that we'd be doing it right early in the afternoon so I was lunch for everybody a good lunch everybody get ready for the sessions this afternoon and San Francisco this evening the thumbs up for lunch all right so I sort of transparently recovered from my mistake and now we can go on to some machine learning modern machine intelligence a branch of computer science that build systems to perform similar functions to human intelligence such as learning reasoning self correcting and adapting and it allows us to do some astonishing things just within financial services which is my specialization it allows us to manage risk much more effectively it allows us to surveil activity much more effectively it's essence it promises to be the piece that was missing for so long where people thought that financial markets was so inherently complicated that they couldn't be understood but you've seen again we've seen the presentations where machine learning tensorflow Google cloud platform give us the opportunity to bring that understanding to very high dimensionality data and of course the caveat is that there's more and more data the amount of data is increasing exponentially the the need to make use of that data is becoming ever more pressing so we need to scale sometimes enormous lis and we also need to move very quickly an experiment and break stuff very regularly and that's where Google cloud platform comes in to give you scale and the ability to iterate which leads us to a premise the architecture for analysis and machine learning I'm not going to spend much time at all going through the boxes on this slide apart from saying that each of these services is fully managed no operations each of them offers best functionality wonderful price and performance what I do just want to spend a couple of minutes talking about are the things that I built over the last 12 to 18 months using these building blocks because that's where the rubber really hits the road and that was the context that I wanted to share with you what I've done can inform what you're gonna build going forward and hopefully it will inform what we'll build together number one I built a proof of concept using dataflow that took 10,000 30 year time series of financial data and correlated everything by everything 50 million correlations of 30 years worth of data three minutes at 300 thousand correlations per second compared to running out on a 16 core server where I turned it off after 12 hours because it was just if he doesn't finish in 12 hours that effectively sort of goes towards infinity to me it was very boring to wait for it I built a proof of concept using pub/sub dataflow cloud storage and big table that ingested a simulated market data feed of all North American market data in graduations of a quarter million events per second did that from pub/sub and I also did it as files took about two days to code that up and it took zero overhead in setting it up and ringing it down interestingly I also built spark clusters to do a grid search of that data using data proc those clusters of 25 servers took less than a minute to come up took about 45 seconds based on my testing which means I was able to do that grid search by instantiating and then tearing down whole clusters of machines finally and recently we built a machine learning model that spans spark'd ml and tensorflow to put an ensemble together to see how those things worked and with that I wanted to hand it over to Matt who's going to talk about what Ken chose doing thank you very much Thank You Cory our partnership with Google on GCP has been a real pleasure I think one of the things that attracted us to Google was the collaborative nature in which we get to develop our products he's also done a fantastic job of kind of showing off the raw power that GCP provides to us so today I'm gonna give you a personal look under the hood of kensho what our mission is how we think about the world and most importantly we'll walk through a machine learning experiment together letting the computer identify what days are like this so kensho is a high-end analytics platform for the financial industry think machine-learning powered hedge fund analytics delivered via the browser if you work at a major bank or a major hedge fund you've probably already aware of our platform if you've watched CNBC you've likely seen the kensho stats box where their anchors use our platform to analyze events in the news while we deliver hedge fund caliber analytics we're not a hedge fund and that's one of the nice things that I can do today is as a start-up as a technology company I can come and give you a peek under the hood of how we treat financial analysis this is making sophisticated financial analysis more accessible intuitive and beautiful now as part of a kensho team and a back-end nerd a data nerd at heart I'm passionate about deleting code and using technology to understand the world machine learning accomplishes both of these goals I joined ten show in 2013 and I now quarterback the engineering data science and design teams and every day I rely heavily on my experience building low-level financial infrastructure high frequency trading and delivering millions of cat gifs at tumblr kensho and our machine learning on Google cloud platform is the unity of these goals make it smart make it fast and make it beautiful now founded in 2013 kensho like almost like any almost three-year-old were curious about the world we come with few preconceived notions were eager to learn and we adapt quickly to new information we do this via search discovery analysis knowledge graph event database we deal with single assets macro events geopolitical weather and today we're going to ask the machine to identify days like this so like any good experiment we are going to start with a testable hypothesis we're gonna plan for our experiment we're gonna structure a specific test and we're gonna let the Machine drive our analysis then we're gonna feed it back into the kensho platform and see how the markets react so if you've read a random walk down Wall Street and believed in efficient market theory you probably think that the answer is no if you recall the financial crisis and the Great Recession you might think asset prices can be grossly mispriced for prolonged periods of time you might think the answer is yes using market events and other features can I gain insight into what may happen tomorrow let's find out what the computer thinks in order to maximize learning there's two things that we really need and for hungry lean companies the most important question is what is the most valuable thing I can learn right now and the two things I need our resources and proper timing so I want to take a step back for a second and talk about those two things to make sure now is the time to act now for resources I borrow a lot from the same challenging questions that face our own education system about educating our best and brightest to keep America competitive and across a global marketplace you need physical safety to explore and experience what the world is around you you need resources to fuel and encourage and to grow with you you need a community of mentors guides and helping you correct your mistakes you need a culture that encourages you to learn from mistakes to try lots of things and to fail quickly but above all you have to always be learning and if we apply this same approach to how we teach our machines and then how our machines teach us right data center security CPU disk and network capacity unparalleled access to api's data the right permissions to not do the wrong things we can do a really powerful stuff that becomes a tipping point in the ecosystem where it feeds back onto itself and becomes a multiplier now the second thing we need in addition to Reese was timing and this is critical and one of the things that struck me was thinking about one of my favorite things in all the world the Large Hadron Collider it took ten years to build it ten years from 1998 to 2008 it was a significant international collaborative effort by CERN it needed years of theoretical testing and at smaller attempts to build smaller colliders it didn't have its first test run until 2010 but it all paid off in 2012 they discovered the Higgs boson and is now being used to measure some of the highest energy collisions in the world I want to compare that to what kensho gets from GCP instead of investing a generation of knowledge and science and instead of investing a half decade of infrastructure build-out in another decade to actually get it running we spend minutes we measure our scientific process in minutes of time and money founded in 2013 we use machine learning on GCP to power our platform now what struck me at this is Google was founded in 1998 as well the same year they started building the Large Hadron Collider and while we can run countless experiments every day and scale up those experiments to the largest in the world like asked myself why are we able to do this how can we go from a generation of planning to doing science and data science and research and machine learning in minutes we're standing on top of the other thing that started in 1998 Google has more than a decade of building out infrastructure security data expertise and it struck me at yesterday's keynote Diane green mentioned that Google spends nine point nine billion dollars every year on capex and it cost approximately nine point nine billion dollars to build the Large Hadron Collider so we are standing upon the Large Hadron Collider equivalent of cloud computing and that means we can start signing customers solving our problems and doing machine learning without having to build 17 miles of tunnel underneath Europe so if we have resources and we have the timing is right because we can do our experiments measured in minutes and we can do those experiments in parallel with everyone else who wants to use this Large Hadron Collider at the same time it's clearly the time to get started so let's move right away to designing our experiment so the first thing we're gonna do is we're gonna gather various measures of sector performance their momentum liquidity volatility pricing trends historical ranges of prices we're gonna want to normalize that data perhaps into log space we're gonna adjust the ranges of the data to prevent outliers from squishing our data into buckets that prevent them from being distinguished from each other then we're going to use unsupervised machine learning to identify key features that have existed in market days previous to this then we're going to use unsupervised machine learning to cluster those and identify what days are most similar to each other and then we're going to take this model feed it back into kensho and we're gonna ask what will happen the next day so let's recall our hypothesis so now we're gonna take a moment to ask you to log out of your retirement accounts while machine learning is amazingly powerful and an amazingly cool I'm an engineer not a financial adviser don't J trade your retirement account so the first thing I said we were gonna do is we're gonna gather and normalize our data we do this in a compositional API driven fashion that is powered by Google cloud platform in this case we take a configuration and we inject it with time series information event from our knowledge base and our event Almanac measures of price and momentum and liquidity and behind the scenes we're going to normalize and clean up that data I said this would just be the good parts this compositional style of machine learning means we can leverage all that Google has to offer in one notebook in this case I'm pulling in almost 4,300 days of market data with about 64 features for every day so we love matrices and so do computers and so does machine learning models in this case we're using single value decomposition to remove redundant data I'm not helping the computer very much here this is the computer telling me hey I found some stuff you really didn't need to include it's not that valuable and in this case SVD says I can remove a chunk of your information and still keep almost 98% of the distinguishing characteristics I'm asking the computer literally to say what can I throw away the next thing we're going to do is throw this into a model called T sneak in longer form its T distributed stochastic neighbor embedding this reduces the dimensionality of the data from 64 dimensions down to 2 but it does not preserve the state of the data it is helpful for plotting it in a human understandable space but I do want to emphasize in just a couple lines of code have told the computer almost nothing I'm challenging the computer to tell me I'm gonna continue to be very lazy I'm not gonna pick or estimate the number of clusters that we should use beforehand I'm not gonna make consumptions about the internal data structure affinity propagation essentially says what information is passed from previous States in the data to future States and the computer will figure this out on its own as we layer these clusters in two-dimensional space the silhouette score will give us essentially what it sounds like it is an overlay of the data and to what extent do the shadows or silhouettes of those clusters overlap or fit with each other now a terrible fit would say I'm in a cluster but I really belong in some other cluster and a great fit would say I'm so happy in my current cluster I would never want to be anywhere else it's not even close and the scores for a silhouette range from negative 1 to 1 in this case we have a 0.15 that's we're doing ok I've seen better we can continue to improve it so let's visualize this in two dimensions this is a pretty graph there's some larger areas where a lot of days appear similar there's some tighter clusters that seem to be really close together now potential deals so much with historical precedence that for me I'm drawn to the clusters that have a lot of days this allows me to go back and measure things that have hundreds of days and similarity but taking a moment that I've already learned something here I can go pluck out clusters that only have to win or three days in them and just the fact that these days seem to be so distinguished from all the others can make them notable as well if as the market day rolls on I find the most similar days are one of these tiny clusters that seems to not be similar to anything else that can potentially be valuable information all by itself so as the computer starts organizing and sorting your data and giving you perspective on what is common versus uncommon it gives you the power as a human to make a judgment about is this rare and if it's rare what do I do or maybe it just sounded rare and now the computer is telling me no this happens all the time we ran into this with the drop of price of oil we went from about a hundred and forty dollars dropped to a hundred dollars and there was there was a big discussion around that because a forty dollar drop in oil is enormous but is it and we were able to use kensho to show that it wasn't an unusual event it wasn't a buying opportunity it was a there was room for it to drop because on a percentage basis oil dropping twenty or thirty percent was not uncommon and as the summer went on oil dropped to $70 to $60 and more recently it was down in the 30s so just the quantity and how rare something is is valuable by itself so let's look at a specific day and in this case I have chosen February 23rd so about a month ago this this particular cluster has a couple hundred precedents in it and in this case I'm looking at other dates and I've bucketed them by month in machine learning as the feeble human who's participating in this task I find histograms immensely useful because it gives me a sense of the shape of the data how it's laid out how the computer has organized it for me and then I can look at this and say that's interesting so what's interesting about it well for one I see there's a lot more days like February 23rd more recently this junk in 2014 and 2015 I don't see much in 2010 2011 2012 little spike in 2008 not much in 2007 and then I go back into the early thoughts I see there's more clusters of data and intuitively I find this makes sense the regime of the market from the financial crisis to about 2014 or early 15 was a long steady march up to the right though he's recovering from the Great Recession people's confidence was going up the stock market was earnings were strong right revenue growth was strong the housing market was showing signs of recovery and stabilization there was an quantitative easing program going on that helped fuel this progress and then as we approached the 2015 window people started to think maybe the federal federal government is going to start unwinding quantitative easing people's confidence started to rebound people start to talk a lot more about jobs right so it says to me that perhaps this machine has learned something about the structure of the market and the overall tone of the sector's and how they behaved with each other as it changed from leading into the financial crisis to the financial crisis to recovery from the financial crisis to now so why don't do is I want to take a moment now to feed this into kensho and we're gonna ask what happens on days like February 23rd can we switch to the demo yep cool so we're not a hedge fund we put out a website we sell our product to major financial institutions and what I've done here is extracted the dates that the machine has told me are similar I have fed them into our analysis engine in this case I have 256 previous dates again you can see the histogram of how they are clustered by days so I'm going to ask the system a very specific question if I buy on the close of days like February 23rd and I sell it the next day how do I do so I'm literally taking on February 23rd I see we're coming into the close right the markets are open from 9:30 to 4:00 o'clock coming into close say it's 3:30 in the day and I look at the stock prices I look at my portfolio I look at what's going on macro levels I look at how the sectors are performing I look at what's in the news I run and I ask the Machine what days are like today and the machine has told me I found more than 250 precedents like I say if I were to buy if this ends up being just like how I think it's gonna shape up over the last half hour of trading what should I buy or if I'm already holding it maybe I should sell reduce my risk reduce my exposure in this case I'm just gonna do some liquid ETFs materials energy consumer discretionary financials technology stocks the results we can analyze what happens on all of those dates we compute on the fly more than 85 different statistics from beta adjusted returns to p-values to to your actual raw or average daily returns we adjust for dividends corporate actions you can see the dispersion in how the various market sectors pan out there's not tight clustering around those events in this case I'm looking at the beta adjusted return this helps normalize the performance of the various sectors relative to the benchmark so I'm looking for out performance or under performance rather than just maybe everything went up that given day so what happens so in this case I'm gonna treat this returns chart as a stack rank and it seems like in this case energy has historically outperformed materials is up there you have down at the bottom XLF is the financials i see health care seems to underperform so in this kind of thing i might say hey let me look at my holdings am i am i overweight healthcare should i do something about that if i was planning on buying energy should i buy it today rather than tomorrow lastly we we give our customers all the raw data output rerun tests t-test both 1-sided 2-sided depending on the scenario that you're trying to measure about looking for fat tail risk and and looking for shifts in the distribution of returns we also look a lot about time series so those way machine learning teaches us about time series we then feed that into a visualization that allows us to appreciate what the machine has identified and overlay that with how the market has performed so I do it I'm going to say sell healthcare and buy some energy or hedge my positions on the 24th this allows me to do something different so now I'm actually asking what happened I bought some stocks on the clothes and now I held them and Here I am on the 24th holding these stocks so what happened to my portfolio so I see materials has done quite well I see energy is still up at the top I see health care has nudged a little bit I'm not sure if there's something here but it does seem like the top has generally stabilized the bottom has generally stabilized and there's some shuffling or mixing around in the middle and it's this kind of thing I've told the computer almost nothing other than here's some market data right we remove what I don't need group it together removes reduce the dimensionality identify signals that are passed from previous States to future States and then fed that back into an analysis engine to say well this looks like this kind of day what happens tomorrow and when I look at this I say this is not a random walk we go back to my slides please so let's bring this all together what we're really doing is talking about understanding the connections of things right how are things connected to each other we're statistically annotating the significance of what has happened in the market right so we're looking at to market assets there's a major a publicity event a company has earnings there's unrest in the Middle East there's a there's a new product announced by a specific company to what extent does that influence an alter how that asset performs in the future through market assets that means events happening in the world news happening now and to what extent does that change our perception of the likelihood or the significance of events coming in the future and then lastly market assets by themselves can influence what will happen as well I'll go back again to the drop in the price of oil where you can see how the drop in the price of oil has caused ramifications in the Middle East around the structure of those governments and their ability to pay their bills it's affected Alaska which is now having to start considering it income taxes and change their budget their budget economics quite dramatically and ultimately through machine learning we're answering the question who should care and how much and if you decide that you do or don't care right when when should you care when is it important when is the time to act and for how long now I got a very interesting question this morning as I was watching the the keynote and the person next to me asked me about machine learning and said is this a biological process or is this a mathematical process I think that's a fantastic question because at the top level machine learning neural networks especially they behave like a biological process there's layers to them there's nuance they incorporate new things as new data comes in you can incrementally train your model in the same way that a child discovering a toy or gravity or getting to visit the Large Hadron Collider incorporates new information into its already existing perception and knowledge of the world but if you zoom in to the micro details of the model you're trying to back out is this machine really get it a really fine stuff that that I can't see and you look at the individual layers of the model it starts to look like math it is math right it's matrices it's numbers and you can peel apart the layers of say a neural network and say each layer what does it know and how does this change what it knew from the previous layer in the same way we might test a child in school to say what did you learn this year and compare that to how they did the previous year and this is our brains as well we think of our brains as a biological process because they are right it's it's lots of connections it's flaws we learn something we unlearn it later but then you zoom into the micro detail and our brains are firing neurons it's ones and zeroes did it fire or not and so when I look at how we've analyzed the connectedness of things in the world the increasing velocity at which information comes in we rely increasingly heavily on the computers to process this information like the prodigy that it is at the velocity that I can't match and it tells me things and that allows me to delete code replace it with models that learn and update and then go back to solving my customers problems thank you very much I can I can do some questions I have about ten minutes if people would like to ask questions yes yes oh so the question is what data did I use to decide what was similar in the past in this particular model I was using pricing volume volatility ranges moving averages various ETFs representing various sectors there's a lot more stuff that you can pull in and layer increasing nuance on top of these things for example you might one thing we frequently do is we'll pull in when earnings and other major shock level events will affect an asset the volume of ETFs has increased and varied over years so you probably want an ARIMA model that normalizes some of the seasonality out of those things there's there's additional informational sectors or even want to study something at a macro level rather in this case I was looking at something that was very us centric so one of the things we find very valuable about being able to run rapid experiments in the cloud is you want to pull in more data we can scale up the computers to do that if the experiment seems not to be fruitful we throw that away we didn't have to build out a huge infrastructure and then scurry around trying to get teams to make sure they're always utilizing it the the second part is it allows us to study sample sets and then if it looks promising then we scale up the project accordingly so I think the big brain is the compositional style allows us to inject different features and different data and play with that and then when we find something we can iterate on that have you have you tried feeding this model with historical data but on meeting some of the significant you know days and see if your model could have predicted those yeah so that's a great so the question is you know can you essentially do a in sample out sample test so you take say 80 or 85 percent of your days you train on that data and then you use it as an out-of-sample test on the other tenth fifteen or twenty percent of the data in this case I think you'd want to pull in additional features in order to do that well so please don't trade this strategy but at the same time that's exactly the right thing to do and the other nice thing we have because we have this complement of machine learning and back testing analysis we're able to check those things and then the incremental layering of machine learning allows you to constantly incorporate new information to be like what did I miss yesterday let me incorporate that what did I get right let me reinforce that and so the model of learning is very much fail quickly do again but machines unlike maybe humans don't develop like a bias we don't get hurt our feelings don't get hurt right if we are constantly wrong computers don't get discouraged and so and then their velocity of running experiments so fast is such a multiplier that they're able to fail so many times until they get it right yeah at a scale that none of us can match Mike so did it predict February 24th Purdue I think it got some things right and some things wrong in that I think the general if you think of it as like layers of sediment the the things that were generally doing well historically were generally toward the top and things that were doing poorly were generally toward the bottom but but I think you'd again want to like I wouldn't go trade your retirement account based on that and I think there's more you could layer in there but yeah I think it generally it seemed to have some sorting there and a sophisticated financial investor would probably look at some sort of weighted strategy that would buy some things in a higher ratio or short other things or sell things in a various ratio to potentially hedge or leveraged opportunity ah so the question is did it identify any days that had never happened before yes so there was a bunch what I what I did in terms of so we have a precedent engine right so we look we use machine learning to identify precedents of when something has happened like this and if there aren't many precedents then you really can't run a test on what has happened before although it may be valuable to know it hasn't happened so when there are small clusters I I just leave basically throughout ones that head I think it was just a handful of days in them because I don't have a robust statistical way of feeding those back in to do analysis on those but certainly say if I were had a dashboard up that tells me you know going into the clothes like hey this looks kind of unprecedent because of these reasons and why that's something that you know at the scale of information coming into the system no individual could do micro yeah so the Python code used showed is written inside a browser and that would trigger the URL of machine learning library on Google yes so so the question being you know so we we do all the questions about coding in the browser using an I Python notebook so we will of API as we love services we love compositional services so one of things we do is we build our own services our own models we use Google services and Google's API and then that allows our data scientists to work in and ipython notebook and that essentially serves as a journal of what this is and they can pull things in and we you know if the experiment doesn't work out we can stash that in and get repo and someone can pick it up later or at least learn from that and the so the way we we do this is we have an API I didn't show it here that we can kick off analysis in the background and pull those back in so instead of the Google sensor tens of role-based machine learning framework you have you manage all your own machine learning models and we do both I got just a couple more minutes right here have you applied your model to other data sets and just curious as an experiment other than financials yes whoo we've certainly played around with it we're very much focused on financial information right now the the financial industry is heavily focused on you know security it's they very high standards for robustness throughput the large-scale data which keeps it interesting there's there's clients you know there's an immediate hunger like answer now and I think that has a lot of analogs across many different sectors and datasets we ingest a large variety of data so it's not just numbers of from market data but we also do world events news we have unstructured data that we parse out and do identify I didn't into the identification and to kind of tease out what's connected we feed a lot of that into our knowledge graph so that we can understand the connectedness of things like proverbial you could like shake the knowledge graph and see what else shakes and for how much and over what amount of time and our knowledge graph has object-oriented a structure to it that allows us to validate things and a lot of those annotations are time-related because we love mapping things the time series allows us to throw into matrices and do machine learning on it microphone when you're talking about doing machine learning you're working with historical data but in the past there were no other companies doing machine learning on data in the early 2000s 1990s so how do you say that your model is valid compared to historical data when all the financial institutions are doing this kind of analysis now sure it's so there's so the question being that lots of people are doing this and I think it's true you know for example Google's been doing this for quite a long time right and I think a big chunk of machine learning is knowing what to pay attention to and letting the computer advise that and I think a lot of what's going to revolutionize the finance industry and many other industries as well is they're going to move to the cloud they're going to use machine learning to remove code they're gonna use machine learning to analyze data they're going to do it in near to real-time and then the humans will be enlightened as a result of that so it flips that like teacher-student relationship right as an engineer I'm so used to basically spelling out a recipe for the computer do a then B then C or tape you know call a and then call beyond that or call C on that and and this flips the rhythm so now it's rather than a couple people trying to teach a lot of machines we basically say here's the framework and data for the machines to to learn and then they will teach us and that I think is an immensely powerful paradigm switch that flips everything there was another question back there I got Tomatoes last last time I had been playing with Watson and the ability to actually type in a question and then it understands the semantics of what you are asking or the context of what you're asking and gives you an answer have you built any of that into your product and have you leveraged any of Google's machine learning algorithms to do that yeah so so we love natural language processing I think it's a very intuitive way we search is a very powerful tool for us and for the financial industry the we have a query engine where you can type in more free forum questions the where I think it becomes exponentially powerful is I shouldn't have to ask the computer so we're evolving from a world that says computer what about this what about this what about this and it behaves like hunt-and-peck and we're coming to a world was they know like computer just let me know when it's important when I should care for how much and how long and so I think they both play off to each other the other area where we use a lot of NLP is on the ingestion side to identify things and you can use hidden Markov models to pull out structure and implications without even nestling and meeting identity identification so there's a lot of cool stuff coming out there and Google's a major leader in that area especially on the Texas side all right last one yes so the question is do we sell the final product or do we perhaps sell pieces of it in the interest we do both I mean so we're building a platform that is very flexible has many layers to it we can scale separately we you know we follow the Google GCP model of Micra services and being able to expose those to have the right layer of abstraction and we potentially open those up as they become ready cool thank you so much everyone you 