 thanks to the organization and everybody for having me here I could have some stories to tell but I'm now going to tell them in detail but what I'm going to talk about comes from some of these stories basically when you have to or in the past 10-15 years where we have had projects with companies with about what this what was called data mining and now we refer to data science even even if both things are not the same there was you have to do a lot of things about data integration data cleansing data wrangling and a lot of these things finally they they take a lot of time that they are sorted out okay so in one way or another these things are sorted out so the the big problem for some of the experiences that we have had in the past is that a complete project even at the end of the project can be spoiled because you don't recognize the utility or the way the project or the models have to be deployed so you have a very bad or bad understanding of how your mother's have to be deployed how they have to be assessed then the whole project is jeopardized and that happens a lot of times and I've lived that and it is from that experience that I have just come up with a series of pitfalls caveats and then I'm going to just talk about some tips that we have gathered in the past years of course you can find some of these in any machine learning or data scientist book but I think it's important to emphasize them because some people just think that you just take a model and you just to evaluate with your prefer metric and at the end of it and as a crucial point the crucial point of a good model is how well the model is evaluated about the story if I was some of the stories that we had in the past one was about a big retail company probably you if you live in Spain or you're having here several times you know it's very well but there were about a thousand products and about 1000 supermarkets so the plan was to had eight million models basically because at the time we didn't know how to have a general model for all products and supermarket what we realized at that point is that there were some things that were changing between one product one supermarket so you knew that the evaluation changed because it wasn't the same for one product just to overestimate that underestimate so just using this mean squared error for every product was wrong because you needed especially utility function for each model and even for each pair of model and supermarket ok and so there was one thing was about evaluation and the other thing is how can it be that we have 8 million models can just we do with just one model for all supermarkets and and have products a similar thing happened in the hospital that we had a project a few years ago we had to predict emergency pressure and you could do that well globally but when you just you delved into specialities or areas in the hospitals you came up with different models and different evaluation metrics so at the end what we realize is that whenever you are just working in a problem you have to predict or you have to define models for different areas of that and whenever something changes you have to start all over again and it is not clear that the model that just gives you the lowest squared error or the low of the highest accuracy is going to be the best model in all of these situations and that just suggested that performance evaluation is a critical part and you have to bring it up front from the very beginning and then perhaps with this you don't do things better than without them but at least you can reach the end and know in advance that you're going to have a success or a failure so I'm going to talk a little bit about em machine learning evaluation basics I don't know because there's a kind of variety in the audience and then I will go into this idea of context I think the previous presentation talked a little bit about context and the importance of context when you just have one model just working for this date and the data changes and you need to know whether this is going to work for the new data and then I will just focus on one specific kinds of context this is that your costs change and your class distribution changes and this is just as an illustration of an area where you can do things and if things are well known I don't know if you are familiar with Roc analysis but I will show that as an example of the things that can be done when you just anticipate your context you understand your context changes in advance okay so let's ignore data wrangling and data integration data crunching all of that all of this takes 50 60 70 percent of our effort in a project and just imagine that you are in a moment when you have all your data integrated and you are about to press train okay that's the easy part well it's not that easy you have to choose between several techniques and parameters and all that but then basically you get your model but the problem is that well what are you optimizing for so you can say that this model is better than other but you need to know for what so the important thing is not the metric that you realize that this is not in the lab where you want to optimize your model it is in the real world in the real application and sometimes with what you have in the lab is just a part or a different view of the problem or just a partial or idealized view of the problem you're going to face in the real world okay when I say in the labs in your platform when you just have your data integrated and you're just training your models and evaluating your models and all that so we need to focus on deployment deployment is not the test data we will see that in a moment so well perhaps you use a different you can use the term production if you like just put a model into production so that's what I refer to where the word deployment and you can use many different metrics depending on the problem but the golden rule and you probably have heard about this rule but a general version of this rule is that never of a state the performance of your model because in the lab it works well because it might happen that in the real world when you go to production when you go to deployment the model doesn't behave so well so of course we need to know the reason why it this this may might happen so in the lab we are well used to these problems of 15 and 15 it is clear the one solution for that especially in predictive data my team my name is one rule that we can call the golden rule but just for predictive stars never use the data you're using for training for evaluating your models okay that's clear there's a golden rule and nobody breaks the rule because otherwise you can have very optimistic assessment of your model and we don't want that this is this is quite easy because we typically that depends if we have time or not but if we don't have time in our data set we typically just make a random partition and we work with that of course there are some problems here when we evaluate we need to choose one performance metric and with that performance metric we select select our best model that's common practice as what we do with any machine learning data science data mining platform of course there are some caveats about this what if we don't have a lot of data that was solved many years ago well as the decisions nowadays very well so you can do something like bootstrap of cross-validation and in cross-validation even you have you can just make the partitions many times and without breaking the rules you can have that your models are trained with a with the with the whole of the data set and they are evaluated with the whole of the data set which seems a little bit counterintuitive before you see that this is just doing this many times well it is also important that you there's no need to do cross-validation even if some platforms just put it as a default thing when your data set is is large well so is this enough to have a good model or just to select a good model well these are these are the testing conditions quite ok but what about deployment conditions what's the difference between training or testing conditions and deployment conditions well the difference is context as a difference okay because basically what they are doing is exactly the same thing the problem is we are seeing many times that our test set is going to be always going to have the same distribution the same conditions the same context than the deployment application deployment data and all and everything that surrounds that deployment data and this is not usually the case so we can have a model then we have a context and we know is this model working in in this new contact then we have another contest do I need to change the model do I need to retrain the model then I have a third context and do I need to do this again and again and again okay so the question is that the evaluation of a model can we make a model that is more versatile and can be adapted to several situations what leave we are able to determine what's changing between these contexts and at least try to automate the things that are happening between contexts for those of you who are familiar with machine learning this will probably remind you of areas such as transfer learning and all of these things this is a little bit about that but for the point of view of evaluation what can we do when we evaluate a model even it even if it is a specific model or better a general model that can work or can be adapted or we used for several context changes okay so the idea is that how can we take context change into account from the beginning the first thing that we need to realize is that there are many kinds of contexts so I'm talking about contest with can you be a little bit more precise about what you mean by a context well one kind of context that you the TCC to realize is that your data you have some data where you train and tells your models may change when you try to deploy your your your models that happens all the time you just learn your model for a date the same when you're going to apply it you apply that to the most difficult part or to the easiest part or to some other part of it is new compared to the part that you were you have learned to model okay the time the data is is is changing and occasionally you can I anticipate these changes and some other occasions is more difficult that maybe changes in the prior distributions are many that there are many kinds of data shift ok many things we can be just the the relation between the input variables relations to the output variables that might be different cases the little changes in the utility functions you are using especially the cost functions in this example about the retailing company it is not the same to overestimated than to underestimate because you have just predicting product sales so the cost are not going to be symmetric that's something that is very clear from the beginning it's not the same just to have something and stock and you cannot sell it and just to break and you don't reach and you cannot sell something because you don't have it well this is something very so can you how we use a squared error for this well it's not the best solution for for this problem but some other changes can change some other things can change for instance you can have more noisy information less noise and more missing data more uncertainty in your data even if the data has the same distribution you can have representation changes new variables some variables change change in magnitude or in labels or even you have new constraints some predictions cannot be done or you have more information about the or less information about a situation or even your task can change completely that's something that reason one of these examples is quantification I don't know you have heard about this task in machine learning or in data science you can predict how much you're going to buy how many products you're going to buy or how many euros you are going to spend but they I want to know how many euros all of you are going to spend there's one easy solution is just to create a regression model for each of you just aggregate all of that and I have a global prediction that is the way I evaluate each of these local models the best way when I want to aggregate all of these models probably not program we need to do some adjustments so when you change but you can use these models here you can just create a model you just plan to use for something and then you realize that you can use it for a different task so this is a more important ain't but all of these changes are context changes so you can parameterize them you can just analyze them and you can anticipate the details these things may alter the quality of your models in deployment of course I cannot go through all of them for just 20 minutes but I will just show a few slides about Brock analysis which is just one example I hope I will go very I will skip some slides here but basically what we do here is that you can evaluate several models that's quite okay you can direct many metrics for that and then you can say okay which classifier is best so that depends on the methods you use the road you use my collaborator you use precision juries recording is do all of these things and that depend on your models and but on many occasions not all your errors are equally costly so this is cost-sensitive learning so you can just say okay can I evaluate or construct my model depending on a cost matrix and you say they're not all your false positives and your false negatives they don't have the same cost so what you can do is just evaluate your three models taking into account that cost matrix and from there you can just say that this is the best model because according to my utility function this is the best model so in this case if we have a context discount is just defined by the cost matrix and the proportion of positives and negatives I can say that this is the best model but what if these things change very frequently and that is the case of you imagine you have the recommender system it is or you have a spam spam filter it is not the same that this matrix is the same for each customer for each user you have a different matrix for each of them so you need to think how can I just create models that work well for a variety of models so one thing that helps you to do that in classification is Rock analysis and basically what you do is you plot your models as point I will skip the details and there you can just see whether it is a good model a bad model or just a random model that's not very interesting the interesting thing is that you can just connect the points because basically you can toss a coin and say okay half of the times the model and this mother cell you can connect the points and when you have a lot of models you can just plot now show these points on this space and see the sum of these models will never be optimal in any possible context so you can discard them forever so there's no notion of the best model because some of these models are going to be best for some context but the thing that you can say is that some of them I'm not going to be optimal in any context so basically what you do is something like this you have a context and you say okay you calculate the slope which is just given from from very simple information or your matrix and the proportion of positives are negative you say okay this is the best model for this situation now you have another situation and you say okay and at the context and you say okay so this is the best model so depending on the context you choose between one model or the other but you previously have discarded a lot of models that are not optimal in any possible situation so the rule is that you don't get the best model but you can discard a lot of them and basically what you get to the end is about four or five models that you keep so well I'm not going to explain how to construct a curve especially for scoring classifiers this is a little bit more technical and I don't have time sorry but basically we do the same here so this is basically when you have a model predicting probabilities instead of just clashes and basically we do the same thing they are dominant areas where we know that one model is better than the other so we keep two models here because there are some areas where one is better than the other but the thing is what if I only want to keep one model the best model well in this case what we can do is okay can we calculate how well this model behaves in a range of contexts depending that we have many customers and each customer they are going to have different cost matrices can I just have a metric that tells how could this model behave in a range of situations well there's basically the area under the curve you can calculate that and say okay B is much better than a there's an area where a is still better than B the well in this case it is very clear that with B you can do very well nevertheless every aggregation metric just reduces information and it is worth and just using the plus and having many models and just keeping and the model that you need for each particular context okay so this is just one example of something that you can find a many rock Oh till it is to plot the curves and to do all these things so you can find packages for this almost everywhere but the ideas and you can generalize that for other areas in machine learning for binary classification for regression they're similar things for instance you can do this for regression for over estimations and under estimation so this is not just the brock analysis just something for binary classification but basically the idea is that we can generalize for some other for some other context but I will kill that because I'm running out of time I will the idea is that we can apply the same methodology for other kinds of context changes it's not just binary classification it's not just consensus of learning a consensus of evaluation with imbalance data cells and all that you can do the same philosophy or methodology when you have some other areas and some other tasks and when you have some other context and might change for instance you have expecting more noisy information you can anticipate that and and truth the model that is going to behave well when the noise is there even if your training data doesn't have noise okay so that's something that you can anticipate and there are ways to do that and and things are changing in the same way that you can just keep several models and depending on the level of noise you can say okay this model is going to be the optimal when I have 10% third Prince end of noise or when I have twenty percent of nor they have a white noise I have this kind of noise you can do that for all of these kinds of contexts that I mentioned if you slay slide back even if your task change you can just anticipate that and keep the models are going to behave well with you integrate all of that so going back to this slide basically you generalize what we have seen very briefly with rock analysis to other areas of machine learning you can see okay try to identify the context is it context changing how is it changing can i parameterize these changes and if you can answer approximately well to these questions you can anticipate like you can just train and specially select your models according to that context and taking into account that these countries are going to change because otherwise you just focus on the data and the context where you took the information then probably you are going your break in the Golden Rule and the government rule is done over state how good your model is because perhaps when you go to deployment this is not going to be that good and basically that's it I'm a little bit out of okay so thank you 