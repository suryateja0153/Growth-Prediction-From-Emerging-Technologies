 well thank you for that kind introduction John and thank you to the organizers for inviting me and the timing is good here because a lot of the questions that came up after Dave's talk are exactly the things that I'm going to be talking about in fact I wish I could change my talk a little bit now and given what what Dave did because I'm going to talk about modeling but modeling can take different forms and there's a kind of modeling Dave did which I would call good modeling which is constructing models that you don't believe I mean clearly Dave doesn't think that the data were generated exactly from that model and yet the structure of the model helps you learn stuff about the data versus the other type of modeling where you which is I have to say statisticians are very guilty of which is write down a convenient model based on mathematical means and now act as if that's exactly the truth and everything follows from that so I'm going to let's see the first I will say just a few words about what's the difference between ml and versus statistics and econometrics Dave already said stuff about that then I want to talk about just a few examples of what people it's hard to say what's machine learning and statistics these days but I want to talk in particular about just a couple of examples the lasso and random forests and then what I really want to focus on this talk is exactly what I forget who it was someone in the audience asked about which is infants you do these high dimensional things how do you actually get confidence intervals and do inference very active area right now there's many different methods if I have time I'm going to talk about a six six of them D biasing conditioning uniform method sample splitting subsampling conformal inference I probably won't have time to get to all these when I was young I could really plan my talk so as I've gotten older I'm just like terrible at this so I don't know how much I'll get through here but I'll certainly make my slides available for anybody who wants them if I have time I'll say a few words about causal inference I'm not an expert on causal inference but I have worked on it a little bit in the past and I know that's something in economics it's obviously of great interest so I may have time for that in may not but I believe Susan and I Victor's victory here yet Victor hasn't shown up yet but I think there talks are going to be about combining causal inference and machine learning methods where I would say machine learning just means high dimensional regression and in particular Victor has recently invented I saw a paper last week a cool term called double machine learning so a lot of this he certainly has picked up on the key fact of machine learning which is coming up with cool names now here's a warning all right I'm extremely opinionated all right and I have a very biased view about things and it's controversial I I'm an outlier and I want to read this quote David Friedman is one of my intellectual heroes he passed away a few years ago great statistician he says investigators who use regression are not paying adequate attention to the connection if any between the models and the phenomenon they are studying and by the time the models are deployed the scientific position is hopeless reliance on such models is Pangloss Ian so I saw that I said I better look up Pangloss Ian so it means characterized by or given to extreme optimism especially in the face of hardship or adversity Friedman was a total you know he David worked on mathematical statistics and did you know beautiful stuff but when he did applied statistics he just didn't believe in models at all he just thought models were a completely bogus and this is what he was referring to I'm also skeptical of assumptions and a lot of my work recently has been about trying to do high dimensional inference without assumptions or baturin to weaken assumptions but again I seem to be an outlier here but I do want to make this distinction between making assumptions writing down a model and then acting like you know now I'm going to estimate the treatment effect to within three decimal places that kind of thing versus again what Dave was doing which is know this smaller was just to help me guide my exploration of the data and to learn patterns in the data and so on so I really should have made that distinction here because I didn't do it and I wish I had given Dave's great talk but I'm sort of giving this a they David Friedman point of view so what is ml well the joke always tell it originally just so does John point note I have a joint appointment ml and stat and stats in Baker hall and MLS and gates and I always we used the same joke every year I teach all these students in both and I start with the same joke I hope they don't they probably have all heard it by now but I say if you're doing statistics and Baker hall you're doing statistics if you're doing statistics and gates then you're doing machine learning it's really very little difference in some sense between the two fields although that's not quite true but I would say a machine learning you know assumptions are pretty light so when you make assumptions you don't necessarily believe them they're just they're just a tool towards an end and a lot of the times the goal is high dimensional prediction not always but that's often a goal statisti tist excited say the assumptions tend to be much heavier and we're interested typically not in just prediction but also inference things like we were talking about at the end of Davis talk and now I'm not an economist but my I'm going to say I'm going to go out on a limb I'm going to go a limb and say that so much tend to get heavier you start to put in instrumental variables and ignore ability assumptions and so on and it's prediction and inference and of course causation is the big thing and of course this is not just economics but I would say epidemiology and biostatistics and so on and that's not meant as a criticism it's meant of that's what you have to do to answer the questions that are being the questions are more difficult I guess is the way to sit so let's work up to this let's just go back to the pure prediction model we're just going to have a bunch of data there pairs X's and Y's as usual very important is didn't have time to talk about a lot of time to is treat I treat the X's as random I'm I'm I'm very against treating X as as fixed as some many statisticians do and there's a wonderful paper by Andres Booya and Larry Brown and a bunch of guys at Wharton called the conspiracy of something rather it's in statistical science and it makes a very very strong case for why it's totally misleading to treat the X's fixed in regression I won't have time to go through it but III just I'm strong believer that we should treat X as random we're going to have the covariate as in d dimensions and of course what makes things interesting these days and makes things difficult is that D could be much larger than n the sample size D for dimension n for sample size Y could be say a real valued thing regression or zero one classification up I'll just focus on regression but everything I'm saying could apply to classification as well there'll be some unknown regression function mu of X the mean of Y given X which we don't know and if I had to summarize machine learning and at the risk of being somewhat of a making somewhat of a cartoon you get a new pair x and y you just want to predict it well so you're just going to look at Y minus your prediction mu hat of X and the key thing in machine learning is that to do that you're balancing bias and variance and every it all comes down to bias balancing bias and variance in contrast as you all know as economists balancing bias variance is a terrible thing to do in causal inference because bias kills you you won't get a correctly centered confidence interval all the stuff about influence functions and correcting bias and so on in semi parametric inference is about getting rid of the root n bias because otherwise you can't even get a correct confidence interval so bias is much worse for causal inference and I think that's a kind of interesting mathematical distinction in fact most people I would say in a general machine learning community to have no clue that you should do something other than bias a balanced bias and variance all right so there's a billion prediction methods now and support vector machines this and that I just want to mention a few I'm going to mention three the lasso I'm sure you've heard of all these alas Oh which is just a way of doing high dimensional inference random forests may be less familiar if you don't have experience and I'm just using it because it's very popular and it's considered I think the best off-the-shelf nonparametric prediction method these days as far as I know like if you just have to run a quick prediction method that's very nonparametric branagh for us and actually Susan's know a lot of work recently about importing the random forest idea into causal inference then there's deep-learning everybody's sort of deep learning depending on who you ask it's the biggest breakthrough in the world or it's snake oil I don't have an opinion about that I don't it no one seems to really understand it deeply enough to say I'm not going to take an opinion on that let me just remind you quickly what the lasso is all right so the lasso you know you just take the usual sums of squares and instead of minimizing that we add a penalty in the key of course the lasso is an l1 penalty not an l2 penalty this is the l1 penalty can anybody even see that laser yeah okay and so this is now so popular that it hardly needs an introduction it's I would say it's as far as I know the most popular high-dimensional regression tool now and why does everyone use it I think there's a few reasons it's it's convex in that problem is convex which means you can find beta hat quickly the resulting beta hat is sparse which is what we want we want to typically we want to zero out many of the parameters and this does estimation and model selections for you simultaneously you get a sparse estimator and quite frankly you can prove things about it if you want to understand its behavior you know because it's the solution of a convex problem you can prove things forward stepwise regression I actually like that because I can even program that myself but it's much harder to prove things about greedy methods so I think that these are some of the reasons why I think the lasso has become the method of choice there are questions what does beta mean and this is a big issue about the difference between machine learning I think in statistics and I'm going to talk a little bit about the interpretation of the parameter I think is quite important how do you choose lambda the typical answer would be cross-validation cross-validation balances bias and variance though which may not be what you want to do and how do we do inference and that's what most I'm going to focus on in this talk let me just do a quick description of random force in case you haven't seen it first you start with a regression tree what's a regression tree just a piecewise constant estimator done by recursive splitting so you can do it for regression or classification so actually just just typical picture would be you take a variable 2x2 and you split it you split to the left and to the right then maybe you'll take X 1 and split that and so you split recursively in the here's X 1 and X 2 that just ends up you're splitting variables you descend it with a piecewise constant estimator it's actually I think one of the nicest and simplest nonparametric predictors because it's very simple and interpretable and well as I say I think Susan you talked about this yeah so Susan will talk more about this but these are very very popular nonparametric tools whether you call that nonparametric statistics or machine learning is you can call it either one I think and forests just combine a bunch of these so in forests you typically draw sub samples from your data and for each sub sample you fit a tree and usually you also take a subset of all the covariance at the same time and then you get a bunch of trees and you just average them together and this just works remarkably well there's been some attempts to explain why but I don't think there's really a convincing explanation yet but it just works well and practice is just no doubt about it so it looks like this you have a observation now you have a bunch of trees and you're going to feed this down each tree and combine the the output and so it's a pretty it's an interesting thing because it's very effective but now we've lost the simplicity and the interpretation and so on and there's sort of a conflict there between interpretability and good prediction all right so the question is if you do any of these things random forest lasso whatever how do we do infants well before I say that do you want to do inference so I would really say if you're in a pure machine learning problem it's kind of irrelevant right if you if all you're trying to do is you have a program that's deciding whether my mail spam or real I don't really care about confidence intervals I just want to make sure I'm reading you know to mail them supposed to be reading so in some cases of course the answer might be we don't need to do inference so I'm going to assume though we do want to do inference there's obviously cases where we want to do inference and as economists you would it that's an easier sell than maybe two machine learning crowd but what parameter are we actually estimating there's actually several different parameters and and a lot of the work on what's called post selection inference or selective inference and statistics in the last two years has been focusing on even what's the parameter some of these papers are estimating different parameters so I would divide it I'm going to divide it into three groups there's something I would call the true parameter view where the assumption is that the true model that generated the data is linear so and remember this is a high dimensional linear model well in that case Beit is pretty clear right it's there it is it's the thing in the linear model I have to say I think this is pretty much a bogus assumption when I you know when I was a student and I learned linear regression from one of the greatest applied statisticians in the world's name is David Andrews men many of you would not have heard of them and this is in the low dimensional days and I remember when he taught it to us the day one he put down the linear model he said we're going to use this to fit today dices but for God's sake don't believe them all I mean that was like day one was the models wrong and now I find you know people are writing down these high dimensional malls and acting as if the model is true and I understand for theory you know maybe we don't have a choice but for making policy decisions and things like that I find that quite worrying why would the true model be linear and you can't by the way these in ball and high dimensions none of these assumptions are testable if they were we wouldn't need to regularize right if you we regularize to take an ill-posed problem and make it well posed and if you could test the assumption it wouldn't have been ill posed to start with which doesn't mean you shouldn't make this assumption it just means you know about how to interpret it but there's a different point of view which has been used I would say throughout the history of statistics but it's become much more popular and I have some papers on this and these there's these guys at Stanford doing what I'll call the Stan forty and view of inference which definitely take this point of view and also there's this group at Wharton who's been doing a lot of this inference and they're also taking this point of view so we're all in agreement on this which is we don't interpret the beta that comes out of regression as any sort of true beta and so is kind of comports with in some sense with what des was saying we just view it as in some sense the best approximation so formally if you give me a new Y and an X I can look at the prediction error and ask what's the best linear predictor that's certainly a well-defined quantity without assuming the truth is linear so we don't assume Mew is linear but we still talk about the best linear predictor or in other words it's just the projection right onto the set of the space of linear predictors so that's called the projection parameter and a lot of the new inference stuff takes the point of view that the model is not linear but we can still try to do inference for the projection parameter and things get more interesting because when we do the lasso or we do something else we're going to do model selection so we're going to choose a subset of the variables based on the data of course this is going to be a data dependent selection and now beta sub s is a really interesting thing it's the projection it's the best linear predictor on the ten variables I selected what makes that really interesting is not only is this a projection parameter but now the parameter is random because s is random so now we're doing inference for random parameters so the data are random and the parameters are random and all right that random parameter and it's also by the way at a technical note it's a not a smooth parameter for you I'm sure a lot of you are familiar the bootstrap you would need the parameter to be how to margie Frenchy ball has to be smooth this is not because typically model selection procedures are you know very sensitive to small changes so we can't just do the bootstrap or some typical procedure there's another parameter though and this is the one I've been writing papers about with my colleagues at CMU I'm sure we didn't invent it but we invented a name for it called loco because we it's a little bit crazy it stands out for leave out covariance it's very trivial so there's an archive paper about one approach to dealing with local parameters and we have another paper we're about to finish I'm going to say next week that will go on the archive and it just it's just very simple and again I'm sure lots people have been doing this for years which is let me explain what this equation is I'm going to I'm going to let's say I fit something could be a random for us could be the lasso so I'm looking at a new pair x and y this expected value is just over a new pair x and y and I'm looking at the prediction error so that's why - my why do they put mute transport this should be mu hat of X sorry about that this is just my prediction of a new pair now I'm going to say suppose I didn't have access to covariate J maybe that's not unemployment I don't have access to how much do my predictions change and so that's a measure of variable importance so I'm going to reef I'm not I'm not taking the I'm not zeroing out that thing I'm rerunning the entire process whatever I did the selection process the variable selection process the model fitting without that variable it's as if I didn't know about variable J and that gives me a new predictor which is I think this is died but anyway it's it's this one and now I'm just looking at the difference and so that has a very clear interpretation it's just what is the prediction inflation how much extra prediction error would I pay by not having access to variable J and what we like about this is this has a clear interpretation without resorting to linearity or whatever the model is it just says how much price do I pay in prediction by not knowing this variable that's why we call it leave out covariant and it has lots of advantages which is it's very interpretable I think it's very simple it's not tied to betas you know I think I keep asking all my sister's friends why do you keep worrying about beta and I think it's because well but they took the first regression course 30 years ago they learned about beta and they've been thinking about beta ever since I think we're going to stuck in so now I'm going to preach a little bit you know I think the way we statisticians I can criticize that assistant cause I am one I think we learned a bunch of tools and then when we met the high dimensional world instead of saying let's try to think differently we said let's try to add assumption so that our low dimensional thinking will transfer to the high dimensional world I really think that's a lot of what's going on so I think you know beta worrying about beta is to a large degree this this fixation on things that we're used to thinking about this is much simpler and on a technical note which I won't have time to talk about the bootstrap inferences are much more accurate for this parameter than for our estimating projection parameters okay let's try to stop preaching sorry about that it doesn't depend on linearity model correctness very interpretable let me just mention a brief some of the literature and this is not complete at all but in the true parameter approach there's there's some papers by these people actually Victor it's going to be here later hopefully to talk about some of that stuff and very fascinating stuff and again it's very subtle because it involves these things about to get the confidence intervals you don't have the usual bias and trade bias and variance trail the projection or parameter approach there's the first thing is there's a series of papers by these guys at Wharton I would really recommend you read their papers they're very very nice then there's the Stanford group that's John Taylor and all his colleagues they have this explosion of papers I'm going to talk a little bit about this I think there's something really interesting happening but I think there's also some problems and then there's I just threw up one of our papers and you know there's other people looking at this projection parameter bus and then we've been working on this local approach but I also want to mention Java sucker and Lucas mensch who have a very nice paper about doing something I would call loco they don't call it loco but it's a sort of leave out covariant thing using and they're doing something very clever they're doing random forests they're treating the random forest as a u statistic of what's called a infinite degree u statistic and trying to develop central limit theorem for doing that kind of inference so there's and I'm sure there's lots of other people doing all these things as well so I wanted to try to go through now some of these techniques in case you haven't seen them and explain them and say what I think are the strengths and weaknesses before I describe them in detail here's a table so this kind of summarizes everything the first method I talked about is what I would call de biasing it treats the beta as if there's a true beta it's based however on very strong assumptions it does give accurate intervals I think they're order 1 over root N and they're very computable but in terms of robustness I put no meaning it's very sensitive to the to the model assumptions then there's these the stand for T and approach or conditional approach which is based basically on conditioning on the selected model and then using sufficiency arguments to develop a pivot I'll explain that a minute sumption z' are quite strong the accuracy is unclear to me because sometimes they're actually infinite these intervals but they're very computable they've worked hard to get computable things again the assumptions they're weaker than the devising approach and in particular they're not assuming a true linear model but there's still some strong assumptions there's what I'll call the uniform approach which is basically looking over all possible models you can select and that's the wharton approach they're less it's it's more robust it's a bit less accurate in return but I'm not it's not clear to me as I'll explain that you can do it in very high dimensions and then there's the oldest method in the world sample splitting and everybody knows this right you fit the model on half the data then you do the inferences in the other half everybody seems to hate that method but I actually think it's the best method because it makes virtually no assumptions I'm a big fan of sample splitting and it gives very very accurate inferences so ironically you actually get most people say it's less accurate you actually get more accurate inferences than you get so for example the I won't have time to talk about like boots track accuracy and so on you do get more accurate inferences but you but how can that be well you could be estimating though a less accurate model in other words I might choose a poorer model when I only use half the data to select the model so I'm losing something by sample splitting but what I'm gaining is the model I selected I can now infer very very accurately so there's an interesting I call it the prediction inference trade-off you're going to predict a bit worse but get more robust and accurate inferences this is all explained in great detail in this paper and promising to post soon all right then if I have time I'll talk about a completely different approach that comes from machine learning world called conformal things which is completely distribution free but my prediction is I won't get there I think it's unlikely I'll get to that method these are so I kind of broke it down to these five or six methods then there's what I call the forgotten methods which everybody seems to have abandoned which is again from the olden days which is we could avoid all of this if we just did unsupervised the dimension reduction first you know like PCA or variable clustering then you're back to low dimensional aggression you don't have any of these complications and everybody is I think that's worth revisiting those things but I won't talk about them okay another conceptual issue is what do we mean by coverage when you say you know what's the probability that your interval covers the truth and I think actually this is something economists are much better at than statisticians is being careful about what they mean by coverage decisions I find are often very loose about this so the ideal coverage is that the probability that theta is in the confidence interval and the frequent descents and theta here could be random itself I want the info that over every possible distribution to be at least 1 minus L that would be the ideal confidence interval and as we'll see there's an order statistic version of loko which actually satisfies this property even in high dimensions then there's a weaker condition which I think Lee called was what the person who named it honest inference and and I I see it used now and then basically what it says this okay maybe the first one's too hard let's look at the info of the probability that Thetas and C over on a big is supposed to be a very large nonparametric model and we'll take the limit of that and demand that B 1 minus alpha and that's often achievable when the first one isn't and the important thing is that the infants on the inside not on the outside because that means you're really getting coverage that doesn't depend on the particular distribution parametric would be the same thing but with a small model instead of a big model and yeah so that's less desirable point wise is just for a particular P it converges to 1 minus L that the problem with that is every how big does n have to be before you get close to 1 minus self what could depend on the distribution P and you'll never know if we're close or not what we really want though this is the another point that I push on that I argue so might you know one of my best friends is Rob Petrie on tea at Stanford he's part of this other approach so we're arguing continuously about this and so he did he would say this is too strong but I call it a robust and honest so it's got the info overall distributions or a big set of dishes but you notice there's another in and the in phys overall selection procedures like how you selected the model this is a controversial issue is I think really you want confidence intervals that are valid no matter how the person picked the model whereas some people say no if we're using the lasso you only want it to be valid over lasso the reason why I want that is because in real life you know nobody sits down with a data set and says I think I'll apply the lasso get 17 parameters and do my inferences now I'm analyzing the data set I plot the data I look for outliers I do transformations I do all kinds of stuff to the data before I actually end up with a model and how can I write that down as some well-defined rule I mean I don't think you can and I think that if you don't have so in other words I'm saying the selection rule is actually a very nebulous complex thing and this is where a lot of unconscious biases come in and there's always talk about the ear reproducibility of science these days and I'm not saying this is the cause of it but it's certainly one part of it I think and it's interesting that I think in this competitive computer science literature they're doing something now called adaptive data analysis it's worth looking at that because it's it's very similar to what we're talking about here and they'd also take this point of view that their inferences should be robust to no matter what how the model was selected and I'm very sympathetic to that point of view but again I seem to be an extreme point here all right so that's the background let's start talking briefly about all these methods so just I'm calling them D biasing there's really a whole class of methods here and I'm just going to give you an example of one of them this one's by Javon mard and montón re and it kind of works like this there's no model selection involved what they do is they just fit a model say by the some sort of sparse fitting thing so like the lasso and you get beta hat now the problem is that the lasso gives you a sparse estimator but it's bias so they do this D biasing step so basically you take beta hat and you replace it with beta hat minus something the details are I guess aren't so important but you do have to estimate the inverse covariance matrix somehow that's involved and do this D biasing thing and then when you do that magic happens if you take root n beta hat minus beta and really I should be writing boot beta hat sub J minus beta sub J for one element you do get something which is normal plus something smaller order so you get this nice central limit theorem which means now we can construct the usual confidence intervals so I think this is really clever and if you wanted to do this over all parameters of course you'd have to bond for Oney or something over all Moz or if you'd selected a model I guess you would only have to bond for only maybe over the selected ones although that's not clear because that's a random selection but at any rate it does give you these very nice confidence intervals very clean method but so let's go through the assumptions it assumes that the linear model is exactly correct it assumes these incoherence assumptions which are often made in high dimensional regression which is basically that you're all these covariates are very close to being independent which I have a hard time with it assumes the model sparse that there's only a small number of nonzero betas it as soon as the variance is constant and it requires a very carefully chosen tuning parameter so theoretically I find this actually very appealing an interesting method but I'm not sure I would want to base like if I was working with a physician and trying to make decisions like for example I work with a biologist I'm not sure I would be telling him what wet lab experiment to do next based on something that came out of this I'm not sure I would trust it now what's really interesting probably the most conceptually interesting thing is this stuff that's going on a Stanford it's John Taylor and his colleagues I didn't list all their papers they have like the writing papers like one a week so it's you can just google that so this is really interesting so first of all they give up on the linee rarity assumption which I like very much and their target is this projection parameter the best linear parameter now they do have a lot of assumptions they are going to assume that the the errors are normal they're going to assume that the variance is constant and that it's known and they're also going to assume that the covariant is fixed and what they do so now they select a model by some method I'll say the last cell but it doesn't have to be a blah so and again they're going to focus on the projection parameter the beta the best beta for that selected parameter let's just focus on one of them say beta s of J and then they make the very following the following very interesting observation they're going to choose an event e very carefully roughly speaking you can think of this event E is the model you chose so let's say I chose variables one two and eight they're going to conditioned on the fact that you chose models one two and eight now that's not exactly true it's a little conditioning events a little bit more subtle than that but morally and they're just conditioning on the selected model so here's what happens so something very very interesting happens let's look at one of the beta J's so we have this beta J in this selected model and I'm just forming the usual pivot and what is the distribution of that conditional that I've selected variables one and two but we started with everything kind of normally distributed now we're conditioning on some event roughly speaking it's going to be like a truncated normal distribution and in particular in the distribution generally would be something that depends on the whole distribution right even though we're only looking at this parameter its distribution it's law could depend on all the parameters but by the conditioning does something else it's by sufficiency it ends up only being a one parameter family so you get we get like a truncated normal and it only depends actually on this parameter and this is also because Sigma is known so if you if you take a truncated normal ticket CDF I could now I have a CDF it's well it's actually a family of CDX index by one unknown pram no problem we just treat that as a pivot we just evaluate the CDF for every possible value of the true parameter that gives us a test and you invert it you just do test and invert I mean in principle you can always test and invert anything right it's it's a very general method but usually you'd have ten billion nuisance parameters and would be a huge confidence set and so on but because of the assumed parametric model what this conditioning event has the property again because of sufficiency that it reduces it to a one parameter family of distributions and there's a lot of interesting if you look at the papers it's all done geometrically it's it's all very very fascinating but it leads to something you can now compute because it's a one-dimensional family you test and invert it and it has these very strong advantages which again they're not assuming the model is linear at all and there's no incoherence assumptions no independence assumptions on the design matrix so that's very very nice that's the nice things about it the disadvantages are it is assuming normality constant variance known variance and also there's a problem which I call fragility and I have to be honest with you I John and Rob and Stanford will not necessarily agree with me about this so this is my opinion but I think it's very fragile because it's it's not just depending on the normality but it's depending on the tails of the normal and let me certainly explain why suppose you had a thousand variables and you just did one step of forward stepwise progression so what are you doing you're finding the correlation between y and x one why next to I next Li what variable goes in first the most correlated one so where are you in the tail that distribution all I mean in that district you you're always in the tail and now if you ask what is the distribution of let's say you pick variable one so what's the distribution of the correlation between y and x one conditional on the fact that that correlation was bigger than all the other ones that's the truncated distribution you're dealing with you're necessarily your p-value is a truncated distribution in the tail of the distribution you're comparing two tail probabilities so let me draw a picture of it you've got this distribution and you're conditioning on this event basically that I chose this model so the conditioning event is this thing now when I compute that CDF that truncated CDF I haven't I have a value of my test statistic say it's here and so my p-value is roughly speaking the ratio of this to this but you can see remember I'm like 10 to the 6 standard deviations out into the tail and I'm very nervous about compute comparing tail probabilities of these normals and in particular if you tried to say well we want the central limit theorem say everything is approximately normal sure in the center but there's no central limit theorem that tells you that the tails behave approximately normal so that's what I call fragility whether it's a real problem in practice is something we're not sure yet I'm just starting a project with my colleagues Ryan tips Ronnie and Matt gazelle and some other guys where we're going to do a really thorough simulation study about these kinds of methods and compare them and see how non robust they are but just to give you an idea of how sensitive things are to the tail here's two distributions one's a normal and one is so close to normal you can't tell the difference but if you compute a ratio of tail probabilities like say this divided by this and look at that as I'm that as I move the event you can see that the that thing is you can get I think that's the p-value function you can get completely different p-values when you're in the tail the probability another way to look at it is if you look at this when you're truncate when you're conditioning on the model so think of these different sectors as different models you're conditioning on and your p-value is just compete in the end is just some function over the sample space but it's different depending on which model you're conditioned on if you actually look at the pivot to actually look at the pivot you're computing that supposed to be has a uniform distribution the function looks like this it's a it's kind of smooth in here but at the boundary of every model it's non differentiable and so you're computing the function you're using to do the emphasis this is really bizarre thing with all these sharp corners on it and so on which suggests again that this is not going to be a very robust method of doing inference in fact if D was fixed if you really did fix the dimension you could just say look central limit theorem says everything's normal this is just a continuous function it's not differentiable continuous so by the continuous mapping theorem you you the p-value is uniform but we haven't we have a paper where we look at this another archived one you know refereeing is so slow in statistics all my papers are still archived that if the dimension increases if D over n goes to infinity and epsilon is not normal then it you do not get the pivot does not converge to a uniform zero one in fact we we have an example where it converges in probability to zero well actually converge to zero with some fixed probability so there's some issues there about the tails and the robustness having said that those are technical things the basic idea though about you know trying to condition on what you've actually selected I think is very attractive and there's something important there so maybe there's a robusta fide version of this that would work much better I think that's an interesting open question uniform methods and this is the approach that's been pushed by the Wharton guys I shouldn't say guys Linda Giles there so I should say important people roughly speaking it work the thinking works like this suppose I could say here's all the possible models I could select so just to have a concrete example I'm going to do forward stepwise selection with a maximum of twenty variables so now I can say okay all subsets of size that most twenty that's my script s in principle if I could now look at the CDF which is beta hat of s that's the beta hat for the selected parameter minus beta s that's the projection parameter and if I look at the soup without overall the models I could select and get this CDF if I knew that CDF that would give me uniform inference over all possible models and in particular if I now select one randomly using some method it's automatically correct because it's uniform over all models and that's what they do so they if I knew that CDF I would just take say my beta hat plus or minus the tail probability there and you and hack automatically have a valid confidence interval so the advantage here is there's no linear model there's no incoherence assumptions and it's got what I call honest coverage as I described earlier the disadvantage is how you can estimate this you could say well can't I bootstrap this but think about bootstrapping it at every bootstrap every time you bootstrapped you'd have to take the soup over every possible model you could select and you definitely do that in every bootstrap sample and that's it those are np-hard problems typically so it's computationally infeasible so how do they do it well they say well let's assume that the data are normal constant variance known variance and then they can actually they found some ways to compute this but even then the last time I talked to andreas I think he said maybe they could do 12 variables or something like that so this is quite limited at this point now maybe there will be some breakthroughs but I think there's a serious computational bottleneck here now the my favorite method and the oldest one is sample splitting I don't know everybody rolls their eyes at this but I think and so we shouldn't write it off I tried to trace the history of sample splitting because it's people have been doing this for so long and it's kind of hard to trace it because I think people just do it and don't write about it but you know I found references hard again 69 Maran 73 and so on here's me if anybody else has other references let me know I thought I liked Barnard's quote this is a 1974 he says the simple idea of splitting the sample - in developing hypothesis and what part in testing remainder may perhaps be said to be one of the most seriously neglected ideas and statistics and I you know here it is 2016 and I kind of think that's still true and so that the actually the paper that we're going to post on the archive in next week or two is a kind of a modern look at sample splitting we kind of want to revive interest in sample splitting but it's very simple I'm sure you all know it you just split the data into two pieces you do whatever you want from the first data you select the model with an arbitrarily complicated selection procedure then on the second half two data are independent so you do whatever you want maybe just fit least squares and use the normal approximation or the bootstrap whatever your favorite tool is and that's it life is easy and that's again that that's automatically both honest because it has this info over P but it's also robust because it does not depend on what you did to select the model so I love that no assumptions robust to the selection rule now there is this concern that in the first step you're only using part of the data to select the model so you may not pick the most predictive model above the lowest prediction error and in our paper we explore this to some degree but frankly there's a lot of open questions there so how to explore this trade-off between prediction accuracy and inference robustness is still something where we're really thinking about and it's it's a serious issue you can it can make a big difference and you can combine this with any parameter so in particular here's the local parameter I talked about but before I had the mean why not use the median without distribution and the beauty of that is if you do sample splitting and if you use loco which is a better parameter and if you use the median instead of the mean you actually can you know getting a confidence interval for a median you can do using order statistics you know in a distribution free way this is the only method I know for doing high dimensional inference and regression where you can actually say that the int it's not asymptotic and there's no assumptions on P for all distributions P all selection rules W the coverage is 1 minus alpha that's a pretty good you know claim I think and and it's simple I mean this is something you can you know run pretty quickly so we're trying a big simulation study because until we do lots and lots of cases we're not going to convince people to do this I think let me just talk a little bit about the sub sampling approach of mention hooker and again I think Susan will tell you about her approach to doing random forests with inference for that this is a little bit different they fit this is a very recent paper just appeared in j JM lr for those of you don't know the leading journal in machine learning is JM lr journal of machine learning research and like and all their articles are free online as should all I don't know what the state of economics is I've been fighting this battle in statistics to get all our journals three and online I don't know what happens in economics but ml as usual is ahead of us and they're doing it the right way all right so what they do is they say well you know what is a random forest it's you're getting these trees and you're averaging things and it's each one is built from a sub sample of the data so what have you got you're just taking an average of why am i pointing here I take an average of subsets of the data well that's just a new statistic now it is an incomplete u statistic you can't take every possible subset and this is especially true because typically you let the sub sample size grow as you get more and more data so the fact that you're only taking a random sample of you statistics actually matters a lot and that's a incomplete u statistic there's also the randomness about the covariance election so that's another issue that makes it not quite a Hugh statistic and it's infinite order which means it's not it's not like a typical user disagree just looking at pairs of data you're going to let it grow with sample size so they have a very nice development of a central limit theorem here for incomplete infinite or u statistics and once they have that they say okay so the MU hat is approximately normal and they do a test that I would call like a local test they say they they look at the random forest and they look at the random forest after you've left out some covariance and say do my predictions change or not and they get they have a you know normal based test because they have a central limit theorem and it's very interesting so I would encourage you to take a look at that the test statistic is just going to be again the fitted random for us with and without the covariance of interest and by the way just I talked about linear models where you could do exactly the same thing for linear models there's nothing really special to two random forests here so the advantage is this is nonparametric now because it's a random forest it's not a linear model it's assumption free sort of there's no if you don't like the splitting this avoids the splitting there are some questions that I've been thinking about which is for one thing it doesn't return a simple model whether that bothers you are not or no some people don't care which is great but some people want to have an interpretive also that I guess depends on the application the central limit theorem here there's a technical issue which is the central limit theorems I was talking about earlier our uniform I don't think this one is although it's not clear yet it hasn't been worked out but so I think there's some more research that needs to be done to understand how strong the inferences are here okay oh my mentioning the susan and Stefan's paper here that does a different approach what should I talk about let's talk a little bit about yeah I'll talk I guess I won't get to anything about causation but that's okay because other people are going to talk about causation I mean the big issue in causation I was just all I was going to say about it was you need the extra assumption of course that there's no unmeasured confounders and so that's one more worry and presumably you know there's papers about doing sensitivity to unobserved confounders and I think an interesting area for future research is can you take the work that's Madonn sensitivity to unobserved confounding and adapt it to these machine learning kind of regression models that was all I was going to say there there's a completely different approach to inference that was invented by volodymyr vote who's a kind of a machine learning probabilists lhasa fir warlord I just threw in the warlord I don't know why I sounded like I was giving a line from Apocalypse Now or something anyway and so we've arrived is - yet another unpublished paper of mine where are we still waiting for referee reports where we adapt this to doing high dimensional regression inference but the idea is beautiful if you've never seen this idea do two VOC it's such a simple beautiful idea and implementing it can be challenging but it the idea is very simple and the idea is suppose you want to get a model free completely distribution free confident for a prediction for a future why so it's very prediction oriented you make up a working model let's say a linear working model but the assumption at the beginning of this is that the model is wrong and there's no approximation so you're going to get exact inferences even if the models wrong that's a VOC and what we're going to get out at the end is a as a set C with the guarantee that the probability that the future y is in the prediction set is 1 minus alpha for every possible P no asymptotics no regularity condition is nothing is really a remarkable thing and let me explain the basic idea because I'm running out of time I don't know how far I'll get into it but it's worth seeing this if you've never seen it before so I'm going to do it first without covariance suppose you just have a sequence of Y's y1 to yn all right now somebody says I want to predict a new Y here's what here's the idea let's take a guess at what the next observation is let's call that little Y arbitrary little Y is some arbitrary point now I'm going to test the hypothesis that the next observation is equal to my guess so what I'm going to do is form the Augmented data this is the key step you take your data set and you augment it with yn plus 1 or ym plus 1 is equal to your guests so it's a little bit like the opposite of cross-validation instead of leave one out it's add one in now you compute some sort of he calls it a conformity score we would call it a residual so for example just take Y I minus y bar but it's got it's got to be based on the Augmented data and now again I want to test this hypothesis but I can you can see that these residuals under the null hypothesis if I've actually put in a true guess that's correct these data are they're exchangeable and the ranks of the the ranks of the residuals are uniformly distributed every observation has an equal chance of being the biggest one or second biggest one so if I just count how many residuals are bigger than the one for the one I just guessed that's an exact p-value that's a uniformly distributed p-value but no assumptions at all and that was for one guess why so now I repeat this for every Y and invert it and that's it now I have an exact confidence set and so for any distribution P any N that's an exact prediction set and I shouldn't say a confidence that I should call it a prediction set and if you want to know more about this I should have put up the references but I wrote a series of papers with my young colleague Jing Lai where we looked at this from a statistical point of view so we developed it for the rent density estimation and regression and classification and we looked at its minimax properties and all that and we kind of reexpress it in a more statistical language I'm sorry I forgot to put those references because the properties of this thing the correctness is always there but the size depends on how good your guests of the model is so this is distribution pre finite sample you should look also by the way if you're interested in this just just Google Volk and you'll find zillions of papers of extensions of this that he and his colleagues have done which has largely been ignored in statistics I have to say now which is unfortunate so a linear regression high-dimensional in regression that the basic thing is the same you have data you do model selection you get a beta hat we augment the data we have to repeat the entire procedure but not really there's some tricks to avoid that but conceptually you repeat everything you get residuals you get your p-values in principle you repeat this for every possible x and y and a lot what a lot of the papers about is how to avoid doing that and you get these valid prediction regions so that's a way of doing inference that doesn't depend on the working model being correct and I don't have time obviously to go through all the details of how you do this in high dimension inference so again I'm just throwing up a reference for our work on that oh and also I'm happy to say we have an R package to actually implement this thanks to my younger colleagues who can program well that's in Ryan's github thing whatever that is but I know if you go there there's all about our code including every example we did and it will reproduce the examples first this is like in the in the spirit of reproducible research so I think I should so this is the causal stuff I'm going to skip I'm going to go to my conclusion so in conclusion there are now are many methods for doing inference for these high dimensional machine learning models or statistical models whatever you want to call them whether they're needed or not I think depends on the application you may not need to do standard errors or hypothesis tests on the other hand I get I'm guessing in economics when you're thinking about presumably think about like policies and what policies to implement I assume that probably these things could be very important in some of these cases sample splitting to me is the easiest and gives the most valid robust confidence in rules but you are losing prediction accuracy of course when you split it I want to make a case for forgetting about beta and regression yeah there's other parameters you can use to describe the importance of features but not everybody agrees there's these conformal methods I didn't really have time to talk about but they're worth looking into if you're if you're interested in this stuff for causal inference I really think you know there is a large literature that you all probably know much better than me about doing sensitivity Donner's or confounding but I think I think exporting that to the high dimensional setting is as far as I know have not much has been done on it I think that's an interesting open question that maybe that's a place where statisticians and machine learning people and economists could collaborate and with that I am done thank you for your attention you 