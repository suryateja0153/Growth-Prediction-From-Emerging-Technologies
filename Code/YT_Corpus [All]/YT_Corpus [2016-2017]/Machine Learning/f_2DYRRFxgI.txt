 thanks for having me here is the sound okay back there yeah great so I'm excited to see the much of the views here but it's been a it's been a fun time over the last actually 20 years I've been working machine learning and today you see basically every company wants to do some few machine learning and so we're asking the question these days how can you possibly trust these algorithms they're trying to learn something from data it seems like a huge massive leap of faith and everybody wants to do it in fact my view is that within five years every innovative successful application is going to use machine learning at its core you might call the machine learning you might call it data science you might call it AI but it's going to be innovative in exactly this way it's going to display intelligence and this is a huge massive thing in fact intelligent applications can provide tremendous amount of value well known example with Netflix is about seventy percent of the views come from recommendations so the recommend is doing a really great thing there however it's also well known they stand about two hundred million dollars a year on the recommender team so raise your hand if you have two hundred million dollars a year to spend on your recommender so this processor is slow to build they require a tremendous amount of expertise and they're just really hard so as we talked to a large number of people over the last year's we realized there's three core blockers in terms of building things they use machine learning they're intelligent the first one is for a variety of people especially I'm sure it's not the case for you in the room before most people mapping that business question to the machine learning algorithm is just hard so for example Fiona build a recommender system and you figure out I must use this thing is called matrix factorization understanding how to describe a recommender system as the matrix factorization algorithm it's just hard requires a lot of expertise even if you have that expertise the second part of the problem is really pretty painful that includes engineering features iterating over models trying different possibilities evaluating validating them and that slow iterative process can be hard and painful to execute on but even if you've done that and maybe your team as crew it's something they feel is really cool maybe use Python or are to do that often you have to package that up and hand it off to another team that's going to translate that into Java or C++ to deploy it in production and that process which might take 6 to 12 months basically ends up with something that had nothing to do with the thing that you built in the first place and so this 3 pain point have really motivated us over the last few years to rethink how this process is done so today I'll talk very briefly about how us at data the company spun off think about the process and then we'll dig into some research that we did at the University of Washington the addresses the question of why should trust a particular machine learning algorithm and that really comes from the journey that will describe in this first part of the process so a data the way we think about it is that we must address three problems the first one is how do you get to value fast even if you have minimal machine learning expertise how do you make that mapping easy and quick the second one though is that you don't want a black box solution where you when you first finish it there's nothing else you can do you're done you're stuck you want the flexibility to innovate build new things and modify it into whatever way you want to do it and finally for us that idea of handing over your code to somebody else to take the production is not empowering it's annoying as a data scientist I feel like whatever work I did is not valued if I can't take it to production myself so we want to think about ways that make it easy to go from the beginning from building your first application all the way to take it into the product production so this is the three tenets that we work really hard on to accelerate that journey now our goal really is to allow innovators so this is you and others to create intelligent applications with a new kind of a joke machine learning system so that's our mission a data and with the goal of accelerating this path from inspiration to production let me talk a little bit about what with you this production path to look like in a more agile way so typically going to start for an application you want to build I want to build a new kind of fraud detection system that's going to update every minute and provide real-time predictions within seven milliseconds so that system is going to in the backend be based on some models that you learn from data now translating those models into a real-time system is pretty hard and usually requires a lot of infrastructure the way we think about it is in terms of microservices you can package your models up in terms of this small micro services maybe tens of tens maybe hundreds of them in millions of them which in real time will provide responses to queries that come from the front end and those queries can give you feedback for example the user clicked on this particular recommendation or this transaction was actually fraudulent and we want to be able to close the loop we're going to be able to monitor when things are working or not working and in real time update the models using online learning systems so what you'll see is the ability to do all of these things in a pretty easy way and actually we allow you to deploying whatever infrastructure you want biryani on premises or in our cloud service like AWS so let's say I'm going to do one quick example this we're gonna do a little bit of coding in a second I'm going to walk over there and then we're going to jump into the research that I mentioned in the beginning so let's say that you want to create a recommender system they're going to recommend movies but you don't want to spend 200 million dollars on that or you can do it is with a few lines of code be able to load your data learn a recommender system and deploy it as a real time service and then if you like as you come up with the feedback you can improve modify it or even add your own algorithm so let's see a quick demo of that I will step over here because there's no wire in the podium I hope this still works let's try it all right so what I'm going to show you here is a ipython notebook so jump the notebook you guys probably seen this before and so it's just a way to show called really easily on my browser and we're going to import GraphLab GraphLab crate is a library that we've created which makes it pretty easy to build this model so i'm going to load some restaurant review data i used to live in the bay area so this is from san francisco and reminds me of the olden days there so I let's take a look at what that looks like so this is from yob there is a few columns there's the name of the restaurant a category price and most importantly the user who gave it a particular rating and there's actual reviews of the restaurant and when it was done so normally you'd slice and dice the data in order to get a particular sense what's in there and what kind of data quality you have because that process was annoying slow will create a little visualization tool where one line of code you can get a sense of what your data looks like so we do one pass catching algorithms to try to figure out watching your data so you see different columns is easier if I will stand in front but you see the most commonly reviewed restaurant in San Francisco is a pizzeria delfina anybody yes yes good spot is part perhaps surprisingly most common reviews are about breakfast and brunch and two dollar signs to the way to go so I get a sense of the data and let's say that I like it and I'm and I think it looks pretty good I want to build a recommender system for it you can choose from a wide library of different algorithms like matrix factorization and others but if you just start GraphLab that recommended or create will automatically pick an algorithm for you and in a few seconds on my laptop I now have a recommender service a recommended model which I can query and ask you know evaluated let's do some procedural recall curves let's do a bunch of different things on earth but instead let's just see it in action so I'm not a coffee drinker but a lot of people in my my team doing coffee and I want to find a good place for us to get together i heard that four barrel coffee it was a good one what are the places in san francisco might not get coffee any recommendations from the locals there if I like four barrel coffee okay well let's see sightglass ritual coffee blue bottle fills now I'm more of this kind of guy anybody know I Malaysia in the mission dive bar it's good got a dive bar when I do a little hot thing what are the places i might want to go well I'm i go to casanova lounge dava elbow room make-out room yeah for sure seems like we got a we got a nice thing going here so clearly the model is working and I trust it and hold on to those words in a little bit so I want to deploy this as a real time service so I can build a phone app that uses it normally I'd have to take care of this all myself and think about the whole deployment piece because the process was slowing down data science we created this tool that we call predictive services where I can connect to a cluster here here i'm going to show your cluster that's running on AWS there's three machines they host a number of different models they have versions you can explore look at their how other working monitor them but interesting with one line of code that can take this recommend them although they just build what are you complaining about oh that sorry i had tested a demo and i forgot to remove them all ok so we phone line of code here i can take this recommender model that i learned on my laptop and ship it off to an s3 bucket on AWS and then those three machines are going to copy it out of the s3 bucket and start hosting the real time service they can call from a recipe I using things like curl JavaScript anything you want you're going to call it using Python and if I do so here it returns this Jason responder says if I like four barrel coffee I'm I like site class my like ritual coffee roasters Blubaugh and others and interestingly you also include this unique ID here which allows us to provide feedback loop where if the user clicks on a particular recommendation you can go back and update your model based on that so that's where you can build a system like this in a few lines of code and deploy is a real-time service now seems pretty good and we have this tools to do it but the question is why did I think I could trust this particular model why do you think that would actually work so let's talk about trusting machine learning models if you think about it normally we think about this pipeline we go from data to machine learning model to deploying real-time predictions but the question is how do i know that this thing is really working and how do I know that I'm not going to get fired once he goes into production my student marker Habiru and I postdoc sameer singh at university of washington have done some really interesting work that allows you to understand why a particular machine learning model makes a particular prediction and they've evaluated that with a wide range of different user studies to show that works really well so I'm happy everybody sign the release form in the back before coming in right we're going to do a live user study here so let's say that you use deep learning to solve a very important task not aware at the University of Washington so this is a pretty important task for us we'll take an image and wanna know what is a wolf or a husky for those who don't know ask is the mascot of the University and well as being a husky to the stadium we want to make sure we don't bring a wolf that's going to kill everybody and we're going to use deep learning for that so let's say that we use some test data and we fry it and the test 88 seems to work well let's say for this example is only makes one mistake what I predicted a wolf and it was really a husky my question to you is do you trust this machine learning model to take it to production and not and risk the lives of thousands of people in the stadium yes trust it thank you any anybody not trust the model if you don't trust them although is it because you don't trust me what's the scary so what the question here is why would you trust it why we do not trust it so the work the market in similar deal is really interesting they they do look in our talk a little bit briefly they do some perturbation of the images of the input and try to understand what pirates were most important to making the decisions if you took at this imports and you look at this deep neural network which is really hard to understand but you asked what was most important to make this decision i'm going to show you images here and and the highlighted areas or the most important parts to make that decision now take a look at this and tell me if you now trust it in particular take a look at the ones of predicted wolf if you look carefully you'll notice that instead of a wolf detector we have a very fancy and deep snow detector and this is not just true with crazy deep learning models how many people know the well-known 20 newsgroups data set so this is a famous data sets from about 30 years old for news groups news groups are this thing it's like a forum which later was like a facebook page where you could post things about a topic it's amazing um anyway does this data set that's have been around for a long time where there's different categories like politics and religion and so on and you have to categorize which based on the text of the article which were the news groups that came from and basically any modern machine learning algorithm gets really good accuracy in this data set so whenever I taught a class i said oh this is a solve problem here's the data set where he always works but when you use mark when Samir's work you notice that the actual features he was using to make those decisions didn't make sense so for example the strongest feature was the email address of the person who posted the article here the fact that Sean at gmail com always posting the politics blog does not tell me something about the red sorry i should say no blog forum does it tell me something about the rest and in fact if you remove those kinds of features you only get about fifty seven percent accuracy so it's not actually that good so understanding why you make a particular prediction can help you understand if the mod is working for the right reasons and can help you do other things in the process of going from model to a deployment so for example Netflix if you go it tries to tell you you might like this movie because you also like Lord of the Rings but if you think about our doctor if you have a system that says the probability this person has cancer is ninety-five percent the doctors basically ignore that system but if you say look at this MRI look at this related cases read these papers you help make your own conclusions that system will provide a lot more value to that doctor and so Trust is very important not just in terms of understanding improve the quality of the model but also in how this machinery predictions get used in the real world and what Marcus Amir did was I talked a little bit about it it can highlight what features are most important and the way it works is by taking the input data and trying to figure out were the key inputs were imported for prediction by perturbing the data fitting models locally around each prediction and this models desa explanations then then group around the space to understand how does the model behave as a whole and I encourage everybody to read the paper to understand a little bit more about how this thing works but let me talk about a couple of examples of what happens when Marco build a visualization of this and we put in the hands of people so if you remember mark if if you took the 20 newsgroups data set and you remove those features that didn't make sense then you got a pretty low accuracy sorry if you if you the way I should say this definitely if you take if you train our training newsgroup state that separate test on similar data that doesn't include Shawn's email address you get pretty low accuracy so what Marco did was clean the data remove all the features that he thought did not make sense in order to make the prediction and ask the following question is it possible to show this explanation to mechanical turk occurs that don't have the emotional in background and with a few rounds of iteration be able to get as close as possible to Marco's gold standard so can a human that does have much to learn background use explanations to improve the quality of the model that was the ? cost and what he found was then filled three hands of mechanical Turkish looking at the explanations they were able to get better predictions the Marcos gold standard so I fired him just kidding he is a great student like it's amazing but what it does say is that the explanations are extremely helpful even for non-experts to understand why machine learning algorithm works and how it can be improved and we've been pushing this in various ways so building trusting models is not just about make it easy to build evaluation curves ROC curves quite a quantitative metrics you want to be able to explore the models like we did with restaurants in San Francisco to qualitatively understand why things behave a certain way but and it's important to get explanations to understand why auto makes a particular decision and whether those explanation those decisions make sense and these are all things that were actively working on in order to build trust in the model I call this 3 e's evaluations explorations and explanations i mentioned how this is useful to improve the quality of model we've also been using it to help people understand what machine learning makes a particular prediction so here's an example if you look at churn prediction so this is deciding if people using your product are they likely to stop using it we can provide very interesting explanation so I'm not going to go through this visualization in detail but we segment the users that are churning for similar we are likely to turn for similar reasons and then provide those explanations so let's say this group of users was always shopping in the toddler section of the website and then stop shopping now a marketing person can take those explanations and create a campaign it's pretty interesting pretty natural what those folks are doing you can talk to target them in certain ways so explanations are also good for that kind of approach and we work with really interesting companies that do exactly this problem current prediction now just want to do one last plug for clothes off how many people are taking the online courses that Emily and I've been doing so thanks for joining them I'd like to share something that would be the Emmy function I've been with Washington have been doing for the last several months nine months now which is build a six-course specialization in machine learning and the idea here is a sequence of courses that takes you from the basics all the way to advanced techniques but this is not like other machine learning courses out there where you typically start with what's the probability of coin flips and build up from there and hopefully by the end of the semester you can quickly mention in the last lecture there are some applications of machine learning in the real world we start from the real use cases like how does zillow predict the price of your house and then deconstruct out to explain regression and the underlying algorithms for it or how does pandora figure out what next song you should listen to and we deconstruct out to explain recommender systems without maturity reduction makes a factorization and more and so you see the sequence we have so far about 60,000 students engaged in it and it's been a really fun process and I uh I hope you all give it a try so with that to summarize I say there's a really exciting phase transition today in this world that we've been working on for quite a while what we're going from an academic and well they were there was about you know my curve is better than your curved for saying I just write papers to real-world applications they're really being differentiated by the machine learning of their car and I think our mission here is to create the new technologies that allow you to quickly create intelligent applications gain trust that they're doing what they're hoping the way both a training time and production and deployment services that you can connect to other parts of the organization in real time thank you very much 