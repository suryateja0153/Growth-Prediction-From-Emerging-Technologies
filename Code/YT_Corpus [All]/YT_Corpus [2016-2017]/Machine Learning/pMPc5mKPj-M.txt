 so yawn unless you've been living under a cave has obviously been one of the pioneers of deep learning and neuronal Oaks and sometimes he's really ahead of his time because you know while a lot of the people you know machine learning was focused on convex optimization for much of the 90s and 2000's he kept on saying why are we afraid of non convex optimization so he's been training convolutional neural networks and these things are getting great results for a very long time so I think we're happy to have him so you can give us some more insight into how that works Thank You Percy yeah I actually had to talk at a workshop a few years ago the number of years ago called who is afraid of non convex optimization that must have been almost ten years ago actually I don't exactly remember the year and I think there's even a video of it which is embarrassing okay so I'm gonna talk about large-scale optimization for for deep learning and about a different you know a number of different topics some of it have to do with just you know learning the training deep networks and and you know others about inference and things of that type so you all know about you know deep learning we stack multiple layers and they're all nonlinear so inevitably we get nonlinear we get non convex functions to optimize and it's and you know we have different ways to use to compute the gradient of course a lot of us use back part but I'll talk about some alternatives to kind of computing gradients in in complex systems and you know we are used to talk about talking about neural nets which are sort of continuous of matrices and mostly linear operators with point wise nonlinearities but there are other systems that we like to train and you know we have trained systems like this you know going back to the early 90s which are composed of combinations of neural nets and structure prediction systems like you know what we now call CRFs and stuff like that so you know think of this as deep CRF if you want this is how the you know check reader is back in the early 90s were trained you know discriminative training at the sequence level where you're back propagate gradient through accomplish on that so these kind of makes other problems appear than just non convex optimization and I think there is you know a whole renewed interest in these kind of methods now structure prediction on top of deep learning for reasons that are obvious we need to have inference you know classical deep learning systems are basically just feed-forward and we need to have some sort of way of performing inference and usually inference means minimizing some energy functions or some you know likelihood a contrast function the score function whatever you call it with respect to latent variables and so in the loop of the of the optimization due to training you have another optimization and what you have to optimize is not you know just need to find a minimum of a function that's well define you need to find a minimum of the minimum of a function of course that's a very you know very common sort of relational optimization so the kind of if we stick to our simple neural nets though what what we have is a succession of in sort of most modern neural nets you have a succession of linear transforms point wise non-linearity which are really the values are really max functions they're kind of switches that choose between the the value that comes into them or zero okay and because it's a max it's continuous but it you can you can think of it as a switch essentially and then we have pooling operations and the pooling operations also the most common ones are is is max pooling and again it's a switch that picks you know the the maximum between multiple functions so you can think of the entire network basically as succession of linear operations and switches that can move around that activate certain paths and deactivate others and of course people have you know you know built gigantic networks with these things and and the running you know it's pretty cute when it runs we need you know when it works this is just you know SGD training the first you see the first the filter is an effort layer okay but of course the as we know the objective is non convex and so the reason it's non convex of course is is that you have products of variables from from the bottom to the top so if you take one pass in one of those networks okay which has again real use perhaps max bullying you have you know weights between between units between nodes and then you you know you might have switches here that active at this node or not if you have values and the the contribution to the output of one of those paths is the product of the input by all the weights along the path and when you have products of variables and you're optimizing with respect to variables you inevitably have hyperbolas popping up so if you take the simplest possible example of a multi-layer net this is a single path network as one input one hidden unit when I put it should be Z and what we're gonna train this machine to do is just to compute the identity function you know plug 1 on the input output 1 on the output and that's the only example you need to do to plug really you'll need anything else and so the obvious solution is to set W one to one value and W two to the inverse of that value for this to be the identity function without talking about there's no nonlinearities in this thing and so the the shape of the the last function here in the space of the two weights is something like this where you have a hyperbola you know on each side that indicates that you can set you know the the to the to divert use two positive values W and 1 over W or both to negative values you have those two kind of disconnected local minima and of course there's a saddle point right in the middle so anytime you have a symmetry in a network you have a saddle point because if you have a symmetry that means you have two solutions that are equivalent and to go from one to the other you gotta have a saddle point or local Maxima but that's very unlikely so now try to kind of picture this in high dimension so now we have multiple inputs we have lots of hidden units we may have multiple outputs but let's just consider one for a moment the number of dimensions of the space increases and the number of solutions also increases for the same if you keep the complexity of the of the task constant if you have a very very large network most of those solutions are connected because there are kind of simple ways of transforming the weight so that you transfer the function from when when you need to another or from one path to another and so you have very relatively high dimensional solution spaces you have disconnected regions due to symmetries and or sometimes connected and you have tons of several points yeah ridiculous amount of saddle points because for every signature you have a saddle point and and and you know there might be in different locations and and so you have sort of community really large number several points of view if you want so that picture I mean I had that picture in mind a long time a long time ago in fact I gave a tutorial in I think 1993 or something like this at nips where I had pretty much the same figure to show that you know how let the shape of the loss function that we were present that we think that we think you know about but you know nice it's not you know it's not as obvious that's kind of a nice ball or anything but it turns out we know quite a bit about the properties of functions of this type as anyway I was alluding to so I'm gonna sort of give you a flavor for some work that that we done with we did with Java now who's in fact the person who should be talking about this really here in my place is an a common scare that says our work she's right here she's looking for a job by the way she's a postdoc in my lab but and hmm and so this it's based on this idea that basically you can view a network with with values sort of a sum of path if you want so the input output function for one particular input to the output is the is the sum over all paths of the contribution of each path ok the country which path is our additively combined if you want and each path is either activated or not it's activated whenever all the units along that path are in the linear region and it's not activated whenever a single unit along this path is is on the in the flat region okay of the value and so so the input output function again is sum of a path of the along the path of the input multiplied by the product of all the units and then multiplied by binary indicator variable that indicates whether the path is active or not so that's the input output function if you want this Delta here which unfortunately depends on all the w.zahn all the x's okay bad news this Delta here this battery function and indicates whether path is active this is the product along a particular path of all the weights along this path and that's the corresponding input at the start of the pass and this is a sum of a path from from you know from particular input so of course what we're interested in is not the input/output function it's more the last function so the last function we're going to use something like a hint rose and a hint routes is another max of 0 and something or max of M and something or 1 and something and so we're going to plug this in and in fact the only difference it makes is that you know this cost function is going to be another value a max of 0 and you know a margin 1 minus the product of the desired outputs that YK is the desired would say binary classifier times the output of the network for a single act with network you can view this as just kind of a another Delta if you want or another input you know another coefficient that depends on the input and now the output and the Delta and now also depends on me it doesn't depend on the output but it depends on the input and the weights and so you can rearrange the terms in the end what you get is that the last function is a polynomial in W where you have products along path so the degree of the polynomial is the number of layers and in front of this you have a very complicated coefficient which can be either 0 if the path is inactive this is sum of a path again so it's either zero is the path the path is inactive or it's a value that depends on the inputs the outputs and and you know not the way it's so the sort of known the fact that it's zero now it depends on the weight but the value will precise value of it doesn't depend on the weight which I guess I should have made more explicit in this in this formula so in the end if you kind of ignore the you know if you kind of take into account the fact that this W really doesn't enter if not for the fact that you know it determines whether this thing is binary or not and this is a polynomial in W with random coefficients and about a year and a half ago it was at lunch with Robin now who was the director of the quality C to tell NYU and maybe it was a couple years ago and and we started talking about the problems that that we are both interested in and he and you know and he was talking about you know the properties of critical points of force spherical spin glasses and where I started to think that looks very much like the last function of a multi-layer net and we're talking a little bit about this and and realized there was a connection and then we kind of decided that we should really explore this and so the the the the results are known about the property of those of those function is that the the is the concern the distribution of critical points at different energy levels so there is a ridiculously large number of critical points whose a number of you know several points who know where the number of dimensions coming up a number of dimensions coming down is roughly about the same and there's a very large number of those in sort of pretty high energy levels okay so if you take some random point the energy level is very likely to be at a stage where you have lots of cyber points that with sort of equal numbers of of dimensions curling up and down and then if you go down in the energy you can start to find several points that have fewer dimensions going down and more coming up and the distribution can be computed when the coefficients of this polynomial is a constant the so called you know when they're Gaussian basically the Gasman example and and so one result is that the when you get down to low energy levels you you start hitting a level where you start having local minima and they are very very they're clustered in a very narrow range essentially so if you plot the histogram of local minima as you know as a function of the last function it's relatively narrow there's a very small number of high energy local minima and these are the ones that are annoying for machine learning because the other ones you can get trapped in and so if you get entrapped into the one of these guys you you get a bad local minimum but in fact most of them the huge vast majority of them is in a very narrow range and so that really piqued my interest because that what that means is that you know it doesn't matter when you do optimization you're very very small chance of getting trapped in this high high local minima and basically every time you optimize you're gonna find one of the one of those big guys what it also tells you is that there is no way in hell you're gonna find in any of those guys because those are so more so much more numerous that's where you're gonna get trapped you know you're never going to go through that that that floor if you want that's kind of a floor and you know it's got a few you know tiny holes in it but you know you're never gonna find them so so to to my students Michael Ian F and Michael met you who are here worked with with Anna and there's some experiments where you know you try to you know just very super experiments to compute the histogram of local minima in a simple neural net as we vary the size of the neural net so this is you know trade on on n list but it's gonna be reduced size n missed because you know all the way it takes too long and we do a lot of experiments and every time we get a solution and we can plot the error rate and the smaller the network the the broader the distribution of energy of the MIMO but as the as you increase the size of the network the the so V McCluster you know that down to kind of low energy that are very very narrow and this is sort of commensurate with what we know about about optimization which is that the back in the old days we were training neural nets where a lot of people were training neural nets we were told that you know besides ditions and we sort of it made sense to us that if you have a neural net that's too large you know we're over it's over parameterize then you're going to over fit and it's not going to work it was that that's kind of wrong experimentally even though experimentally we were kind of really observing this phenomenon that when you make the neuron that's bigger the the test error goes up it's only true for certain types of of neural nets it wasn't roof accomplished on that it wasn't true for relatively deep nets you make them bigger they generally just work better like if you see you know if you look at the statistics of how big neural nets people train these days even with sort of relatively limited amounts of data a gigantic networks with the huge number of parameters more many many times more than the number of training samples of course they're regular eyes with you know dropout and various other things but but they're way way bigger than necessary and in fact that has become the mantra of deep yawning you there is a big difference between neural nets in the old days and what we do now is that we train gigantic networks that are way oversized for the problem and we just regular eyes the hell out of them that's that's the strategy to employ and the reason for doing this is of course if you have neural nets that are way oversized it's much easier to find a local minimum of the of the the loss function so the intuition I have which is not confirmed by experimental theory but it would be interesting for people to work on this is that the the number of dimensions that curl up in a neural net if you want is determined by the complexity of the problem but not by the architecture itself so as you increase the size of the neural net what you increases the degeneracy of the memo so you're gonna have more minima that are flatter in many more dimensions but the number of dimensions that curled up is just determined by how complex your problem is it's not determined by the size of the network and so as you add dimensions new parameter space those dimensions add dimensions where the minima are flat those mean you might become much easier to find because it's just more you know more exposure of them you know there are a bigger proportion of the space if you want so that's the intuition and in fact you know it correlates with things like super vector machines you think about a super vector machine it's a it's a machine that's been built to you know it's such a big size that you're guaranteed there's going to be some you know minimum that actually learns your training set and if you make the the if you design the kernel properly so it's the same idea make the network you know bigger than necessary then regularize the hell out of it so those are the theoretical results you no lie allow us to compute the distribution of critical points in kind of ideal case where those coefficients are Gaussian and this is the the sort of theoretically computed distribution of critical points near so this is kind of the critical points that have a small number of dimensions curling up okay so these are the minima these are the ones I have one dimension sorry smaller my dimension curling down and sorry so a saddle point that's Mima is a saddle point to one dimension curling down set up on with two dimensions coming down 3 4 5 etc they're all very very very close in terms of distribution and energy and she's a zoom on on this distribution which is again the the the histogram of local minima and critical points of small index with a small number of dimensions coming down so this is now getting into domains that really are not my specialty and I have to you know be a terrible confession to make which is that I don't actually fully grasp all the mathematical medical techniques that are involved in convening these things but there is sort of two regime in which you can compute those histograms one is single replica symmetry breaking and the other one is so full replica symmetry breaking this is for four for one replicas major breaking methods the curve is very different for full symmetry epic simply replicates which working and it's not clear which of the cases applies but we are sort of this door hints that maybe the full Africa's major breaking situation applies okay so that that's it for this part I think what's important about this you know I'll say so one thing I should have said by the way is to to make those neuron that look like they approach the conditions of the theorem that applies to do this we have to make sort of ridiculous hypotheses one apart is this we have to make is that basically all the the paths are our independent should probably show the other one no that's good so we have to make that but insists that all the paths are independent basically we explode the network into kind of a gigantic number of inputs where you know there's a whole bunch of paths that converge to the single output and they're all independent of each other and then we have to assume that the coefficients are kind of Gaussian or close to Gaussian which we can you know do the trick on the value of the weights but they it's gonna run a pretty bad approximation and so we're basically assuming the and we assume that those coefficients are independent for the pass which of course is not true because the pass share you know a lot of sub path with between them so we're basically assuming that all the paths don't interact this is you know it's kind of an ideal gas model of of neural nets if you want where you know you know in an ideal gas the particles don't interact this is sort of an ideal gas model of neural nets but you know physicists do this all the time mathematicians they'll pull the hair when you do this but you know we're allowed to put our physicist hat on if we have one okay the next thing I want to talk about for which is a very sort of curious phenomenon that we observed so there was a paper a poster by one of my students teaching Zhang at the anime in conference on a method called elastic averaged SGD and it's a technique for paralyzing SGD over multiple nodes it's a very very simple idea it's you know a DMM when you take out one when when term essentially so that is super simple you you assume you have N or P processors or P nodes you know P workers that independently minimize their own loss function and they all minimize the same loss function using SGD but they see the samples in a different order so there's no reason to believe that they all do the same thing in fact we kind of make sure that they don't all do the same thing and what happens is that there is an elastic term that attracts the the parameters of each of the worker the local parameter of each of the worker two kind of a central parameter if you want okay it's just an elastic term so that's this term this quadratic term well each worker is minimizing is its own local last function plus its regularizer that sort of is l2 regular as well tracks it to the center and the center itself can attracts the average of all the workers but kind of slowly if you want with sort of a moving average formula which is something like that so that's kind of a moving average where the the central parameter can attract the average of all the all the local workers and the local workers just you know get updated with you know usual stochastic gradient with this l2 regularization and it works really well and the the advantage of it is that is very robust to delays in communication between the parameters so between the parameter the local workers and the and the central parameter server you know in the absence of any communication each of these guys is doing its own you know nice optimization it's just that this elastic term can attends to you know bring all of them together and one interesting phenomenon about this is that it's you know it allows us to paralyze over over many nodes but but what it allows us to do is use a very large learning rate so that the worker can kind of oscillate like crazy because they're bound by this but it's you know elastic band they can't really go too crazy they can't they can't diverge you know out to lunch they they they kind of are bound to the center and allows that allows us to kind of crank up the learning rate without adverse effect and then the the central wait you know plays the role of averaging like in averages Gd where you know it's sort of smooth so that the the fluctuations of all the all the workers the workers are exploring like mad because they kind of bounce around and converse pretty fast but they don't converge they they they are kind of you know distribution sort of convergence but they but you know it's it's the the the product the central parameter vector that kind of tracks that by the way you know and I was also involved in this in this work and Sitchin is sitting right next to her so if you want to talk to them they are here so here is the mystery so there's two mystery you know we can sort of explain you know why this is stable and things like this but but the mystery is that experimentally when we train your large neural Nets with this we have two versions of it when that you know where we run regular HDD on each of the node and another one where we run momentum Nesterov momentum lgd on each of the node and the one that uses momentum energy on each of the node goes a lot faster like really a lot faster and then the regular one and we we don't know why we don't understand so we need help you maybe will figure out but you know it's just as good if someone else begins it out it's a bit of a mystery but experimentally there is no question that the the momentum version works really well so let's see Nana you know if you got some theory about this that can I tell you the real you know this study the stability as a function of the parameter is the two parameters that are important of the three parameters are important for this algorithm the running rate the regularization constant that attracts the weights the central one and the the number of the number of workers and the delay in communication between the between the workers let's see Santino's who proved that a DMM which is sort of a well-known version of this where you have in addition to that a constraint term you showed that this is actually unstable when you have delays in communication so it's so this constraint term is really bad for you you you have to get rid of it and that's basically what ESG is it works really well I'm not going to go through the results of switching presented them at the main conference but you know it gives a pretty good speed up and this is on a relatively small number of CPUs 16 GPUs actually it's our GPU experiments now this is for imagenet which is kind of the stress test if you want for a lot of deep running techniques and it's you know pretty healthy speed up for you know test error when you run on eight GPUs compared to kind of other methods like dump or and and you know sequential sequential HDD and then the other thing that's really interesting which caused a bunch of companies like Facebook and others to implement this since the archive paper came out is that it actually gives you better results in the end it not only does it converge faster it actually gives you better lower error rate of the the test error and that's another kind of mystery it's not quite mystery because people have worked on this but but but it's this idea that you get a std gives you better generalization error than kind of smooth gradient if you want because the fluctuations because of the fluctuations down the bottom of a minimum the the when you when you have a kind of a high filtration algorithm with a large running rate the the system wants to converge to a flat minimum if you have a very narrow minimum and you have a few questions your average error is quite high since the algorithm wants to minimize the average error is gonna is going to choose a flatter minimum even if it's are the same even at the bottom is at the same height it's going to prefer a flatter minimum than a narrower one and that's good for generalization because you know your your cost function or your training set is like this and of course on your test set is slightly offset and so if your minimum is very narrow your error on the test set is gonna be high okay we have a flatter minimum it doesn't matter if you move it a little bit you get more or less the same error so this has a regularization effect SGD has a regularization effect and we've all seen this that you know when you train with a GT you get much better results and again your training with large mini batch or with batch batch gradient when you I have zero okay okay so in the zero minute I have left I just mentioned one thing called target pop and and target prop is kind of it's something that I've been working on since the first days I worked on their own nets in like nineteen eighty-three or something eighty four and this the idea of not backpropagate ingredient but propagating targets virtual targets for four units or layers if you want and it has to do with sort of a you know elegance tools but you know basically you can think of a multi area neural net as a succession of functions with the set of constraints which is that the output of one layer has to be equal to the input of the next layer and you can write a lagoon or Lagrangian that actually has those constraints this what's written at the top here that's the output of the ice layer and it has to be equal to the input of the eye the output of the yeah ice layer and that has to be equal to the input of the I I plus one layer and then at the top you have a loss function but what if we relax this constraint as a penalty so now we don't force the input of the layer to be equal to the output of the of the previous layer but we allow for slack between them like a you know a square loss or something it's quite error so you get this kind of thing where instead of the constraint now you have you've cost and so now you have extra variables with respect to to which you have to optimize your your you're now augmented objective function and those extra variables are the Z's and of course you have a coefficient here but all those those Z variables and this is like kind of like e/m it has the same effect as um you know if you train a mixture of Gaussian model with gradient descent you can do that it doesn't work it gets stuck in local minima which you do a.m. which means you you have extra variables indicate the assignment of each of the component to the you know to each point to a component then all of a sudden you're doing a minimization in much bigger space with this augmented innovation free energy and and all of a sudden it works it converges and and the reason it works better is because you've added dimensions and that allows the system to get around local you know several points and not get stuck in local minima the local me man I've become several points so it's the same idea here instead of constraining the although you know or being all those constraints you relax the constraint and all of a sudden you have more dimensions with which to play and the local minima problem becomes less of a problem so we use this in number of situations in the past in particular for sparse auto-encoders we use his predictive spots decomposition this was a paper in 2009 which was never accepted in any conference and we use this also for time series prediction for recurrent Nets so we have a recurrent net and instead of containing constraining the output at time T to be equal to the input at time T plus one we have the slack variable and we can do inference over the entire sequence and train a recurrent neural net this way and this has several several advantages which are it sort of feels a bit like hmm when you do sort of an inference after you observe a sequence and more recently yoshua bengio and a couple other people had papers on this idea of also kind of computing visual targets for intermediate layers or intermediate steps in a recurrent net I have a lot of hope in the fact that those techniques will you know come to the fore eventually because there is other than two adventures to them but they're more expensive funnily enough then just doing back pop in many cases okay so I'm into a negative time so I'm gonna stop here and thank you very much 