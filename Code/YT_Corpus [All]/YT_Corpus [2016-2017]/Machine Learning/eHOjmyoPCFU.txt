 okay so this is so the presentation is called double machine learning for causal treatment effects I guess we try to be creative with the title right here so it's it's based on on to two papers mostly on the second one also having the same title it's a joint work with Dennis chess belicoff Esther the floor Christian Hansen in or the mirror and Whitney nuit was it was put in there heave 2016 it's a fairly simple peeper but it took many people to write it so it's a and then and then we have another paper which is very complicated complicated and it's for coming in America because of this it's a it's called program evaluation and causal inference was high dimensional data its joint work with Alex Bologna he won Hernandez well and Christian Hansen okay so I'll be mostly focusing on the second paper I'll be mentioning what some of these items are covered in the first paper so our goal here is to provide a general framework for estimating and doing inference about a low dimensional parameter theta naught in the presence of high dimensional parameter at the node so this high dimensional nuisance parameter at naught will be estimated with the new generation of nonparametric statistical method and now branded as machine learning methods I resisted the terminology for a long time but then I gave up when and I said we're fine machine learning so the so these methods include random forest boosted trees law storage neural nets boosted trees you could also consider aggregated versions of decimals or or cross hybrids for example you can run a lasso take out a small strand and then fit the forest to residuals that's going to be a cross hybrid and so we would like to use methods like that to do to do estimation of high dimensional nuisance parameter and the target would be some low dimensional parameters theta naught we would like to construct good estimator for 42 not as well as construct confidence intervals for theta naught and of course of course we build upon and extend the classical work in some way parametric estimation which also analyzed a similar problem where you have a low dimensional target parameter and you have a nuisance parameter which is estimated by traditional nonparametric methods such as kernel regression and things like this so this goes back to to work of Bickle Rita Welner in econometrics and this linton Nui Robbins and Rodney's care van der Vaart under one Ruben and many others so oftentimes the way they handle estimations of nuisance parameters they say well under certain conditions the these estimators will fall in a nice set that don't care said it's a set whose complexity is bounded so the so the the complexity of the functions that does not increase with the sample size and in fact it actually rolls out most of these methods that are listed here so for example if you if you take these assumptions like the the estimators take values in a don't square set blonde square sets limits the complexity of the function of the function space and this basically rules out only all the examples that we see a see here like all the high dimensional examples so we would like to go beyond beyond those don't care classes and there's been we did a bit of work on we and others did a bit of work on lasso which goes back to like five years ago or something like this there we use the very heavily the structure of Warsaw there's been relatively little work on using other machine learning methods and so this is what we would like to do now is to is to try to push this this problem to to two other machine learning methods and today I'll make two two main points point number one is that the the machine learning methods like off-the-shelf machine learning methods seem to be remarkably good in in terms of prediction however good prediction does not automatically guarantee that you will have good performance for estimation in in an inference about the sum cause of parenteral or what dimensional parameters in fact the performance can be poor so that's going to be a negative point which will serve as a motivation for the positive point which is which is that by doing double machine learning or orthogonalize machine learning combined with some sample splitting and we can construct high quality point and interval estimates of the causal parameters low dimensional parameters and the I will illustrate these points in a partially linear model it's a it's a it's a it's a nice running example later on I'll generalize it to moment condition models so here we have the outcome variable Y we have a treatment variable G or policy variable so this is the policy or treatment variable theta naught here is going to be the target parameter of interest that here is going to be high dimensional vector of other covere it's called controls or confounders and this yeah called controller confound confounders depending on the department in which you sit and and then that here are the compounders in the sense that g is related to that so conditional expectation of G is equal to M naught of that and I'm not it's not a zero so in as will typically be the case in observational studies in perfectly setup randomized control trials this M naught vanishes and many problems disappear otherwise otherwise they do appear and this this dysfunction is not reported zero okay so we have this setup and so what what I do next I take I take a good a good prediction rule based on this model so for example I can build one using random for it so I can build a prediction rule for y using D and that and obtain a prediction rule D theta hat not plus G hat not of that it's obtained by by combination of least squares and random forests right so I can I can iterate on these squares and run no forests and obtain this estimate it respects the the partially linear for it it has excellent predictive performance for predicting why using G and that why does it have in this example well in this Monte Carlo example I have a G naught that the true function generated as a linear combination of trees so random forest is great for approximating functions there are linear combination of trees so it's it's it works it works very well for predictive performance however the estimate of the slow dimensional parameter that we get here is not so good so if we look at the finite sample distribution of theta hat naught minus theta naught the estimator - the truth the finest sample distribution is shown by this blue histogram and you see it's it's biased relative to to 0 now it's not a crazy estimator this one is not a it's not a crazy estimator but it's it's heavily biased and inference using this estimator could be quite misleading certainly we could hope we we would we could hope for a Gaussian shape like this centered at 0 but what what we have instead is this and then it's clear that the inference using using this distribution well using this estimator could be dysley misleading unless we can somehow bias corrected or something so so this is the so the pretty prediction again so what's happening here like the pin this is a good estimator for prediction purposes it's a regular as a regularized estimator in order to optimize prediction this this this function is heavily a regularized it's it's kind of pressed down towards zero a bit and because of this regularization happening in this part it has a it has a non-trivial effect on estimation of this other parameter here so area Eric so this this this this estimator is biased and it primarily it's primarily caused by by the regularization bias happening here so in some sense it's like it's analogous to the emitted variable bias here it's accepted it's a regularization bias okay so how can we fix this well we can fix this bye bye dude bye bye bye bye bye double machine learning and that's the second point here so we can predict we can solve two prediction problems instead of solving one prediction problem so here we will predict Y using that and we will predict G the treatment or policy variable using that obtain the hazard quantities these estimated functions so here we using random forests but we can also use other best performing machine learning methods then we residual eyes so you you take a wise abstract of predicted value take this abstract of predicted value and then you regress w hat only had to get hit a not check so this is like a fresh wild Lavelle style approach to estimating the the target parameter in case you didn't know the fresh water well was a machine gunner who worked back in the thirties and so so now this distribution of this estimator theta hat not check looks like this so the blue histogram shows the final sample distribution it's centered around the true value approximately centered around the true value and the shape of the distribution is a approximately normal right and if you look at the risk of this estimator and the risk of this estimator yeah this has a smaller variance but very big bias this has larger variance but almost no bias the risk of this estimator is smaller than the risk of the previous estimator so this would be a better estimator and plus you get the content in central this is a estimator okay so this is the double machine learning here so we're solving two prediction problems then we we're using the results to to construct the estimate of the low dimensional parameter so what's going on in in these two approaches well we can look at the moment conditions associated with this two to two moment so two approaches and analyze this a moment conditions in econometrics would like to work with moment conditions with GMM right so so the first in the so the first approach is based on this moment condition so it's it's kind of a regression adjustment condition so it's y minus D theta naught G note of that times D so given G naught this equation identifies theta naught so this is think of that as a regression adjustment equation the second approach is based on this residual ice quantities so we take take Y it's abstract of predicted value take this abstract of predicted value and so on so this one is the regression adjust adjustment approach in this approach we can call the fresh Walla well or we can also call it the Niemen orthogonal approach in a sense that the scores or moment four conditions that we use satisfy the Niemen erdogan allottee condition which I'll describe later so explain this terminology why we use this terminology it also happens that this this score here that we see is actually as I said semi-parametric efficient score for estimating heat and not under homoscedasticity there's also some other equations that you can use for example propensity score adjustment but this will also not work so well for the same reason this the she doesn't work I'll explain why okay so so let's focus on on on this or this these these are the two moment conditions that underlie our two approaches so both of our approaches they generate the estimators of theta naught by solving the empirical analogues of this moment conditions one and three where instead of the unknown nuisance functions we plug in the machine learning based estimators of this function so we have G naught in in in any case of this moment condition we have M naught in an L naught in the case of this moment conditions where L naught is the conditional expectation of Y given that right so we have two conditional expectations entering this moment condition so we we analyze this two estimators so the the the estimator based on the moment condition 1 can be written as least squares of why I minus G hat not that I on G I so this is the formula and we can rewrite this estimator minus the target time through 10 as the sum of two terms a and B a a plus B a is is going to be approximately normal and centered at zero by the central limit theorem and term B is is a kind of a bias term it's a bias term it contains this estimation error and we can further approximate like if we do some sample splitting if G hat comes from a set aside sample we can we can we can do a simple approximation where we can approximate it as B by by an expression like this so what did we do we we took GI and replace it by the conditional expectation I'm not of sorry I'm not of that I write so this is not this is this quantity isn't is not 0 and and here we we have this estimation error so heuristic Li if we think about this problem heuristic Li this is the this is the estimation error in in the sky dimensional or non parametric setting this error is going to be a 4 and 2 into the power of minus Phi Phi is going between 0 & 1/2 and so you have a sum of n terms you dividing dividing this by root N and the term B will scale like root n times and into the power of minus Phi so it will diverge off to infinity so it means that the the this this estimator the prediction focused machine learning estimator theta hat not it's not going to be written consistent it's still consistent but just it's not it's gone it's not going to be root and consistent and it will have because of this bias the bias the bias dominates and pushes pushes pushes theta hat not far too far away from the true parameter so so basically the estimation based on this moment condition doesn't work so well here also you could make a similar argument for the second moment condition where you do so this is called regression adjustment there is also the propensity score adjustment where you you you take D and subtract off the predicted value here so just kind of do do this so this approach will also not not work so let's look at the second approach why does it work so for the second approach we get an estimator which is the least squares of one residual on the other so the residual so the one residual is the residual eyes why another one is the residual is D and under under under mild conditions we can rewrite this estimator minus the target times root n as the sum of three terms a star B star and and an another term which I'll discuss later the term a star is going to be nicely behaved so it's a it's a it's a sum of independent centered variables under under conditions that permit is essentially limit theorem this will be approximately Gaussian centered around zero and now you have another term which is which is the bias term so the bias is still here but now the bias has different structure for the system here we actually has a product it's it has a product of the error estimation errors so there's a there's an estimation error for the conditional expectation of Y given that and this is a summation error for for the propensity score for the conditional expectation of D given that so that there's a product of two estimation error and so potentially it's a good news because before we we had one estimation error appearing and now we have two estimation errors appearing here so this is smaller this is smaller term so if we do back of a back-of-the-envelope calculations were four beasts are roughly speaking it will scale like root n times and into the power of minus Phi n plus Phi L where these two terms are the rate of convergence for estimating m and L and oftentimes you could get you could get this rate to be faster than n into the power of one minus four and so some from this back of envelope calculations from this heuristics we could we could guess that the double machine learning estimate either had no check is going to be root and consistent and approximately centered normal quite generally but of course general generally depends on your own definition of things obviously it's it it's going to be routine consistent and approximately centered more generally than the previous system here but still there's there's still serious limitations here so I don't want to oversell it but it's going to be it's going to be a lot more robust than the previous approach and in it so this claim will be will be true under some formal conditions okay okay so so we we we could we could make this conjecture and we can prove this actually as a formal result under under formal assumptions to be to be stated later now I did mention this two terms a star and B star there's also a third term and in order to to - have a nice behavior for this third term and to have wide coverage of all kinds of machine learning estimators we have to rely on sample splitting so we rely on sample splitting to get a third term to be stochastically vanishing with only the rate restrictions so by using sample splitting we eliminate conditions that that bound the entropy complexity of the realizations of machine learning estimators okay without without without sample splitting without sample splitting life would be very difficult okay so what so the question is why sample splitting here so we have these three terms as I said as I said and if we look carefully carefully this remainder the remainder here contains terms that look like this so it's a summation over I of you I times the estimation error for estimating M naught and if we don't do sample splitting this estimation error can depend on UI and the analysis becomes very difficult so it's still possible to do the analysis you just have to write down explicit rates at which the entropy bounds grow so we have to say that well yeah you have to be very explicit anyway you have you have to make you have to make heavy use of the assumptions for example when we did the analysis for law so we we had to rely on approximate sparsity assumptions very explicit conditions that that that allow us to to limit the degree at which UI and and this estimation error talk to talk to each other right so this is all done so if you're interested in in some ugly mathematics there's we have a paper in doing doing the results without sample splitting and it's forthcoming so it's just yeah you have to like it's hard yeah but you have to write you have to write explicit bonds and things like this but then it's very hard to check those conditions for it like if I give you a hybrid of random forest and lasso if checking the the the conditions on the entropy of the realizations of the system it might be difficult in practice so in order to avoid doing this we we can rely on sample splitting and with the sample splitting what's what's happening is that this estimation error is independent of this and so you can use just a blush of inequality and the fact that this is considered this is going to be well and and as long as this is consistent for this right this term is vanishing not very simple right so with simple splitting it's easy to control and claim this is stochastic vanishing term so without a sample splitting we need to use maximal inequalities to control the supreme of the empirical processes that look like this so we take the supremum here over m falling in assets clipped em and we're script em n is the minimal functions space space that can pay our function set that contains realizations of this machine learning estimators with probability one and so we need to control the entropy the rate at which the entropy of this set grows and it does grow in modern high dimensional settings the traditional assumption that that people use like if you if you I guess in econometrics you open down endless handbook chapter or newest handbook chapter they assume that this set is P don't care it doesn't work it doesn't work or or I guess wonder what book also like has the same assumption so it doesn't it doesn't work so we need to control the rate at which the entropy grows it's fine we can control it but it's just practically it's very difficult to to verify these conditions so anyway so we we get rid of these conditions here and then go back to the analysis of our two strategies one based on the moment condition one and one based on the moment condition three so why does one strategy work and another doesn't work I guess the key I think to in in in in our view the key difference is the condition we call the Niemen orthogonality condition and the condition is that if we if we if we take our new system it Urso this is a two liter regression function is our nuisance parameter the true valid is denoted by at a naught and we compute the directional derivative of this moment condition with respect to or the moment function with respect to this nuisance parameter so formally it's the Couture derivative right of this moment function with respect to the nuisance parameter evaluated at the true value this derivative vanishes so for the moment condition three this derivative vanishes so what this means heuristic Li is that the moment condition the Demong condition remained but remains valid under the local mistakes in the nuisance function so small small perturbations of the nuisance function they don't they don't change the moment condition too much and this makes sense because in in in practice you don't know that the the true value you're going to be plugging in some approximation to the true value and you need you need to reduce sensitivity of the moment condition with respect to the plugin right and so this is the condition in sharp contrast the if we compute this gitaji derivative for the moment condition one the the other moment condition the obvious one with respect to the nuisance function G it's easy to see that this derivative does not vanish so the two moment conditions that we had they have very different robustness structure this is not this is not a robust moment condition with respect to perturbations of G naught and this moment condition is robust with respect to the perturbations of this nuisance functions likewise you can also show that this second moment condition here is not robust with respect to the perturbation of the nuisance function here so the robust one is this one it's a it's one of the rub it's one of the scores that it has this robustness property so the in Eamonn Neiman our tag analogy here is the key and we will use that to to general generalize this example so we know that we need to work with score functions that have this structure and then then under some some conditions we can do fruitful inference on on the target parameter he cannot so we do we do we carry out this generalization we say we're going to work with moment condition models of this form general abstract form so if theta is a vector valued generalize score function or moment function theta naught is the low dimensional nooses parameter at a node is going to be a true value or denoting the true value for the nuisance parameter living in some abstract convex set T equipped with some norm it could be a function or vector of functions we we impose we require that this score here si has the Niemen our technology structure so formally so here we define this pathway pathways or directional derivative the derivative operator and then we require that this derivative van vanishes so it's a it's a it's an operator in this by directions it needs to vanish so it needs to vanish along all pre miscible directions and again so heuristic Li this conditions means that the small deviations and nuisance functions do not invalidate the moment conditions we also rely on sample splitting the sample splitting if one through capital and denotes the set of all observations then our letter AI denotes the main sample the set of observation numbers of size little n that will be used to estimate theta naught I complement is going to be the auxiliary sample it's going to be set of observation names of size capital and - little and used to estimate the nuisance parameter and and we assume that I and I complement they form a random partition and set one through capital n again the user sample splitting allows us to simplify the analysis and enormous Lee and cover a wide range of machine learning estimators for for it for for the nuisance parameter then we have a bunch of irregularity conditions there are more or less standard right so there's like parameter not in the boundary condition there are some mild differentiability conditions the neumann our technology condition that we just stated identify biliary condition we need some mild smoothness assumptions on the score on the score function but but the smoothness assumptions are committing a scores that for example using median regression so some non smooth scores are allowed so so here it's kind of much weaker form of smoothness and there are some some rate conditions related to how fast the moments grow okay oh and also there is a candy there is a point where the entropy condition which is a more or less standard condition because well it's an in principle if we if we use smooth score like that differentiable score score functions we wouldn't even need that okay so so all these conditions are more or less standard and so it looks it looks messy here but if you with some with some song with some work you could you could you could deduce that the rate the rate of convergence here and needs to be faster than n to the minus 1/4 then so then we form this the orthogonalize machine learning a similarity they had no check it's going to be indexed by the set I and I see right it it does depend on how we partition the data to carry out the estimation of the main parameter and any nuisance parameters and the estimator is going to set to 0 the empirical analog of a moment condition because this side may not be different differentiable you you may not be able to set it exactly to 0 so therefore we need to write this more general stuff here but basically the idea is that we're setting this as close to zero as possible and the main result is that under the assumption stated this estimator is going to be after scaling by root N and some other stuff this estimator is going to be approximately linear with the influence functions I sidebar given by this expression here and and it's going to be Gaussian centered Gaussian and the result is shown uniformly over the set of probability measures that live in assets creep PN and the set is allowed to grow so this explains the complexity of some of the notation the analysis uniform it's exactly uniform over the an expanding set now if you look at this estimated is this square root there is a square root of little n here and obviously you can deduce from that this estimator is not fully efficient because it doesn't make the full use of the the entire data set so this is the reason why people don't like to use sample splitting you lose some efficiency however you can restore the the efficiency but by doing the following so we can we can do say 50/50 split of the data into equal parts obtain the first estimate that I just described then we can reverse the roles of the main and auxiliary sample so you just switch the I and I see here obtain another estimate and then you average the two estimates and both of this estimates will will will satisfy this expansion and then so you can deduce from this theorem as a corollary that this will this will attain full full efficiency so you would be able to replace little little m here by the capital n and you can generalize this to a K way split of the sample you can also do extensions to quasi splitting because because well if you if you work with moderate moderate moderate a large sample sizes then it's tempting to not to steal cheat and kind of use the main sample to pick the best machine learning method right so maybe you train it maybe you train a bunch of machine learning methods on the auxiliary sample but it's still like to verify their performance on the main sample and you can in principle do this so you pick you can pick the winner based on the main sample of course this breaks the pure sample splitting so the entropy is back but the entropy is back in a gentle way because so a kind of gentle means square root log of M way and and so the results go through as long as this rate of convergence for the for the nonparametric part of the model times square root log M goes to zero so so the results hold up under under this quasi splitting so again so here the the we are not doing pure sample splitting but it's a it's it's it's it's a way to split the sample in a two in a kind of control controllable way in a in a nice way I have a five minutes left so let me let me mention a couple of things so so why do we call this Niemann Niemann orthogonality so this idea of constructing scores that have this orthogonality property they go back to the work of Neiman back in the 50s and 70s what he was I guess his problem was that he was having well he was working with a likelihood problem parametric likelihood problem he was he was having a nuisance pariah dimensional nuisance parameter but if he was interested in the efficient inference on Peter the main parameter and instead of bad he didn't have am Emily he used something else like as a plugin right and of course and so he asked the question can I get the full full efficiency somehow and and the way he constructed constructed the his estimator basically he he said well III I can use the original score' then I can correct the score I can I can subtract out this term times this parameter mu where mu can be really rewritten in terms of the elements of the information matrix and this this correction what it does it it still it still allows me to identify theta not the the target parameter but it has this additional structure right so the this orthogonality structure and this allowed him to to to to to solve the empirical analog of well to construct the estimator for the main parameter where he used arbitrary plugin for this nuisance parameter not a maximum likelihood estimate some other plugin like maybe a maybe a badly behaved one you still needed some rate rate rate conditions but this this could be much much worse behaved than the the maximum likelihood estimator and so actually this this was generalized this construction was generalized like in semi-parametric settings you can construct scores like this that have this property by by taking original scores and projecting them on to the ortho compliment of the tangent space induced by the nuisance function so if you interested you can read up the under part or you can read with chamber line so there's this so these ideas do generalize to to fairly general framework and we have a bunch of examples worked out one of them is average treatment effect in a partially linear model that I just talked about another one is average treatment effect and average treatment effect for the treated were you what do you have a binary treatment but you have no restrictions on this function and so you're interested in this average treatment effect or average team in fact for the treated there are orthogonal scores that you can have there they also same as W Arbus course we also did the extensions to local average treatment effects and local average treatment effect for the treated and some other examples as well and all of this work so all of this is practical I mean there were many theorems but all of this was put to tu-tu-tu-tu-tu-tu an empirical use so here's an example quickly so here we're interested in estimating the impact of 401k eligibility on net total financial assets so why is that G here is the indicator of working at a firm that offers the 401k pension plan that includes a bunch of controls listed here data is the is the data is due to Patel by at all and so here we estimate a partially linear model and in the interactive model without restricting the the the effective to be without imposing the partially linear function of you see five columns so this random force here means that we have estimated nuisance functions by random forest here by by post la saw here by boosted trees here by neural net and here's but we use quasi splitting to pick the best machine learning method to estimate the the regression function for for--why and regression function for D and it turns out it's not actually it's a kind of hybrid so for for estimating a digression function of Y on that you use la saw but to estimate the propensity score you use neural net because neural networks better and so this this is this is there and you could see that the numbers are fairly close to each other they're quite a robust and so on so it kind of works in in a way that we would expect from the from the theory so let me sum up so so we provided a general set of results that are low root and consistent estimation and probably valid asymptotic inference for causal parameters using white class of flexible modern methods for non parametric estimation the construction will be light on three key elements number one the use of Neiman art organ of estimating equations number two we need it fast enough rate of convergence of estimators of nuisance functions and number three we had to rely on sample splitting to have a to have the to have the conditions stated just in terms of rates and this allows the wide coverage of the machine learning methods without exploiting heavily too much the structure of the of the estimator you 