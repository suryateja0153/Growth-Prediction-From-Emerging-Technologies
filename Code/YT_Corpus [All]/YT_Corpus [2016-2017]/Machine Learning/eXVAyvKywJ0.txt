 hi welcome everyone its I know it's early morning so really happy to see you know quite a sizable crowd here so welcome to the non-convex workshop today I want to I give a brief overview of some of the you know exciting reads recent advances that have happened in this area and then leave it to the great lineup of speakers to give details on a number of different approaches so in optimization we all know lies at the very heart of machine learning right almost all machine learning tasks can be framed as some sort of optimization problem if you think about unsupervised learning you know you can think of clustering that's optimizing some modularity score on our or a similar set score you can think about probabilistic models and then you want to optimize the likelihood under the class of models you're looking at you can think about of course training neural networks then there's some loss function you'd like to optimize unfortunately almost all of these interesting problems turn out to be non convex right so the moment you have hidden variables you cannot expect convexity because once you have hidden variables you have multiple equivalent solutions and that cannot be a convex problem so how do we go about proving or even analyzing methods to solve these challenging problems right almost all the theoretical guarantees we know is for convex optimization and non convex is still a very in a nascent area in terms of analysis and guarantees but it's heartening to see that if you look at the just the trend of like the search write the word non convex is in the upswing compared to the convex so I believe this is the right time you know to be working on the analysis of non convex methods and of course with the you know great success of deep learning we are more and more you know interested in asking you know despite non convexity how can we expect to get such tremendous gains and great solution by the way also wanna advertise there is a very active page on Facebook on non convex optimization now there are lots of discussions and paper postings so I welcome you to visit the page and contribute to the discussion so I want to keep this going even after the workshop so convex optimization we know has this nice property of having a unique globally optimal solution so if you do local search methods such as gradient descent you're guaranteed to converge the globally optimal solution on the other hand non convex methods non convex optimization does not have any of these guarantees right there can be lots of local optima and that's why it's challenging if you do local search methods you can only hope to go to a local solution and it's not clear how that compares to the globally optimal solution right and of course this is a np-hard problem so if you try to look for the worst case scenario this is hopeless right so but this is where I'd like to contrast theoretical computer science from machine learning you know we're not after worst case instances you know learning is hopefully very far from worst case then the question is can we characterize the set of assumptions and of which we can actually get to the globally optimal solution or near that that so that's kind of the thinking we'd like to adopt to overcome this problem of NP hardness and so what are the main challenges when it comes to non convex optimization right so if you characterized the set of critical points or the stationary points those are points where the gradient vanishes so if you initialized at these points gradient descent would stop here that's why it's called a stationary point right so now these points unfortunately can explode in high dimensions and that's why non convex optimization is hard in high dimensions right when you have exponential number of such points it's impossible to say where you land and you know what progress you make and within the class of critical points you can classify them further right so one class or this set of local minima and local Maxima so for minimization these are stable points the local minima are the stable points so meaning in the local neighborhood you're guaranteed to get to the locally minimal solution for gradient descent but on the other hand the other class of points are these saddle points so here in some directions you'll be making progress meaning you'll be making improvements to your objective function whereas in other directions you'll be actually you know making your objective function worse right so this is not an optimal point you'd like to move away from it and there are some deep hardness issues involved in also escaping saddle points so there are two kinds of saddle points one is the class of non degenerate saddle points so these are saddle points that can be discovered by the Hessian information so the idea is if you look at the hessian or the second order information right the second order derivatives of the objective function there are both positive and negative directions so this will distinguish it from a local minimum where all the directions are positive right the Hessian is positive definite in the neighborhood of a local minimum but not for a non degenerate saddle point on the other hand for a degenerate saddle point this is no longer the case so there's hardness of even distinguishing a saddle point from locally optimal solution and this is what makes non-convex optimization challenging in high dimensions so despite such challenges what can we hope to say right so there's been a intensive set of you know steady intensive study in the last few years proving guarantees so in what cases can we actually overcome this hardness overcome having exponential number of critical points and go to the globally optimal solution so I'll give an overview of this class of spectral methods which involve optimization on matrices and tensors and say in what cases you can actually get to the globally optimal solution and we'll see that it has relevance in a number of machine learning tasks so a simplest non-convex problem you can think about is finding the top eigenvector of a matrix so you have a matrix M you'd like to optimize the Rayleigh quotient right I mean this is you know you can think of it as optimizing the quadratic form on the sphere and you would like to ask what are the critical points for this optimization problem because the optimization is on the sphere this is non convex right so it turns out that indeed there are multiple critical points but the critical points are bounded so instead of being exponential it's just at most the dimension of your matrix right so how does this come about so it comes about from the eigen analysis so the critical points correspond to the eigenvectors of the matrix and in the presence of the eigen gap there's only one local optimum which is the top eigenvector so that's what makes matrix analysis tractable right so that's why we have a algorithm that's guaranteed to give the eigenvectors because in this special case the local optimum is the same as the globally optimal solution so if you run gradient descent so the power method is really a variant of gradient descent you're guaranteed to go to the globally optimal solution as long as you avoid the saddle point and it can be shown that just a simple random initialization Awards this set of saddle points so that's the nice thing about matrix analysis that even though this is non convex you can characterize the entire optimization landscape and the only sure saddle points and that can be avoided by random initialization so how do we go beyond this so we can now ask ok you know we have nice properties for matrices can we ask the same about tensors so what are tensors they're higher-order generalizations of matrices right so the idea is you can think of a matrix as you know a representation of pairwise relationships I offer variables so pairwise or correlate correlations of a variable right where I am now for tensors you can think of it as representations of higher order relationships so a tensor a three-dimensional or a third order tensor is now encoding third order correlations between this random vector X so what can we say about optimization for tensors so you can again ask a similar question of finding the top eigenvector of a tensor so in this case you maximize a cubic form instead of a quadratic form all right so now unfortunately this already becomes np-hard if you now ask this for any general tensor this is no longer tractable but if you limit to the class of orthogonal tensors so these are tensors which have decomposition into orthogonal components then we can say more right so there is already a difference now between matrices and tensors for a matrix every matrix has an orthogonal representation but for a tensor the representation with the smallest number of components being the smallest which represents the rank of the tensor need not be orthogonal right so can have non orthogonal representations that are low rank that's the general class of tensors but if we limit to orthogonal ones we can study this top eigenvector problem so it turns out in this case the optimization landscape is much more complicated in fact if you ask about all the set of critical points it's exponential right so the critical points are now characterized by a quadratic equation because if you take the gradient of a cubic form it becomes a quadratic equation and that has exponential number of solutions but fortunately if you limit to orthogonal tensors the local Optima only correspond to the components so each rank one component corresponds to one local optimum and there are no other local optimum so that's what distinguishes a tensor problem from general non convex optimization right we're almost at the cusp of like between easy problems and hard problems that and we can still say guarantees for this so that's the idea now for tens of problems we do have exponential number of critical points but we have the local optima or the stable points to be only the components and now this leads to guaranteed tensor decomposition so what you can do is you can run gradient descent and converge to each of these different components and in fact if you deflate meaning you click over one component subtract it out and run gradient descent again you can guarantee recovering all the components right so this gives us a method now to recover the decomposition of this tensor of an orthogonal tensor and it turns out you can actually extend it beyond you can go to non orthogonal tensors and still recover them there's a simple transformation that can implicitly converts a non orthogonal tensor into an orthogonal one and this can also be computed such a transformation can be computed by a simple SVD operation and this becomes an invertible transformation if the components are linearly dependent so the idea is now you have a guaranteed method for recovering tensor decomposition even when the components are non orthogonal as long as they have linear independence and linear independence is a very mild condition right so this also now shows the power of tensors you can now identify components that are non orthogonal which wouldn't be possible with a matrix decomposition you can identify a matrix decomposition only up to an orthogonal transformation and this is not just a theoretical construct these tensor methods can be run on a large scale they're highly parallelizable and you can exploit all the linear algebraic Hardware accelerations for tensors as well in fact if you think of tensor operations they are a higher-level Blas right right now we have Blas 3 which is matrix matrix multiplication now you can ask extensions of Blas which make tensor operations get the best Hardware gains so that's one of the you know set of things we've been looking at but there are a number of implementation including implementation on spark for large-scale sensors and we also had a poster on this Thursday that showed how you can compute randomized sketches for tensors in a streaming way and never have to store huge tensor objects but maintain it implicitly as a sketch so what are the implications of these guarantee tensor decomposition you can do unsupervised learning for a wide class of latent variable models and these are some of the first guaranteed methods for a consistent recovery of parameters and you can think of Gaussian mixtures hidden Markov models topic models community models mixture of regressions so a number of researchers have been working on this for the past few years and you know for the first time we have algorithms that have polynomial time sample and computational complexity for these models right and it also turns out that these are much more efficient than running variational inference on a number of models for instance in many settings such as community detection you can get orders of magnitude speed-up of running time compared to variational inference and you'll see more of these details from spectral methods in the talk by kevin chen and its application to computational biology in the evening today so it turns out there are also many other interesting problems you can solve with tensor methods a recent one we'll be looking at is reinforcement learning palm dps or partially observable Markov decision process or some very challenging models and for the first time you can guarantee regret bounds under a class of memory less power planning policies and it turns out that when there is low dimensional hidden representations it's much more effective to run these spectral methods compared to cue learning or other methodologies that directly operate on the observed space so it turns out you can use the tensor methods to also train neural networks with guarantees so with backpropagation you know we have no guarantees in terms of what solution it recovers but on the other hand if you change the lost function to a tensor factorization for the first time you can guarantee that you get a good network with guaranteed risk bounds so the main idea is instead of directly using the input you can't answer eyes it you can construct tensor features from the input and you can train these tensor features through unsupervised learning and then you can correlate the output with these tensor Erised inputs and decomposing this right factorizing such a tensor will give you estimates of the first layer weights of the network and then you can build on top of that and in fact I'm how you know excited to have Andrew berends later in the day talk about the classical theory behind neural networks and how that connects with the latest results or guaranteed recovery right so there is this I guess very interesting exciting open problem on whether there is really local optima and deep learning right so if you think about some simple toy examples it's easy to construct bad local optima I'd so have a very simple example here where there are positive labels and these green ones are the negative labels and indeed with a 2-1 layer two neuron Network you can construct solutions which are correspond to the globally optimal solution and here you can separate the positive from the negative clusters on the other hand there is also a bad local optima which separates the clusters in this fashion so here the middle cluster now has is assigned a negative label which is arbitrarily bad training error so even in such a very simple example you can see that there are a bad local optimum for training right and you can in fact visualize it with this lost surface for back propagation on the other hand for the tensor method you are assured of not having such local optimum and I think an exciting question to ask is does this still hold this idea of having local optimum still hold in high dimensions there is some beautiful theory from you know by mathematicians of injurer and Ben aru's saying that if you think about random Gaussian functions right polynomials which have coefficients which are normally distributed there's concentration in terms of the values that critical points take and throw roughly what it says is all local minima are take similar values and I think Yann LeCun would say a bit more I think in terms of what this means for deep learning right but I'd like to give a note of caution that we are still far away from you know proving algorithmically we'll get to the right solution and because what is the challenge even for this nice random Gaussian function we only know that all the local optima have similar values but we don't know what the basin of Attraction is so the current theory still requires the exponential number of random initializations to get to a one of the local optima and in addition there can be all these degenerate saddle points which require higher-order gradient information and that scent be hard to compute so you may not even be able to escape such saddle points so there's still a lot of theoretical challenges in being able to resolve this non convex optimization in high dimension but I think it's a very exciting connection on being able to analyze the optimization landscapes of high dimensional problems so there are a number of other approaches that have been derived in the recent areas in terms of giving guarantees for non convex optimization later in the day Sanjeev Arora will be talking about a number of interesting models such as dictionary learning models where you can first initialize with a different algorithm so you kind of use the intuitions that are specific to the problem and come up with an approximate solution and use that to initialize gradient descent or another you know heuristic algorithm a local search algorithm and a hope that it will go to the globally optimal solution and in one of the posters my student niranjan and young will be presenting recent results on robust decomposition of tensors so in this case you have a tensor you want to decompose it into lower rank and sparse components and you can alternate between finding low rank and sparse decompositions and it turns out that if you initialize it in the right way you can go to the globally optimal solution and in the last nips we had a paper on robust matrix PCA which showed again with if you initialize with just the PC on the entire matrix you can get to other good solution so in a lot of these problems the art is in terms of getting the right initialization and proving that you are in the basin of attraction of the globally optimal solution another important challenge in non convex optimization is even getting to a locally optimal solution if you do not want to do restarts right you have to make sure you escape saddle points because near a saddle point gradient descent slows down right because it could take exponentially long time to escape it so though the main intuition is now you want to use some of the directions of escape and may ensure you go away from it and the class of easy saddle points are the non degenerate ones so in this case the Hessian matrix has both positive and negative eigenvectors and some of the earlier work used the second order information so once you discover what is the negative eigen direction for health the Hessian matrix you can use that direction to move away from a saddle point and there is a very beautiful work by Nesterov and Polly AK that shows how you can do this in an implicit manner and guarantee that you'll converge to a local optimum in polynomial time right I mean even convergence to a local optimum for non convex optimization is challenging but it turns out if there are only non degenerate saddle points you can do that with second-order methods and one of the recent exciting results is to show that in fact you don't need second-order information my student firangee with my collaborator wrong and others have shown that just adding noise to stochastic gradient descent works right in polynomial time in fact you can give a bound on what the time is you can go away from non degenerate saddle points and now the open problem is to ask how to escape degenerate saddle points and the general if you look at a general problem that sent be hard but can we again look at subclasses of problems and ask what kinds of did non degenerate what kinds of degenerate saddle points can we escape and what are the methods so that's an open problem to resolve so later in the day we'll see a number of other approaches in recent years that have shown promise right so the classical notion for a non convex problem is to convex if I it so you can like look at the convex envelope which is the best convex lower bound for a non convex problem and if the function is continuous you know that it will achieve the globally optimal solution but what is the challenge the challenge is even computing that convex envelope in general is np-hard but there are some very new partial differential equation formulations that at least give us new tools to analyze how to compute convex envelopes so the idea is if you like kind of run this PDE ultimately it will converge to the convex envelope and for general problems it's exponential time but we can now ask at least for a sub class of problems can can it become polynomial time and then there are approximations to convex envelope using smoothing operators so you can think of simples you know Gaussian smoothing and ask how that approximates the convex envelope and whose high and mobile he will talk later in the day to make this precise like you know like connection between convex envelopes and smoothing Andrew barrer later in the day we'll be talking about annealing methods so these are probabilistic search methods the idea is ultimately you want to go to a target distribution that concentrates along the global optimum but of course the challenge is to get to that point can take exponential time but at least for some subclass of problems can we overcome this and I believe andrew has some exciting new results that shows that it's possible to do it for neural networks so I'm very much looking forward to hearing that another nice theoretical tool is the class of sum of squares problems so you can think of this as the hierarchy of optimization problems with increasingly higher computational complexity right so you can think of it as higher order semi definite programs and there's been a lot of exciting work recently to ask how many levels in this hierarchy you need to go up to solve different kinds of problems including tensor decomposition problems and the question is can we you know make this computationally efficient currently most of these well you know analysis is not practical because they require very high order in the lesser hierarchy but they are there ways to make this more efficient is another open problem so I'd like to now for you know open up to all the invited speakers and also some of the posters we have and just end this by saying that in the recent years we've seen tremendous progress in analyzing non convex optimization this includes you know looking at the optimization landscape for matrix and tensor problems as well as asking when local search methods I know can escape saddle points and can get to a good locally optimal solution which sometimes may even be globally optimal and there's a number of approaches such as annealing and smoothing which for specific problems can get you know overcome the NP hardness so that's the main message I'd like to end this by saying that NP hardness should not stop us from pursuing theoretical analysis right so we'd like to ask what is the sub class within this set of problem instances which we can solve tractable and are they interesting from the machine learning perspective what kind of machine learning tasks to do this fall and thank you and I hope you'll all be around till the end of this workshop thank you 