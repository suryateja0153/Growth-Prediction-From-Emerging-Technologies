 what about today so the first a half an hour is gonna be really some like blah blah just warming up I'm gonna show you videos and very few equations and then you're gonna start to introduce a list of concept these are basically the main ingredients of statistical learning theory which is the framework we use and then after the break we're going to encounter the first basic set of algorithms namely local algorithms and the whole idea of model selection which is another word in what I was Kay about to say parameter tuning which is one of the annoying things you have to do when you use machine learning algorithms so as I said the first part is really just very high-level and I just want to give you a vague idea what is machine learning and of course it's going to be a biased and personal perspective and the thing is hopefully it will emerge is that there are somewhat two words one is the word of intelligent system and artificial intelligence and one of the word of data science which is a fancy way to say essentially statistics for data analysis and these two words from what collapse okay in the last year they somewhat start to converge the technique that allows you to develop intelligent system and that the leak technique that allows you to do data analysis start to blend in in some way and if you were to say this to people probably like I don't know when the artificial intelligence barn in the 50s it will be completely come as a surprise okay no plea at the end of this next few slides you will see is not much of a surprise and this is one of the idea machine learning is the thing is that stands in the middle between artificial intelligence and data analysis in a way I just want to start with a very brief story of like snippet of story of where artificial in terms of machine learning come from and one probably cannot avoid noticing that we have no idea how to define intelligence okay there is not smart way of doing it and by now to some extent we do accept this proposal was done by Alan Turing but basically just giving an operational definition of intelligence basically advocating the one kind of intelligence we we assume we know which is our own intelligence okay and so the idea here do you all know what is the Turing test most of you probably know but here the idea is essentially that you have a human interacting in some way through a device with not humanoids like computers but some kind of artificial machine and if the human is not able to understand or to realize is interact with a machine he means that the machine passed the Turing test very high-level this is the idea of the test and as soon as you try to be a label you say oh this is very rough let's try to be a bit more precise and you immediately start to see there is going to be a pretty long list of things the machine needs to be able to do to receive the human and here they're just a little less but probably it needs to be able to parse and understand a bit what the language is and the meaning in the language it probably is way to memorize and represent information in some way probably the way to reason in some way it needs to learn you know - providing with an experience that he hasn't had before I need to be able to adapt to the new experience and to react to the new experience and then you can start to make it a bit more complicated because you can send something like a picture maybe and ask what's in the picture okay the whole idea of capture capture that's the thing they're making it complicated and now they're making way they just show you a picture and you're like you're kidding me that's an apple okay but a machine is not really able to do it too much these days so that's one other way and then you can make it even harder you can if you start with a physical interaction when it can pass object and things like this then you will need the artificial entity to have some kind of capability for example manipulate things and things like that so the interesting thing is that to some extent inch of this in the years has become a separate field okay oftentimes not interacting with each other and we see a bit why I mean the reason why do you think are very small because all the details are not important but this is just a list of names as always says like if you go to a machine-learning cocktail party and you want to just drop some names these are probably the one that you might want to remember people like good thing Rosen Bluth is not Britain ladies Rosenblatt is bener know him and I think one Newman and touring are probably among all of them the one to people that somewhat are recognized to be the starter of these all discipline especially lately because you're happy because things start to work so we can look at our our shames in a bit more of a confident way people often recall this one meeting in 1955 do you know about this Dartmouth College anybody knows about this nobody so what happens in 1955 a bunch of people and here I wrote very smolder names decided that they were going to solve artificial intelligence in the summer they went to Dartmouth which is a beautiful place and they decided to get a bunch of people like you and they said ok you guys are gonna solve computer vision and you natural language processing you guys can do robotics okay done and by the end of the summer you're gonna build artificial intelligence it's laughable now but this gives a sense of these people were not laughable at all okay and this gives you a sense of how much we just miscalculated the entity of our gonna let them alone poor thing Joe McCarthy Marvin Minsky closed Shannon and Nathan Rochester so Marvin Minsky I think is a is what all of them are a super big shots this gives you a bit of an idea of how much people just completely miscalculated the entity of the problem okay they just said thought it was easy and it turned out that it was super complicated and this is one first thing that is like put you know it's like the beginning of this story and everything we give you a beginning of the story is that one other thing that came out is that things that for us are complicated and we thought they were very complicated to do for example playing chess at the master level we were able to solve quite a quite a long time ago almost twenty years ago okay whereas as you see in a minute think that for us are completely trivial we are not able to solve with machines at all so these two facts affected to our preconception of what was how hard it was to build intelligent machines was completely wrong and the things we thought were tired were actually kind of easy and the other way around are two aspects of the story of artificial intelligence this is a short actor of a video I stole from Tommaso poor Jo and this is research that was done in his lab 10-15 years ago more 15 and 10 and here you see a first system to detect pedestrians okay basically the system was trained it would be shown a bunch of videos and then after some processing would basically run time try to put a square around people when we would see one crossing the street or walking around and what you see here that is doing a fairly decent job is a small snippet so is not significant but you can see also that basically sometimes where it is some stuff which is elongated it will just put the square around it okay and seems to be convinced that that's the pedestrian so it was working okay and it was around that time that I'm not sure so decided that he was actually gonna put the company and I pull just for those of us in this video that I show every time I give this presentation and then we set up a company that would make these things really work and it's been working the last few years doing this now is he is his company he's in the stocks is doing pretty well and this is now a system that you can buy in commercial cars it's an option like air conditioning okay and it's a system based on vision and he just a little ad all right the systems work and work really well it's sold it is you can imagine it must work really well because you imagine it doesn't work okay and if you actually kill somebody you can sue be sued for a lot of money and the company would be done overnight so the fact that this product is actually in the market means that this thing is remarkably reliable and the system basically work in 99.999999% edge of success so this is like one example of how are we doing today okay we do have this kind of stuff let's see if I can show you this so this guy gets his common experience of a few years ago and this is an ad for so this is kind of where we are today okay so we have stuff that actually works the one that I will show to my mom and she's really happy so I'm going to show it to you - is this one here some of you might have seen it but it's surprisingly many people haven't seen it so let's go during poi L core so sarin italiano so my fault not one it's an internet problem we're in a bunker all right so you can go vacation and speak whatever language you want and it will translate you so these were exactly pretty well so it's pretty much Google Translate with Syria whatever Google Voice on top so it sucks the same way Google Translate sucks and it doesn't suck that much I mean I think if I'm gonna be next time I'm gonna be in Vietnam like it was last year and nobody was gonna speak English yeah well he's okay I can ask where is a restaurant where is the bathroom without you know so this actually works pretty well I think and this is where we stand today okay it's kind of an exciting time to be in the field and the number of people in the room I think is another is another proof the number of company that are born every year and acquired by other big tech company the number of people that start with 10 machine learning conference it's it's a huge it's a huge proof of the success of this kind of thing so it's kind of a good time to do this kind of stuff because for the first time since probably the beginning of artificial intelligence we're far from understanding what intelligence is or even from just replicating in some way in a complete way but we do have snippet of intelligence which are good enough to basically augment our capability to interact with the world and solve potentially complicated questions so what I want to show you now in the last few slides it just the first example give you a vague idea of how we do this ok what is one of the ingredients we actually many many ingredient go and make these things work especially in this kind of hard applications but it is just one what are the ingredient to this system and basically there are two main things or what changed what changed in the last 20 years well mostly that we got very good computers and a lot a lot a lot of data okay but the other things that perhaps we could you know I convinced myself that happen is that now we have also has techniques that are allowed to process this data with computers and try to answer complicated questions so here is an outdated list of things that can be done and the idea is that machine learning is the set of techniques that I basically learned from data rather than explicit program to do a task and so you see where data start to come in is that then we try to provide answers like those who would like to obtain and then you try to you have the Machine somewhat inferring what are the rules what are the technique that are needed to answer this kind of questions and here as I said there is kind of an updated list of potential application and here in the corner in particular you see things which are essentially plots and data visualization not particularly to do with with machine intelligence but there are actually kind of data that you treat it exactly this kind of technique that allow you to do things like for example voice recognition so I'm gonna try to start with a very very simple example stolen from Coursera and you bear with me for a few minutes and then we start to move on to something a bit more interesting and complicated so everything starts with data okay and one simple situation is the situation here where you are given observation of houses dimension and prices and given this you would like to predict the price of a house in the future okay so for a while you record how big is a house and then you're basically say okay there's a new house I know how big it is so you record how big is the house and what is the price and they're given a new house you know how big it is and you want to give it a price okay how do you do that suppose that I give you new house which say is 2,000 square feet how might you think you should cost how do you solve this question linear regression but suppose that you're not even there good okay how would your mom solve this question experience that's a bit black magic though like what why not 14,000 she just took the last line I'm gonna I'm gonna tell your mom you said that but my mom what she would do is that you probably would take the house which is closest in size okay that's the simplest thing you can think how you say mm well let me check oh but this one is close enough so it's gonna be somewhere around there okay that's a mummy way of solve the problem and it will see in a minute this is actually an algorithm called nearest neighbor okay it's it's what we're gonna see today linear regression is much harder we're gonna see it tomorrow and so this is the basic idea and so the basic idea is that you start to have some symbols you have some input I call it X and output Y okay in this is the inputs are just the dimension of the house so x1 is this number xn is the last number and y1 is this number and yn is this number are so good in this case I can plot these things so I can put here the size of the house and here the price and you can even just try to solve this graphically and okay we can try to do that supposin now you all really want to solve this problem okay it's not just the game there's something truly naive in the way we're trying to do this process suppose that you really want to give the right price to a house because you want to become rich what is the first thing that we start to do to improve this system doesn't look too smart but not because of the way we solve the problem but probably because of the data this is not very rich right so what what is that you would do to improve this set again how do you wanna increase the data so somebody said increase the data but how do you increase the data more sample so this would be each one is a sample is a pair and it would take many many many many many more absolutely yes anybody else has other ideas consider are the characteristic of the house I think this is actually probably gonna do a big difference right so you say certain amount of you know you take material certain space and time chunk of observations and then you look at number of rooms if it has a view if it has a balcony if it is central heating or not your conditioning you start to write all this stuff okay for example it is just an example you look at the rooms and your you really see that things can get complicated this can be real numbers this can be essentially natural numbers but the main thing this example want to show you is that it's fairly natural to keep this one column as it is and start to add other columns so these X now is what x1 is going to be what it's going to be this number and this number okay so I'm going to use always the same notation I'm not gonna put little arrow above my symbols but sometimes these are going to be erased and in this case x1 would be an array made of these two numbers and x2 is gonna be an array made of these two numbers and so on and so forth okay can we draw this yes but as soon as you start to add other stuff you might not be able to inspect the data anymore because this can become say two three four six ten hundred dimensional array that you're not able to visualize anymore can we still do solve the problem the way my mom would solve it what would my mom have done here well if I give you a size of the house you would check the closest size and assign the same price to the new house right what is the equivalent things you have to do here so the new house you're not only going to have the size but you're also going to have the number of rooms then what do you do right you still the only things in so he said you search the closest one and it's absolutely correct and the only thing is that what you mean by closest because before you just have a number and now you have to find some way to measure distances between not numbers but array of numbers so for example you have to say this line is closest to this than it is to this for example and here for example you can just take Euclidean distance you take the difference of each numbers and take the square and sum it up and take the square root okay or you can invent something fancier than that so everything will go in the way they find close but conceptually you can solve it in exactly the same way but you start to see one trend which is that you start to go from low dimensional problems to potential high dimensional problems where the dimensionality refers to dimensionality of the input space ok so this example might seem over simplistic but if you get this example straight this is like contains pretty much everything conceptually or what we will want to do so the game you're going to play in the next three slides is basically that we're going to change the setting the examples and keep and we're just gonna try to see how we can fit different problems into this framework where there are x and y we're going to ask what is X we're gonna ask what is Y and at some point once we have this we forget how we got it and we just for example just see how we want to measure distances and we're just gonna solve the problem ok is this clear any questions about this if your last year you're lost so this other example is so the game we want to do now is we take one problem and we try to fit it in this X&Y business so the problem here is let's say detect whether an email is a spam or not okay so imagine that you have a situation where you spend a few days whenever you get an email which is a spam just click spam and throw it into a folder and otherwise you keep it in your main directory after a while you have these two folders the main directory and the spam folder where you have a bunch of text okay and what you want to say is once you get a new email we would like to say if it's spam or not so the first idea could be all just going to put rule if it contains viagra and something else probably to spam but the thing is that or you know if it's a king of Kenya that wants to give me two millions that's probably also spam but then tomorrow's gonna be Kim the king of San Marino okay or the king of whatever you're gonna change just a little bit so you have to be robust so when a system that is able to adapt okay so that's why when I use learning to adapt to the different situation and learn in a in a varying way what we mean by spam so we want to try to use exactly approach before so what's going to be the output before it was what we wanted to preheat was the price of the house what is that we want to predict now spam or not they actually are behave a boolean output are binary so 0 1 or minus 1 1 ok so it's not a real number it's just one or two possibilities what about the input how can we turn a text okay into a vector of numbers if you already know it don't cheat I heard ya but then if you know backwards you're cheating but now it's too late you spoil it for everybody so bag of words is the way you do it so the way you do it is basically to take a bunch of text or a dictionary okay and just write this long list of potential words you can find in all possible texts it's gonna be a very long list of words okay then what you do is the following you check the first word and you see if you find it in your text if you find it you put one then you check the second word if you find it you put one if you find it again you put two and so you just do counting of words what is going to be the most frequent numbers you find here zero because most words this is gonna be a long dictionary so most words will actually not appear so this is gonna be a very very very long vector because a dictionary can be pretty long and it's gonna mostly contain zero so but it's gonna be very very high-dimensional so if if this is now a text each of these number is going to be the occurrence of the word in the dictionary okay and another line is gonna be another text that makes sense to some extent now we're done because notice the x and y x is occurrence of words that's what it's called the bag of words and the output is just going to be 1 or minus 1 given a new text you first check which words appears and you turn into an array and then once you have that you try to predict whether it's going to be a spam or not okay in practice by the way people don't really do this do what they do is that they actually did they really do something like this but they typically have a pre-processing that cut the word to the root so that begin began beginning beginners actually cut to what call a root the beginning which I guess is the beginning and this thing actually works surprisingly well okay if you think about it it's a pretty rudimentary way to represent the text you destroy any grammar syntax a lot of the semantic semantic knowledge a lot of the syntax in the in the text but it actually works pretty well if you want for example do sports versus politics this is typically enough if you want to say whether somebody if it's worth worth or Shakespeare then you might get a bit trickier okay if you want to say if it's more romantic or less romantic style of writing if you're gonna go to these more complicated Hugh's probably something like this would not be enough but if for example for topics it works pretty well and they made some test where they take a text and they scramble it and they show it to people and see actually people can do the same if they just have to figure out what is the vague topic of a document they're good we just build a few keywords which is basically what's going on here and pointed to one of the key oh the whole story we're gonna see which is we actually have to deal with very high dimensional data but we kind of hope that's one way or another the number of important things in the data is gonna be so many so in this case the vector is long long long long long but perhaps there are a few keywords in each tags are gonna matter okay oh I don't wanna see this so this is another example same game as before they state an example we try to fix it in our XY framework here the example could be I have images of say the letter 0 or oh no and then Z ok and the plan is that once have one of this or this I just want to say whether it's O or Z and of course you have a bunch of observation where I do know the answer so again in this case we can think of a situation where the answer is binary say is one or the other how can we turn an image into an array of numbers not the smart way so again I just do smart so again what's the first word understand numbers yeah so you just unroll the image the image is going to be a collection of numbers and I think I even have a picture oh this picture by the way are stolen from somebody almost never do so if you zoom in an image you're going to see a collection of numbers if it's black and white between 0 and 255 6 1 what you do is that when you take all these numbers in you're enrolled them okay you put one line after the other and you just turn an image into a long vector it sounds like a good idea or not not very much right why yeah so one reason it was you have might have images of different quality okay or different size in a so on let's say did you normalize them all you scale them to be the same sir I still thinks it's a bad idea why right so absolutely so what he's saying is that image and even more than text have some kind of structure so when I take this and when I take this and I put it afterward I'm destroying that I did is actually close to this in some way okay surprisingly enough and in fact this kind of a naive way of treating the problem one really work on a hard problem but you can actually give it a shot and see that if you just wanna consider the distance the Euclidean distance of these two vectors this order one matter okay you're just gonna compare this guy with the cool you know corresponding one pixel with the corresponding of another image and other Peaks we're gonna corresponding over another image and so if you just gonna just do some simple distances this one matter and this simple method actually give you something decent not great but we'll just use something decent when I say these enemies that we respect to your typical intuition or say this one never work well take a bunch of my images in a bank of candle image and just try to say compare distances okay what we're great but he actually does a decent job which is analytical message of this kind of stuff and it's like one great mode of machine learning first always try something that is naive okay and over simplistic and see what you get with there so that once you're smart you can actually show that you're smart and you're just not recovering something simple in a complicated way the main point that we want to observe here aside from these remarks is that this vector is again potentially very very very high dimensional because it's as big as the number of pixels in an image and unlike the one before this one be sparse it's not true at all that the most of the numbers in here are gonna be zero where here are gonna be some value of the pixels okay and here again we observe this kind of situation where if here n is number of what images are pixels and P is going to be the number of pixels and you might have been a situation where the number of pixels is huge okay and in fact today's we start to have situation where also the number of images is huge but typically one difficulty is that you have to deal with extremely high dimensional vectors okay so things like the linear regression that you learn in high school or early on your college studies a can become a different word it's become this case where the dimension of P could be I don't know in the order of thousands or even tens of thousands if you start to extract more numbers so how do we deal with that the last example I want to make is the example from biology and it might be a slightly outdated example the idea here is you have a bunch of patients and you have for each patient some measurements of the expression of genes so what you want to do is that the patients are dividing to court when it says disease tape type a this is type B and you want to be able to disentangle these two class so when a new patient comes in you want to take a sample of the tissue measure the expression of the genes and based on that predict whether this is going to be most likely type A or type B okay so same game as before this is just a picture what is a microarray is it essentially is a technology to measure the gene expression and roughly speaking what you have is that pixels in this image the intensity of the pixel roughly speaking correspond to the gene expression okay so you can basically turn the intensity of a pixel here in expression of a gene it's not quite like that but pretty much so what you're going to have here is that each of these is going to be a patient and each of these is going to be the expression of the first gene of the first patient the second gene of the first patient and so on and so forth the output again in this case could be say 1 or 0 or 1 a minus 1 just whether its type A or type B so again in this case the number of observation can be huge and one one challenge in biology for example is that oftentimes these are people and these are diseases likely sometimes it's hard to collect samples which means there are not that many people with the disease or worse which means that hospitals don't agree on sharing data so sometimes you have 50 patients and then the number of genes you have it in the orders of tens of thousands okay so now you might have situation where this is 50 or 100 and this is 50,000 okay so from just a basic calculation of you know degrees of freedom and number of data points this seems hopeless and so one question is how can we go around this curse of dimensionality which is this idea that you have many more observations then so many more dimensional many more parameters than observations so this is just to conclude this little snippet of how you somewhat frame many different kind of problems into supervised learning that's how we call it okay you basically have many different problems in which the game there is an art step which is look at the problem and think how you want to squeeze the problem into this X&Y framework then from that moment on try to develop methods there are so much blind to the nature of the data and try to find ways to separate them in classes or predict a real-valued output okay so this class is gonna be about the challenges and ways of doing this kind of stuff so again we start from the the basic housing price example to move on to text images and biology and what you see that these data are very different okay and in fact you will have to put some extra information to solve the problem well we're just gonna at first I just see how you can phrase them in this way so far so good any questions complaints yep yeah but so this is what they show you at the beginning so basically the material here so these are just examples so pretty much I collected them on Wikipedia kind of style processing so you just go around and look at stuff the mainland the the reference I show you the beginning in one page at some point they contain this so the elements of statistical learning is one possibility Coursera is another possibility though I only stole that examples I don't know what else is there yeah these two I think you should be enough especially at this lovely Oh Coursera is machine learning yeah yeah when I say Coursera is entering class and machine learning so these two reference will be enough any elements of statistical learning to go on and on with a bunch of examples okay other comments so and here is kind of what we wanted to get at which is even in this just few examples you've seen there are some of them you feel more like talked about intelligent system and maybe some others like the biology case where you talk about more data analysis maybe and what you see here is that again to some extent what we want to talk about is the kind of statistical techniques that allow to go from the data to some form of decision making or information retrieval okay data mining these are all kind of sino means in my head of the things you do once you get rough data and the protein is rough data are never never a complete picture of the world they're just a snippet and out of those you want to extract or if you want generalize and be able to get a picture of what is the phenomenon the model the world that generated that data okay this is going to be our game and the challenge is going to be high dimensions and potentially massive datasets to interact with in some sense machine learning is the taste test with a computational twist okay you are gonna use a lot of statistical ideas but together with optimization and algorithm idea to be able to handle in a computational feasible way large amounts of potentially high dimensional data questions so before killing you with the questions I'm gonna show you a couple of videos this is this is really more a break okay so if you have any question you can think about while entertaining with songs these work done at MIT a few years ago up and this is apologies you've seen this you know this song from Fury so this is Dido and this is not Dido just a song so this is an artificial system what it does is that it records so you basically feed the system with a few minutes of a person speaking and it learns the way the person move moves the mouth okay the rest is superimposed is engineered okay di the movement of the face that's engineered but the way the mouth moves is learned the system is much more complicated you're just deciding what's X and what's Y but that's one of the ingredients these work done by tomorrow pause and Tony has it quite a few years ago like 15 years ago so what you should see we notice is that the the letters are pronounced correctly okay if you hear the word and then look at the lips they actually pronounce the right letters and here I'm just going to show you a few examples so this is Mary it's her it's her voice and the way she move the mouth the snippet of video I think is a bit longer than this and once we have it we can have Mary singing but we can also have her singing in Japanese we're Chinese but so the video of marilyn is much shorter and so the quality if you see here is not as good as Mary Mary is actually pretty good in Marilyn so let me tell you what this says first so this Reema helis is an anchorwoman from I don't remember which station in the states and she's gonna basically what hopefully you understand because otherwise so I'm not sure how much you're here so what she said is basically that the new technology can process anybody's voice and making come out from anybody its mouth okay and the next person you're gonna see is Rima Alice all right so this is actually so that was mostly done for fun and as I said the lots of reviews a lot of fun because there is a lot of nice sinking of the movement of the face with the song but the scientific part is really more the the lip movement so this is actually a small Turing test where they actually talk snippet of the real person and the artificial person and try to see people could actually recognize them so if you see they're fairly short they were shown 53% I'm bidding with this kind of little test what you see that people are not really able to recognize it was not a lot of people I think it was learned 23 subjects and basically what you see that there are 50% chance of saying whether this person is the real one or not of course if you give them a much more time and they start to study the thing they probably will be able to caught up small imperfection in the video but at this level in some sense Mary passed the Turing test all right so the min T the meaning of this first hours of lecture was just to give you a vague idea of potential application of machine learning and the way to cast different problems into a common framework a lot of the art that goes into solving problems that they will not go in my in the classes because of course this polygon specific so we're going to try to extract some of the general principles that you can apply once you cast your application it is XY formulation which is what we call supervised learning okay so that's what we want to do next then just to wrap up this first part you know the two things I want to mention is as I said this is the moment where a lot of high tech company are jumping on this kind of techniques and basically Facebook is opening offices in Paris in New York essentially to do artificial intelligence and machine learning this guy the missus abyss is the boss of Joel who's coming in a couple of days his company was acquired for ridiculous amount of money by Google a couple of years ago Twitter is doing the same so the trend right now visit at each high tech companies while Google has huge artificial intelligence group isn't done itself with essentially artificial intelligent machine learning group so there is a lot of opportunities there but the other thing is that in the mean time what you see is that to some extent both in Europe and in the States there is a substantial amount of fundings and efforts put in the scientific side of the story because as you see from the application I show you while we can legitimately say that computers do have some form of intelligence the idea of robot killers and the singularity is not quite there yet okay we're now we really don't have a good understanding what intelligence is we are not able to replicate a fully intelligent system but not even that if they're able to replicate a system with two kinds intelligence you know speech and vision in a decent wave with a common software energy consumption is a huge problem amount of data is a huge problem most of the system the one that works are based on the federal companies such as Google can collect ginormous amount of data which are now at disposal but it's also very costly if you're not Google it might be very hard for you to get the same kind of data so the challenges out there and one of the reason why we're here to try to contribute to it and this is basically where I wanted to get it and I said this is just to warm up I start to think what we're talking about when we're talking about machine learning ok and what I want to do next is to move to the next part so any questions so far while I change my slides question/comment but somebody's awake because it's so dark all right so what we want to do next okay is a step first so right now we introduced examples but the only symbols we introduced our X&Y we want to put a little bit more juice in what we're doing okay this doesn't work so we're gonna look at this system which are based on learning from examples with visi means the idea of having these mostly these input-output relationship and try to extract some decision-making or predictive rule out of this data and you can consider this problem in quite a few said things we're gonna consider what is called the statistical learning theory setting and the main idea is again that you have some kind of input-output relationship some esse okay that you would like to learn and the key point is that you have some new world we have some data that you've seen in the past and you typically call this set of observation from the past the training set again n is the number of observations and X here can be an array okay typically is an array of numbers so in the spam images text example this was always a big array of numbers and what you want to do is to learn an ephah but very importantly you don't care that much although you do in some way in you'll see in a minute to predict this data but you really care about making predictions about the future okay you don't want to be able to predict very well the data set you have on your laptop you would like to be able to create a system the ones new data comes in and you don't know why you can predict why well a new email comes in and you do know where it's spam or not and you're gonna say well you try to make a prediction where there should be one or the other similar you get a picture of if you got a picture and you say whether it's me or Carlo okay so one key observation here is that it's obvious but if you want to basically say that the data can give you information about some unknown functional relationship you have to explain a bit better what is the connection between the data and the functional relationship you have to assume that there is a model of your data because there is no such thing we cannot really start asking the question and a key point here is this word here is uncertainty while you do try to find the deterministic relation between input and output you basically once you get an email you like one way to basically say whether it's spam or not this is actually the relation between input another can be intrinsically noisy okay or affected by various kind of perturbation they make the proper the problem uncertain so you need a way to cope on the one hand with uncertainty in the data and on the other hand with the possibility of potentially making errors and the game is not going to be being perfect but it's going to be make the least number of errors as possible so we need to make this concept and again the other point is going to be that the errors are not gonna be so much on the data you have now as much on the data you can see in the future so we need to introduce a way to model uncertainty L way to measure errors and a way to measure errors now and in the future and once we have that we can probably state what's the problem okay so that's what we want to do next so basically the next few slides are going to be extremely boring and I'm gonna give you a long list of names so I'm just gonna try to attach concept to the names okay so as much if you question the names is the best way to try to see if you can remember them in the future the first bunch is fairly simple we're going to assume that the input are coming from some input space and the output are coming from some output space the input space doesn't have to but we typically gonna assume that it's actually vectors okay it's RDS just array of numbers and the output is going to be typically a subset of the real numbers perhaps just two values say minus one and one we call the product of this space the data space that's where we get data from we haven't done much so far okay so there is the input space and the output space turns out that if you change the input kind you don't change the name of the problem but I think we change the output kind you actually change the name of the problem in a way funny enough we typically call the problem where the outputs are real numbers regression and we typically call the problem where the output are binary say minus 1 is 1 but this is just a bit really binary classification the generalization of this to many possible outcome and we only touch briefly upon it he is what you call multi class where you basically instead of just two kinds you have three possible kinds say recognize faces not only between me and Karl but in me Karl Alessandro and Julian okay this step is is very important what we want to do is introduce a big big new concept which has not been out yet and is the idea that in the data space there is a probability distribution okay that relates the input and the output is a joint probability distribution between x and y basically everything we don't know about the data we put it there the key point is that this distribution we assume it exists we assume it's fixed but we don't know it all we know is the data X Y X sorry x and y that are sampled from this distribution okay we know n pairs x and y each one sample from this distribution this is a purely theoretical step okay if it's useful it's useful if it's not used to should drop it's an assumption it turns out that it's pretty useful in many situation to guide the way algorithm to work not only we make the assumption that the distribution exists we also assume that it can be split this way and once you split this way you start to think of X as an input and Y as an output and as I said here we put in the distribution P with model everything we don't know about the data my to split it this way for example what you can see is that if you fix X and you look at the conditional probability of Y given X it basically means that if they give you an X I cannot say for sure that there's going to be a y associated to it but there is a whole distribution of possible values some are more likely than some others that make sense so you can think of these as some kind of noise on the ratio between the input and the output the nature of this noise the words noise might be misleading because you might have a situation where for example little you have an image and you add noise okay but you have a situation where I stole Carlos glasses and it grew a beard and we actually look alike and then it will have the situation where two things get close to each other and that's also some kind of probabilistic effect but it's not perhaps what you would call noise or you might have a situation where have images and you just put a very low resolution rate and so by basically by compression to have some form of noise which is more of a perturbation form of the probablistic blurring of the identity of the image in the image but it's really not again what you would call noise it's just some other kind of perturbations so this common are just to say sometimes I call it noise but really a lot of different stuff goes into this distribution that connects the input and the output this make sense ok this is a very important conceptual step assuming this distribution the other thing we can do is look at the marginal distribution the distribution of the input ok and you can think of this in many ways but the easiest ways to think of essentially say when we sample the space of images ok or the space of houses you might not be able to have a very uniform sampling for some reason ok and you might just see some samples and not some others and you would like to make prediction everywhere and so this actually gives you for example there might be some area where you are more likely to sample images or houses than some others ok and this is model by this it's nothing to do with the price of the house nothing to do with whether it's Satan images contain myself or Carlo and it's only the fact that for example there might if you want to say you want to predict the temperature in this room and you want to put a bunch of sensor there might be points are easier to get than some others okay so the distribution of sensors in the room might not be uniform okay and so here you put all these indetermination in this list is really a partial list of the potential things which can shove in here on one hand you can complain and say well but these are all really very different things why do we put them all in the same basket right well because you can see how much offer you can go putting in them in the same basket and see how much you can fit all this probably in the same way there might be points where you really pays off to say oh now look this is really quantization is not noise where this is really semantic confusion in my data it's not compression okay but for now we're going to just put everything together and see how far we can go the basic basic example of distribution okay the basic example of data there are a simple acquaint to distribution is this one here you're probably going to see it in the lab or maybe not oh no no in fact you think you're gonna see this fixa function okay for a minute you're god okay you know the function of star you know the noise it's a this one is a random variable this X is also a random variable say you fix the interval 0 1 take a uniform distribution over 0 1 fix a function say the sine and then just add Gaussian noise and then you get the number Y do this several times and this is going to be a bunch of x and y's the region between x and y is not deterministic because it's completely governed by the noise and the f star tells you to some extent what is the best choice in hindsight if you could know the model of the data this is what one would call the regression model the distribution over X and epsilon defines the distribution over Y and this is going to be your Joint Distribution over everything okay so this is the its basic special case or what I just show you is the basic way to define a P of X comma Y and that's what you do is doing the classic case of regression okay and these choices are completely arbitrary okay this might not be a Gaussian noise it might be a Poisson noise or any kind of noise this is just one example in the case of classification notice that why typically is only two values okay for example one and minus one so you're not having a lot of possibility for your output and these two are related their probability of 1 given X is 1 minus the other the case of somewhat a noiseless quote-unquote situation and classification is the one where the probability is always either 1 or 0 it basically means that the relations between the input and output is not really probabilistic into the case where epsilon here is 0 given an X there is a right answer here it would be the same if the probability is 1 it means that it's Laurent's 100% if it is 0 means that absolutely it cannot be me okay and here you will see in the lab different way of building this kind of situation in a very simple way so the data model we assume that we have this probability distribution we can split the distribution and see that we can put a noise and sampling as well as many other effects inside this distribution and here are two main examples regression and classification so I forgot so one thing I didn't say is as light disappeared but I want to write this at least once an agreement that you will hear a lot is I I D it stands for independently and identically distributed it describes the way the samples are assumed to be generated from the distribution of P of XY and it's a very important assumption it means that when you see x and y these pairs that you get in your training set they always come from the same distribution that doesn't change over time it's always the same there is one p of X Y it's fixed unknown and you just can take sample from it okay I mean physician this but these are there are a lot of other situation is clearly not true time series is the classical example a process that inherently changes over time in a substantial way and this is the first I identical okay you always come from the same distribution the second eye is independent when you take a sample the next sample is going to be completely independent they're not going to be they're gonna not gonna influence each other okay and this second assumption can be relaxed in many many many many possible ways the first one so I think it's crucial up to a point the first one is really really crucial because as you will see in a minute the fact that we assume that R is just one distribution fixed a priori we basically tell us what is the ideal solution of the problem it is if the distribution does change over time there is no such things and the product gets much much more complicated so in some sense the independence assumption can be relaxed this indentical distributed assumption which is if you wanted some form of stationarity is really really crucial all right so the next step as I said is so this is where I promised you three things introduced a model of the uncertainty that's what we have just done and then built introducing a way to measure errors because we look for a deterministic relationship in an uncertain environment so we have to accept the fact that we might make mistakes and so the loss function the notion of loss function is what formalized this idea and in words a loss function basically says you get an X you predict f of X but then you are told that actually you should have predicted Y how much do you pay what is the basic example of loss function say in the house pricing example okay you're given a size of the house you predict the price then you reduce the true price and you want to measure how much mistaking your what would you do the difference the difference can be positive negative so what would you do the absolute value to or the square one way to make it positive okay this is the least square loss function and it are going to be the default choice in a lot of other problems what about classification if you have to make a decision whether a picture contains me or Karlo okay so now I got your picture you predict me and then you revealed its Carla how would you pay an error the simplest way is 0-1 you just say if I was right I'm gonna pay zero if I was wrong I'm gonna pay one we'll see in a minute that this lost function which is basically what it is called a missed classification or loss function is very nice theoretically but in practice leads to unfeasible problems so we'll have to treat it in some way so the loss function is good because it gives us a way to measure error the problem is just it's an error on a fixed X and Y and I don't care about the fixed x and y I want to be able to talk about the error I will make in the future because ideally that's what I would like to control and for this reason one introduced this theoretical object the so called expected loss or expected risk in words what it says is take the loss function and then now average over all possible Y and X okay so here there is just the expression it's an integral because x and y can have continuously many values if you are bothered by the interest you think of it as a as a as a sum over all possible values of x and y and here you have weights the way they basically say if I get a couple XY which is very likely to be sampled this should cost more than a couple X Y that never shows up even if they have the same loss okay can you compute this quantity democracy ah no answer sit again yeah so by definition it absolutely depends but for today because I said loudly that I'm gonna assume that this is fixed but unknown you cannot compute it okay so unless you are in a god mood where you actually generate your data or make stronger assumption you won't be able to make any calculation about this yet this is the theoretical object that it will tell you something like does it exist an ideal solution to the learning problem there isn't an optimum I can shoot for okay and the optimum would be something that makes this small if there exists an ideal solution it would be the one that makes this error small which makes basically says that I can do well in the past as well as in the future all kind of x and y that i may be able to see and we would call this a target function it would be in some sense a minimizer of that okay so again this is now an important conceptual step we introduced uncertainty in the problem and now we say we introduced a theoretical measure of error which is the the ideal one is the one where you can average out over all possible x and y this will not be particularly practical but the moment where you ask things like there is exist does it exist a optimal choice of the parameter of my problem of my algorithm you're gonna split the answer in the question in two first you're gonna be a theoretical you're going to be a theoretician and ask that does it exist what is it depends on and you will need concept like this to be able to have a framework to say what is the what does it even mean to be optimal and then it will be a separate question which is ad and more important which is how do you do things in practice okay so this will be our framework to cast all mathematical questions and then we will try to see how this framework suggests way to solve the problem in practice so just because I don't have it on the slides an object that we will often introduce in practice is the empirical counterpart of the expected error which is the quantity we obtain if you replace the sum the integral the expectation over all possible x and y with the sum over the training set point okay so the difference here is that the x and y derive a my training set and I now can compute this quantity I just have it once you give me an F I can compute it so oftentimes it will be the quantity I will use in practice as a proxy for that okay this is something you can compute and in some sense there's a whole theory that tells you that we will not touch upon my to tell you to which extent this is a good approximation of that yeah sit again it's hidden in here so let me give you the and waving version and then we see if it's needed to write an equation when you write the average what you can do is you can take all your samples all of them and some demap or you can divide it in groups with the same values and now sum over the possible values and put the frequency of each value okay here I use the first version where I take all the samples and put them all together suppose that they have points with the similar value okay so the values of Y and X the possible value of this couple are V impractical won't happen but let's assume for a second it does happen and what you can do is that you can do the sum from 1 to V and here you have what you have to put the weight of that couple okay so in my way so this is equal to one over n sits this is one way to write an empirical mean this is another way to write an empirical mean here it's a more thorough observation here I just sum over all possible values here you don't see the weight the density and here you do see it okay so just this is very easy so just think one second about it offline and if it's not clear you let me know okay yeah so it's so the words are identically and in yeah first thing first the name is identically and independently distributed so identically means that every pair XY comes from P of XY you can assume a situation where you get a pair XY and then somebody changes the distribution P and give you P Prime I didn't know what happened because all the example are identical identically simple from the distribution independently means that when you get x1 y1 this doesn't influence the probability of getting x2 y2 with certain values so these two samples are completely independent that's it ok so you can have a situation where you put a camera on top of this door and you have people entering and probably these samples are quite so did you say over a course of this class you're gonna be always you're gonna be always you and this bar gonna be already the same distribution more or less and the fact that she enters or the fact that he enters let's say that it's independent okay if you want to predict the fact that might be sunny or rainy and that's probably not the kind of situation where samples are identically an independent distributor at all you can still wonder if you by making this assumption you can get something useful and that's a different story but collision assumption might be violated in a lot of situations good I need your questions I'm not a lot of questions have to say yeah I'm assuming because everything is perfectly clear so now few names okay so let's let's recap the name so far the priority distribution we don't give it a name lost function is one name or cost function I think economists are actually optimistic and called utility function but we call it loss function because we are stench a list then we call the quantity we can define over all possible pairs expected the loss or expected risk and we call this guy empirical risk or empirical loss it's kind of make sense we give a couple names we call the procedure that given data returns an approximation f of X of my target function we call the procedure learning algorithm and we call the function FS estimator it's an estimator of the target function ok let me point out here that this terminology is kind of annoying a bit because it doesn't quite make a distinction in statistics people call estimator a theoretical procedure and don't really look at computational aspects ok so what I mean is learning algorithm so what I want to put the entity is that there's going to be always two ingredients in what we do on one hand is the principle statistical principle that allows us to do some kind of estimation and then we have to say what are the computation and tell by that principle okay and so in some sense when you think of a learning algorithm I like to keep into account both aspects of this story so what is the kind of principal which is the objective what is the kind of procedure I want to do and how much it cost me and I would like to see both aspect because it might be for example a procedure which is optimal but it cannot compute in practice the way around it might be something which is very cheap but it's not very good and so we want to keep into account both these aspects and so the learning algorithm part tries to the learning over the name try to keep into account both these aspects a fairly important point is that while we assume that the target function exists we are not really interested in estimating the target function but rather to do as well meaning that the target function is going to make a certain error and we would like to do an error which is more or less as big as the one of the target function in which case we say that the solution generalizes if you generalize if you predict well in the future one possible way to do this is basically saying that my F of s the one that they pick on the data the real one that I can compute should be close to F star but not in every possible value but with respect to the error so this is the expected error okay again this is not something I can do in practice but it's something that theoretically I could require if my solution would have this property or make this small my algorithm would be good so TP we want to invent an algorithm then you try to show if it somewhat is able to put your solution close in this sense to the ideal solution okay so it is more the theoretical side of the story now here there's something missing okay from a mathematical perspective because this makes sense in words this says the error of my the ideal error of my solution is close to the best possible ideal solution the best possible idea there but the thing is that these guys random okay so different data sets would provide me different things so now what does it mean that I make this small does that make it small for one specific data set for all data sets on average I have to specify in some way what does it mean this is a random quantity so if I want to say that this has to be small I have to say in which sense and the easiest way is to ask for this to be true an expectation which means that for the average training set or for the training set on average my solution gives me a pretty good solution okay this mean well that might have bad cases okay or the my solution sometimes can be very different but on average is fairly good yes let's start is the one that minimizes the expected error but this Patil is just there and the F star is the one demeaning right there okay so the expected error is just this so given a function it tells you how much miss how many mistakes you'll do in the future if star is the one that minimizes this yes yeah and suppose that I give you one and I ask you is it good or bad how do you do that no but we're being theoretical now okay so we're just trying to introduce quantity so what these are just theoretical quantities and you will see in half an hour how we use them okay but what I mean is suppose that theoretically we want to try to say what is a good solution what is a bad solution so you define the problem you say oh the problem is to minimize the expected risk and ideally have F star which is the best of the best then given that I provide f of s and then I ask myself is it good or is it bad I plug it in here I go and check ideally the error but then I still have to know if I'm far away or not from the best possible how do you know what's low is it fifty low or is five low or is a hundred low if you know that the best is five five is low but okay but what I mean is in some sense here again we try to define what is the quantity that's um what we want to measure to see if we're far away from being the best or not so you need to find that start with the lowest possible quantity suppose for example they want to check if polynomial is better than Gaussian okay or it's a that linear regression is better than new linear regression if I can show that constantly this distance of the distance of one to the battle is bigger than the distance of one to the other this is one way to say that one model is always potentially worse than the other if I don't have something to you know refer to why I'm missing like what the zero one is what is the the reference point of this game okay you will see in an F an hour when we try to understand the behavior one such algorithm how having F star helps out okay yeah so this is what what does it mean no no for example let's check this case for example okay suppose that I build the data this way no matter how I do it f star which is the best you can check that it's the best here is always going to be affected by an error is going to be y minus F star X is going out on average of all possible Y and X is going to be excellent so you always have an error okay so you always that because the input the iteration between input and output is not deterministic you're gonna have a certain probability of making an error and that's it even the best function is gonna make an error so this is still true that I might consider a relative error I can divide everything by EF star but for now I'm just being simplistic and what I wanted to say is if you put your theoretician set you introduced the quantity we like to minimize ideally you would like to prove that your estimator is you say that your estimator is good if it's close to the best possible you can do but to say what does it mean to be close you have to be a bit careful because the quantity you are dealing with is random and so you here you just say the simplest thing to do which is it's low in expectation okay I'm late let me just how much what the hell okay lets me let me say one more things and then we stop in practice again we're being theoretical here okay so we introduce the quantity we would like to minimize and then we just introduced to basic idea this stuff is basically an appear in disappear is just going to be reference okay we say that the algorithm is consistent if as you give more examples is gonna try to get close to the best possible and if this error goes to 0 we say it's consistent okay this is a basic requirement it basically says if you get more data you're going to get closer and closer to the best possible solution on the one hand is the basic sanity check on the other n is an asymptotic statement it doesn't tell you how many points you need to be that far away from the best possible solution and so if you have learning rate or if you want finite sample complexity in essentially that you're able to make statements such as if I give you a precision epsilon you will need these many points to be close epsilon to the best possible solution and this gives you more information so these are kind of guarantees that one would like to put on top of a learning algorithm okay so what we want to try to understand so now we just define what is the ideal problem we'd like to solve one of what I do next is does this help at all and try to understand how to design learning algorithm because for now we just said what is a learning algorithm and which since they might be good but we haven't said it all how to build one and we like to know how to build one and now to understand what are the properties of this learning algorithm and that's what we want to do next 