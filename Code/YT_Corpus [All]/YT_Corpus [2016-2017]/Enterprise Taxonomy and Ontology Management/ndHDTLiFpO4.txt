 to organize the process of requirements collection features specification and dragging of changes semantic web company uses the election tools confluence and Chira the goal in the aligned governance use case is to express the information managed by these tools as rdf data using the Alliant metamodel this will enable us to query requirements and issues in a standardized way by using sparkle and to easily combine them with other data sets for example the process related data can be annotated with concepts that's original from the taxonomic waited in the pool party thesaurus manager these annotations can help in increasing the quality of results when searching requirements documents or the interface similar requirements or issues semantic web company collects the requirements for each version of the pool party thesaurus manager in the pool party development space in hierarchical form as shown here the root page of the requirements documents is titled requirements PP which is on the next level followed by pages containing high level description of a specific feature which again are followed by more detailed descriptions most of these pages are structured based on standard templates defined by semantic web company annexin very high level requirements description looks like this it contains the title a textual description of the goal from a user perspective a list of steps that need to be performed to fulfill the requirement and they involved stakeholders on the more detailed level the requirements pages are structured in a similar way to contain a title linked issues from the JIRA issue tracking application as well as a textual description from the user perspective the specification also includes a list of preconditions that must be met in a more detailed description of the feature containing text and mock-ups furthermore a detailed requirements page also lists acceptance criteria and test scenarios and is usually followed by some comments discussing things that are not clear to the developers when a requirement is specified so that implementation can be star did new documents so-called issues or tickets are created within the tree replication this is an example of such an issue it contains structured information about for example the type of the issue is priority the current status and the responsible developer issues also contain a detailed description about what functionality the cover and optional attachments to better illustrate the problem or to provide test data to represent the data contained in the confluence and Shiro installations as RDF gear the design intent ontology is used by the unified governance tool implemented by semantic web company deal has been created by Oxford software engineering as a part of the Alliant metamodel to capture the knowledge generating during various phases of the software design lifecycle Oxford software engineering and semantic web company have also created do PP which Maps the pool party conceptualization with the deontology this is how the various parts of the requirements pages get converted into their rdf representation the green flames around the page elements illustrate what information is represented by what elements of the vocabularies while some elements elective requirements title and goal description can be represented by standard vocabularies such as DC terms others like the cheery issues to which a requirement is linked to are formalized by attributes coming from deal also for detailed requirements pages we can cover most of the information contained with deal finally we can also cover the content of zero issues for almost every element of the issue we provide a way to map it to the standard ODF representation making use of d upp perform the actual data extraction and conversion process we contribute three tools a java project that can be compiled as an application which may be run from the command line connects to the conference and JIRA instances and thumps they contain data in RDF format the tool can also be compiled as a library to be used in other projects this is required for our second contribution a plugin for unified views which wraps the data extraction library so that it can be included into custom data processing pipelines unified views is an existing open-source extract transform load framework with a graphical user interface that allows users to define schedule execute and debug RDF data processing tasks as our third contribution we provide such a unified views task which encompasses data extraction data annotation using a pool party thesaurus and loading the annotated data into a remote trip restore the extraction task is organized as a seven-step pipeline in the first step our extraction tool indicated with the red box is used to provide the data which should be annotated the next processing step is that box below which filters the extracted data so that it only contains text literals which serve as an input for the annotation apio it passes its output to the next pipeline stage which is a processing unit that connects to the pool party extractor annotation API and returns a rdf graph containing the identified annotations for each resource after that the original extracted data and the filtered annotated data are merged and converted to a file this file is then both written to the local file system for debugging purposes and uploaded to a named graph in a remote for joseph server so that it can be queried using sparkle semantic web company provides a virtual installation that will hold the extracted data in a named graph at aligned dash with user dot pool party is using with JoJo's graphical user interface the data we expected can be directly queried by spouter this allows us to run various statistical analysis for example we are able to find out the ten issues that have stayed unresolved for the most number of days in the future we plan to use the annotations from the taxonomy to detect duplicates or similar requirements or issues our immediate next steps will cover the exploitation of the extracted data to formulate additional valuable competency questions these will also drive the further development of the extraction tool as well as deal and D upp because we expect the currently extracted data will not provide enough information for answering all of these questions we especially want to identify miss structured or erroneous requirements so that we can increase the overall quality of the software development process at semantic web company in cooperation with University of Life Tech and Oxford software engineering we will propose a methodology for linking the extracted conference in zero theta with the test cases formalized as for example RDF unit tests fractal shapes or spin rules which establish the link between the software development and to data development life cycles 