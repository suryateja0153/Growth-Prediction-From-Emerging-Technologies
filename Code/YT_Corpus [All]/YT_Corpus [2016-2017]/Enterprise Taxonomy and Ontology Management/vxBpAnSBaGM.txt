 hello everyone we're at the grand opening of the 2016 ignite conference and we are inducting this book measuring and managing information risk a fair approach into the cybersecurity Canon and we have the two authors here in studio with us Jack Jones and Jack throwing guys welcome and thanks for coming thank you and congratulations on we're getting this award thank you so you all have tackled I think okay one of the biggest problems the biggest blind spots that we've had in the industry for since I started doing this and that is how do you assess risk right and your book talked about establishing some rigor in that entire process I've been doing this for years I just kind of make it up as I go you're gonna make me you know do math and things so tell me why how you started to get this done well first you know I submit that we're not gonna make anybody do math alright goodness the framework fair enables good solid math but a lot of organizations a lot of people who use fair use it qualitatively you know they use it sometimes they don't even take it there they just use it as a way to normalize terminology normalize people's mental models and and as a framework for critical thinking so that they're not just coming up with stuff you know without thinking through it well I mean you say that right you one things you say in the book is that we're it's doing a risk assessment without any accurate analysis all right we're just kind of making it up on the fly right yeah so so if you think about the opposite of that if you think about practicing risk and an unwritten scientific ways as bad a connotation as that has I think what we're really talking about is relying on our instinct that accumulated years of experience that we have to make a good decision really it's common sense and the problem with common sense is that while allows us to provide a narrative around the way the world works it undermines our ability to understand how it works and that that's really what we provided with fair is model to help people understand the way that risk works and bite-size miniature pieces also sort of decompose the problem into measurable pieces I think it's one of the big evolutions in our industry is that people like Gus are coming their realization that we're not trying to protect against cyber attacks we're really trying to protect organizations or reduce the risk to our organizations right and as cybersecurity professionals this is something we should be good at and we really suck at this right right we have a hard time assessing it we have our time convincing people what it is right and there's there is a skill set that will excel at this sort of thing I think anybody can do it but as with any other skill some people are going to be really good at it they're going to be wired for analysis they're going to be you know their critical thinking mental muscles they're probably been applying them all along right those people are going to take to this like ducks to water other people or it won't be as natural to them but it can still be very helpful to them and thinking through the problems and making sure that they are their own mental models are calibrated and they're not over looking things are over biasing things and those sorts of you know those sorts of things so I think it's really I mean for all guys like me but trying to learn new things right you know I learned what these terms are and then you define them you define terms like threat frequency and vulnerability and locks magnitude with some precision alright so there can be no ambiguity about what we mean when we say that can you talk about it why you thought that was important well if you don't have that sort of precision in terms - in order to define that the problems that you're trying to analyze there's no way on earth you're going to put good measurements against them I mean I can't so you've heard you've heard the phrase you know what gets measured gets managed and well you can't measure something you haven't defined clearly right and when you know people say I will hear people say oh I tried for your first heart right and I'll talk to them about it and they won't have grasped that that fundamental need to get real clarity around what it is they're trying to put numbers - or or qualitative labels - that sort of thing and and so they struggle and then they say well it's hard and it's really not well I think it's a that's the the big revelation for me when I read the book was that it is possible to get to some precision even though some of these things are really hard to measure but you can get a better idea than saying well I think it's this alright so how did you how did you get to that point where you figured that out well how did you know because you know you're an old guy to Jack like me how did you figure that out pain and suffering lots of mistakes I mean you know when I was first trying to figure this stuff out you know I I was fumbling and making mistakes and and I have old notes and and such from the early days of developing this stuff and I look back at them and just it's embarrassing you know how bad some of those stuff was and and and fortunately for me I was willing to be self-critical I was curious and persistent right and and really looking for something that would pass a laugh test or smell test right and finally that you know actually started to happen you know you you'd be quizzed or the poked and prodded by people who were skeptical mm-hm right and finally it got to the point where I could defend this against anybody who cared to throw stones I think it's go ahead oh I say and and but that's what scientific inquiry does for us it it causes us to challenge precisely the way we think that the world works in order to make a defensive or and that's as complicated as it may seem on the outside the variables that we talked about with fair are really decomposed versions of what you learn for a basic assist exam you have to learn that basicaly equation but the problem is when you go to apply that you never know where to get the numbers from yeah I mean I've been in situations where I you know we had these great algorithm that we put into a spreadsheet right and we'd crank all these numbers out but it was based on some really wrong assumptions right so so what analyst will do is they'll pick a very large number for how bad it could be and they'll choose a probability based upon what they want the outcome right so those decomposed there it will give you measurable pieces to help describe things better and they also the framework is also marvelous at poking and prodding at risk statements I mean this is gonna sound like I have a bad attitude or whatever that I mean right but you know an auditor or a regulator or consultant when I was a CSO would sit across from me and say oh these things are high risk and say really tell me about that and I would start just quizzing him how did you arrive at this and drilling into it they could never stand behind they would always they would always back down in fact I over the last two years or so as I've been going into organizations and working with them 70 to 90 percent of the time depending on the organization that someone says something is high-risk when I start poking and prodding they can't stay behind it it goes away it becomes a lower medium and when that's the case and we talked about prioritization when 70 to 90% of your high-risk stuff isn't there's no way you know what's really important in term Angie and I hear people saying oh we're losing the cyber war well of course we heard you know we can't prioritize correctly and we're miss spending our limited resources and that sort of thing of course we're gonna lose that war so I in other talk we're having here at ignite I'd like to say that the primary mission for all network defenders is to prevent high-risk material impact to our organizations right but if we have no way to tell what the high risk is like you're saying then you were wasting your time you've been doing spending lots of money on things that are never going to affect you or at least have no impact to your organization all right so it's really a question of if I'm gonna back up why don't we just tell everybody what fair is I'm halfway through the interview we haven't really told anybody what that now we have to explain so it's it's essentially a decomposition or as Jack said it's a codification of what risk is and how it works it's it's a Bayesian network just a you know a tree structure if you will a root structure with risk at the top and two components loss of in frequency and loss magnitude and then the things that contribute to those and things that contribute to those so that it's it's essentially giving you again this framework for critical thinking for dissecting whether otherwise very problems right and and making sense of it making sure you're not overlooking things in such a and it also then because it's in that structure it defines the relationship between these elements and that's one I think critical difference between fair and a lot of the other models or frameworks that are out there you hear about taxonomy z-- all the time I so Aztec so my taxonomy taxonomy z-- put things into categories right but they don't define the relationships as one of their so there is an ontology it not only provides these categories if you will but also defines the relationships between them some of which are you know boolean of one sort or and other these sorts of things and that allows you to do the kind of analytics allows you to measure this stuff I would tell you that I did not understand the difference meaning ontology and taxonomy I read your book all right all right so thank you for getting that done for me all right so it gets down to defining our terms all right so you guys say this that risk equates to loss exposure is that fair okay and so then the definition of a risk is the probable frequency and probable magnitude of future loss is that right that's that's exactly right and and that's a very that definition I spend a lot of time thinking about that before I chose those words and that sentence and first of all that's what we're tasked with helping organizations manage its future laws right and there's a frequency component and a magnitude component that's fairly obvious but both of those are probabilities the reason I put probable in front of both frequency and magnitude is to emphasize and I've been criticized for this but it's there to emphasize the fact that these are probability statements these are there is no absolute certainty around it I mean we can be as scientific and rigorous as we like we can have data out our backsides right and we still can't foretell the future right we can have a better understanding of what is most likely right it's about probability so I was told go ahead no that was I was told one time that I got the equation wrong because I had probability on both sides of the equation that it has probably x impact obviously I'm talking about nice no that's correct because I don't know either one of those answers absolutely so there's an uncertainty that we can only measure with probability that exists on both sides of this thing well I think one of the reasons we're really bad at this as a community right is we don't factor into frequency component okay so we can guess on how bad so there might be when it happens and what we do though when when we do add the frequency and here's but here's what happens where I see over and over and over again is they'll take worst-case impact that's what we'll focus on right yeah people in our industry we're stressful and then we'll type for frequency we'll take the most common instance all right and you throw those two things together and you grossly blow up the risk statement I mean it's completely inaccurate at that point that we exist as an industry for probably you know as many events as we can count maybe three hands there's really big long tail events or well exist the common have any sort of things we hand and we move on nobody loses a lot of money from them there's no loss associated with them but management invariably sees that there is some big boogeyman out there of a big loss amount so risk is always about loss it has to be customer and and and there's list of risk that talk about assets like this one time and we talked about it in the book you know VoIP is a big risk right I think no I know I refuse I refuse to be threatened by inanimate technology it's just a thing I could use it to do great things and to provide value for my organization but people that misuse it could cause loss to the organization I say the same thing about insider risk insider threat okay yeah reason afraid of that but you know the frequency is you know there's not a lot of data on that you know so how'd it can be afraid of death sent me pretty low yeah yeah to your point so something we're doing regularly now with organizations is helping them either establish a review their top ten risks and invariably every single company I go into their their list of top ten risks or a mash-up of technologies threat communities like our control deficiencies right never loss of it no I never lost the closest thing you'll get for a loss event is data leakage right not specific enough to be all that useful and from a measurement perspective and so they have this list of things and I'll ask him I'll say ok this top-10 list which one's your top one I don't know I said well what was number 11 that didn't make the list yeah I know and why didn't it make the list right and they have no answer whatsoever right there's no way these people can defend their top 10 so fair can help you get there is the point so that's gonna make our jobs easier you won't feel an easy going to the board you'll know the answers right to that that's what I like about it alright so one of the things you said in the book which I really like you said that doing a risk analysis is deductive eyelash you're like homes right now you really like the idea that I could be compared to Sherlock Holmes okay yeah so tell me what you mean by deductive reasoning so there when you think about analytic models what people are most commonly thinking of is things like insurance models and such where they have boatloads of data gathered over millennia right and they look at the data and they can say okay the data and when we move around says the world works like this okay so here's our model well building fair I didn't have that kind of data right that's an inductive model when you're you know deriving your model from data right when you don't have that there's a way to deductively arrive at your model and it's simply say what is it we're trying in this case what is it we're trying to analyze risk what is it comprised of well loss then frequency loss magnitude what's lost it in frequency comprised of both red event frequency and vulnerability to other ones what's you know so it's just deducing from at each layer the layer below and so that's really what that means and and there are pros and cons to inductive versus deductive reasoning or they're both valid but they're both valid and and both have their strengths and weaknesses but when you don't have data right you have to work deductively and as as we begin to apply the model and we're seeing that all the time as we do more and more of these analyses and get more data it allows us to refine probability statements and that sort of thing refining the model and and so you get the most best of both worlds so touches oh go ahead Jesse and I think all this this challenge that you talk about Jack about challenging honors for their statements that's the duck as well you start from this higher level this is a high risk thing and fare gives you a language to sort of interrogate and ask questions about that so is it high risk because it's happening all the time or because we're gonna lose a lot of money think that so you think it's happening all the time as those frequency of law so freaking say have attacked and those help ask very pointed questions help you measure things bad yeah it makes you think about how we were trying to protect you know they think you're trying to protect yeah so we touched about touched on this a little bit but now you're coming back around to it you mentioned that in the book that you don't like being called a predictor you much rather be called a forecaster so let's talk about why that we need that distinction yeah that's really making the work that we're doing more palatable to management you know you know we make predictions all day long we're in Las Vegas now as matter of fact yeah that all sorts of predictions made about sporting events political races economics public policy who's gonna win the cannon or deny know each other's a lot you know our eeriness to make predictions is probably only matched by our reluctance to be held accountable when those predictions don't come true that's true it's not so much about making good or whether we can make good or bad predictions but really identifying the things about which we're able to reliably make a prediction for and I think using a term like forecasting relieves management of the expectation of precision and I think that's a really important thing because I can't tell exactly when you're going to be breached when they're going to have cyber events happen to you organization but we can put a range around it and that sometimes enough often times enough for management to make good decisions is certainly better than red right any time wait I use on my charts all the time we do - yeah you know I mean there's a place for qualitative and such and a lot of times management likes it really simple give me a color red tells me I have to pay attention to this right and great so I knowing that about management when they're like that that's what I'll do I'll give them what they're looking for but underneath that is going to be the analytics and the numbers I can back that color up right right there's going to be red is going to be specifically defined explicitly just find us - what it means what ranges what ranges fall within red so that if somebody wants to dig deep we can defend it I like to say that fair gives you a discipline for how you choose high medium low red regular easier yeah yeah so anything everything that red that comes out of your your group is gonna have this rigorous approach to how we pick it and it's gonna mean something and it's not gonna be the kind of thing that you're accused of being a chicken little for number fourteen so what I really like about the book that you guys have opened it up to outside the inspection okay it's not too crazy guys come up with an idea about how to do stuff you opened it up for your own stand or giving it to a standards body and I want to know can you guys talk about why that's important well it's important because this is a big problem in our industry right and and in order to evolve if I had kept it proprietary which people were I mean people had encouraged me to patent it and exactly stuff right and I just thought that was wrong for one thing but on the other hand too there were challenges with that because if there is an accurate codification of risk right then that you know trying to put a patent around it would be like Copernicus trying to patent his observations this solar system that would make any sense right and probably wouldn't get approved but but it you know putting it out there in that group allows it to be more publicly kicked around poked and prodded and evaluated and make sure it's it stands up so and the analogy I've been using recently that I think should be meaningful hopefully to our audiences you wouldn't use proprietary encryption right everybody knows that's typically a bad idea I mean someone's proprietary encryption could be amazing but probably not probably right and and the same with risk analysis and models right I mean if someone's model is is proprietary and held back or so complicated that you know it takes a PhD to understand it do you really want to think hard about you know what's going on there and that's and so really it's it's this notion of alright let's put it out there let's people poke and prod and if it's if it's not if it's broken in some important fashion I want to know that so it can be fixed I don't want to put some useless piece of garbage in front of people and having them make worse decisions than they already are great so well I think that's it's the best thing for and I expand testing you guys did that so we're about out of time but I just want to say public that I I think this is the future we were talking about this before this video thing began we are really bad at this as a community you guys have given us a path to get better at this so thank you for doing that congratulations on being inducted into Canada tonight and thanks for coming and doing this interview I appreciate it 