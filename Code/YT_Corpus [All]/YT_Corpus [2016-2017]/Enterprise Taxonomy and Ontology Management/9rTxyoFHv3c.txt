 I'm David Booth today's webinar by dr. Parr Sameer is one in a series of webinars about various aspects of the Yosemite project Yosemite project is a collaborative effort whose mission is to achieve semantic interoperability of all structured healthcare information it articulates a roadmap for achieving interoperability based on RDF as a universal information representation in today's webinar dr. Marashi will discuss work that has been going on at Montefiore health systems and Albert Einstein College of Medicine to build and use a semantic data Lake based on w3c standards best practices and knowledge representation in healthcare this work helps to demonstrate what can be achieved through the use of rdf as a universal information representation for healthcare data welcome dr. mahadji thank you very much David and the interest of time I will start the background and quickly so we can get into the introduction of this DL very quickly this is the ecosystem in which Montefiore and most accountable care organizations operate we have a lot of patients that are frequently interacting with digital media and generating a lot of themself behaviors and their environment frequently they come with some devices that are attached to them like Fitbit or Apple health a kid or other technologies that collect a lot of biomarkers from their heart rate their sleeping patterns to their mobility and a lot of other information that unit is generated by them living their lives and that's very important many patients come with their genetics tests done and a lot of genetic information about themselves and about their family members a degree of histories of many diseases that are now relevant in our understanding of the patient's phenomena and some kind of vehicles and devices that they take home and collect a lot of data about their health and at home and this all happens outside of the hospital but when they entered the hospital we interact with them even more broadband the massive amount of chemical diffusing or EMRs with an epic system we tie them into sensors detectors devices monitors in different settings inside the hospital that collects a lot of interesting logical data about these patients we have lab systems that creates biomarker data waveforms that we generate images because we collect pathology information down to the point that patients are dead and even after that we collect a lot of information about these patients within different departments medication data as a accountable care organization we have a massive care management organization where nurses and practitioners interact with the patient's after they are discharged from hospital to coordinate their care among multiple participants on the real care process within health homes or in a nursing home or in patients their own settings we generate financial data in fractions of our institution with the payers and the legal liabilities and lawsuits and the risks dedicate by interacting with patients these are all kinds of information that we collect we maintain a venturi of all of the devices and their conditions their quality their calibration that are all important in an understanding and providing quality care and reducing not only reducing the aerobic also being in compliance with some of the regulatory aspects of the law real clear process our interaction with our own staff creating the financial environment where we can pay them and promote them in their career and the relationship that they they have with our institution management of our assets provisioning for utilization of those high-value assets like our imaging facilities our chemotherapy seeds down to the level of parking slots that we have available so that we can optimize utilization of these resources within the institution and what files of all of these systems as they interact and transact in a in a daily manner as an example of why these log files from these systems are important I can reflect on one of our projects that was looking into clinical errors in prescribing wrong medication that were due to physician looking into more than one patient's data at a time in their screen and the only way we could actually discern how many windows the physician had open when they were doing the medication what were the content of those windows and how did its relate to activities that they were conducting at the time didn't each one of those windows and whether or not they have anything to do with the clinical error of your tracking was only achievable if we could link between content of the database and content of the log files to create a piece together a history of events and looking to coincidence of those things so this is basically the big date and warm and the amount of your phrase and the very basic question is so what is the characteristics of the system that can help us deal with this copious amount of data and information that they're generating consistently after every every system every organization has a level of sophistication in terms of building infrastructure to ask and answer questions like the ones we are interested in I was trying to categorize those technologies and this is a very neat way of looking into the information space and the tool sets that are operating in those the the point I'm trying to make by this slide is that there are many categories of data and some of them are bigger than the others some of them are inside the hospital or inside the institution some of them are outside but and there isn't really one easy they are creating a single source of truth so you can reliably and consistently access data and interpreted among multiple use cases historically technologies like itv2 or OMA and some private sector technologies like CLG has been they love to create a sales service and some economy of scale for practitioners inside the hospital the focus only on the EMR they took things that are clinically motivated when the patients are interacting with the health systems frequently most of the larger healthcare systems have invested in building enterprise data warehouse that does cover or almost exactly the same area plus administrative and financial data that would be relevant for the operational support of the institution but there isn't really if it was infrastructure that I know that can cover the entirety of the space in terms of dealing with the real-time data generated by those devices images free text data from can omics connectivity to national networks and most importantly linked open data the massive amount of a heterogeneous data that is outside of the institution very relevant to the practices that we have inside the institution but not very straightforward in terms of putting it into a analytic framework that is ready for analysis and our notion of semantic data Lake is basically an idea about how we can create a single source of truth for accessing and delivering these varieties of data into our most research setting and operational setting and consistently another way of looking into that dilemma is by just looking into the maturity model that organizations should evolve into if they want to really become a data-driven learning healthcare system the step one obviously is just to aggregate it off fragmented data points and put them in some central repository that would essentially enable a basic reporting so you can't go there and create some dashboards for executives and some ways of creating views to those data the next level would be basically scale out which is do the same but in a much larger scale covering internal reporting and external reporting automatically that automation will require a lot of investment in building standards terminologies harmonization methods and registries with consistent and standard definition is that everyone can understand and subscribe to you and that can become the basis for building automated mechanisms to leverage that for many different use cases and that is where once this is done the scale and the data has been democratized and delivered in the enterprise that's been the culture of using data as part of a workflow process will emerge inside the institution and then you can expect that we can build facilities that can really do change organization systematically for you can start building waste reduction models and reducing variability in delivery of care processes that at the end of the day these efforts are the ones that will make a return on our investment in building the data infrastructure and actually utilization of tmr's and realize in large institutions and anything above that is these are the value-added services that you can expect from investment in analytics building characterizations population health management models predictive models that are critical and core enablers of accountable care managed care or value-based or quality driven health care system that's basically Dale's care reform and the underpinnings of all of those are these kinds of infrastructure that we need to create and of course these days everybody is talking about learning healthcare systems precision medicine which is application of linked open data and genetics in making personalized personalized interventions at the individual level the point I'm trying to make with this slide is that yes it is an incremental process and each milestone adds value of its own common but it is not it is not sufficient just to create the massive data repository data is only one component of the solution of any consistent terminology management consistent metadata management consistent knowledge management and linked open data for this vision to materialize and again the question is what is the construct what is the architecture that can be used to scale in data integration as well as enable terminology management metadata management modeling and knowledge management and linkage to informational assets both information and knowledge assets outside of your institution these two ideas are the ones that actually have motivated design and implementation of semantic data like we have really thought about architecture that can help us navigate this maturity model I'm not I'm not making claims that we have done it it is complete and it is ready to use by everybody but I can claim that for each one or for most of these layers we have competencies that we have developed that can demonstrate that semantic data like is a appropriate kind of architecture to enable institutions to go in that direction so what is it that we have done it is basically an architecture based on principles of big data analytics Internet of Things and cognitive computing it is a graph based technology that is built on top of technologically on top of Semantic Web and utilizes HDFS SPARC and graphics as its core technology to distribute processes and integrate data management efficiently and scale out and data analytics and both in memory and enable transactional data management in parallel possible so you can consistently collect data in real time as you are doing running the amount X and on the infrastructure it's as technologies specifically directed as towards biomedical field consumption of biomechanical terminologies and oncology's with very robust URI level security and authorization and provenance facilities I'll show you a little bit of that and we have made sure that we are capable of digesting both structured and unstructured data mainly clinical but is still very an integrated approach to clinical text understanding as well as data integration on structured data we've built specialized temporal analysis use partial analytics and machine learning algorithms that are now inherently integrated to this platform so they can be involved in scaling the large dataset and it's basically a multi institutional collaboration between month of your friends Intel and Cisco so what does it look like I'll make some assumptions as I move forward with this one of them is that I do not need to speak of what is a graph representation what is rdf but i'm assuming that the obvious knows a little bit about these things but i'll keep it fairly non-technical i'm trying to open the hood and show what is underneath the principal design architects architecture that has enabled semantic data lake is to understand how a single data point is represented in semantic data laid obviously being semantic it is linked to a term or an ontology that defines what that data point is and represent a literal representation of that for him and used for example i'm just reflecting on one data point here which is about a medication which in this case is amoxicillin but then it's also linked to the 10 that was used by the source system to reflect on this if the source system has used a coding system for nice eco to represent this amoxicillin we represent that too so you can assume that our system has a library of these terminology systems to link to and if if he normalized to represent this term in a standard way rather than the way it is represented in the source system via also a link to normalize or the standard representation of that so by now I know I am a moccasin for the patient and two code set that defines it in each one of those code sets by the nature of being driven from some knowledge base or bring in a lot of metadata and knowledge about them so you can actually look into arts north to see the classification and the hierarchies associated with these medications or within NDC you can look into makers and packaging of these medications if you have made any transformations the the rule of those transformation is associated with that time what applications in what use case or role scenario can access this data point is also attached to that data point we allow different user communities to annotate their data evading want we have a generic way of annotations but we also enable people to annotate their data however they want we have an annotation framework which enables for example if if in the side a use case the rule requires linking of the state upon you if the Beavers databank because there are a lot of meta information and ontological representations with infrastructure and the indications of contraindications of the medication that might be relevant for the analysis we enabled that extensions of the ontology or linkage to other oncology's if it is necessary for the analysis we enable that every single interaction in the system is time-stamped so every data point it comes with a timestamp on when we did the transformation so you can build a history of those and we have a very granular representation of every single data point as well metadata about the lineage of this information what was the series of ETL or data transformations that has happened before the data has been represented in RDF format is also linked so you can actually look into the source information coming from EMR going into enterprise data warehouse and coming to the STL and how many transformations has happened what was the original date representation of the source and what was the intermediate representation before it got to semantic data legs also links if if we use this data point in any analysis downstream results of that analytics driven by using this data point as an input is also linked with so we can by looking into any data point you can see in how many different places this single data point has been used and what were the results associated with that and of course every data point is linked to another for example in this case this amoxicillin prescription is perhaps related to an encounter in which this medication has been prescribed so so that's just that was an anatomy of one data point let's see how multiple data points connect to each other and become semantically another example from chest x-ray of course coatings used to represent this in multiple sources and all of those mentions there for an x-ray and we can use this annotation to link it to you for example the cost structure of this x-ray for research setting for industry sponsored clinical trials or pay by Medicare so you can you can actually create multiple cost structures for that one service and say if this is done by Montefiore this is the price if we charge it to industry this is the other chart duration of the image that charge is valid and if we are billing this to Medicare what is our conference so the terms of the contracts multiple different ways of creating invoice for that one service that can be annotated by financial people a clinician may and okay something else but this is basically how you start contextualizing every single data point in our setting you can add calibration for example information about devices used and the schedules available or the time slots available for that device that took this image can be also part of the annotations you can actually use this annotation framework to manage a lot of meta practices around every single data point now that we have that x-ray you may want to say what was the person that x-rayed was taken and what was the address of that patient that we did that x-ray for and address itself can be annotated by geocodes longitude and latitude of that census tract information and household information so how large was the house how many bedrooms does it have what's the neighborhood like what is the average price of a square foot of a house in that neighborhood in that census tract so you can basically start implementing linkage between the social socio-economic constructs with the patient data here so we have that covered we can link it to the encounter in which this happens diagnosis made for that and then you can collected analytics you can run a predictive model that would predict what would be to readmission risk for this patient based on everything you know about this patient the clinical representation of the patient neighborhood this patient lives in qualities and costs of providing the service for the person you can use all of this analytics information to create analytics and risk the scores for the patient you can link it to providers and then you can bring information that you have about the provider to better understand these practices facilities in which these services are provided and then you can link it to another higher level analytics which calculates what is the distance between the that place this personal lives and facility the den business person is being cared for and what are the relationship it's been that and the readmission risk of the patient so this is basically how our overall method data splash knowledge /length open data management framework links together to create a consistent representation health it and as you can see is a very iterative process it isn't the same architecture repeating itself to create more complex constructs that once you put it together it really becomes a community and population-based data set that evolve longitudinally for every single data point there is a knowledge base that further contextualize that knowledge based and linkages that's one can provide it's been that knowledge base and outside sources to annotate these single data points further or analysis that you run outside and you can link it directly to the patient data we have also a mechanism you probably noticed that we call it and out of the tapestry which is a consistent process of determining results of analysis every time that it happens back into the semantic like and closing the loop on collecting these information as part of our larger vision to build a big data Mart this whole construct is what we call our evidence generation paradigm which brings to bear on data that we can bring in and notate contextualize and analyze in a learning that happens the in the system that can be returned back into the system so so that was overall overview now let's take a look a little bit into the architectural components that I briefly introduced of course we have a knowledge base that's the core brain basically behind the system it is a knowledge base of currently we have over one and a half billion triples only the knowledge base a big part of it is geographical information from census and other socio-economic data but the entirety of the umls 182 vocabulary systems from you MLS is integrated into an entire Olaf terminology is there clinical trials own name drug bank and several other knowledge sources first databank and those are also part of this in this quarter presentation it's a knowledge representation framework that is being used is simple knowledge organization system and other w3c standard that uses a Semantic Web and RDA as a way to create Desiree's method is the reason and mashups between them so it's a huge match up of all of these things very quick introduction to its structure as I mentioned its expansion to skulls so you can expect that all of those host concepts concept schema labels and labels structures and manipulations all of them are there we have taken those skulls concepts and extended that to create an ontology of biomedical vocabulary by aligning it to umls so we have basically translated umls to scopes and transformed entire umls includes cost representation complete with the cement network and semantic relations that are incorporated in the method phaser e-myth a thesaurus part of the umls and then each source vocabulary inside mental thesaurus is also turned into skulls and linked to the meta teasers so this way you can look into the meta deserves information from the Cui's perspective to see all of them collage achill relationships within a concept like abdominal pain but then you can change your focus from the metatheory level to specifically snowman's perspective or mandra's perspective or icd-9 perspective of these phenomena and basically narrow down on a very specific part of the knowledge base that he would believe has the information you need to make the analysis in the side we have also linked that to Alma Alma is a subset of UML s which is being curated by a big data analytics group called Odyssey especially to enable observational research and data science using clinical data from EMR so it is a lot more clean more high-quality terminology that is validated across several large scale announcer to use cases to consistently represent Telenet call findings so we have made the linkages and that is basically the internal link point for all of the biomechanical concepts within semantic data Lake and once we is linked to OMA they also are linked to sno-med join to ICD and to the Missouri fizzers and we can basically bring everything make it queryable if you want to look at it this is how it looks like in the left hand side you would see the semantic network which creates a very simple but useful classification of these terminologies within within the network and metaphysics information like synonymy definition of the concept preferred labels all of the matches matching between multiple terminology system they're all consistently represented I made a very quick remarks on its composition this is the current state of what it is without census tracts it is close to 600 million almost half a billion and triples with census tract and geo a coding data it is more than 1 billion triples so if I want to look more closely inside it if you just query for something like that because mellitus you will see diabetes mellitus in multiple different representations from multiple languages as well as which coding schema in what version has been used to construct this representation you can see that in the left hand side you see tracing of each one of those labels to the terminology system version of the terminology system code use that in that from another system inversion of the terminology system so you can trace these concepts in the left-hand side you see the semantic relationships according to the method is heurists relatedness rather natural relationships within the concepts such as put in a row party and different representations of writers and peripheral neurology and everything else in the right hand side there's taxonomic relationships here are key of terms is possible by navigating different parts of the graph and once you have these interconnectedness you can do pathfinding queries this is one query this is just an illustration of what is possible now that you have a knowledge base like this available within the system that shows the value of having knowledge bases like this one this query you can actually start reading it it's very intuitive to read is saying that for a medication that you take it movie five medications that are indicated or contraindicated in diabetes mellitus and for those that are contraindicated find their mechanisms of actions it i have not selected all of the medicines just a few of them that are enough to illustrate the case and I will ask the system now that I know what are the contraindications and indications of medication different medications for diabetes mellitus for those that are contraindicated see if you can find a pathway based on the mechanism of action this will so this is that all of the red nodes in the right or basically results of a search algorithm that has into the mechanisms of actions and their pathways to see if there are any common pathway it's been any one of those nodes Emily you can immediately see that for example there are generic agonists our common pathway within these two mechanisms of actions and there is a common pathway around the g protein-linked receptors across all of these medications and all of these mechanisms of actions so there's a very quick illustration of how these men and the ontological mashups can give you further insight that is not possible to gain without having these knowledge base nonce racism or you can ask questions such as given a medication tell me everything that is there is to be known about this one medication and as you can see we have all of the packaging formulation information those form information indications contraindications and McKenzie's or actions of this one medication coming from multiple sources in one single graph the nice part of this is that once you link it to link it to patient data then you can really have a lot of insight about patient data by having the knowledge base associated with every single data point here we are looking into a one single patient that had a peanut allergy diagnosed in a an encounter so this is basically showing there is a person that has an outpatient encounter the Internet encounter we have a diagnosis and a diagnosis was allergy to peanut allergy to peanuts is linked to the ICD code of the allergy peanuts based on ICD not but that one linkage is enough to connect the two the entire knowledge base of how these allergy to peanut is classified within different representations you can actually see that we have now gained insight about the one code normally this is what you the only thing you see in clinical databases this is where the buck stops in relational representation of a data warehouse but now we know this is these are many different ways that allergy to peanuts in a clinical database you can build a lot of interesting NLP algorithms based on this from OMA you can actually solve causation information what causes it and the disease classification I can at any given time I can ask a question such as what are the allergy disposition of what how many patients with the allergy to a substance are there and how many of them are have food allergies these queries are not possible because of these linkages and then in more robust query based on those path finding features that I showed you would be take one patient than say find any patients that is similar to this patient and rationalize it tell me why those people that the system finds as similar patients are somehow related to this patient as you can see based on a retinal detachment event in this individual we have found four other individuals with different forms of the retinal detachment that fall into the same category of disease therefore system belief they are similar and again this is impossible without having those knowledge bases linked to the data and algorithms that can navigate the data space information space to create these results as I mentioned our our representation is a highly iterative very well organized and structured although it is a graph but it's a very structured graph of its own this actually shows you that if you want to change the lens from graph to let's say a third normal form in relay tima design you can actually create a third normal form view of the graph of the patient information in the cement that typically I'll speak a little bit about the consequences of having this which is ability to in real time create just-in-time data marts on a graph based on results of a very complex graph query and that would be persistent in any relational database for people that want to see information in relational but can't harvest that from graph because they're not familiar with the idea structure or the technology to be effective in that respect so this shows one patient their encounter diagnosis in that encounter with the timestamp two types of the diagnosis and encounters and what was the diagnosis made in that encounter very structured and this can be transformed into a comma separated file or a relational representation very quickly I talked a little bit about linkage within data and knowledge here I'm showing the metadata management some of the facilities that enables tracing and tracking of data within technology constructs as you can see every single data point here in this case this is the visit ID our patient is linked to the fields format of the fields and everything else that you may want to know that you kill time the rule for that kikyo process and as you can see every single data point is stamped with tracing lineage information that would be necessary for necessary for quality or audits the big picture is that as many times that the data exchanges hands before it gets to the semantic data like here you can see what was the field what was the quality of the data at the field the source this is the EMR this is the interrogation of the EMR before the Det L at this time how large was the data set of complete was the data set what was the last time that this was updated and then it lands in enterprise data warehouse the quality of the data at the time that it was landed in the enterprise data warehouse and this is the data person typically so every single data point is traceable back to the source and the quality of the data at the source and this is the mechanism that enables building automated quality control facilities that can detect whether or not for example ETL pipeline has a stopped and you no longer are receiving outpatient diagnosis data or outpatient lab data because something has gone wrong you can detect those kinds of anomalies without waiting for a surprise to happen and it also accounts for multi-source data if you are pulling data from three fields into one it traces and adapt to some really cool traceability functionality any given data point because of all of those metadata is traceable to to the field that may contain that so you can take any CPT code for example and say I have a CPT code I have no idea where can I find this in my enterprise data warehouse and it can this is actually basically saying that that CPT code has representation these two fields within the two tables two different tables two different fields have the same CPT code in our enterprise data warehouse and consequence of that that we can actually create dynamically contain a dictionary and catalogue of all information this basically shows how if you are if you're looking for patient diagnosis these are the fields that have that information procedure date these are the fields that you can see the procedure date in your system and this is all harvested automatically rather than someone making a note somewhere because of all of those metadata that is links and which enables automation and scale out we can ask really interesting questions such as such as so what do I know about this patient tell me everything there is to know about this patient 360-degree view of the patient this is not probably one bogglingly complex use case but I want you to notice the simplicity of the query and how interesting how intuitive it is which we basically say just count the types of information and and group them by type when it comes from this patient and as you can see it says I have one indication of a city 33 patient diagnosis and as you add more data about this patient no matter how complex the graph is to get into these data points you can actually trace and track and find exactly where the data is for the patient's diagnosis patient's date of birth patients gender and build mechanisms to retrieve that or quality control that and and so forth this is also one of the key ingredients of our patient engagement approach where we want to be able to provide reports the patient's on what do we know about them and how many times this information has been used in different settings that's basically part of the HIPAA and we have created infrastructure to be able to no matter how complex our data infrastructure is be able to trace and track and what do we know about our patients so that was our knowledge base metadata and data management how this all comes together is that we of course we have this knowledge base that we use it for M notations and linkages we have created a annotation engine it is also familiar with with the notions of time as I mentioned we have a specific view of the temporal relationships that we would like to represent so that we can build analysis on top of this annotation engine takes all of these data sets and allocates links to oncology's that we have are you automatically or using some level of human interaction and this is the software that we have developed to do that it is very similar to any vanilla ETL software that everyone would be familiar with navigate and you go through a couple of steps to do the ontology mapping and characterize without the annotation frameworks you want to use for these data sets and it does the rest and creates the semantic data lakes graph the only thing that is necessary for the modeler to know and to to be able to operate on is to understand they tap the source we do not require our modelers to know anything about technology about ontology is about graph or RDF we just expect them to know what the data means at the source and that the rest is handled by the system using those interactions to have them annotated and this is literacy in the interest of time because we don't have much time I will skip some of these slides our notion of analytic tapestry as I mentioned is basically to link downstream analytic processes to database so our semantic data linking corporate data and all of the value-added analysis as well we have created a another software that again interacts with users ask questions about what exactly is it that they want to incorporate in their analytics what are the transformation methods they want to use imputations filter functions and validations that they want to do how they want to run their analytics in the system and creates an ontology of that analytic process and that ontology is one that didn't ignite another algorithm that can push the information to any and our framework that has been used 9 h2o or SPARC SAS we don't care we can push the data first find the data from STL format the way the user has requested push it to analytics and once the results are back consume it and put it back in this deal as I mentioned we can write it into a data Mart so people can do this on their own it's relatively mature their model but once is ready put the results back into the STL or send it to a service somewhere that doesn't take care of the data analysis get the results put that in this field so that's basically the framework for another tapestry we have automated the process we believe that it has turned over semantically from a system of Records to the system of insight because we know everything about patients I'm gonna give you an example of an elf tapestry query this query says for this one patient tell me what were the analysis that we did for that patient what were the input to that analysis and whether or not there was a secondary analysis that took the output of that analysis and input what other input was used what were the secondary analytics and what did we do with the results of those we send that input somewhere else I'm just gonna show you the results this shows that there is that one our score that has been created by a random forest algorithm for a project called approve these are the inputs that we've pushed into that random forest the score was calculated which was used by another analytics this is a rule-based analysis actually so this rule was a fired based on this input and several other inputs of its own at this time and created a notification and that notification resulted in creation of a email that has been sent at this time to somebody I just took one of the data points used to invoke this random forest and show all of the lineage information that you have seen previously so and this is this is the critical aspect of our clinical decision support when you are sending a note or a page to a doctor that you need to intubate this patient you better to know be able to answer the question of why why didn't you believe that this patient needs intubation you can really provide all of the data metadata and quality information that will convince someone whether or not this is a reliable application this is basically evidence behind any conclusion that we create in the system and it helps us with other with other audits and in protections so all of those data points are there the proof project is basically the use case that we are validating this amendment data Lake it makes a prediction based on 68,000 training data it's a thousand admissions data set of whether or not patients will die or will need resuscitation respiratory resuscitation in the next 48 hours it is a highly sensitive and very specific we have only one percent false positive and we all perform any published algorithm that do similar things this is basically a view to our end of the tapestry using nine which is another another framework that we have integrated to STL so this is our analytic integrated software that uses the tapestry information to create to query the data take the frame description which is what I showed you an ontology that describes what is the input to this compile the frame to the imputations do all of the new derivatives run random forests and the Bayesian analysis that is needed to run join and push that basically the result of scoring back into this deal that was the version one this is the new version of the same and outlook for the proof project as you can see we are actually injecting a classifier and this is all done by frame description users just entered that questionnaire they say they all I want to do this I want to do that and this in Aleks are enabled and pushed into the software and now we can actually create all kinds of new functionality what we are doing here we are running five different versions the same model implemented by different concepts and different perspectives and we actually compare the results of them at the same time before we use any one of them as our downstream process this is a process be called super learner which looks into results of many many decision support systems logistic regression random forest techniques to create one integrated fusion of those scores before so if you can basically optimize your your analysis by by comparing multiple versions of the model to find the best prediction rather than going only with one model we have currently implemented the topological data analysis as part of our tapestry mechanism so you can actually look at the topology such a shape of the data as an indicator for clustering of the data again part of the process I think so yes and these are these are graphs that are generated by HDL and the software that is not currently monitoring the performance of our approved process at the hospital level for for delivering basically the the analysis at the point of care by by using all of that the information that I just mentioned using a steel framework I'm gonna skip because there are so many I think we ran out of time the last slide here is that what I showed you I hope is an indication that what we have built and we call semantic data Lake is basically motivated by a architectural understanding of what it takes to mature into a learning healthcare organization by bringing data metadata knowledge bases in length open data into one unified and outer framework this is very I need to stop thank you doctor mahadji will take just a few moments for some questions we have a few queued up here a couple of questions actually are around the advantages of RDF in particular why is already F critical everything that you've talked about here is about analytics why is already at the critical point what are what advantages do you see in using RDF over other data representations the critical adventurine there are other than elegance and simplicity I think that the notion of ability to link data to metadata to knowledge and to the external data sources is and doing it in an efficient and in a rich and expressive way is the number-one value add it is literally impossible to do this if if you are not using a graph based representation and RDF is a very good way of creating robust graphs I would say that RDF alone is not enough to do what you're doing the notion of triples with graph let's call them quadruples is not enough to go to this level of representation for reasons of optimization scale and and security we have to add more nodes in our architecture I didn't even talk about the back end how the Hadoop / graphics integration works of the distribution of graphs or multiple machines work but in order to support all of these you have to extend the notions of rdf to achieve this level of complexity so you can have many many different views on top of the same data without having to rewrite it or reformat it every time that you use his changes which is the second value add any other representation which is not a schema less will result in creating many many silos of data because those use cases will require a different schema and not compatible schema with your original schema you'll inevitably result in those kinds of silos that was why I started with the notion of single sourcing this information because this is one representation that I can mold into many many different representations that's why we don't call our either into transformation processes ETL you know they are ELT which means transformation happens as in the last step of the way when you are delivering the data to analytics or to application you just extract and load no transformation beyond just pure simple RDF grants okay next question how are you currently using or planning to use this semantic data like what in other words what use cases are you first addressing so I mentioned I actually showed you the creating a clinical decision support framework enterprise-wide the algorithm improve outward who is currently running on entire hospital system seven hospitals all non pediatric patients regardless of the point of entry are being monitored by the approve right now the same framework is as these figures being applied to the scoring risk scoring for readmission the disease and community-based rehab mission very very granny bar basically where predicting risk of readmission for the patients for predicting rupture of an ER isms for for sepsis for predicting relapse of relapse of c-diff colostrum deficit these are multiple these are really inherently different use cases but architectural framework wise data-processing wise they use exactly the same architecture we don't really need to create another take home art for sepsis and although they come art for and ERISA for neurosurgical decision-support we just use the same framework we just add a frame description and push it to analytics and then bring back and Alex into the system and it's not queryable okay thank you so next question an early page mentioned Internet of Things but I don't think it was there wasn't much and said after that what linkage is there to Internet of Things other than it's a buzzword for a class of data sources yes it is currently currently it's exactly that's the buzz word I said it is based on principle of implementation of that and I said it is a work in progress we have seen it as a facilitator to adapt and to align when the technology in terms of thing itself evolves beyond the buzzword and the use cases that we have in mind is by linking it to by understanding the source of these data points more precisely so we know what what is the device that is being carried by person a and how it is different than person B what are the differences of different monitoring devices in terms of their resolution and their calibration their quality that are generating exact same data about the same patient or categories of patients and linking that linking that to our decision support algorithms the the scenarios are yes basically better understanding of the source and being making them dynamically available for analytics basically in general the use case that we have in mind to do that is this is some Kaveri negotiation with Microsoft to see if we can get some help to integrate their API is to use bands to to collect mobility data and use the mobility data along with some clinical data to create a classification and depression classification scheme for patients so why they are just seeing and comparing their sedentary mobility pace not just number of steps but the pace with which they take steps and the distribution of those steps during the day we believe that we can we can connecting that for the clinical data we can actually detect depression interesting thank you doctor mahadji we have a few more questions you might have to have briefer answers here in order to finish up in time here well we are over time already but in order to finish up next one is how is the system more likely to get linkages correct avoiding false positive and false negative relationships especially there's a worry about patient identity cross referencing including improper data as part of a patient data or excluding improperly data that is actually about that patient yes we are using the best of the breed technology to do the linking which is human curation right now on top of that I'm not claiming that we have done a lot of working we have created enough metadata to be able to automate some of these things that we know can go wrong and/or can be detected automatically one example of those metadata that we are playing the planning stages is to create a curation automated quality framework for our master patient index where understanding whether or not these are the same people or or different people very similar characteristics is very critical and we would like to first understand how best we can detect these these errors in the mapping and once we understood and let's say we marjoram read patient information how we can ensure that the clinical data is also emerged correctly this can only be done if you have massive amount of metadata and processes that can actually scale when your tracing different in multiple places about the same patient and deciding when and which one of those that you want to out or in for specific patients but we haven't really done much other than just conceptualizing it and having conversations with our business intelligence team to start implementation so yes we I think we have some breadcrumbs that we have put to find our ways when we automate but currently we have just you're using human and the patient's human curation okay three more questions and I think we're gonna have to close it off after that next is what ontology czar you using can you mention some of them we have couple of ontology built on our own the wrists are basically everything that we could get from sno-med and we have revealed several of the ncpo oncology's we couldn't really find anything reliable that we could reliably interpret to our platform but majority of our basically oncology's are the ones that we have generated created on our own and some knowledge bases that we have adopted from you MLS there are all of the hundred eighty-three terminology systems within you MLS are here the semantic relations which is over two million relations that Olaf has he rated our here as well and it is a very rich very rich ontology if you will that we are not offering ourselves we are just reusing from existing sources the ontology that we have built are basically the work product of almost ten years of research and they're basically built to not only work with creating linkages in the structured data set but also in parts of speech and in clinical texts okay next question how many triples are used in total in your data Lake and how many patient records are on average how many triples are used per patient record names and how's our patient I don't know our our system has 2.8 million patients and I think 12 billion triples and we haven't really come find everything only those things that we needed to support some of these projects you have to realize that scalability issue especially when you are running concurrent worries and you are also in production some of these algorithms are real and so we are trying to not to complicate the world for us our next version of the backends which will be hopefully released by end of the summer visual optimized the baby partition data partition graphs and annotate graphs for security and authorization so we can do quite as much faster will enable us to compile it in more data and I think we will have something around 100 and 250 billion triples pretty soon for example for right now we don't have any device data in there we don't have genetic data in there not all of the clinical findings basically the ele tables that catch all tables of our um our that people put everything else in there are compiled into this and those are the largest sets of data okay final question how do you deal with patient privacy issues in using this data like that was a big narrative that I it's kids we have a dual representation of data when his date shifted so we don't we don't really have the actual dates anywhere in the system we have a unique shift for the patient and the shifted data if the use case requires actual data we unshifted and the encrypt everything and these processes are partly the ones that I'm basically telling you that our next release will optimize so some of these can be pushed into the back ends at the query time so some of this can be actually part of the multiple nodes that we use for representing any facts so we can do these at the back end and optimize the process but our crowns representation is a dual representation where everything is de-identified and an identification happens only just in time when we needed based on those shifts and unique memories okay great well thank you very much dr. Marashi for discussing your work and thanks to our audience for participating if you'd like to receive future notices of Yosemite project events just visit Yosemite project org and click on the link to join our email announcements list thank you everyone and goodbye for now 