 thank you all for coming so we're just give me some information about who I am start off with because I'm hoping you came for something other than the free pizza so you can never be quite sure and so as Erica staying there I'm from Ireland quite need the life jaws and so I graduated from at Trinity College Dublin back in 06 then I worked in Google for about seven years working on various systems and I kinda ended up as the monitoring guy because I need monitoring to do work it wasn't my primary role but you know if I want to debug a capacity issue or figure out a problem you kindly monitoring so I was the guy who ended up fixing it after Nash I worked at a local starter from Dublin called box ever or as in charge of systems infrastructure and from there I actually found my own company called robust perception focused on Prometheus monitoring system language and I have over the years contributed many many open-source systems so when you talk to people about monitoring and they can have very very different ideas but what you need I had people wondering am i doing employee monitoring like you know the people who check the proxy logs to see if you're looking at facebook during work it's like no no that's not the sort of monitoring or involved so I kind of see monitoring in this context to mean for teens the first of them is to know when things go wrong so if your customers are knocking down the door that's something that's gone wrong if that's going to happen in five minutes time that something is about to go wrong and one of those things that happen well the second thing is you be able to debug ish and figure out what's going on you want to gain insight into the problem so you can fix it on the more longer term you know uttered in plano the customers are trying are there at hitch forks you'd like to prevent that likes preferably several months ouch and for that you've trending so you want to be able to look it over the last week or month or quarter or a year and say hey we're kind of getting close to capacity maybe we should do something before you run out in two months time or we say hey our cache hit rate is twenty percent that means we can actually turn off some of these servers or make other business and engineering decisions the forged kingdom on rings for in this kind of varies it's kind of just generic plumbing and it doesn't quite fit but people use it a lot so for example if a hard disk fails you can't do anything in smart at the end of the day you have to send a human into fixed ash mostly because the robots aren't good enough yet and but those other cases like you made me also to a security test other things I dash where you're just plumbing true in the ones on just how you're making that happen so if we're going to talk about microservices there is a few things that are kind of different about micro services compared to other more traditional architectures one is the UN tend to have many many small services normally you might have 5 10-ish services that are monolithic microservices you could have tens of services may be getting 100 depending on how broken things out are the services are all talking to each other rather than one mod lush that's just talking to itself there might be a whole pot of language is involved like you might have Ruby and Python and no tengo and C and C++ and maybe someones you even use something of Lisa tarika and of course because there's all these micro services there and you already might have tens of them already well there's lots being added removed all the time as teens Irving refactored and change so it's a quite dynamic environment and then the thing is that if you talk about what you need to do to monitor microservice let's look at what more the common lounge drinks problems people have our at all for any system so in my context of my company and just as an open-source developer and I've talked to lots and lots of companies and there are some common trends that are well common monitoring tools that are out there they tend to be very limited their limited technically and we're limited conceptually as in they force you to take a certain way and that certain way is necessarily very good they don't scale LOL and are generally just really really annoying to manage and generally these result in the fact that way you do Tings just doesn't align with the business it doesn't make sense now the sample I always use is this if you have a system like nagios that can only alert you when cpu usage is too high and what you actually care about is user an end user latency is too high you've got a mismatch there because yes maybe CB usage is ninety-five percent but that's just log rotation little clearer in a few minutes but maybe also you've got a deadlock and CB usage is low but your latency still sucks because we got a deadlock so this is not a great method because there isn't great correlation so on top of microservices add additional challenges on top of us so if you're adding certain removing services all the time that may stay pretty low friction if you have to submit a form and wait a month or two for your ops team to take care of that that ain't gonna fly and you've also got machines me ajan removes all the time so you're gonna want that happen completely automatically without any human intervention preferably there's also going to be each team because you're managing drilling services and making your own architecture decisions then each of them also has different requirements in different ways they look at the world having one operations team in the center or centralized the monitoring team deciding what the world looks like not really going to work out so how do we support all this so what we kind of need to look for is a monitoring designed for a dynamic microservice environment right it needs to support all the languages you have all the systems all the server types all the databases whatever else all your teams to decided to use and allows you to decentralize that across your teams and you know we want to be able to do a nice bit of math across it and if you do have commonality between your services because you are using common or PC libraries right at least in one language you should be doing that when you are do have common services like common libraries like that let's take an advance to the fact that hey everyone's using the same library let's take advantage of that everyone's life easier because the thing is a lot of monitoring systems when you look at them they have a big black cloud that you can see what's going on inside them you can see on the outside yes or requests coming in we're getting so many requests per second to this end point in that endpoint with so much latency and the other end is talking to postgres in the middle we're not sure but we think it works because you know just still some pitch forks for sale so the thing is noted in reality services have internals just like your micro service architecture kind of looks like a tree the in terms of your service is a whole pile of subsystems look kind of like a tree so the idea behind prometheus is each of those subsystems is instrumented and then you pull that off ouch by Prometheus the second idea is that you don't think about machines you think about services because fundamentally I don't care that this machine is running your service because it's just one of many of those and obviously the Micra service architecture you might be multi tenants on the given machine you want to just tank no no one machine doesn't matter it's the whole service and how it affects that it really matters one machine being slow is not a problem less is doing it's so slow that's dragging the rest of the service down enough to matter because you don't want to be woken up in the middle of the night by one machine that's you know it's not performing optimally but it can wait in the morning so previous itself is inspired by a Google monitoring system called Borg one which is well over a decade old at this point it was started by X scooters in 2012 the project and as an open source project with Sanford and it's mainly reckoning go because they were having problems with Stassi and graphite and it was publicly knowledge in 2015 and it's completely in pet independent of any company and so you're wondering excuse creating this is because when Googlers leave sre need Google they just recreate Google technologies it's just kind of what we do I personally be involved in five such projects for me this is just one of them although the one which I've most seriously don't and so since we launched publicly back in january twenty fifteen hundreds of companies are using committees in production and i don't mean that one guy is running it on his laptop for fun I mean more than one person that companies using it which i think is a much more reasonable definition it's also a very active open source community we have over 250 contributors just the official repositories and then there's over 50 third-party tools you can use with it as well which i think is kind of cool so the idea I thought I push and talk to here is that we don't have that big black cloud that we actually use inclusive monitoring and monitor everything so if you have your PC library right then one person on the client site instruments at once and everyone can use it the matching library on the server side once again someone instruments at once everyone gets to use it your business logic hopefully not copied around quite as much and but once again someone instrumented and you get that information for your system if you have say your client library 4pcs instrumented and common across your entire infrastructure that leaves some handy bonuses for you is a developer who's using this library this is pretty handy because it means you're going to get some monitoring for free because someone's already created dashboards someone's already created alerts and rules the other benefit is actually is a library offer let's say that I'm trying to make some decision about compression and I'm the maintainer of the OPC library I can add a new metric or two that will help me at this let that propagate out by a release cycle points to Prometheus servers and everything pulling those just two metrics I care about and do some math and then they get trade-off for CPU versus network or everything else so that's sort of horizontal monitoring is also an option when everyone's using the same libraries and talking about just but adding in the instrumentation and but then you know everyone hates having to write code but someone has to do it you know it's great to say yes let's just tie the bell on to the cat's neck but who's actually going to do it and so from atheists clients that shouldn't you which witch you do instrumentation are not the same as most clients out there we don't just marshal data and chuck it over the wire we actually provide an instrumentation library to make your life easier it takes care of things like concurrency and stay tracking and bookkeeping and all the other things you need to worry about taking care of for you where possible where a language is powerful enough or has a feature will take advantage of it so reply an example context managers and decorators make our lives easier and make your life easier so let's take this example here this is a complete example of using a Prometheus client library from Pikeville you pip install at community its client and you import stuff pretty standard and the next line then is creating a summary so this is the way the track latency here and you'll notice here it actually has a description requests eration in seconds because that's helpful to figure out what this actually means which matters lost when you start having hundreds to thousands of metrics across your system because if you're doing this right you will have that many and then we just use the decorator to time this particular function which does not even because the slide is kind of limited in height and then we just expose it over HTTP so that's how easy is time how long all the calls that function takes effectively two lines of code when you ignore the boilerplate which i think is pretty cool as similarly as other common cases not just timing things you want to count the number of exceptions and you can say this particular type and how many requests are currently in progress oh pretty easy to do because we know these are common use cases so we've made them easy there we go and the other thing is like you're thinking awesome this previous thing I want it right now go out and use it across all the languages but you know vendor lock-in because you know that's always great to have and but the thing is that I have a very strong stance on this which is that we don't lock you in you can use Prometheus client libraries and not use any other component of the media's ecosystem so you can actually be spiking in Java have entered so far the api's are there for everything else and you can actually output the graph flash so with tree lines of code in python vs 14 HTTP and in java it's actually easier to do graphite then HTTP to official way for prometheus mostly because setting up surveillance is annoying in java and whereas you know i could make things easier for graphite so if you want to take advantage of these stuff unlike you know already from UTC ash but you're still using graphite off you go the library's allowed four ish maybe in future use Prometheus maybe you won't Maeve got some different monitoring system right a few lines of code outputs the dare instead so this gives you lots of options so you can take advantage of all these benefits without being tied into community itself and it goes the other way as well because let's be honest prometheus is unlikely to dominate all monitoring in the foreseeable future as much as I wish otherwise and as much hopefully by the end of this talk all you wish so too and so we have lots of things that would convert that it the other way what are we graphite stats the SNMP jmx you can take data in from those and output them for previous and then the other things we can do is the view of the existing binary that does premedia stuff you can read that in parse it and check it out to grafische so an example of this is the land of zeidman as the land o is a german startup do clothing in your primarily not Ireland even though different office there and but think nagios written in piping to scale and but they do itself does not use for meteors however is lando are running node exporter which is our machine vibrate library as explosion exporter and they just pulled some stats from that by the platen cleaners and alerting them there is no Prometheus in this loop but they get to you to take advantage of the fact that we'd figured out how to monitor one particular system and pull that into their monitoring system for free because one of the problems I have as local source monitoring system water is that well great I sold you'd wear is like 50 different integrations out there probably about 60 or 70 by now those all have to be written for prometheus and New Relic and cloud watch and that a dog and open TST be and collecting and this is just silly every single monitoring system out there needs to implement all these integrations and that's an n-by-n problem that city so one of the things I'd like to propose is hey let's get one standard and then have it as a clearinghouse to spin it out whatever you want and as well as I mentioned there are lots of integrations for example we support minecraft so if you want to monitor how long those tix are taking by your frames per second is off you go you can do that as well as the more practical stuff depending a course and how much you've added your ledger and like--see adviser java jmac spiking my sequel post res cloud watch anything else I gosh you want an interesting as well as most of these were created whether the core team knowing about them we only found out after people are already using them in production that gives you a leave just how easy they are to write and also why we now have guidelines and how to write them there are some things they were little non-obvious about writing so another aspect to for meatiest compares to those who are used to graphite and sue most people here flash at some point and so it'll use dotted strings so this example here if I have something a metric about tech in Dublin there'll be metric detective them and these can go on so long and it might say oh the region is this it's canary or prod and so on and you kind of have to know which field means wash / media is instead actually uses structured data labels for this there's the fuel systems like open tsg be Netflix Atlas think data log has it your relic has some way and but most systems are still on dotted strings which is a bit of a problem when you're trying to look them up because you have to remember which field means wash and hope that no one put a dot in anywhere to mess things up because it turns out most people don't know about escaping and sanitizing but the advantage of this is that because it's just key value pairs in a map or a hash and you can look them up any way you want so it's much more powerful rather than having to use Globes and the other thing is that these closing of instrumentation like if you think about your PC library it might be which method or any point you're using or it could be coming from the service like oh this is canary or this is Europe or this is easy to or whatever so I can come from both and those are merged in unified so you can get them either way so if you want to know hey how is the slash fetch me going across everything you can or if you want to say hey how's the entire of prod doing you can do that as well because it's all one unified namespace so here's an example here we've got a metric hold node network receive bites so that's the number of bytes received in the network interface and devices here so this is Anna docker examples we've got a docker 0 eth0 low so on one machine happens to be 192 168 1 73 and it's a node as a job so you might have hundreds or thousands of these thing on how many machines and devices teaching them half then if you're going back to the instrumentation example it's about 20 bytes to add in a label so this stuff's really easy to do you do the declare in advance what label names you want and just because you don't want to know how difficult would be if you didn't your dash as a user because that just makes it really hard for later analysis if you haven't chosen them another interesting thing there you'll notice that the numbers here are kind of vague particularly for low and eth0 because that's clearly not all I did inside last minute so Prometheus has a notion of a counter a counter goes up goes up and unfortunately not all systems go up which is a little problematic with their counters so by the fact that we the counters only go up we can do something to increase our reliability let's say that we're doing scrapes rice every minute and one of them fails for whatever reason the network stodgy someone you know hit off a cable maybe with server was overloaded without the moment and we missed a scrape but we know the total count from the previous scrape in the next straight so we can tell oh yeah we can kind of tell what happened there so we lose some resolution but we don't actually lose information that's kind of handy there are other monitoring systems out there there's a few different approaches about a blog post on this that for example every 60 seconds they'll push what happened in the last 60 seconds so if you lose that push well you've lost it good luck and there's other ones that will reset every time you scrape which is real fun once more than wanting tries to scrape so this approach is much more resilient and the one challenge you have is that if the server restarts everything goes back to zero so you need to be able to detect dash which Prometheus does and that's all fine as long as counters only go up because if counters go down we can't distinguish between a counter going down and the server restarted so that's why it's really important that counters only ever go up so now we've gone to the data model we've talked about the instrumentation getting all the data in that's great what do we do would now like we print it out and you know stick it to our fridge now so we've got more powerful options got a language called prom ql which you can do basically anything wish a tinker almost complete feature set at this point so you can multiply add subtract divide multiply do label operations anything you want so if i was answered a question of what's the 90 percentile latency in the European data center I can or how fold it it's been four hours now anyone who's ever said disk full alerts will know that they are noisy and annoying because there's basically two approaches to setting them to normal approaches one is you set a threshold like five gigabytes free the other is you set a percentage such as five percent free problem is if you set five percent on a really really big disk that's very wasteful but if it says in a really small one it can fill up in no time and what the exact opposite problem which using the absolute amount of five gigabytes what / 80 is you've an option of using linear predictions so in look at overstate last four hours data do a least squares regression and see there's hey we think in the next error is going to be X full and if that's over one hundred percent full ten page so that's a much better way of doing things it's not perfect obviously but it's still far better than static thresholds and other things you can do like if you want to know the top five uses a cpu I'd stay by da Korean British is something no problem you can aggregate these in any way you can think of the other important fact is prometheus makes no distinction between graphing and alerting if you can grafische you can go recognition you just need to add a filter so here's an example here so across this entire for media server whatever its model ring which is probably an entire data center we're going to take a race because that's the counter of everything running in docker so saying this system slice stalker averaging that over five minutes added up by image so this is across all docker images so this is across all machines across all containers across all apps everything using the same doctor image give me the top five for CPU so that's sort of aggregation and power you can do and this is a simple example in fact this is the world complicated example is run in day-to-day usage so it's nice rizal to talk about previous language and so on but if it takes you you know if you have to sacrifice a chicken every time you use it that's not going to work out too well the good news is we've not require either human or animal sacrifice to use Prometheus and so Prometheus itself is a single binary its goal based is easy to run it has no network dependencies right it doesn't depend on Jupiter console Cassandra Hadoop or the internet because think about it if you've got an Eric petitioner other problem do you want your monitoring system falling apart or do you want your monetary system doing the best I can if you've got something like a Catholic you there that's the CP system and it's going to fall apart as soon as even our partition prometheus because all it depends on is local disk and network access it's just going to keep on chugging along as well as it can so if you've got one of them on each side of the network partition hopefully at least one of them and get alerts out and get on page you which is kind of handy for debugging and as I discussed there the it is stage rather than event-based so if you lose a scrape all is good it's also decentralized so committee says pull based a lot of monitoring systems out there are pushed paste which has pros and cons paul also has pros and cons but it does mean that if i want to test out prometheus on my own my workstation i just turned it on and canned air and see the exact same thing it's what's in production so that's really handy for developers it also means if i want to do hey CheY i just run a second one side n t'kul all works simple no clustering or other things that that which can fail but because it's so simple that means each team can run their own I mentioned earlier how each team can have their own view of the world where Prometheus you just give one of them each or potentially also have it inside one per media server but let's be honest everyone want the road and now you all will ask the question this is all pull based how do we know what to go and scrape because we push weld everything kind of just finds the monitoring system with pull that's obviously not going to work out and so we've got service discovery that can look at ec2 or console and marathon or zookeeper or whatever else you have I get a list of all the targets and then you can use relabeling to basically figure that out because one of the problems we have is that everyone has different view of the world in any given company there is probably between 10 and 100 different ways that each team views hauser systems are organized like an infrastructure team is going to say yes this machine its on rack foo in data center bar that's it and if the variable structure team de wenis what things are called however database team might say no no that's the canary server for team baths and Prometheus both of those world views are possible even in the same Prometheus or in different committees servers so you're not being dictated by what the infrastructure team of the side it's kind of handy now as well it's being easy to run and it's also extremely efficient so our new vibration coding is down to one point three bytes per sample on average that's actual production data and so this is actually based on Google as I on Facebook's gorilla encoding and we tweaked it a little bit get slightly more efficient for the constant use case so slightly slightly more efficient and as well sink refer media server and handle eight hundred thousand samples per second for context that's probably enough to handle ten thousand machines where the load so that's a lot suffice to say you this unlikely you will ever need any horizontal scaling on this because it's so efficient which is kind of handy in the event that doesn't work out there is a way to do Coruscant as horizontal scaling it's just slightly annoying but you know most of the time we'll just say vertically scaling so just give each team their own and at last forever now of course with all these / media servers all over the place and you're going to ask how the hell do I get my data it's going to be all over the place and so this Federation's allow you to pull aggregated stats to model for media servers or more commonly if you've got a data center Prometheus in a global one you can just pull those all up for one central view but for anything important is still drop down to the data center one and get all the detail there's plenty of nash boarding options so on the left we've got the built-in expression browser it's good for ad hoc stuff wouldn't use it for anything else on the right console templates they're really powerful kind of hard to use because it's a full web templating language in the middle is prom dash which is an older system with Ruby on Rails that's all your pointy clicky stuff at these days we use Ravana and recommend dash so we have first class support aside graph Anna so if you use the authority of graphite same setup too often a query it works and the rain tank guys who produce Agrafena are very very happy about prometheus so it's going to get tighter integration in the future so we looked at so far is the instrumentation for variety language easy to use we got a pretty nice data model and way to look up data alerting is also pretty awesome and every team can have their own change their own view the world without having to be bottlenecks by a central infrastructure team or a central monitoring team and you know as new services are added and removed and even as machines or as remove things also math you're being picked up as long as there's a lower price so this means prometheus is a pretty good option to monitor your micro services so I've also want to go over some just general tips from monitoring microservices that these are not meeting specific he's applied to anything out there that you have and just so you know some things we've learned over time because when you start getting lots of data from all these different microservices it can get a little overwhelming and just like you architecturesoftware you need to architect your monitor just so you can think about these and scale it up because you know if you try to think Tooting is an EVE way that's going to be 0 n and that just doesn't work as you get more machines and services so the first tip I say to you is know what your key statistics are users do not care in the slightest going back to previous example that one of your machines is overloaded that is irrelevant what I care about is that your service is slow so that's the statistic you should have in your dashboards now you can debug and figure out that odor service is slow but that's really what you're taught to top level you should be caring about the same things or users do similarly related to that once you start having let's say lots of services each with lots of machines having each of those as a line on the graph isn't going to work out our brains are limited in capacity we need to realize this fact and appreciate it so rather than having a line for every service in every machine have a line for every service that's going to be in this example here like an order of magnitude less start of the process I once you figured out oh it's the web service that's broken then you can look at further down and see oh yes that machine it's dodgy killer but it's just like thinking about how do we change this operation from an N by M to a log n solution and a sort of way to think about us axillary relates to this is avoid what i call the wall of graphs and this is where you have a dashboard that just you know it looks awesome but it's utterly unusual unusable and so the one I've personally seen those worst had 600 grafts and this is declared to be an awesome dashboard because all the information I never figured out how to use that dashboard the worst have ever heard about happening was 1200 graphs someone had a reference some some talks and worst of all I mentioned this yeah humans can't deal with this right you'll be lucky if the person who created this bash board can even use it a week later it just doesn't work so when I say instead is you've got a microts architecture it looks vaguely like a tree hopefully no cycles maybe not sure and so what you do is let's say you've got a problem like high latency you start at the top you look at is insane hmm is this service overloaded no while I read us back ends oh that one's slow grand look it's this dashboard is it overloaded know what is back in that this one is this one slow this service is it overloaded oh it is grant let's look more this service is a particular machine is it a subsystem the service and so on once again this is taking a login approach rather than just showing all the information and hoping that someone doesn't fall over from overload so it's a much more scalable thing so what I found is a good rule of thumb for every single dashboard no more than five graphs and no more than five lines on each of those graphs preferably only one or two you can get away with more snacks graphs and tables with like 20 30 stats on the side or handy as well but this will generally provide sports that are pre easy to understand if you find yourself going over dash you need to either trim or break things out and getting then a little more into matt quantiles cannot be aggregated it is statistically impossible there's lots of these fair few systems and prometheus is among them and which offer client-side quantiles this is where the instrumentation library inside your process calculates the quantile we're all going to discuss how that's done which is also kind of interesting but let's say this is correct and hazard rights properties do you have these this quan child-safe 95th percentile latency across all these servers in your monitoring system what mathematical operations can you actually perform on these that are meaningful the answer is there are none not possible you can take a minute max and kind of get bounds but that's vanish you can take an average because that's wrong because you don't know I give one service particularly so then that's where your ninety percent are lighting is going to be you can take an average you can't wait them doesn't work if you actually need quantiles the way to do it and the Prometheus histogram does this as this thing from the summary create buckets like 10 to 20 boxes is normally enough and just count how many queries are let's say lower than 10 milliseconds stored in 20 milliseconds Snowden 40 milliseconds and so on take those up those are all counters those can be aggregated those who me aggravated cross machines across services across data centers or globally and then based on dash you've now got your histogram and calculator Quan Chau that's how that works so in Prometheus we done using history of control and ratio something to be aware of those other monitoring systems there's very few to get this right another one is well quantiles and percentiles it can be a bit misleading so a federal question I have a simple service with two back ends now my name is fear Center latency on the front end has gone up because one of the back ends has gotten a bit slow if I look the 90 percentile agency on the back end what will I see there any guesses no guesses which is actually the correct answer because the answer is it depends it entirely depends on how the query requests are correlated between these two and which ones are causing it to go slow 95th percentile means you are throwing away ninety-five percent of the data and if you're doing that are two different levels it's there's no guarantee it's the same ninety-five percent based on queries so that means that it's really hard to figure out what's going on and in an emergency trying to reverse engineer this and figure out these correlations good luck not going to happen the good news is averages don't of this problem so if you have a search Ainge that there has been a latency increase via percentiles or walk nosh averages are always going to show that up so your top level if you see 50 millisecond increase in latency due to a back end if you look that back end you will see a 50 milliseconds increase may be 49 1951 but you will see it so keep that in mind averages are very easy to reason about they don't have all this complex math in the way and as you get more and more complicated systems you know you kind of want to keep things as simple as possible now maybe once you have determined which services to blame you can look again at your percentiles and see what happens because knowing that oh no it's only the slope Erie's or the bass queries word affected is kind of useful and another one Dan is you need to consider costs and benefits it will be awesome to have one second monitoring of anything or even 100 millisecond monitoring of everything but is that really retro resource cost like when we're down getting onto one second monitoring we need to worry about things about how much CPU using on the client like I would say let's say if you have one second monitoring vs 10 second monitoring is that really 10 times better because it's going to cost you 10 * resources both humans and machine and I'd say no it's probably maybe ten twenty percent better similar between 10 seconds and 60 second model shrink maybe twenty thirty percent difference there so just think about what these trade-offs are how much money you have spent monitoring and what works well personally I find that 60 milliseconds sorry 60 seconds I'll 60 milliseconds that will be a very expensive monitoring system 60 seconds is a nice good balance in the nice starting point for I'm sure where to start that's normally a good batch there is stuff for one second granularity where you doing is so there's this thing called microburst and that's where a lot of network happens in like 10 millisecond 200 milliseconds you're never going to spot those from 60 seconds Marjorie you can spot a fair amount with one second and but that's about the only time you really start to need one second monitoring or really read oscillation for those you actually want is highly aggregated start screaming for your clients like just total Eurasian not broken out by anything and keep those at 1 seconds and nose are handy also really really useful for load tests but that's the only really want maybe at most 10 metrics coming from every single process at one second monitoring just from a trade-off standpoint now of course you can do your own match make your own trade-offs this is what I've personally found to be a good approach and once again going back to math there is a thing called the new quest Shannon's sampling theorem by the quest and Claude Shannon who was it leave as 100 last week he invited information theory he also then took care of most of the field in the same paper which is kind of awesome and what it says is that if you're trying to resolve a pattern a wave and that wavelength is take 10 seconds then you need to sample at least every five seconds to actually figure out its shape this means if you're scraping once diminish you can only spot patterns that are at least two minutes long so this can cause all sorts of weird artifacts and caused confusion and so on just be aware of it like when you actually see this very often if you're in graph the founder of whatnot and hit refresh and it's not like one second later like it's still working off the same 60 second data but all the shapes change and that's because of the sampling issue in the quest Shannon you just don't have high enough resolution to spot of what's actually going on now what option here is you increase your resolution because like weird stuff does happen and but you never know how accurate we need to go how small resolution so at this point if you do think you have are having one of these problems like a microburst or so on or fluctuations this is a good time to use logs and because they will give you the exact times that everything's happening and they have arbitrary resolution but it's they should have arbitrary resolution I'm not sure I've ever seen an issue that's sub-millisecond add that probably happen but they should be good enough for dash because there are limits what you can do a metric space systems like primitives and the other one and so coordination is localization and so we're good for time yeah okay grant and because I skipped this otherwise so I have a rule of my head now some of you might have heard this before and and you'll give me a sequence of numbers and i'll tell you if it conforms to rule right so i'm going to say that 246 conforms to the rule so let's just take the four people in frontier guess you can tell me sequences of numbers and now say whether they can form or not don't be shy hmm any one sequence the number 222 does not conform one two tree conforms 6h 10 conforms 1 10 100 conforms 20 40 60 conforms 642 does not conform so the interesting thing so the syndicate guess at the Rope Martin monotonically increasing yeah basically any non decreasing sequence and so you'll notice a lot of the questions that were asked there or they all got a true answer in the conforms because humans are really really good at spotting patterns particularly patterns that aren't there we aren't good at spotting actually a pattern lightened be there like it's not unusual when you ask this question and to be honest ask me to a large group get really biases in your favor real couple rules like they increase by 2 or increase by tree or who said it has to be three numbers right these all need to be told about so you need to allow for the fact that humans are really good recognizing patterns that aren't there so anytime you see a pattern or a correlation think about well if this is true that also strikes to see X and is X happening so if you were looking at a graph and you see like Otis is spiking in this is spiking as well that's the correlation but it's a causation maybe those are just two cron jobs running on the same schedule and it's just pure chance that are happy growing at the same time maybe if you look back for the last 10 runs they've never coordinated before so it is pure chance and and the other ones we all heard of that is actually fairly common is they are actually raised some causation there because they're too batch jobs or something and one of them is slightly increasing contention which is just enough to push the other one over the edge so you're actually having a load issue but look out for this sort of things always look back in time to see zoom out a bit see what happened is this a regular event or is reaction correlation there because if you look back and see actually ninety percent at a time these two do correlation that's a pretty good sign another one is that previous is a metric space system right we don't do logs and and lots of systems are things out there those advantages and disadvantages to each approach their complementary you should really have because the thing is logs have information about every single event but this means you're kind of limited by bandwidth how many fields you can have analog to maybe 50 100 before you just run out advantage but you can have any values you want in there any cardinality no problem it's a batch processing system it's actually handled ash metrics on the other hand there aggregating across time to give you a counter and so you can have many many metrics even of 10,000 hundred thousand metrics no problem but you really need to limit that cardinality for the same reasons so they kind of focus on different things metrics going to give you a high-level overview of what's going on and you be able to figure out that oh yeah it's the guest requests to the / foo endpoint that order problem and then look at logs and say oh yes and here are the customers are ticularly not problem ask them to stop or fix it so metrics are good for zooming in or what the problem is and logs for her garage okay we don't know what the problem is which queries in a problem aren't that sort of thing so they work together in the same way metrics and then profiling but using our entirely metrics or an entirely logs is not really a good approach unless you have to choose between one cos got no other choices just limited time but try to have both each of their strengths each of the weaknesses and they complement each other nicely the final tip is a bit more organizational and there are going to be lots of alerts that just don't justify paging someone like hey where it's seventy percent disk rather than ninety percent or we're going to fill up this can you know a week rather than four hours and what normally happens to alerts like this is to send to emails routine mailing list and then everyone was sparking the team which is hopefully everyone will filter them away or delete them never to be seen again until the outage happens because there's always an outage and it turns out one of those emails that everyone was ignoring and you know it hurled the problem and if someone had looked at it we didn't all been good and of course that's one of the 10,000 emails always received that day so you know well actually be honest to worst I ever saw person he was about house at them today and so what's happened here he's got a whole pot alerts or going to somebody will look at them honest and they're just going into the void ignored because the signal-to-noise ratio is atrocious because there's no incentive to not have these alerts right it easy to say acer emergency you want this alert how you handle this and in the same way that pages you make sure to someone gets them because you got someone call these you also want some solution that is credible that someone is actually following up on them so there's two approaches i have seen two generally work one of them is you send these alerts to a thickening system and make sure they're taking been taken care of in hard way you normally takes handle tickets if most of those tickets are getting resolved which it's not happening anymore then it's time to adjust your thresholds in the upwards direction at the other approach I've seen work this is less common is once a day have an email or 12 hours thing how your shifts work and that says here are all the current firing alerts and the neon call just process the demo once a day I've also seen that work I have only ever seen one team who ever managed to do straight email for alerting then that team speaking to sober two teams who are doing it straight from alerting but that's the only time I've ever seen it happen so it doesn't work once you get beyond like two or three alerts but these approaches do work the final word and just because monitoring especially we're using summer prometheus which allows you to instrument everything or any other system is that you can over complicate things and get in over your head just as with any other technology keep things simple right if you find yourself creating big long complex systems or having to spend a lot of time just running your monitoring system rather than run during your actual production of a system that your customers are paying for then it's time to simplify monitoring is a means to an end it is not the end of your cut end goal of your company so keep that in mind because you should show me you be spending you know maybe ten percent five percent your time on monitoring itself you're spending more than that it's time to simplify it's probably a simpler solution that's good enough and just say as well my company we help with previous and monitoring generally and if you want to contact me there's details here as well as medias website got a demo and any questions and we'll just do a quick demo so this is down without robust perception on here which is live oops it do is it here's some actual monitoring output from the known exporters so here's machine stats yeah we've got lost to them so here for example you see how much time is the expand to reading discs on dev vda there's only one SSD in this machine so it doesn't look very interesting I see that's a nice big number here is the expression browser so this is handy for ad hoc graphs but you know it's not the best you I ever and it's also grow fauna which is much prettier in point-and-click so here's the prometheus itself and in the top left there you can actually see the spike rose accessing the data a few minutes ago and just to show you an idea of just how powerful or meet his query languages and this is Conway's life implemented in committees query language and then rendered in consoles templates for those of you who didn't study computer science and Conway's life is known to be chirring completion and a much bigger squid than this which means that if it is possible to compute something and with any programming language equally computers with Conway's life and because I've implemented Conway's life and prometheus that means it's possible to do any computation that is computable inside Prometheus please don't do this in production okay sorry so do we have any questions yeah okay so the question is if I'm sending events stats t do i need a sense mean it's nasty compatible and so we have something called the stats d exporter which can take in that in stats deform obviously try to pick out labels out of it and then / media subscribe dash and unless you're actually using PHP or something else CGI based it's better to direct the instrumental because the problem with the status the approach is ascending a UDP packet for every event and when the previous philosophy a a single user request or whatever other requests are coming in should probably be hating about 100 metrics just as it passes through all the libraries that's going through the system you generate start generating 100 UDP packets for incoming requests that's going to take out your network so the way Prometheus works and is that it just has counters in memory and then you go come scrape those once a minute or once a second or once every 10 seconds or so on so we're stancy is useful is squared there is no multitrader binary sitting there that you can maintain memory stage in such as PHP any other questions okay so the question is how does the service discovery mop and work typically with the pool model so let's take easy to service discovery assume most people here are familiar with ec2 and so you go to ec2 and once every I think it's one minute or five minutes we go and say hey give me a list of all the instances and we get like all the tags which includes its name we get the security groups and we get the BBC ID all that information and then those are all given to the user in relating and they can access those all as basically hidden labels the user can then use relabeling to copy those into the labels that lack should be used and they can also filter by missing like hey only labels that are if the description for example matches this regex keep it otherwise it sausage and the other ones worked the same so if you're using console exact same principle and if it'll go and say hey give me a list of all your services we come back then you start filtering on them and coughing you over the labels is relevant to you cuz like maybe your team has decided a tag called envies product canary and you want that in a label called product an area or maybe you put it in the description field or maybe a space and security group or maybe a serve ace in vp cid whatever you have you can manage to convert that into something that's meaningful to you the problem we have is that because this is so utterly non-standard our only option is to offer an interface that this generic because otherwise we'd end up with well soundcloud default sound Claire's team a SoundCloud tob tells you see 1017 d a sound healthy and that's the center and then we have every other company with their own variants for all these options so that would obviously not work you'd have a compliment or an explosion of flags we try to do this and so instead it's just like yep here's a key here's your set of labels and here's some operators to move these around with red x's next question so that one over there at all oh yes session area okay so Arturo durations for things like machines and databases and docker so for machines we have the node exporter nothing to do with nodejs and that will basically dumped large proportions of / assistant / proc for docker your main option is C advisor which directly exports medias metrics and from databases like Cassandra you use the jmx exporter my sequel we've got a great exporter like Percona guys who are like to my secret consultants they've done a lot of work on it in fact they've also released your own monitoring solution on dash there is a post res exporter I people have been talking about the Oracle one I think there's a Russian company is built when internally but not contribute to the back yes but they do plan on and there's a Redis exporter there's a memcache the exporter just probably more but these things are generally fairly easy to write as long as the system exports some form of interface we can get the metrics out of we can converse it because generally the way to view exporters is we wish everything was directly instrumented and but in reality it isn't so it's Porter's like just little proxies you send in the request it goes like talks to my sequel gets the data lunges ish Chuck's back out again next question there's another one okay I'm not seeing any more questions so thank you very much 