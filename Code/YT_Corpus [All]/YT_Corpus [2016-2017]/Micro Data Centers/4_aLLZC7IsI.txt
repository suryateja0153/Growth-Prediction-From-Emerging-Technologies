 each year Microsoft Research helps hundreds of influential speakers from around the world including leading scientists renowned experts in technology book authors and leading academics and makes videos of these lectures freely available you all right good morning good morning so you've been hearing a lot about wireless and you will continue to hear a lot about wireless in the next several days and I thought I will do something slightly different they talked a little bit about mobile which Islam says is completely related to wireless now everything I say is a fairly fundamental and very applicable to any wireless systems that you might decide to build or do you might decide to go while most of the talks that I as I've heard have focused when you think about wireless most people tend to focus on the physical layer the mac layer maybe the architecture right I view wireless a slightly differently are you being viewed from a systems perspective so to me it's an end to means and yes I'm going to optimize the layers below and make sure I extract the most use out of them but to me it's about connectivity so yesterday when I gave a talk you can sort of think about I I didn't I did talk about Mac protocols a little bit right I did talk about discovery and things like that but really it was a larger access view it was about access and how do you do this today I'm going to talk about mobile computing which is that once you have the access how to use and what the issues are I'm going to reduce the new concept called cloudlets you've all heard of the cloud is there anybody who's not heard of the cloud if you raise your hands please leave okay so let's start let's get it going a little bit so first we fly desist you know again one of those obligatory slides so I got this slide from Satya if you don't know him he's a professor at Carnegie Mellon he's a pretty well-known guy he's done a lot of work I'm about to bring for a long time and he was presenting this and I thought it was sort of a neat graph so you heard about Moore's law you hear about you know how you can't these days you actually hear the process of speech you can't really go up so you're gonna have multitude of processor specialized hardware but the interesting thing about this graph is that all through the centuries the one thing that is not really changed in will ever change is how human attention is it's and so what mobility has done for us is it's actually put a lot more tax on the computers than it ever had in the past right so when you have a phone and you're looking at it most likely you're actually doing other things while looking at the phone you're walking you're driving you're doing other things as primary activities and then this is happening so you don't have the patience to do the research for example if you do search on a desktop you can wait and if you get lots of links with different things you can continue pressing on links on mobile you actually need to find when you do a search you really want an answer right away right and so the point is that the because the human attention is of such a limited resource and you want answers it very very quick there's a lot of stuff that we have done in the software over the many many decades that can that is directly applicable to mobile space now the so some examples I list here machine learning activity conference context awareness and as you sort of look at literature here you start to see that all these things are applicable and they are got some really strong good algorithms do great things and so you want to apply them but the problem is that this device even though these smartphones these days are pretty powerful they're not powerful enough to do a lot of this stuff they just can't either they are battery limited or they're bandwidth limited because it can't get that much data there or they sort of like just a processor limited and you're not gonna be able to do everything very fast okay so that's the whole whole point now so that just says that okay so we need help the device needs help so one thing is well cloud has so much resources is garlic you know thousands tens of thousands of servers and you can so what if you offload some of the computation to the cloud then you can do a lot more on the device make sense that's great but between the cloud and the device there is the Internet and there is the wireless access channel right and the thing is if you don't manage that well then no matter how fast you want to do it's not going to still solve your problem if you augmented reality if you're looking at the phone he wants something to show up he wanted to do it in a cloud if it takes forever for the data to go back and forth it cost a lot of money then it's not going to happen all right so what we're going to do is I will talk a little bit about competition in the cloud but I will really focus the stock on latency it's fairly simple fundamental concept in networking you know we're gonna see how latency impacts and then how potentially we can improve latency all right so before I do let me just start with this few quotes these are pretty well known personality Maresa Myers and now the CEO of Yahoo so you know and this is the quote when she was at Google similarly it is somebody from Google search this is a guy from Microsoft and as you read this you sort of see latency actually translates to money and in fact the reason I have this code is not really for you I actually presented this talk to several executives and most of these business execs are much more about the money right so I'm gonna spend you asking me to spend so much money to improve latency but show me the money and it turns out that there is money here there's a tremendous amount of money and some of these guys have and we have quantified it even more to say that if you you know if you don't do things fast enough you're gonna lose revenue because potentially you can't even offer those services and secondly if some of your competitor is offering it then you're going to be able to live in I go to that place all right so so this is not just an abstract sort of a technical concept and good thing good concept now going back to the competition part these are some of the applications you can are familiar with you know you talk to Siri Cortana is now in Microsoft's you augmented reality I talked about some interactive gaming this is for any kind of internet of things sensing that sort of thing in each of these instances you need lot more computing horsepower than you have and so even though these applications exist today they are not even close to what you can do if you had tremendous amount of computation power now here's an example of a very simple thing that a guy in my group Maathai method did so what he's doing is he's walking around he's got a variable computer a variable camera and he's figuring out you can't see this I know I tried to put the PowerPoint didn't come out this is oh yeah you can see this red dot so you see the red dot and that is where he's moving okay so instead of doing location determination like the way I talked about our some of the other stuff that we will talk about with our SSI signal strengths or you know time of flights or things like that this is actually done based on vision so essentially the camera is looking at what's happening and based on that it's telling you where in the building you are now it's it's a toy application not done for any product use but it's sort of done to show the power of computation and the ability of a computer to be able to do that now for this you needed a desktop class machine you know heavy duties with lots of CPU lots of memory you know bag that close to 100% CPU trying to do the analysis in real time now if you were to build a wearable device okay you hear so much about Google glass or anybody else was to build a vision analytic thing you can imagine we don't check to actually have that horsepower now while I'm at this I'll also show play this graph this one this shows what it what is the state of art and vision so envision the interesting thing is really what are you doing so in this case you know sort of like you think the system is recognizing it's a thinking it's a wallet it's a tea bag it's a so a lot of this vision algorithm guys are looking at things like you have put a open pick up this glass of bottle then they want to know that it's a glass of water because once they have that annalen analysis they can actually say something to it that tool requires tremendous amount of computation okay now having said all this introductory stuff let's now start with some more meaty stuff so so you know let's start with ground truth we started with a certain ground truth lat yesterday so we start with now three so let's just say that there as I said if you have better quality path in the internet from your device to your cloud you're gonna have better quality services okay high latency is going to degrade the services to some point that is actually useless so if you were doing something like location determination you know not as a toy application but as a product application and and the system could not react fast enough because it couldn't get the data to the computers it's useless to you right so latency matters and then prove poor performance affects revenues and things all right now let's think of it very simply I drew this graph will do this picture a little differently but let's see what are the components of latency there's a client there's a network Peter dear sandwich requests count the car client does some processing sends it over the network the data center a cloud does some processing since the results are back and then the client gets it right so it's pretty simple so now well you can optimize for companies like Microsoft for example and Google to some extent optimize the heck out of what is happening in the client optimize the heck out of what is happening in the cloud right but they don't they don't know really what to do because this is much much more complicated Beast so in fact folks in my group have done quite a lot of groundbreaking work that if you want to get into this area you could read papers about about how you optimize networking in the data center so that network says not the bottleneck at all inside the data center which is that the older servers can run online rain late line rate and send the data so we haven't done much of the internet now I did this experiments of about a year and a half ago maybe a little more but I haven't repeated it but I suspect this is pretty similar so all I did was I took in my smartphone and I did a trace route to some Google site this is the result of that going over Wi-Fi I did the same thing over 3G and this is the result of that it's going to the same IP address and I guarantee you that if I do this experiment here this list will either just go on and also get stopped here someplace it's just so slow now the interesting thing is for big large companies we have no control over this right same application getting about 11 hops here is 25 hops lots of distance that's that's the problem now to really understand what's going on you must understand the internet and most of you know it but just as a basic so you have the data centers and the way the internet works is you've got lots of these ISPs ok I mentioned AT&T Sprint Comcast put in their ear to put in their whatever local species you have and then these ISPs may deal with one another there's a protocol actually called BGP which allows you to to implement those policies and deals so for example when packets go from here or go back here depending on what D Comcast has with APN people either send it here or else it in Sprint or it'll dynamically choose depending on you know what time of day it is and who's had a congested link and things like that and that's why because it has no sort of a clear path that's why you see different hop counts and different rates different results at risk and it's very complicated and then there's a company here there's a thing called Pier one if you go and check their website this is an old number but you can see these are s numbers think of them as networks this is the number of networks they had discovered and this is actually a conservative estimate all right and this is the number of peering connections between the various networks so now if you have a packet that is flowing from your client to your to your data center and it's going through so many peers you're going to have a latency problem right okay now add to that so I haven't said anything about mobile rate so far all I've talked about is latency cloud devices but now you guys work in mobile or you work in Wireless let's talk about Wireless let's talk about cellular wireless for now cellular wireless stack looks something like this now of course I mean changes for different things is just a sort of representative architecture and you can imagine so I have taken the internet and just put a bubble here that whole complex Internet I put a little bubble and all this is what is happening in this cell network so this itself adds tremendous amount of complications and latency in itself in fact there have been many studies here's a sample of some studies that were done well Susan of attempt few years ago this came out last year talks about what is the latency stew some of these things and so for example a TD has 350 milliseconds round-trip latency okay about average and then t-mobile is about 450 milliseconds that's a long time it's a huge time as you will see at the end of this talk now people say LT is great at tea trees but even LTE has immediately didn't see what 70 million comes with a lot of jitter in there so that itself is also pretty large okay so this is this is actually done by researchers and you know if you get into this field you can do it you can perform you know some experiments because these things keep changing but nevertheless they are pretty large okay so now if you know about TCP then you know that this lounge this RTP is actually where more than three fitness when you say the audit is 350 milliseconds the actual latency is lot lot bigger and the ways because that reason is the way we design our protocols so if you take TLS or TCP you know it's a very chatty protocol it has all these handshaking going on and so by the time it does all its stuff and starts to move the traffic you if you plot here this kind of shows for example in approximate 50 millisecond difference in RTP translates to 530 millisecond real latency and moving the objects okay and this graph shows tends to show for example there's an attempt of what the RTP is versus what the actual transfer time is based on TCP so somebody says 50 millisecond latency you can think about 50 milliseconds I mean what the hell that's not too much but it's actually a lot it's a lot because as you transfer objects okay so so you have to bring that down now if you don't believe any of what I said there's actually we did this thing I haven't tried it here you can download if you have Windows Phone you can download or any Windows device you can download this free app run it and it'll tell you what your networks are giving you so when you hear anything in popular press don't believe any of that that's all SPR this will give you hard data what is going on in your networks and then you start to see how bad these are okay so this is what you know about that so now now that we've established that hopefully establish latency is important and hopefully status latency is really bad and hopefully establish that in mobile devices or wireless systems wireless networks it's even worse then we have to think okay what can we do given this really complex set of fares here all right so let's again start with the basic so the idea then is when the packet leaves the client they want to get to your computing device right away want to get to the cloud right away right any time that is spent in the network is wasted time and we want to avoid that so you would think well let's bring the cloud closer to the client makes obvious sense like you know if it's gonna go through all this stuff not not worry about it so then you say well let's build lots of data centers around the world and place them in strategic locations that's exactly what what everybody is doing so is this little picture thing about bees the mega data set what I call em is mega data centers or mass clouds they have close to 200 thousand servers in several buildings that are serving the world and so lots of companies Amazon Google Microsoft are spending billions of dollars building these things around the world right and then they're placing them in different places and lots of countries want that this thing should be placed there there are all these other issues which like if the data is going somewhere else you know all the people from my countries data is going somewhere else that's not a good thing so you should place here so companies have to deal with all that stuff because effectively it's the economy economics of the thing you have to spend a lot of money that's called capex to to build these and then you have to spill a lot of topics which is the operating cost to manage these data centers across the world and that costs a lot of money too because you know machines get old you got to replace the machines you gotta get people to service them enough myself so that's why this is what you're in the middle of is a transition from the old world that we had which was of what shrink-wrap software and so and and desktop devices or computers under your desk to a world which is all about cloud and all about going in the center of the universe and as a cloud and we are sort of in the middle of that transition right now and so it's actually quite exciting to him but that opens up tremendous number of technical problems that people are solving that's where all the research is happening if you could all the conferences and think that's what is happening now so the question is is this enough right is this enough yes no why like what yeah right right right so the question is select your right so you know is this enough and the answer is no and there and then the question is what do you do next you build more of these right but really you can't really build more of these because they're really expensive they are very very expensive it cost a lot of money the money has to come in from somewhere for it to be spent right and as engineers or you know if people who are either going to go into research or academia or go into industry this is the real problem that you don't have unlimited amount of money so the next level a question which you may be thinking of is likes forget this mega data centers let's build very small diversities let's call them micro disease and build thousands of these and build them all over the Internet ok so this is my sort of an attempt at trying to draw a picture try to visualize all this stuff but imagine instead of this bigger things we build thousands of these things which I call cloudlets or micro disease and then what we do is we try to work very hard to create from there cloudlet - this is mega clouds we create some tunnels there the company says can sort of building so many so many mega datacenter is spending money there I'm going to spend money on paying these ISPs to give me a very very clean network from my cloud led to my cloud or alternatively if you don't do it I'm just going to invest money and put fiber there and move it that way right so then that way you can start to build now these cloudlets can be anything from one server to 40 servers in fact if you think about it very intellectually your research cloud would give me anything a cloud can be my life excuse me my laptop here right it could be a cloudless for my phone that is in my pocket a cloud it could be something in the access point but anyway that's what you do so I say for 240 when you get imagine I went to 40 or more cloudlets in this this is actually a product that we we have Micro has this is a server rack which is about in this particular case has 20 servers that on the rack that we sent to an ISP he plugs it in and then Polk they have a cloud like Ulysses so cloudlet is defined as a resource rich computing infrastructure with high-speed Internet connectivity to the cloud and I say resource rich I mean has a lot of computing power in it and then the mobile devices uses infrastructure to augments its capability to be to enable the applications that we didn't think it was possible in the past because now you're gonna try to cut down on latency you're gonna provide computing very close to it and they're gonna offload stuff in there all right so what can a cloud lit do one thing that they actually do already in some sense are the CBN's you've heard of the CD in networks right so really what they do is do content caching so in CDN for example in YouTube you if you want to download something there is no reason to go to the cloud if a cloud or in this download it has all the popular content then you can just pass it from the cloud let's so if you feel that the performance is pretty good because it comes right away so this is the kind of stuff that people are already doing the other thing that so the other thing that people understand is to do a split TCP connection so in disappea if you know understand TCP you have this round-trip thing and they have the window size and things so what happens is when you send the packets a package get lost and the answer comes back then the TCP will adjust itself but if the path is very long it'll take forever right it'll there forever as in there you know if the performance will be very bad but most of the losses happen in the last hop so it happens in Wireless part of the system for example so if you have a connection that splits right there and then you have a more persistent connection from that cloud led back to the their data centers then any losses that happen that are localized you can actually fix very very quickly either in the link layer even with the higher layers but they don't affect the round-trip latency right because the backend is very helpful right so the end that question actually comes up all the time and it's very frustrating to many researchers like why the heck are you doing the thing is I think a lot of it has to do it's a sort of the simple answer is about legacy there are so many applications out there that do so much the other is people have attempted to come up with to decouple for example the congestion control part of the TCP protocol with the actual semantics and they've tried to push that in but the adoption is very slow because people are I mean unless it gets you a very painful point though Changez don't tend to happen as a as easily right it is simpler in fact in fact when I say split TCP the connection from the cloud led to the cloud doesn't have to be TCP okay so that can be anything you want in fact they're proprietary protocols that people do because both ends you control if you control both ends you do whatever you want it's only between the client and the edge that you can because who knows whose client you are using right you could use any operating system with anything and because if sanitized thing you have to have TCP yes yeah yeah it may potentially improve it but one solve it the even like said you're still so long as you're still held back with the the actual protocol the way it works and adhering to all the standardization you're not going to actually solve it you know yeah there are people have written unlike hundreds of papers on trying to improve TCP performance so wouldn't say it solves it it just exactly but you know that's not the main issue I don't want you to get hung up on TCP there are other other things going on but well what I did want to say to you is that this thing people understand so for example by doing a split TCP or it's also called SSL domination so if you're doing HTTPS then we found that just from a little bit of experiments that you could shave off about 30 milliseconds round trip which is pretty substantial because 30 milliseconds then amounts to taking that 30 milliseconds and using it for computation and that then gives you relevancy which is pretty large to see the other thing you can do is our read routing and I'll say a few words about it and then if you can go to the web and search for CD ends you will see hundreds of companies starting with alchemy line like cloud front level 3 edge cast they provide these CDN services content distribution services which do caching so there's this is a very very mature market people already do that now so what I want is make a distinction was between cloudlets that are that are classic CDN so they do some more caching but really what they are is are about competition these guys talk about new variant computation they don't actually do any application of a computational but but cloudlets are do okay so in terms of just just to sort of complete what I was saying here about overlay network diversity so if you have multiple cloudlets and these cloudlets are connected to one another then they can actually collaborate with each other to figure out what the best path is so a simple example of that is there's these cloudlets and they you're trying to get to let's say you're trying to get from site one to site three then you know you can kind of go this way or you can kind of go this way or you can go different ways and so if different outlets are keeping they're keeping the quality of the path from there to there's particular site then they can collaborate with each other and dudes make routing decisions on top of their routing decisions that the ISPs have to do some improvements on it okay but let's meet let's talk more about mobility in Wireless because that's really what yeah you guys are most interested in so let's talk about very something very very fundamental to to wireless and mobility and that's battery life right I don't know if there's been any talk here if anybody talking about energy or battery life but huge topic massive topic very difficult in tons of work you can give a full lecture a full day in the worth of lecture on that topic but I'm assuming you all understand the issues maybe not a precise thing where we are so in battery so what happens is I want to tell you a little bit about fast dormancy so the way things work in in civil networks is that you are so this is number of Verizon for 2011 this is 1.6 watts here this is zero point zero watts so when you have try you you you want to transmit data you have to transfer you transmit it then the system allows you to go down to a slightly lower power of one watt then you have to stick around for a certain amount of time then you can go to the low power now what I'm showing you this is because really if you're not transmitting you really should not be using any energy right ideally you should transmit turn the system off when you want to transmit again or receivers whatever you wake it up to the thing and it turn it off that's how you save battery really that there's no other magic to it it's just sleep as much as you possibly can okay so so but to explain to you how cloud let's help improve battery life I have to give you some history here okay so some history is that several years ago four years ago actually I guess there was a lot of bad press in in the u.s. about how the iPhone network how the iPhone brought down AT&T cellular network okay so this is just cut out the DOS this is September 2 2009 December 12 2009 customers angular iPhone old or AT&T 80 nation blame so you know a TV network was really lived up do you does anybody know about this is no you you do do you remember why this was do you know what that's very good yeah so she's actually right let me repeat what she said and very good so I told you how you know the sort of the fast dormancy work so when I phoned came out first and was very popular near see these guys were thinking I gotta say battery right from my phone so what they would do is this is again the Verizon number they would transmit the data most of the web traffic everything you do with you if I keep doing this because I have my phone although I don't know where it is so when I do this that's what I'm trying to get my phone anyway so when you look at the phone and you certainly do a web transfer it's a very bursty interaction right it sends comes in scum and so what they did is they said well I'll send the data as soon as the data is done I'll bring the phone down to the lowest power level and let's do it this way that's how this it made sense right because you don't want to waste energy is just hanging around well what happened is the network the Cerner networks are built inherently built as a circuit switched networks not packet switch networks long time ago when the telephone network was brought it was a circuit switched network and it actually worked beautifully so what a circus which is network is you never you make a call you dial the some number a hole there's a lot of signaling that happens to set up resources along the path as the signal set from from the client to the destination right and then when you are done it brings down its another signal is sent to bring down and clean up all the resources so the next wall can come well so the wireless network inherited that same thing so what would happen is you would connect a signal would be sent across the entire network all the resources would be set up and as soon as you see that did the transfer you brought it down and so a signal would be sent to bring all the resources down ok it's not imagine how much data transfer people do when they do where they're just doing all the time they bring some packets and look at it they bring it from back again so now all these spoons is millions of phones started to send signals up signals down signals up sequence down signals and the system just could not handle it it just broke basically it stopped functioning because you know it got into all these states that they had never been tested for and it just broke and then you start to hear all our other stuff so these mobile operators realize this right away and said this is this is really problematic and so what they said is okay well you can't really bring the system down right what do you have to do is I mean do radio you have to keep it up I'll let you go to this one point this one wat thing and then you have to stay with me for a certain amount of time before you bring it up now this does two things one it sort of fixes the hysteresis and it allows you to sort of wait or the phone does not send a signal for a while and because it doesn't send a signal for a while and the the there is not as much or load and then if your for example if they choose this number T right then as more transfers happen then you've basically eliminated these other down signals that you needed to send and so there's no set up front going on okay so the T that they chose for LTE was approximately 10 seconds so once you start a transfer you gotta be up for 10 seconds after the transfer is over right that's what they did now that's a lot but it's I mean in terms of saving losing energy 10 seconds you just waked up and at the highest power level or you know 1 watt and not doing anything so this is the now this is not a good thing okay this is not a good thing so the patch will send in fact all your phones do this pretty much really what you want is this behavior you do want the phones to shut down right but you can't sort of fix those networks that's a lot of cost so then cloudlets can help because what the cloudlets can do is if a cloudless is at the edge it can be the proxy for that Network and if it's a proxy what it does is the phone actually can go down right I don't know this picture shows the phone phone transfers goes down but the cloudless suppresses that signal and keeps the network thinking that it's up and then after 10 seconds are over it then sends the signal out and so the network gets what it wants and the phone gets what it wants and you are able to save some energy so if you do this sort of thing then if you look at it's a very simple equation energy transfer is 1.6 watts plus the speed up the speed up comes from the fact that you actually have a cloud let's say you know you can send the data quickly and then go to sleep and you're not waiting for a round-trip so that's your speed up let plus 1 watch times 9 because that's the time you would have had to wake up for four that means about for every transactions you save about ten point six joules of energy now if you take a regular phone like a samsung phone and you apply this thing in the new sort of say well I'm going to take 20 Network transfers in so that could be notifications and emails it's a trying one hour which is not too much actually if I think about the way I were using a phone I'm using it all the time when I'm not doing I'm looking at it and I think you know that surface 20 is not too much you can save 26% of the battery and if you work in battery and if you have ever taken any core so if you understand this is a huge number people have been struggling for a student to try to improve battery life and they're not able to do it but this this sort of saving is huge now if you sort of go to the liver and you grab draw this graph view this is the network on the horizontal axis the number of network transfers per hour this is the battery saved you start to see this what happens if you didn't do that system that exists today and you can save up to 70 depending on how many transaction you can save 75% of your battery life time you can improve this is huge and this is true for all mobile devices okay this before so this is a big big saving this is so these are the things you can do and yeah this slide shows that battery power has been tremendously hard to solve so this is how many years and this is sort of how the battery powers into there is no magic bullet here in fact if you put too much energy into a battery and make it shrunk make it small small it's really making a bomb you know you don't want to make bombs right you want to make berries so in the time that the CPU has improved by a factor of 250 almost battery is improved by like 2x or 3x so it's really bad okay so that's one thing a cloud let you do another thing that a cloud can do is I'm going to beat up a little bit here is competition offloads I started the topic with that so the question about competition offload is that you want certain parts of the application to work on your phone and certain part of it to be work on the cloud so now the questions from a research or engineering perspective is what do you have float how much would you offer out what do you offload how to dynamically decide whether it's a good idea to offload because network can be really bad and if you're trying to offload is going up and down that's not a good idea I don't want to do it and then how do you do it without making the life of a programmer very hard if the programmer has to think all the time about oh I should offer you can oh not a win so this is just a code I guess I picked up doesn't any matter but disguise there is a huge game developer and really what what this code is saying is that I don't want to deal with any of these issues that you're telling me about right because I just want to be able to program my games and just focus on the game and so all this stuff has to happen in this in the system itself without the application writer have ever having to worry about it so there are several ways to to do cloud offloading and just pointing out a few we had the system system called Maui and I'll describe that to you and there's some papers on it to the UCS had a system called DISA Intel had a system called lone cloud and there was another one called Orleans and the way they saw work is for example the Microsoft system which I'll go into in just in a minute basically offload methods like think of them as procedures and things like that this one does threads there one does tasks green so there are different ways of looking at this problem and you can decide which one you like the best so in terms of the Microsoft System and the Maori which I was involved with I built so you build the the application the programmer builds the applications and then without thinking much but what it does what he does is when you're doing in dotnet there's a tags that you can actually can add some hints in the program so when you start a method you can say this method is removable what that means is that as you're writing a program you can sort of say this is a sort of a method that is self-contained I don't actually have to there is no much dependency this probably can be put can go somewhere else and so you just a get on top in a more table start begin right that's all you do now that doesn't mean that's telling the system that you it actually has to do the system is going to make that determination itself but we'll get a sense of what is what things it needs to look at more carefully okay so for example is you're writing some code you will write this you will just add this tag called removable and you can define these tags in visual you don't want free loving things like that so what the Maui runtime does is when your application is trying to run or it will actually look at these tags and it'll split the methods okay at runtime in terms of optimizing for some characteristics like it might be optimizing for performance or it might be optimizing for energy and like I said and okay so then now there is some subtlety here the subtlety has to do with deciding that somebody would ask the question that why does it have to happen in run time why don't we just sort of decide upfront and then do a static split and the answer is actually you can't do static splits because what if you have moved something on the cloud and that the network went down then what were you do in that case for example if you're buying something from Amazon and you down you the part of the code that goes and purchases or moves the credit card into Amazon is it's on the cloud and it goes and does its thing and network goes down what are you gonna do let me go to another purchase or not so the semantics start to do matter so you can't really do this in in statically you do it dynamically I explain to you how so really very quickly in the smartphone on the device you can have you can have a sort of a laser application here's the run time you can have which contains a client proxy a profile and a solver and on the cloud side you have the same thing you have a server proxy profiler and the solver and think of them as being talking to one another to some RPC calls or whatever so when when the application is run it gets intercepted by the damaja runtime and then the runtime will then decide whether to actually run that particular piece of code here or run it on the server on based on some basis I'll tell you and then it mean if it runs on the server it will synchronize the stable to the server in the client and essentially run it now the profiler let's sort of talk about the profiler really quickly so the profiler source has this call graph which is how all these methods are set up and then it has it also takes execution time which can be determined offline so these things are by the way all offline this is online the amount of memory amount of things it needs to transfer that's your state service there's CPU cycles how much device energy at that particular method will consume and what is the network latency at the particular time and what is the network type Wi-Fi 3G or whatever right based on that what it does and it so this is the call graph call graphs if you understand this is method a calling method B which might call method C which also might call method D so it's going to tag this call graph by something like for example it says takes four hundred or forty five millijoules to run this and it takes about 30 milliseconds and the state transfer from here will be from A to B is about 10 kilobytes so it will create this and then data for itself okay so once it does that then you have a decision engine which will then has to partition I said partition the thing so what it does is now it looks at this whole car graph instead thinks of it as a linear program now what is the optimization function the optimization function is that you want to maximize the energy that is saved so let's say optimizing for energy at this point I maximize the energy saved subtract by the cost of the offload because there is energy used in computing and there's energy use in communicating so if you want to send data out that's going to consume in an energy right so if you and if you send the data all the energy that is used for running that is attributed to the cloud and you're not using it so that's your energy saved the cost of offload is the energy that you use in offloading that data right that's so you you want to maximize this because you want to maximize saved energy in such a way that the execution time for the whole system plus the time of offload right takes to go and then come back that is below a certain threshold because you want to do all this I mean if it's gonna take forever then you don't want to do it if you if it takes less than certain times then you do it and so you the reason to show this equation is not to sort of give you a sense of anything but my point here is that a problem which is the kind of a software problem that traditionally one would think about Landen self pretty well to some good theory work that he can use to decide what the system must and that's exactly what what is being done here is being everything has been modeled in a particular way you get all the right numbers parameters you measure them and then you based on this you you make the decisions and runtime okay now let me show you results of the earth this stuff now of course you want details there are papers i can you can read the paper so you can understand the details and you might decide you can actually do a better job and that's perfectly reasonable in fact people who read this paper have come back to me and say hey can why don't you do this well you know that and that's perfectly reasonable ya know why yeah I mean yeah yeah it's not trivial to solve but they why is it enemy but this is not this is setup so I'm not actually sure why are you singing P Hart alright let me get back to you okay because I just realized I'm really way behind but I will definitely take this question with you just off my night because there was me go into a rat hole yeah a little bit very quickly so anyway look looking at the graph here what this shows is this shows for example the execution duration this shows this is a face recognition algorithm by the way right and what it's showing is that when I say smartphone only so everything is being done in the smartphone when you actually trying to recognize the face and this is done in the cloud and what you see is these legends that I here represent how much RTT you have to the cloud so you got 10 milliseconds you are looking at this if we got 25 milliseconds you're looking at this if we got 220 milliseconds you are looking at this so you can clearly see that you can do things much faster in the cloud right then you can do it in a smartphone even when the RTT is so high because the smartphone just doesn't have the horsepower to do it now just to prove to you that the system actually works this is a different graph this is about energy that was about execution speed same legend right and here what he sees when the RTT is large then it is actually better to do it say do it in the phone you will save energy here in fact our system does not allow it to go here because it was higher so we had to hard-code and just remove it and SUSE to generate this some graph and here it is showing that no stay in the smartphone side don't send it because the amount of time it was sent to communicate and wait for the radio not to be turned down and things like that it's just too much all right so here's not going to show you a thing that my intern did Cesar enough you know Dina qaddafi which is our student not the necessary hurry Balakrishna student and so this is let's see I hope it works now yeah so what is going on here is she's just taking a video for smarter people walking right now she's going to do first recognition the cloudlet with it which is only 5 milliseconds latency it's gonna go very fast or what it's doing is recognizing seven faces in real time it's recognizing the same now we're gonna move the cloudlet about eighty five milliseconds and help latency I'm gonna do the exact same state of art algorithm and then you seen still recognize it but now it recognizes only four faces right which means latency just because of latency you got higher person recognition and versus you got you know goes to fifty five sixty percent left recognize it matters okay now the interesting the reason I want to show you that is because it is not just about face recognition or any computation intensive algorithms it is about everything but let's let's decompress this or decode the face technician so in first technicians you do face detection right then you do that lies some sort of alignment or things they need to feature extraction from that and then you do feature matching or a pattern matching to figure out what features it corresponds to so what face we're looking at and then underneath is is the time for each of these things now this thing shows for this much of an image if we just do it in client-side this is about in how many seconds if you do it in the server size one thirty so it's clearly doing this in server is a good thing now this graph is the most interesting graph that have we generated here what this shows is how much time is required and then what is the accuracy of the system and what is the feature count so what this is doing is as you can see as you increase the number of features you extract from it your accuracy goes up of course after a while it's law of diminishing returns but you can see that when you extract something like fourteen thousand fourteen you have fourteen thousand features it's the accuracy is close to ninety ninety two percent versus let's say here right and but you also noticed that you need more time in the computation correct so that's what I've been sort of saying again and again which is that if you if you put this time in the network versus in the computation you're not going to be able to succeed so any company anything that is able to cut that network speed considerably will win because they will just put that time into the computation and get it now what is interesting is this is true for many things not just object recognition which is the next generation of applications even for things like speech recognition or search which is what we do today it's the same thing it's the same boxes really in search I don't know if you understand the way search works you when you do a search many main threads are spawn in like this large massive index that are sitting in the server and there's a think of it as baton recognition algorithm it's going and try to match against something and now for a certain time 200 milliseconds it cuts it off and says give me all the results you have takes the results renders it sent it back to the mobile device when you see all the results now when you say Oh Google results are better or micro results are better the relevancy algorithms there is not much difference between them or us there isn't because you know we all understand how to do things everybody's smart they're smart we're smarter you know I'll do that what happens is who's spending more time doing the brute-force search if you get more search there more time there you get better result and so people spend millions of dollars trying to actually fix that and that the same thing is true for speech recognition too so this thing will come keep coming again and again I just want to show you object definition because if somebody ever says to you whatever or this is for the future absent so you talk with somebody and saying hey you know this is not we have it pretty good now no you actually don't okay so now going back to the cloudlets the things that i shall talk about mentioned very clearly was early was that let's build a systems where we have a network that is much better from the cloud ledge to the cloud we can do only routing and this is a path diversity thing we have a picture on this - path diversity shows that if if a cloud net is connected to multiple ISPs so for example if there are three dcs is that - is piece you have six you know you can do absolute six different ways of getting to the cloud and you can sort of decide which is the best path you can do all this stuff so this is all good okay so let me say a few more words about offloading about about bandwidth okay now banner is very important in countries like India it's very important in all the but in fact it's very important everywhere depending on the pricing structure but but so what a seller never guys do is they try to offload as much as they possibly can to Wi-Fi and we talked a little bit about that yesterday as well now so so one technique is you offload the other technique is you compress aggressively Ram is actually done some work there is to sort of do compression so that you're not using as much as much bandwidth another idea is to do what is called what we call is procrastinate now the way this thing work is when you do an app for example a weather app when you run the app what it does it just assumes you're gonna bring a lot of these maps that you're going to look at so if we go to the next page of the app you will see a radar map of the place right or the next page you get another some other map so what it does it it aggressively in order to improve performance for you it'll bring all these maps now you might decide that I only want to go now gonna do the map you just look at the first weight it tells you what the weather you shut it down move forward but all that bandwidth has been wasted because and in you're paying for it right that's not good design so the idea is then that this MVC or this cloudlets which are very close to you now they're not 70 80 milliseconds to hit you there in the performance they will bring everything for you but they will not pass it to you but now when you sort of like if you go to the next place because there are only a few milliseconds away they've said to you and you will hopefully not see the difference so really so this graph for example shows you know we tested this against many applications and what we found is for example some application it doesn't matter the green is what it does in terms of how much bandwidth or how many bytes it brings down and the red is how much it uses the blue is how much is safe if you do what I just said which is don't bring it down till it's needed you procrastinate right and so in some applications you could see the Blues are pretty large is pretty large here pretty large in the weather app in some other like you said today where it'll content is a lot more dynamic and things like that we only bring after you've clicked a link you don't actually gain some as much things and to tell you how it actually feels like so show you real video here so this is how the world works today you got a weather channel you know you start the application so it's bringing in now he goes to the next page and it takes it takes time so this is what what we did in this particular demo is to sort of say we only bring the data when he goes to the next slide right I'm gonna run it this one more time so because I bring the thing you've been this first page now when he hits it he will bring me so there's a delay and now the map shows up you don't want that application Reuters don't wonder so with a cloudlet this is what happens so you have an app he goes to the next page oh sorry I don't know that I because my focus isn't something else I'll do it again he does the same thing he looks at the app he's and now he hits it and realized there you don't even see it you don't even see and his exact same thing has happened here by the way it still brought it but the cloud light is saving it okay now there are other things too but I need to wrap up quickly so another idea is sort of you if you want to run some applications in the cloud and you want to play them on the mobile device then what you do is you bring you if you're closer to the play in the cloud lid then you can actually do much for example if you if you had an app and if it's a sort of a touch app and if you click if you sort of move it like hungry what is it called hungry birds or something if it's running the cats I'm telling you my age okay yeah how many birds I play I'm like level 10 oh whatever any of you played so if you have a lot of if it's running in the cloud and he was sort of like you try to get the bird to move with things you will see it's just not going to be playable so it's useless so you can do that by by running this in the cloud okay the other things here are I'll leave the slides that you can take a look at them later but really to make it all economical some of those problems one has to solve is you can't actually devote one server per machine per client you can't do that right because it's just too expensive to do it so what you're gonna do a multi that what is called multi-tenancy that means you can have one server to order to many many many app the more you can do in why one server the better it is from an economics perspective so then you have to think very hard about how do i schedule these things how do I decide who the service first and how do I decide how to pack it all it so that's all multi-tenancy stuff and I do a bunch of other things too now so there are many benefits of this cloudlet concept that i've talked about right so cashing in SSL definition is already there compression people already do but all this other stuff in the black is not been done this is all open territory this is where again I you know I may be sounding too much product here but let me just say that if people want to do startups or new companies and things like that there's a tremendous amount of potential to build these things and do a lot of good stuff here by adding all these things and this is all going to come ok let's talk very quickly about deployment of these how will it work so in wireless networks the way things work is you've got you got down what we call this dumb access points what I call this dumb access points and smart wireless switches ok that's for example Wi-Fi this point here the access point is just a simple dumb device takes the packets and I do a switch switches got the all the intelligence and so you got all these lines talking to a PAP talking to a wireless said well let's put a cloud lid right there because your packets are coming there you can start doing this that's one deployment strategy so you can sort of like work all these Wi-Fi winter skin now and when they sell you the switch they can actually sell you a server one server to server depending on what your needs is so that's one strategy to do and it's all very good now I'm going to show you a cloud-based gaming and I want to show you once again how how you can do cloud-based gaming which is a pretty good deal to do but why you can't do it with the clouds ok so let's do this so this is the stuff punching this guy wardo he's you know here's the latency I will sing latency as he clicks in it you can see the thing punchin everything is running in the cloud right now so increase the 30 millisecond latency I don't know if you can notice the difference now and it's click in the punch right 80 millisecond latency you can certainly start to notice a kick the punching now let's do in a real gaming or turn ignites with me actually let's he'll just go to a real he's like a seasoned gamer and I'm gonna show you how how they impact is but you can see he's struggling at any many seconds he's starting to struggle with game so now this is the game you really want to play on your device so but five minute listings and five millisecond latency he's shooting I don't know what game it is what is it oh my god don't three okay this is me but you can see him killing right he's doing really well he's shooting this because he's a seasoned gamer he'd shoot everybody everybody's dying Bloods coming out heads are rolling so now you have 30 milliseconds late so now you see that he's actually getting hit more same guy he's actually getting hit because he's not able to react as fast right I'm he can react but the game is not reactive - see I was getting hit again think he can't get him I don't know if you notice in the back previous case he got and imagine what happens now then he goes - he's getting hit again that's blood his blood coming out I think okay now I really miss it he's gonna die same guy he can handle it if you so you've basically changed the whole game right and he's gonna say screw it hang on please dude I focused so anyway so I talked about Wi-Fi so the way we turned it did the five millisecond always put our cloudlet in the Wi-Fi small cells which is the topic that's coming up so I'm not going to say too much about small cells with small cells is yet another way to do it but before I do that let me tell you about cell network because that's where we always say if you do it in the cell networks you really can't do because lt70 milliseconds you got you know this kind of bandwidth now of course you can get larger bandwidth I know you can but let me just sort of say that a lot of the measurements and different places that we have seen this is what measurement manner which we get now but you can now solve this problem too small cells small cells increase capacity they actually do a lot where where small cells are and again it is very quick because this probably the topic of the next presentation is that you can imagine a cellular network with much smaller footprint size in terms of the cell being a lot smaller and so you can put it in the theoretical you can put a small cell in fact they do femtocells inside the house right now so this is kind of like what Wi-Fi is but it's not a setter device so you don't have to do any Wi-Fi your same cell phone works everywhere but now you can actually get a lot better things so we have deployed hang on so see a similar footprint as Wi-Fi it has all the good stuff with the billing authentication works in the license frequencies can work on unlicensed super license frequencies so on is the self-organization networks etc so we deployed this in our on our campus to small cell in fact with the help of Qualcomm and what you start to see is we have 0.3 milliseconds delay between our cloud between the device and the small cell and so now you can put a cloud let right there and then as opposed to for example if you're going to the internet this is what it's showing and these are some numbers that we have seen for example this is right there is LTE and here is your smaller performance that we have seen so we've seen up to hundred and ten megabits per second throughput and then delays of 1 milliseconds a small cell again are a potential place where cloudlets can be added and you can do a lot of the computation offloads and when there's a whole lot of smaller people now this is not just so I wanted to give you a sense of this whole thing about low is is is a very big deal okay so I've seen many talks now it was a it was a turn the corn by us means specifically along with Satya we have an original paper on this but nevertheless I see for example here's the chief scientists in winter giving it Intel giving a talk with devices and a server sitting right there which is sort of a cloud then this VP actually uses the word cloudless based on seller or seller infrastructure and then you see other things like Nokia Siemens has designing certain things it's computing to post base station computing which is again putting computing right at the base stations to boost performance and then there's an IBM Nokia seem there's another picture from our where is this from this is from somebody liquid 9 okay Siemens so you see on the base station you see a software track now this is very different from the world as the world used to be the world that I was part of for a long time now you know because none of these AI speeds want to be like a pipe they want to be able to do things and charge you more money for it so now in the and the base station they actually writing software stacks that can provide you many of the services that I talked about so this is one product that they are trying to sell I just pointed out base station application architecture I mean who heard about any of that stuff so going back the and if you look at some of the mobile operators you will see that they are buying out companies like Akamai bought out this company called lost youth they also bought out this other company called endo for so much money in this company was just removal acceleration for mobile devices so then cloudlets really are classic CD ends as I said they have been a lot of multi-tenant edge computing with overlay networking they can be so many service as I mentioned and they can be really good for mobile I want to leave you with one thought the thought is the following which is that the world as we have known as you may have heard about is this forget the edges this is this lots of cloud lots of data centers mega data centers that are being built but now in the next 5-10 years predict it's gonna change it's going to change drastically and lots of little players are going to be able to play in the old days only the big player with lots of money in their pocket could play because they are the only ones who can build but imagine a franchise model where you rent a place and you buy or go to the Microsoft's of the world other big Google's of the world and say I want to serve create a micro DC center or a cloudless server and go and write to them and they give you a rack of servers you put the rack of servers you provide network to it provide power to it and you become a cloudlet which is part of this big data centers not unlike how the ISPs grew up in the world ISPs there little man pie ISPs that have existed that thousands of them in the world in villages entrepreneur people who will say well I will provide the internet connection and do the right thing for the community to provide make money so this whole cloud computing is going to become a lot more like disaggregated computing now the challenge technical challenge there is of course to make it all happen but right now we spend tremendous amount of time to optimize here right now you have to optimize across space so even if we say there's a 40 servers per rack and you have a thousand bees there's 40,000 servers 40 dozen servers are not a small amount you double it that's 80,000 servers somebody's got a managing 80,000 server somebody has to make sure that they actually are working well they're doing exactly everything these lead to tremendous technical problems then you have to think about you know the mobile part of this how are you gonna work with the mobile part you think about the networking part of it so there's a slew of problems that one has to look at very interesting problems very big problems and they've consumer they will consume us for the next several several years so I'll leave you with that thought and hopefully it allows you to think a little differently from some of the other things we've talked about ok it better be good one question I know that's waiting was because I was worried all the time any thoughts you don't have to have questions and he thoughts any comments if you want to dispute what I said this is like now it is this mobile platforms are growing rapidly that means we can have 1.2 gigahertz of processing in the mobile itself so what do you think this offload thing is really going to matter I mean do you really offload because those 1.2 gigahertz 2gb ram 4gb ram all the things are coming up so are you really going to because numb in at the graph timidly what mattered is energy in the long run so as the mobile platforms are going rapidly so do you think this offload thing will actually make sense I think you're answering your own question does anybody want to answer her question yeah so I think you answer their own question by saying yes so there's there's a computer angle to that and what you're saying is the hey the computing horsepower is going up can we do more now remember Bill Gates who's the richest man of the second richest man in the world actually he's famously huh it is richest yeah he said he's famously known to have said you don't need that much computing power you know so even if you think that today they were computing power that you coming out with is enough it's not enough because you haven't even touch the surface of the kinds of things when one needs to do for processing that's part one the part two is that you have to admit that the service will always be more powerful than the device no matter what the second thing is that yes energy and even the heat I don't know if you know my thing gets very hot if you have a mobile device which is computing very fast gonna get very hot that's not gonna work too so I think it's it's a combination all that stuff that says that no because you can't because this thing doesn't even last me for a day you know you are going to have to offload that's one and the other thing is that when you do for example face recognition you're doing matching against massive sets or databases you're not going to download all that database on your cloud say again right so you're saying it's very application dependent in which you want to recognize ten faces and I already have the ten faces sure you can do you can do that there's nothing fundamental here except the fact that you're trying to optimize against certain parameters right and so I'm just sort of saying that you're not going to get to the point where you will have a win-win situation of doing everything on the phone so long as you can do it vikrum over lunch Ania continue the conversation yeah sorry Thanks 