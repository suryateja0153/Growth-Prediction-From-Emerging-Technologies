  [MUSIC PLAYING] WILLIAM VAMBENEPE: Thank you. Good afternoon. I am William Vambenepe, and I run product management for Big Data services on Google Cloud Platform. And in this session, Igor and Neville from Spotify are going to describe how they use [INAUDIBLE], how they use dataflow to ingest and process hundreds of thousands up to millions of events per second for Spotify on Google Cloud platform. Yesterday during the keynote, on this stage yesterday morning, Nick Harteau from Spotify was here with Brian Stevens and he explained that the main reason why Spotify decided to come to Google Cloud was because of the data processing services. And so yesterday afternoon, also on this stage, Emily and Kenny from Spotify described how they use BigQuery, and they illustrated the value they're getting from the serverless operating model from the performance of BigQuery. And today in this session, that's another example of the same Big Data services that Nick referred to at work for Spotify, not BigQuery this time, but Pub/Sub and dataflow, and you're going to see how they're being used by Spotify. Before handing the microphone to Igor for that, I just want to give quick updates on two important events for dataflow recently. So when I describe dataflow to people and how to use the Java SDK to write pipelines that can run in batch or stream mode, the question I get most often is, what about Python? Well, good news. Dataflow is not just for Java anymore. The Python SDK is now on GitHub available for anyone to download to run using a local runner. If you want to run it on Google Cloud-- thank you. If you run it on Google Cloud, there's a form you can fill and we'll give you access. It's in early access on Google Cloud, obviously moving towards general availability as a first-class language, just like Java. So Java, Python-- [APPLAUSE] And if you like Scala, there's some good news coming as well later in this talk, and Neville has some good news for you. Scala as well, all right. Not quite as much as Python, but there's some Scala happiness. So that's one big thing. The other big thing is around open source. So you might have heard of Apache Beam. Apache beam is the result of a submission that we've done with many others-- Cloudera, Data Artisan, Talon, Cask, PayPal. Together we've joined to contribute the dataflow SDK to Apache as an incubator project. It has been accepted there under the name Apache Beam. And so now the SDK, starting with the Java SDK, the Python one will follow, is now completely independent from Google. It's an Apache project. The program that you write in Apache Beam can run at Google, on Google Cloud Dataflow. They can also run on Spark, using the Spark runner from Cloudera and Paypal, can run on Flink. And the communities forming around Beam to add runners, to add languages, to add connectors, to add libraries of function, and we're very, very excited to see that as well. So a lot of the innovation of the programming model is now moving to Apache. Obviously we're very engaged there, but it's open to everybody. The name Cloud Data Flow remains the name of the Google-hosted service where you take your Beam pipelines and you're able to run them on Google infrastructure in a serverless mode, where Google deploys and scales everything for you and gives you the best performance and the best [INAUDIBLE] for your pipelines. But don't take my word for it. Igor is going to describe how he's using it in real life and how it's working out for Spotify. [APPLAUSE] IGOR MARAVIC: Thank you, William. So I'm really psyched and honored to be here at Google Next. It was a really good conference so far. My name is Igor Maravic, and I'm one of the software engineers working at Spotify. During the past year, I've been a part of the team which has been working on maintaining and developing Spotify's event delivery system. This session is going to have three parts. In the first part of the session, I'm going to talk about our journey for moving our event delivery system into the cloud. In the second part, my colleague Neville will talk more about the tooling that we've been using to analyze the data once it's delivered. Finally, I will get back on the stage to talk more about the learnings that we gathered while rebuilding our event delivery system. Spotify have had a phenomenal user growth in the past years. Only in the last year, we almost tripled our user base, reaching 75 million monthly active users. With the user growth came the data growth. Today, we are shipping around 60 billion events every day to our Hadoop cluster. Oh, OK. All the events are being passed through our current event delivery system, which is based on Kafka 0.7. All the events that are being delivered are being generated on our clients as a response to certain user actions. So for example, when user creates a playlist, subscribes to a playlist, or when it finishes listening to a song, an event is generated. This event is delivered to our Hadoop cluster. This system worked fine until a certain scale. But with the load came the outages. Well, by observing how the system worked, we observed a lot of problems with it. Most notable problem was that this system is way too complex. It has way too many components that one event delivery system needs to have, and on top of that, most of those components are having limited coverage, and they're operationally mature. Other problem is that our event delivery system is stateless. That means that the state is only kept on the service machines themselves and on the Hadoop cluster. If the Hadoop cluster goes down, our event delivery stops functioning, which is an operational burden for the event delivery team, because we are not responsible for the uptime of the Hadoop cluster. When you combine those problems with the data growth that we had, you get a recipe for disaster. So at this exact point here, we figured out the system cannot keep up with the growth. At that point, we start filtering the events, and this is the drop you can see. Considering the shortcomings, design shortcomings of the system that we observed, the only way forward to keep up with the Spotify is a complete rebuild of the system. When we started thinking about new event delivery system, we knew that the new event delivery system still needs to keep the same function as the current one, that meaning that all the events from the clients still need to be delivered to a Hadoop cluster. To have an easier migration, we wanted to keep the same API. That means that all events are still persisted to log via syslog, and then delivered to Hadoop in hourly buckets. We also wanted that our event delivery system have a persistent state implemented as a reliable persistent queue component. That way, if the Hadoop goes down, we can just keep on functioning without any noticeable issues. Finally, we wanted to keep the system as simple as possible. Simpler the system gets, it's easier to operate it and to scale it. Simple as that. We had the basics laid down, and the only thing left to do was to get our hands dirty. First, we wanted to figure out, what should we do with the reliable persistent queue? Building one is not an easy task. Building one that needs to keep up with the Spotify volumes is even harder. So we wanted to use some of the available technologies out there and use them as a reliable persistent queue. Our adventure started with Kafka. There were three main reasons why we decided to experiment with Kafka. Kafka is a proven technology that's been used all around the globe. There are many companies being successful at using it right now. Kafka is an open-source project which has a strong community built around it. It is a first-class citizen in many of the open-source projects we were using today in Spotify-- Spark, Flink, Storm, just to name a few. Lastly, Kafka 0.8 was designed to be used as a reliable persistent queue. This is a huge improvement compared to Kafka 0.7, which we are using in our current event delivery system. So we went through the docs, followed all the guidelines and practices, and we put a system in place. It was relatively simple and painless. But is this how we solved our event delivery system? Well, unfortunately not. As soon as we pushing production volumes through this system, it started breaking apart. Mirror makers would just drop data if we had problems with the destination cluster. Mirror makers would go in some confused state and completely stop mirroring the data. We had issues with the Kafka library, which would go into an unrecoverable broken state if we would remove or restart one of the brokers from the cluster. This was a lot of problems, and at this point, we were at the crossroads. Should we significantly invest in Kafka and make it work for us, or should we try something else? Oops. At the same time we were experiencing problem with Kafka, other teams in Spotify started to experiment with Google Cloud Services. At that time, we heard about Cloud Pub/Sub, which seemed like a good alternative for Kafka. When we went through the docs, we saw that Cloud Pub/Sub can retain and deliver data for seven days, and that it has at least once delivery semantics. With these two features, Cloud Pub/Sub did everything we wanted. But this was not it. Cloud Pub/Sub is a globally available service. That means that messages published in one region can be directly consumed from another region. This was important for us, because that would mean that we could skip flaky cross-Atlantic internet links when publishing from our US data centers and consuming in EU. Also, Cloud Pub/Sub comes with a simple REST API. We saw this as a big benefit, because if we didn't like Google-provided library, we can easily write our own, because HTP is a well-known protocol. This was not the case with the Kafka library. We were completely stopped in our tracks with the bugs we found there. Lastly, the best thing about Cloud Pub/Sub, if we would use Cloud Pub/Sub, we would have no operational responsibility for it. Whoever carried a pager with them knows how important this is. And after having spent so many unslept nights with our Kafka cluster, we appreciate it even more. Asterisk here stands for that even if we would not be operationally responsible for Cloud Pub/Sub, we would still be operationally responsible for the services that are directly interacting with it. Nevertheless, we were completely hooked on-- seriously, there was no other alternative for us. We just wanted to proceed with it. But then again, we backed up a bit and, well, stopped being so excited and started thinking, would you actually trust your car salesman? Was this just a sales pitch? On top of that, at the time we started experimenting with it, Cloud Pub/Sub just exited a beta, and there was no reports that any other company is being successful at using it. So if we wanted to proceed with Cloud Pub/Sub, only thing that we needed to do was to verify that Cloud Pub/Sub is going to work for us. So considering the Spotify's data growth, we were mostly interested in seeing how good Pub/Sub is at scaling. At the time, we performed both publisher and subscriber side tests. But to make this presentation even more interesting and exciting, I decided to do the live demo. For the demo, I'm going to push 2 million events per second and then consume them. This number is not just a random number. It is actually based on our current event delivery volumes. Currently, we're delivering around 700,000 events per second, and on top of that, we added our growth margins and disaster recovery margins. For the demo, I'm going to use GRPC, which is still in alpha, but it looks really promising. It is a huge improvement compared to the autogenerated pub/sub client, and it is also a performance improvement compared to a sync pub/sub client that we wrote to maximize performance over HTP and which we are currently using in production. Disclaimer here is that even if I'm doing the demo here with GRPC, we are still not using GRPC in production because we are playing it safe and we are waiting for it to go-- to become GA. But now it's demo time. Can I get the demo? Can I? Yay, OK. So for the demo-- for the demo, I'm going to use 10 nodes to push the million requests and then 20 nodes to consume them. So that means that every node is pushing 200,000 events per second, and that we're consuming from every node with 100,000 events per second. The graph should show up here, but because it needs some warm-up time, you can't see anything here. But I came prepared here, unsurprisingly, and I started running the demo before I went on stage. It is exactly the same code and is the same demo. So you can see from 0 to 2 million without any issues. It just scaled. It just worked. The same thing is on the subscribing side. So here we can notice it was a bit slower to start, but as soon as picked up, it consumed the whole backlog and then started consuming with a steady pace. These were exactly the same results that we observed when we did our test. Now you can switch to the slides, please. Slides, please. Thanks. These were exactly the same results that we observed when we run our initial test. So based on those results, what was our verdict? Did we went with Pub/Sub or not? Unsurprisingly, we decided to proceed with Cloud Pub/Sub and to base our event delivery on top of it. After having putting it through many hard labors, we were certain that Cloud Pub/Sub is going to deliver. Putting it in place was relatively simple. We already had the event delivery service in place from our Kafka experiments, and the only thing that we needed to do is to replace the Kafka library with Cloud Pub/Sub one. So in our case, you can say that Cloud Pub/Sub was just a drop-in replacement for Kafka. After we had put the Cloud Pub/Sub to a good use, it was time to start thinking about writing our ETL job. For the ETL job, we decided to experiment with Dataflow SDK. Dataflow SDK is a framework which provides us a high-level API for writing data pipelines. It has unified bash and stream model. Dataflow SDK comes with a promise that it can be run on any number of the runners that are implemented. Currently we have runners implemented on top of Fling, on top of Spark, and finally, on top of Cloud dataflow. Cloud dataflow is a managed service. It is able to execute the pipelines written with Dataflow SDK, and it comes with a promise that it can seamlessly scale them. Back in the days, we decided to use Cloud Dataflow runner simply because we didn't need to put any external resources in place before using it, and because it was most stable one. When you write your data pipeline with Dataflow SDK and submit it to Cloud Dataflow, you get a fancy graph. On this graph, you can see how your job is structured. Our ETL job in essence does really simple things. It consumes data from a single pub/sub subscription and writes it in parallel to both HDFS, which stands for Hadoop Distributed File System, and GCS, which stands for Google Cloud Storage. Our requirement is that all events needed to be written to Hadoop. But since we are planning to migrate to Google Cloud, we are in parallel writing to cloud storage. By doing this, we're enabling other developers in Spotify to start experimenting with Dataproc on moving their pipelines from on-premise Hadoop into the cloud. All the events that are being written to Hadoop and cloud storage are being written in hourly buckets. This is the legacy from our first event delivery system, which just copied hourly log files from all the service machines and put them in Hadoop. This is the API that all Spotify jobs expect today, and that means that we're going to have this API for a long time. Difference here with our new ETL job is that we wanted to start incrementally filling the data as it arrives. By doing this, we wanted to achieve lower latency for delivering data. Before the data can be consumed, buckets, the green ones on this slide, need to be closed. When they're closed, that means there are safe to be consumed. In dataflow, we have a notional watermark, which you can see as a yellow, I don't know, thing here that's moving, which allows us to do that easily. When the watermark passes end of the hour boundary, we mark the bucket as closed. Once the bucket is closed, we don't want to backfill any late data to it. This is because most of the Spotify jobs are reading data from an hourly bucket only once. If we would do the backfill in the already closed bucket, we could consider this data as lost. So in our case, we want to write late data in a youngest currently open bucket. By doing this, we are skewing the event time stamp, but we have no data loss. This is all really nice, but how did we implement event time-based hourly buckets, incremental bucket fill, bucket completeness, and late data handling? Amazingly, they mostly ended up being implemented in a single transformation. In this picture, this is a window transformation, and what does this transformation do, is windowing. Windowing is one of the most powerful concepts of data flow, and I have to say one of the most magical ones. All this stuff, bulk of the implementation for all the concepts I've been talking about, ended up being implemented in only distance of lines. So in distance of lines, we define, how big is our bucket? How do we close the window? How do we do incremental bucket fill? And then let lastly, how do we handle late data? Skewing of the time stamp is not happening here, it's happening in the transformation after, but nevertheless, the bulk of the job is done here. Windowing is available both in streaming and batch, but only when using streaming, the full power of windowing is unleashed. This is because we have a formed watermark which is being constantly propagated as the data arrives. This allows us to detect and handle late data. Having our ETL job being implemented as a stream job makes perfect sense, because event delivery is just a constant stream of data. Us having our current event delivery job implemented as a batch job is driven by the limitations of the framework we were using. The ETL job we've been talking about is just in a prototype phase. We still have few hurdles that we need to overcome for it to go in production. First, we need to be able to restart our ETL jobs without losing any in-flight data. Today, this is currently not the case if the update does not work. Other big thing is that we need to have a good continuous integration and deployment process for enabling the safe and fast iterations on our ETL job. This is even more important if you consider that we would like to have one ETL per event type and that we are currently having 1,000 event types in Spotify. Nevertheless, preliminary results look promising there. So on this graph, you can see the end-to-end latency from when the event was persisted into the log file and when it was actually written to Hadoop. There is multiple lines here because we are currently running four ETL jobs in our experiments. So during the biggest spikes, the latency goes as high as 25 minutes, but in most of the cases is below 10 minutes. Biggest spikes here can be attributed to the fact that we are writing to Hadoop via flaky VPN links, and the lower spikes can be attributed to the fact if we are still tailing files, which is inherently unstable. They are not actually related to the way we structured and run our data flow job. To put this in perspective, end-to-end latency for our current event delivery system is around two hours. So that would mean that with our experiments, we have actually four times the latency improvements. Delivering data is just a first part of the process. Once the data is delivered, other teams in Spotify are using that data to produce really awesome recommendations to our users. Neville here is going to talk a bit more about the tooling that they're using to do that. Here you go. NEVILLE LI: All right, thanks Igor. So next, I'm going to talk about Scio. It's a Scala library that we built on top of Google Cloud Dataflow SDK. So a little bit origin story. We are big fans of Scala at Spotify, and I work with the music recommendation team at Spotify. That's that team that built-- discover weekly related artists and a few other music recommendation features. We also use-- we've been using Scalding for three years now, and some teams are using Spark as well for machine learning use cases. Right now, we have about 50 users using Scala for daily data pipeline processing. A lot of them have no computer science background. They're from business or math backgrounds, so not great at programming, but they enjoy using Scala for the concise syntax and the libraries. And we have 400-plus unique jobs at Spotify running daily. So early 2015, about a year and a half ago, when we started working on GCP, Google Cloud, I started hacking around dataflow and built a prototype Scala APR on top of that. So compared to the other options we have, what are the pros and cons? So today we have Scalding. It is very popular. It's open source by Twitter and used by a bunch of other tech companies. It runs on Hadoop, which is very stable, mature, and proven. It scales to-- it has really high scalability. And the downside, of course, is the operation of the Hadoop cluster. Today we have a 2,300-node Hadoop cluster, and it requires a lot of maintenance. It doesn't handle multi-tenancy very well, so the Hadoop cluster is overloaded during weekdays but underutilized on the weekend. And of course, there's no streaming mode in Hadoop. Some of you might have a heard of Summingbird, which is a Twitter library that runs both on Scalding as batch mode and Storm as a streaming mode. But the downside, of course, is you have to maintain another storm cluster, which adds on additional operational cost. So Spark is really getting really popular these days. It works on both batch and streaming mode. On top of that, it has interactive repo, and also supports SQL. It has a very large library of ML, machine learning algorithms and graph algorithms. It also supports Scala, Java, Python, and even R, so it's very popular among data scientists. The downside of Spark, of course, and many people complain it has too many parameters, a lot of different operational modes. The common practice of running Spark on cloud is to set up one cluster per job. It sounds easy, but to manage that many Spark clusters is not an easy task, and there isn't as much automation or tooling for that. So that's another consideration. So why do we want to use dataflow with Scala? We already talked about dataflow. Cloud dataflow has a hosted solution, so it runs on Google infrastructure and there's no operational cost. You submit your job from a laptop. It sends some request to the cloud. [INAUDIBLE] VMs does all the scaling under the hood. There's no operation cost whatsoever. It also has a tight integration with the entire Google ecosystem, so you can read and write from all these awesome data products, like BigQuery and Bigtable, so that's very nice. And the cloud dataflow has a very simple API that handles both streaming and batch in the same abstract data model, which is also very nice, so you can write your application once and run in either modes. And we also like Scala, of course, because it's easy to build a very high-level DSL on top of Java libraries. It's very easy for developers to pick up. And also, Scala being a functional programming language, is encourages the reusable composable patterns-- makes code easy to troubleshoot, easy to maintain in the long run. We also use a lot of Scala libraries, like Breeze, which is a statistical numerical library, similar to NumPy, and also [INAUDIBLE]. It's a Twitter library for approximate algorithms and also parallel processing. So this is how it looks like, the Scio library on top of dataflow SDK. So on the top layer, you have this very same layer of Scala API that wraps around on the Java SDK. We also added a few other things, like enabling the user to use Scala libraries like Breeze and Algebird. We added interactive repo in addition to the batch and streaming mode. And also we added some extra features that we like, for example, reading and writing HDFX, and also some simple primitive for doing job dependence and scheduling. All right, so what exactly is Scio? It's pronounced shee-oh, obviously. Thanks to Google Translate, it's Latin, the kind that the pope speaks, so very classy. And also, it's the name of a Scala data library that starts with SC, ends with IO, so totally buzzword compliant. So we like the name. And the core API is actually very similar to Spark-- I would say about 80% compatible. And also, we borrowed some really nice ideas from Scalding as well, the two libraries that we really like. So it is open source on GitHub. And next, of course, this is a data library, so I have to show the word count example. For those of you who have experience with Spark, the source code looks almost exactly the same. So you read a text file, spit out lines, do some filtering, count by value, save it to another text file, and that's it-- five lines of code. Of course, this is a Google conference, so I have to mention page rank as well, in 13 lines of code. So your input is a collection of source and destination URIs, and the initialized page rank to 1.0 over 10 iterations. You distributed the page rank to the destination pages, and also added on top have some dampening factor, and that's it. That's page ranking-- 13 lines of code. Very easy to write, very easy to refactor and debug. There was a few talks on BigQuery already, or some BigQueries. And of course we like BigQuery as well. And we even want to use BigQuery as much as possible. We actually at Spotify have been through a few different SQL on Hadoop solutions. Of course, at the very beginning there is Hive, and we use Hive on top of TSV or Avro data. And the obvious problem with that is that TSV or Avro, they are all row-based storage, which means any simple SQL query will require scanning through the entire data set, and that's super inefficient. And also, there's no integration with other frameworks, so it's as easy to use SQL inside a data pipeline, for example. We've also been experimenting with Parquet, which is a columnal storage. It is actually inspired by Dremel, this Google paper which is basically the technical basis of BigQuery. So Parquet gave us like 10 times speedup over regular Avro Hive. But there are also some obvious problems. The integration with Hive is a little bit immature. There are some rough edges. It works really well in Spark SQL, but the problem is Spark is another platform that we have to tune, and it's slightly harder to scale up. Of course, we want to use Parquet inside the current frameworks that we have, like Scalding, but there are some impedance mismatch with, for example, Avro. And also, the Parquet API is Java-based and it's a little bit clunky. Basically, instead of having to write SQL query, you can select where and all that stuff. In Parquet, you have to write a lot of Java code to perform these kind of column projection and row predicate. All right, so we want to use BigQuery with Scio, of course. The idea is that BigQuery is so performant and also scales independently, which means we don't have to maintain any cluster. So the idea is to use BigQuery for most of the slicing and dicing. And of course BigQuery has a lot of commercial tools support and has the web UI, so data scientists can easily prototype, say, a data pipeline inside BigQuery and then import that code into a data pipeline like Scio. And of course there are certain things that's hard to do in BigQuery or in SQL in general, like custom machine learning algorithms or fancy statistical analysis, things like that. So in that case, we can copy/paste the SQL query into a data pipeline and seamlessly integrate with a dataflow. And because we use Scala and Scala gives us macro, which generates type safe data types, and I'm going to show it in a bit. On the left side, you have this typical pipeline that handles BigQuery as JSON objects. So obviously it's a map of streamed objects, and you have to access fields by string and then cast them into whatever you think it is. It's a very tedious and error-prone process. So you can easily run into [INAUDIBLE] errors, and it takes a long time to actually run the pipeline production and troubleshoot it. On the right-hand side, I have a class, but no definition. It just generates the class schema from the data type inside BigQuery, and then I can treat my data in a type safe manner. And in this case, if a composite works in general. The next is an example of a Spotify running feature. So this is when you open up your phone app and run to a certain beat, so Spotify will recommend tracks that you might like based on the speed of your running. So some SQL, right? We have 60 million tracks, say, 30 million active users. For each of the 10 tempo buckets that you might be running at, we want to find a certain number of tracks that might fit your taste. So we have a lot of features like audio analysis, metadata, and also collaborative filtering feature from latent vectors. We want to leverage all these features to recommend tracks to users. Of course, as a data scientist, I can start prototyping in BigQuery. So this is a fairly complex query. I can play around with the criteria and just figure out tracks that might look good for running a product. Next, I copy and paste the feature, the SQL query, into a data pipeline. This is how the pipeline looks like from a high-level perspective. So on the top row, I select tracks and the features that I'd like to use from BigQuery inside Scala code, do some group by based on recording, basically filtering out duplicate different versions of a track. And it also finds tops tracks for each artist, groups them into tempos, then I build a vector representation using the locality-sensitive hashing algorithm to build basically an index that is small enough to fit in memory. Then I can use a side input to replicate that to each [INAUDIBLE], which then selects from another table of all the users. So I scan through all the users using the locality-sensitive hashing index to look up related tracks the user might like. And then at the end. I can store the data to either datastore, Bigtable for serving the live traffic, or I can store it to a BigQuery table for interactive analysis. So that's the idea. So this is how it looks in the dataflow console. And one nice thing we added on top is the file name, and also line number in the transformation name. And because it's Scala, so each transformation is usually a one-liner, so by just looking at the code and skimming through the code, you can easily visualize it to a diagram here. And you can see we have two type BigQuery inputs, a few filter and maps, typical functional programming terminology. This is the second half of a few further MapReduce, of course, side input, and then save it to BigQuery. As you can see here, the last transform here, save as BigQuery, it's line number 152. So the entire pipeline is less than 200 lines of code, and half of it is SQL. So it's actually less than 100 lines of code for a pretty complex machine learning product, and that took like two hours to write. So it is pretty nice. [APPLAUSE] Of course, there are a few catches. This is a very new project, so there are some rough edges, and it's still under heavy development. So right now there is no interactive mode, although we are working on a shield repo, so similar to Spark, Scio, or iPython notebook, where you can do interactive query and processing. Of course. Google provides Data Lab, which integrates BigQuery with iPython or Jupiter. So that's another alternative for doing interactive analysis on Google Cloud. Right now, there isn't enough machine learning algorithms build for BigQuery, but the instance of flow, also from Google, which is now offered as Google Cloud ML. So in the future, it's going to have dataflow integration. So potentially you can just run your dataflow job and integrate with instance of flow. Of course, the library is open source under Apache 2 license, so feel free to check it out. There's even a prebuilt binary job for the repo, and feel free to submit pool requests or file issues. That's it, and next I'm going to hand it back to Igor. [APPLAUSE] IGOR MARAVIC: Thank you, Neville. So I'm going to steal a bit of your time to talk about our learnings. First, I'm really glad that we're put both Kafka and Cloud Pub/Sub through iterative testing, and then at the end, that we chose the best product for the job. Can I get the demo? So this is the demo I started earlier doing my talk. And the nice part about it, and this is the part I like about Cloud Pub/Sub-- it just works. So as you can see, from 0 to 2 million in live demo without any issues. Same thing here is for the consumption-- a slower to start, but nevertheless, it picked up, and then 2 million events steadily-- really nice. Can I back to the slide? Even if you're not in production, our event delivery system, choice of Cloud Pub/Sub is already paying off. It allows us to move faster on the real-time use case. Currently, we use Cloud Pub/Sub as a drop-in replacement for Kafka in few of our storm topologies, for doing the real-time ad targeting with great success. From the dataflow perspective, I think windowing concepts might be a bit harder to understand, but I think they're worthwhile investing in. This is because they allow developers to obstruct really hard problems and focus on the business use case directly. All in all, I had and I still have a lot of fun working on moving our event delivery system into the cloud. If you're interested in learning more about this, I would recommend you to go on labs.spotify.com and check out the blog posts. There are three blog post there describing the whole journey that we took. I hope you enjoyed, and I hope you learned something new. If you have any questions, feel free to catch me or Neville. And that's it. [APPLAUSE] [MUSIC PLAYING] 