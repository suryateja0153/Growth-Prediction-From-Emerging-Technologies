 welcome thank you guys for joining me today at engine it's calm I'm here to talk about dynamic load balancing and deployments with nginx and zookeeper I'm here to tell you why you should invest in your traffic controller and talk about the heart of the systems that I build in my everyday work life so first of all Who am I this thing doesn't work great that's why all right so my name is Derrick tungee I'm a cloud solutions architect and nginx enthusiast so much so that my advocacy has made nginx our default go-to for web serving and proxy in my organization I'm certified by AWS and I work for a company called right brain networks we are a advanced consulting partner and managed service provider for AWS and Azure we help companies migrate into AWS revolutionize their workflow by bringing in C ICD and infrastructure as code as in infrastructure management and configuration management so we did we typically deal with SAS system services that are provided to the world etc I'm also in embedded consulting roles with larger organizations as well as running lots of systems for smaller organizations so today we're gonna have a few topics we're gonna briefly cover some reasons why we're doing things like this we're going to talk about who our actors are and what our goals are we're gonna cover some basic cloud concepts and DevOps and then we're going to talk about what I mean when I talk about dynamic load-balancing we're gonna explain some of the obstacles around micro services and auto scaling distributed architecture I'll detail some of the processes around releasing code and you know what that means in dynamic infrastructure and how nginx and service discovery can be utilized to impact these solutions that we camp so who are my actors my actors are your end user right when we deploy code we want to make sure that they get the best performance and the best excuse me but all right so what do our users want they want zero downtime they don't want to have to wait for a maintenance mode for new new features or bug fixes and they certainly don't want malformed requests our other actors are the teams that are developing the software they want to spend less time deploying they want less effort for deploying they want to spend their time doing what they do best and that's developing and moving forward with the product so what do our goals here we want to automate live index and code deployments we want to update things more often we want to excuse me we want to eliminate the need for lengthy deployments the resources around those deployments have you guys ever seen a war room setup stacked to the brim with snacks and people sitting there for 36 hours straight right we need to get away from that we want to eliminate the need for over over over capacity of data centers and we want zero services so disruption and we want to make sure that every request is sent to the right application server and the right version and we also want dynamic load-balancing because in my world everything's built in the cloud then might not be how everybody else is here but this is a problem that I have to solve day in and day out so let's cover a few cloud and DevOps concepts everybody here is brilliant individuals so this might be wasted but with the drive to the cloud a lot of companies are decoupling they're monolithic application servers and going with micro services and this concept makes many different distributed service services that all work together to provide your service or product micro services allow you to scale on a very granular level which saves on cost and allows people to developers to get their work done more cleanly and push it out more efficiently so with that micro services is a great thing I mean with cloud you know cloud and Microsoft services go together like engineers and coffee engineer might not actually need their coffee but they work a little bit better with it so the same idea with cloud and micro services so being able to scale at that granular level just saying ok we're getting a lot of authentication requests we need a scale up are just our off the portion of the application rather than you know some some worker on the backend that might not be under such load so this micro-services definitely causes a little bit of complexity when you're deploying and you're in your environment as a whole so let's take a look at you know pretty simple architecture and what it might look like - you know for a packet to run through this we have a request hits a web layer back to engine X goes to our API layer and then finally to our data it's important to note that the data that I'm talking about in this particular example our solar shards they're immutable they're static data and we've got multiple shards multiple replications of that chardon cetera so with that our packet travels back out through the stack back to the end-user you can see how much more complicated this can get if we add another service or attend services deploying to these is going to be a little bit more complex when you want to offer that zero downtime high availability so what's the idea there continuous delivery you'll probably hear it day in and day out for you know the rest of the conference but continuous delivery is a completely automated system it allows you to spend less time doing release work increases your productivity and ensures for reliable releases it decreases your time to market for things like new features bugs etc and that improves your product quality it increases your customer satisfaction because you're squashing those bugs you're releasing those new features faster you don't have to wait for you know deploy time where we have to put up that war room and tons of people man hours spend time deploying and it gives you more time to focus on what really matters moving forward with your product so another concept of this continual deployment zero downtime etc is the Bluegreen deployment it's one of the most sought after deployment styles of today this deployment style allows you to seamlessly direct traffic to a new version of the application code running on separate sets of infrastructure the fulcrum of these deployments is the traffic router whether it be DNS or something like a proxy with nginx nice deployments offer zero downtime and the ability to switch back to the older version if things go wrong this lowers the the stress of the engineering team and increases your again customer satisfaction you have zero downtime and that's exactly what we're looking for right so this is a quick diagram of what Bluegreen might look like you have application version 1.0 application two spins up you switch traffic and the old version goes away same idea with say nginx but the idea here is that nginx is able to do this a lot more cleanly than DNS because we don't have to wait for things like TTL there's a lot of inconsistency around that you don't know exactly when things are going to switch what application servers are going to switch at what time etc so this is why doing it with nginx is the preferred way for me auto scaling will just cover this briefly because it does present a lot of problems and while it solves quite a few as well so some of these obstacles when you're scaling horizontally to increase your performance under load things to that effect you're going to have very dynamic back-end and infrastructure your your deployment has to change its not SSH into boxes and swapping out code or just get polling anymore things are vastly different so this is one of the problems that we've been trying to tackle is being able to load balance dynamically this means when we have servers spin up we want to make sure that they get put into the back-end pool we want to make sure that you know if they die or scale down they're removed from the backend pool and with products like the elastic load balancer or nginx plus that's soft for us right instances come up they check in to nginx and nginx starts passing traffic to them so why nginx right being that we're at an engine X conference I hope I don't have to convince any of you but let's talk about why not the elastic load balancer real quick so this elastic load balancer offers a lot of great things like that dynamic load balancing where your instance comes up it checks in it starts getting past traffic but there we go alright so that solves a lot of problems for a lot of teams it's a really great product and it does what it does very well however we have some problems here like only a single back-end pool no context switching and one of the biggest pains is that once a instance registers it has a minimum of four seconds before it will start Pat being past traffic this is because of the health check right it has to pass two health checks and the interval must be two seconds or more with that that provide that that's a huge issue when trying to use an elastic load balancer for dynamically switching out versions unless you're doing something like DNS to point to a different load balancer which again we come back to the issue of TTL issues and DNS caching the need for a morph assist sophisticated deployment and traffic router is greatly increased by the use of micro services because we want to be able to do the context switching we want to be able to have dynamic load balancing we and we need to be able to do deployments so nginx like I said hopefully we don't have to convince any of you guys but for me nginx is this perfect Venn diagram here equal parts rocket ship and bicycle high speed and low resource footprint it's scalable and highly configurable the Lua aspect of this allows us to do so much header manipulation that we can really do whatever we need to the other part is this seamless reload this is this is a feature that a lot of other proxies are missing we're able to do decision-making context switching routing these these requests to exactly where they need to go seamlessly so let's move on to the interesting stuff everybody here knows about nginx but how does service discovery fit into this now I'm going to talk about zookeeper and a lot of people here are probably asking why not console well the answer is I'm a consultant I get put into larger organizations and I don't always get to make all the technology decisions in this particular case the team was moving towards solar cloud and wanted to implement zookeeper so that they were ready for that zookeeper is a centralized configuration store this is where you might store all of your metadata about a server all of its roles all of its you know data models that is running thanks to that effect zookeeper clusters and scales really nicely it's a pub/sub system so that means that we're not only able to push data to it and pull data out of it but you're able to subscribe to different events within the system and you register callbacks to be had when something event takes place or changes in the system or data are presented all these all of the services in the environment are always connected to the the zookeeper if a if a node so fall so falls off off the face of the earth the connections dropped and that that node is no longer in connection to the zookeeper so with that we have really what I'm talking about here is the socket video between the two so things are really fast there's not you know a connection being opened and closed and opened and closed where we're always connected and it's really fast millisecond fast and that makes the ability to scale and have this dynamic to your environment really speedy and graceful so Z nodes are what the data is actually called it's a blob of any data limited to one megabyte but these Z nodes are stored in a very unix-like directory structure which is pretty comfortable for most people you have child and parent nodes the parent node can actually hold data as the child node can as well so let's take a look at a Z node example let's go ahead and get the root we have a few things we have a few Z notes here API web nginx so let's take a look at what nginx is holding for us here in my example I listed out a unique identifier so these are all of my nginx nodes that are currently running connected to the system let's go ahead and ask for the data within one of those nodes and here we'll see things like IP release version things to that effect so how do these systems work together you guys like diagrams right great I do too so here we're gonna stand up zookeeper we're gonna build our nginx boxes and nginx will check into zookeeper we've got our web layer again checks in the zookeeper however here we have the nginx box has a subscribe to the web layer Z nodes so this event is pushed out to the nginx box's nginx client we'll know what to do with that information and template its configuration accordingly same goes for the API layer and the data layer so let's talk about this note client what are we doing here we're grabbing data anything I can find about myself we're doing self-discovery we're going ahead and putting that into a Z note structure that's based on things like the environment it's running in the role of the application and then finally a unique identifier these can be built in very modular ways for instance most of my node clients were very common all they needed to do is just dump data into the system so that nginx could find it and act accordingly the information we put in here is actually pretty important now we want to be able to know exactly what the IP is and definitely the release this is going to build as we get further into exactly what's happening with the deployment you can really put anything pertinent into it same things you would might find in an environment variables things to that effect given that just that discovery the Z node will be excuse me the node will be able to construct the Z node path in a meaningful way and with that we'll have something that looks something like this we have what we call ephemeral nodes inside of the this structure meaning that if the connection is ever severed between the node and the zookeeper that node will then disappear which means that we don't have to clean up after ourselves it's done for us and if there's ever a blip in connectivity then we're no longer routing traffic to that host I decided to store my my configuration data in JSON format because it's independent to languages I was using an Orchestrator written in Ruby and all my nodes were in Python so it made it a very common language to be able to communicate across so let's talk about the nginx client denon the nginx client provides all of your functionality and decision-making the nginx client might do the same things that the note client will by importing or putting his data into zookeeper setting up the structure however one thing that it's going to do differently is that it's going to subscribe to different nodes and different parent nodes in the zookeeper directories this client would also ask questions getting information about the different services in the environment so that it's able to template its configuration and make changes accordingly this is done via templating and configuration files my nginx client included the templates for these different configuration files things like templates of pools variable changes mappings and even being able to produce full servers right so I talked about my data example being solar now running a solar cluster you might have two nodes you might have ten nodes your dev admirer environment might have 10 nodes and your prod environment might have 200 setting up the the nginx rules for those is boring nobody wants to do that so with this we're able to dynamically find out how many shards are there and template out all of the servers so that we're able to not have to type all that out and it just dynamically generates all of the services the entry next client might want to know things about health for all those boxes if something's not acting appropriately or serving requests appropriately you know they can do health checks on the local node put that into zookeeper and say hey I'm healthy past me traffic or you know what we're getting five hundreds over here don't send me any more traffic we need to figure this stuff out it also wants to know when things register to the system and when things disappear from the system that gives us the ability to template out those pools very efficiently you can also do things like I was kicking around the idea of load if you have a API that takes large data set small data sets and ones really cranking away at things you know you can say you know this server is under a lot of load let's let's send less requests to this point somewhere else the nginx will register callbacks around all these different parent or child nodes that they're subscribed to and that callback is going to work appropriately to reconfigure its its configuration and this is this right here is our dynamic load balancing so let's talk about the upstream pools so the pools are created dynamically based on the Z nodes in the environment instances are pooled by application and version that's something that is a little bit different I would say we're not really load balancing over the service or number of servers but really what we're doing is we're load balancing over the application version and the application all the upstream services must pass health checks etc so here's a little bit of pseudocode on templating this out very very simple you can also put port in there you can see how this might drop into something like a containerized environment and I'll end up with something like this so our requests what happens when a request comes into our system it's going to take that natural flow moving through the application stack and then being popped back out to the user so here we're doing context switching on you know what's the what's the hostname I was provided okay we're gonna go to this particular application and also we're going to take a look at the request and what version is bound for so what happens so the version header is mapped to the application pool by different versions of this application that were load-balancing over and we're going to route that traffic appropriately so here's a map of say you know what the request version might be for a particular request and directed to a pool so that we're able to proxy this request to a particular pool based on not only the application but the version as well so this is the meat of it how our deployers really work a new server is built health checks are passed nginx templates of pools then orchestration might change the nginx version by making a alteration to the nginx z node the nginx client is actually watching its own z node for changes so that an outside force can actually change its version at that point nginx will go ahead and read template its configuration saying that my current release version is one point two point four where the previous version might be one point two point three this is actually pretty important because of how the request actually gets tagged the request is tagged if it doesn't have a version within the system that request will flow through the different services tagged with a particular release the as the client notices a change to nginx is a release version it goes ahead and returned plates in its configuration and directs traffic to the newest version for any new request now what happens when you have a request that's in mid-flight between all these different micro services you just switch versions on it that seamless switch for the Bluegreen is great in theory but if you have a coupled service that is making a request to a version that it's expecting to reply it in a certain way and you switch its version it gets a different reply you're gonna have a reformed request and that's the issue we're trying to solve here all right so a typical request might might be going through the system and we will see new boxes bill but where is it my images got messed up I'm sorry the idea here is that if we were to build services in the middle of a ray micro service request that it might end up going to the wrong node we don't want that to happen so if we switch versions even though nginx is on a newer version we're able to direct the request to the 1.2 version appropriately the outcome here is that we're highly available we don't have any maintenance windows we have zero downtime zero malformed requests we update our data and our code more frequently faster and of course with less headaches effectively turning deploys from something like this and do something more like this the impact is higher customer satisfaction higher engineer satisfaction higher productivity and a better product thank you my name again is Derek young he worked for right brain networks suppose we got a few moments here for questions wasn't quite prepared for that but certainly no there's an external process that's continually running it's a daemon that keeps an eye on all these things it'll go ahead and reload yeah exactly not at the moment it was built for particular company the idea is mine it's their code the client actually has a built-in and the the Python library for zookeeper it's called kazoo it's actually got a really nice retry mechanism that will if if there is a severed connection or timeouts there's different things that you can do a probe to take the appropriate step to reconnect yeah absolutely if it can't talk to zoo keeper it's not going to read template its configuration no more yeah the idea here is that you know if you've got if you're running microservices you may have upwards of 30 different services and in your environment for your product offering or service and to be able to do that with ELB is you're going to need to run 30 lbs that's a that's a huge ways to cost it's harder to work with and work around so the idea would be to replace all those multiple lbs with a system that is dynamic and does contact switching over multiple pools different application servers and versions so that is through the pub/sub system publish and subscribe right so this is an idea where you can tell zookeeper I want to get updates about any event to any of these BZ nodes and that actually sends a message back to the client and the client has a callback function that runs and does that yes so it's a it's a small client that runs on each node right the first one is really just the the node client is there on each node doesn't matter what service as it is it's just sitting there you know at a any interval you want it'll just check its health and make sure things are appropriately you know send information back bank things changed when something does change zookeeper will notify the client on the nginx boxes all of them yeah yep any other question so what I would say to that question is that puppets chefs saltstack is art go to those are used to set things up right they're used to configure the nginx box and set up the the client and there's they're used to set up the nginx nodes and cluster them thinks that effect if you were to run something like this with chef or puppet my feeling is that it might be a little bit slower and might be a little bit more engineering work to kind of do that discovery and figure out what's going on and route things appropriately it's definitely doable absolutely doable but with service discovery the pub/sub is really what makes things fast and reliable where you know I know chef is moving towards a push-pull model Salt's already there I'm not entirely sure about puppet but you know having that that subscribe is really what makes this a big difference all right the zookeeper clusters yeah so it you can connect to any number of them and they all have the same information and they all they have chatter or gossip between them and they're just in a cluster together I think there is a little bit of work to be done around actually having scaling zookeeper the the clustering itself Netflix has an open-source product that does it for you I think it's called Excalibur or okay exhibitor that's right any other questions great thank you guys for coming and listen to me talk 