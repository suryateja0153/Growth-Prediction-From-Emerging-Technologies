 all right so welcome to our webinar docker gives us a measure so what the heck are containers and how the heck do I use them well start by defining micro services introducing docker mezzos and DC us and get into a little more detail about each one so I'm joined by Aaron Williams head of advocacy at mesosphere and I am Ryan Lee cloud infrastructure engineer at 10th magnitude thanks for the invite before we too deep into the how around containers let's talk for a second about the why and I love this quote from Satya it talks about and he's not the first one to say it by the way this is a quote that was have been repeated by many smart people but this talks a little bit to the point of why we're here companies these days and not just software companies are competing on their software and on their technology and that that creates a real opportunity for us we're here to help those companies do a better job of competing not just on the products they produce but on all the applications analytics and software that they need to run their business and so that that means that these companies are really competing on the value of that software and containers and micro services is proving to be the best way to leverage that competitive advantage so that's what we're really talking about here and I think that's a good place to start so Ryan tell us a little bit about micro services sure thing ok so micro services is the new buzzword to get annoyed at for those that don't know at its most basic it aims to simplify your big monolithic applications and break them apart into smaller feature focused services if you're familiar with service-oriented architecture it's basically that applied to infrastructure so these purpose-built micro services usually talk to each other via an application API but that might not be the case of the micro services a containerized application itself yeah and and they're generally built to be lightweight and scalable but there's no industry standard service model to fall back on more just like guidelines at this point so if you find it you're trying to move to the cloud or adopt a more agile development cycle and you're trying to deploy your first application with a two gigabyte installer microservices might make sense in that case but let's take a look at how we evolve into micro services to see some a better picture of the benefits this next picture helps to tell that story so traditionally you know we build and ran applications by creating a small number of very large components or services that were highly interdependent upon each other and these were these were monolithic architectures this this meant that when you went to deploy your application you deployed everything all at once you upgraded everything all at once and the monolithic architecture is really has functionally distinguishable aspects but are not architectural II separate components they're really all inter woven together so what this approach means is that you end up with applications that are very brittle they're hard to scale it's hard to make changes hard to upgrade along the way so what Micro Services offers is the ability for you to break those things down into teams that are focused on specific functions rather than being focused on a an entire application at once and that gives you better scaling because you can actually apply your resources to the specific piece of the the overall architecture that you care about you get faster innovation because each individual piece can scale and and can roll out at its own innovation pace and that creates less risk your lessons oddly invested in a specific piece if you want to come along and replace it with another piece you have very clean interfaces for being able to do that so once you have your apps broken down into these micro services you need a simple way to deploy them individually and that's where containers come in yeah that's right so containerization is designed to allow you to declare the bare minimum environment to run your application well this allowing your applications from unauthorized direct communication with each other but containerization is nothing new it's been around for decades in the form of C groups Linux containers and more recently openvz it's similar in intent to virtualization or you separate your components to make them easier to manage but it's very different and very different on a technical level containers might have their own IP address but they directly share CPU RAM and storage with a host machine and directly communicate with the host kernel so without having to run an entire virtualized operating system for each and every app you can fit a lot more of them onto the same hardware so we've got two diagrams showing the differences between VMS and containers on your left you have the classic model where you've got your cloud or on Brammer you've got a virtual operating system and a virtual hosts or virtualized host or guest operating system sitting on top of that and in that operating system you've installed all of your libraries utilities and tools that your application requires the ones specifically on that VM so in the new model you still have your cloud or on Brammer but sitting on top of that maybe in another VM but not necessarily you've got your docker capable OS and that's just any operating system it's with the docker engine installed and inside of that container running on the docker type capable of s you've got your application packaged up with its libraries utilities and tools into a single file so let's take a look at one specific container technology and that's docker docker is a tool was created in 2013 so not that long ago and originally based on LXC now it's built around the doctor created lid container and docker creates a file system with anything an application might need to run so this is libraries and user land utilities and system tools and scripting language interpreters etc and it packages all of that up into a portable file so that when you run a containerized application using docker on your laptop for instance you send the image to me and I can run it everything should run exactly the same in my docker environment as it did in yours right and a docker container will start in seconds or less since the operating system and container environment are already running you only spend time waiting for your application to so docker containers are usually not to be stateless that is they have no persistent storage of their own they don't store data they only manipulate inputs and give outputs for example managing a login process other containers are stateful meaning they do store permanent data for example a containerized database or a video converter you can can I get around this with a new feature from docker called data domain containers and those are basically just empty containers that only store data that you attach to your stateful docker containers so docker itself does not provide a central data repository for long-term storage but you can keep those data only containers on specific docker hosts and so naturally as darker became more popular a lot of projects sprouted up to handle things like provisioning sharing orchestration tracking logging you get the picture containers are great but they aren't the end of the story yeah I think that's an important point Ryan some container platforms are managed and hosted like Azure offers its own containers as a service called the Azure container service so does ec2 and and the important piece here is that containers are great for running your application but there's actually a lot more that you need in order to make this modern application architecture work and document even been developed a technology that is recently launched called the docker cloud which is based on their acquisition of a company called to them it's a cloud agnostic platform and a dashboard for deploying docker containers now there's numerous other self hosted platforms that have sprung up as well so a lot of them work rather well with each other which actually raises the question what kind of infrastructure we run all of this on well meanwhile at Apache Mesa was being developed by Ben Heineman at Cal Berkeley and open sourced through the Apache foundation and I think the quote here is is good a good description of what Mesa is so that you may chose abstracts the CPU memory storage and other compute resources away from the machines either physical or virtual enabling fault tolerant and elastics distributed systems to be easily build and run effectively okay it's a mouthful but pretty catchy I would say so basically to break it down a little you've got a small cluster of Linux VMs with the mezes master agent installed in addition you have any number of VMS linux and soon to be Windows with the mezzos worker agent installed these agents take stock of the CPU RAM storage resources whatever is available to it and phone home to Vanessa's master cluster saying hey I've got eight CPUs 32 gigs of ram and 100 gigs of storage the frameworks written for the mezes platform are able to directly use any amount of resources belonging to any number of meses workers so to say the least that's an oversimplification there are other components that go into building and maintaining a mezzos cluster yeah but the description is a fair one I think it may so she was pooling the resources of your data center it's making all of those resources available for running applications and that turns your entire data center into kind of one giant computer and maybe the next picture will help sort of illustrate that a little bit more so this is what we see enabling that modern application it's not just that you have micro services in containers but it's also the fact that you have big data analytics etc that help you create an entire application and if you do it right then you also get sort of this ecosystem of apps and a ton of sophisticated environments that are running underneath it and that's what sort of mesosphere is so excited about is is being able to enable those companies to focus on these these modern apps as we said at the beginning and then Saki is quote you know this is really about enabling companies to compete on the things that are differentiating for their business and you know that operating system that set of technologies that manage the compute resources of your data center that's not the the competitive advantage the competitive advantages in having great modern applications and so we really want to focus our customers and focus developers on that piece of the stack I think I think Satya would be proud mm-hmm and to give you an idea so Netflix eBay Twitter PayPal Airbnb Vimeo Open Table just a few companies that run at least some of their applications directly on Nessus but what does it all really mean what are we getting at here some of the frameworks built on top of mezzos are designed for container orchestration so guess what Moses also natively supports as an execution engine docker so I'll go over the components I mentioned earlier that make up a pretty standard mezzos deployment exhibiter is a fairly straightforward program written and open-source by Netflix to simplify the management of a program called zookeeper zookeeper which is another Apache project is designed to provide a centralized source for configuration state data synchronization and war mezzos and it's frameworks you generally use uzuki per to store their stateful configuration data and it's also used in the election process if a master becomes unavailable marathon is your cluster wide and NIT and control system contains the definitions of how to run your apps monitors their health restarts them if necessary and manages your upgrade strategy and Chronos is a fault-tolerant job scheduler yeah that's a lot of components but one way to visualize this Ryan is using an operating system analogy so mesas functions as the data centers kernel in that case taking the request for resources and allocating the correct resource to the correct application marathon takes the place of your init system so for example you know upstart on Linux or the services manager on Windows Chronos is your crontab or past scheduler on Windows and all these components together are kind of like an operating system for the data center and that's where DCOs comes in so DT OS is mesosphere product it stands for data center operating system and these many Maysles components are pretty much used in the same way and in a predictable fashion so the mesosphere DCOs collects these components into a single installer configured using a single file so by default is configured with the best practices that have been learned over years of production use of the DCOs product and that's really important this is not just open-source software it's battle-hardened and it's it's being used in large-scale clusters right that is important so this this platform has been used for years by all those companies today I mentioned a little earlier and they've all some of them have even contributed back into the business project to make it even better so taking lessons that they've learned and the mesosphere team have learned we get this battle-hardened software and it also provides a unified interface providing insight on all the frameworks running on your DCOs cluster it also comes with a sink or simple way to install more of those frameworks so say say i i want to instigate a CI pipeline and i want to use Jenkins I can type D cos package install Jenkins from my laptop and 30 seconds later D cos will download the Jenkins image run it and provide an air phase for me to to log on to it and that'll be running in my PC us data center so I can populate it with predefined jobs from a shared location store it on maybe another mezzos framework like Cassandra or Hadoop which I can install in the same way DCOs package install Cassandra and by including a new name in my deployment I can create as many isolated Jenkins masters as I want all of which have the full business cluster at their disposal which as a note is how eBay provides Jenkins masters to each of their developers so each of the developers get separate Jenkins master yeah that's great and this is also another place Ryan where we can use the operating system analogy so you know for instance if I if I want to run a browser on my laptop I don't tell my browser which core I want to run it on instead I let the operating system handle that and so I really shouldn't have to do that on my in my data center either I should just say I really want running pleased operating system figure out how to make that happen and how to allocate resources for that and that's really where the DC OS comes in this picture shows a slightly more detailed look at kind of how the different pieces of the cluster are set up so you always have an odd number of DC OS masters this is what is doing the primary work of distributing the workload out into the cluster then you have both private and public agents these are set up to actually handle and manage those workloads they can be set up to run containerized applications they can be set up to run services and frameworks the beauty of it is the DC OS masters actually handle distributing out that workload across the entire cluster makes it very easy of course to bring up and scale your cluster up by just adding additional agents also makes it easy for you to manage any faults that might happen across those agents if something goes down it's easy to bring up something new and get it running inside the cluster and by being able to split out the private agents from the public agents gives us a very clear way for the public Internet to get into that that cluster so now let's talk for a second about how operators actually use this cluster right so in addition to the DC West command line the GUI supporting all the frameworks and docker architectures a standard mezes cluster supports DC OS has some other interesting internal features that simplify your application architecture like internal proxying routing and load balancing the killer feature I think in my opinion available now in the private preview was called Minuteman which provides Navy her native service discovery and just vastly simplifies the process of networking your containers together in a very dynamic way so just like all of the other platforms DC OS runs docker and solves the associated problems in its own way but it also allows you to run a number of those platforms directly on top of DC OS yeah that's right so if for instance you like the way that kubernetes groups together micro-services into logical units you don't have to build a separate class to run kubernetes you can run it on top of DCOs alongside with your other containers in marathon the goal here is to use DC OS as an operating system and put your favorite your favorite container orchestration engine on top like so then it becomes a platform to us or for us to run all of these different Suites so for another example if you want a distributed fault tolerant highly available storage you can run a Hadoop on DC OS or if you prefer Flocker which manages those data only containers we mentioned before as a developer you can use Kafka's messaging system you can use SPARC for your large-scale data processing and analytics and as a note SPARC is a big part of Azure hdinsight so we've got that available to us and that's why the data center operating system is such a nice little package it gives you that common foundation in a single distribution for easy installation that allows you to build all sorts of micro services and containers on top and that's why I'm here so it's time for the shameless plug for meso sphere the company was founded in 2013 we're the company behind a handful of open source projects and I think that's really our heritage is in the support and the fostering of these open source projects we combine them together to create DC OS and to make a complete easy-to-install distribution but we're really a company that's committed to these open source projects and committed to the great work that's happening in these Apache projects ok enough of that let's take a look at the trade-offs and the impact on the software dev cycle print circular so great now what would your application and environment and dev 2 prod workflow look like so the current way the old way is easier to understand because it's all we've been doing architecture for a while individual of the ends provision and clusters configured post-deployment you have to worry about and get insight into the configuration process at that point why this work so there's a lot of overhead why would Weibull the server if you don't really have to what we care about are the applications running on those servers and the rest is just ecosystem built around them something dies there's usually manual steps involved in fixing it or requires or extra orchestration on top if your application changes you either need to orchestrate an upgrade plan or wait for the new VM or maybe even entire environment to rebuild but nobody builds infrastructure for the sake of infrastructure we build it to either make something easier faster cheaper more reliable and I think we can hit most if not all of those with this new architecture so it's harder to learn but it's easier to maintain we don't care about the environment at the data center just run our apps everything is defined in code everything is checked into source control - the stateful data like your your converted video files or databases or what have you the entire system could fit on a flash drive because it's just text a developer can push a commit and the CI server may be that same Jenkins server that deployed in DC us earlier rebuilds the docker image pushes it to wherever you store those images and then tells marathon to use the new one marathon can can start that deployment the new container with a rolling update and keep say 80% of the containers online at any given time yeah so if the new container fails a health check then none of the existing containers are killed and none of the traffic ever goes to the failed container so ops no longer needs to worry about building and deploying the developer code they just need to make sure that the services can all talk to each other so that's enough talking Ryan let's let's take a look at the demo great pull that up so this is exhibitor just to give you an idea of what it looks like there's not much we can do here since it's already been configured but here we have the DC US homepage UI gives you insight it gives you a very very clean interface into what your data center looks like right now so since this is just a demo and we have one node connected I don't have any frameworks running except for the default marathon instance that comes bundled with these EOS which I can take a look at here gives me a quick overview and I've got that open up already in another tab so this is actually a marathon this is what it looks like so from here we could see all of our applications if we had any running which I'm going to deploy one right now so remember that command that I mentioned earlier we got DCOs package install I'm gonna install another copy of marathon and I'm going to give it a couple options that I defined a little earlier it's gonna ask me if I want to continue I'll hit yes and pretty quickly we should see our very own copy of marathon running here so that's an entire framework that is now running on top of your PCOS cluster so the application that I'm going to demo is just a simple hello world it was developed by to them for exactly these kinds of use cases and I'll show you how simple it is to configure that and deploy it so looks like our marathon is done here I'm just gonna create a new app we'll give it a simple title like hello world I need to point one CPU share maybe 128 Meg's of RAM and then I'll give it the image name now this is polling from the public docker hub but you can configure your own you can if you want to keep private docker instance in her containers you can tell it to pull from that instead but I'm just going to use the public example we're gonna give it a bridged Network that DCOs will actually decide which port it's it's being given we're gonna tell it the the application inside this HelloWorld container is listening on port 80 but since port 80 is reserved in our case I'm just gonna leave the host port and service port blank and we'll see what happens there and since it's listening to TCP I'll I'll give it that as well so and we're dodging now we had create so that's a simple way to create an application and you can see it's already running if we want to get a little more detail about it we can go into the specific app page we're looking at the configuration this is basically what I specified if you're writing apps or app definitions from hand or by hand this is basically what your JSON file would look like and you'd you'd push that up to the marathon API so here we've got no page I've got up you can see that it was given its own port that is not port 80 that is one six six three seven and the reason it does that is so you can have say five of these running at the same time all in the same host all listening on port 80 but it exposed as a different port to the world so if we go here I gave it that port there it is this is our hello world old container we've got a unique of host name so it's that helps to illustrate that this is a unique instance but based off of the same container so if I wanted to look at another one of those I could get the other port you can see how the hostname changes so again this is the same code same container just a different instance and normally I would have load-balancing setup in here so that I wouldn't have to type in the port number I would just hit a public endpoint and the internal proxying and load balancing capabilities of DCOs would do a round-robin or whatever kind of load balancing that I wanted between all of these five different instances every time i refresh I would get a different one and that specific example you might have seen the URL is run on Azure for my demo purposes getting DC was running on Azure is really easy you can use an arm template to deploy your customer closer which we can share to anyone who'd like to test on their own you can use a chef's extension to automatically connect your nodes to your chef server using the DCO as a cookbook I've written to automate the configuration which at some point will be open sourced alternatively you can use core OS which is just a stripped down Linux distribution that only runs docker and conduct a manual installation you can grow your DC us masters and an availability set and behind a public load balancer and you've got yourself a single fault tolerant highly available endpoint to your system you can also pre provisioned your DC OS workers and use auto scaling to turn them on and off it's necessary allowing you to elastically adjust the size of your cluster saving money since Azure instances are billed every five minutes instead of an hour or per day just a simple diagram of what this looks like we've got a sure running in the background on top of that we've got our DC LS worker which not pictured will connect to the DC OS master cluster for its configuration we've got our App definitions those are the the JSON files that I showed you in the marathon UI we'd have a collection of those and we could just upload those to the marathon API and marathon will then tokens F definitions download the docker image and run as many as we want them to wow that's a great demo Ryan so so just to recap you know I think the the ability to get the service up and running with a single command the ability then to take a simple JSON file and use it to spin up your application and then a simple button click to scale it up to five different instances boy that's the easiest demo I've seen to actually get containers running and make them work so love that demo so just in terms of setting up an azure cluster so you should you talked a little bit about that I think you know it's pretty easy to do if you want to do it on your own but if you'd also like to get the taste of the functionality of Michelle's marathon and docker on Azure you can also check out the azure container service so this is a fully hosted and managed service from Microsoft that runs on Azure it's currently in public preview and it's powered by DC OS so you get the same interfaces the same ease of use in terms of getting those containers up and running so encourage everybody to go check that out if that's something you might be interested in especially just as a as a way to sort of kick the tires it makes it really easy that's that area again since that app configuration is represented with simple JSON you can write definitions for your apps store those in source control say along with the application code itself and have that same Jenkins server we talked about before run a job that deploys every single application and framework that we run on DC OS if you wanted you can run the same workflow on your laptop to have not only whatever version of your project but a good amount of your actual infrastructure as well so this is this is easily virtualized locally you might not have you know 50 ml or DC OS workers but you can at least have what would fit on your laptop to give it a production like environment for you to do your testing on so a couple of the limitations that we're working on right now or that I'm researching and I see other people working on you still have to manage your infrastructure VMs if you deploy it onto traditional Linux distributions for example my my case I deployed this on CentOS and I had to do some some preliminary work I had to disable ipv6 I justit's Linux into permissive mode install a few packages but a lot of the DCOs framework is also available as docker images so that core OS distribution that I mentioned before that's that's purpose-built ready to go ready to run your docker containers so zookeeper with exhibitor or your master your ms-dos master for DCOs those are available as docker containers that you can run directly on core OS without really having to worry too much about your provisioning process database support is maybe not as mature as a traditional VM based architecture but it can be done with working on your data container or data only containers with with flocker or putting it in that central data repository like I mentioned before you can have your dock rice can hear your docker eyes databases have whatever access to data that they need additionally security patches can be a bit of a pain since this docker images are usually immutable meaning after you deploy them you don't change them so if you've got to install a security patch you generally need to rebuild and reprovision your docker images however there are a few other projects that are working on this since since docker containers are basically just a bunch of layers if your security package that you need to install is located on one of those layers you can deploy that specific layer but I'm not sure that's production-ready I'd need to get an update on that but there are there are ways coming up to get around that and here I think we'll open it up the strings unless you had anything else ahead Aaron no I think that's great thanks Ryan looking forward to drilling into some of these questions 