 and to get us started in looking at the technology I'd like to introduce a Gordon wetstein who is assistant professor of Electrical Engineering and computer science and is the leader of the Stanford computational imaging group it's an interdisciplinary group that's focusing on imaging microscopy and display systems he's going to talk about light field technology in virtual reality so Gordon welcome well thank you very much for giving me the opportunity to speak here today as Martha just said I'm the director of the Stanford computational imaging group and most of the projects we work on our computational optics computational imaging for microscopy for cameras but some of our interests is also in the space of computational displays and the most exciting space right now of course for computational imaging computation of this place is on near eye displays and immersive experiences so when I think about computational imaging it's really this bridge between modern signal processing and optimization on the one hand the software and geometric optics wave optics on the other hand that's the hardware so where this intersection of software and hardware and Jeremy was already talking about you know a little bit of the history of VR earlier today and he's been working on this for decades and that's a that's very remarkable I had a in my first immersive if I call it immersive experience in 2003 when I started my undergraduate program back in Germany and what we worked on there was also VR and and and this you can see that here the illusional is what researchers in our group build was basically cardboard box with a hole on top a CRT monitor underneath you had these stereo shutter glasses a stylus and that you saw a couple of Lego blocks that he could pick up and they would fall down and it was the coolest thing I've ever seen then and I just decided I want to do I want to know everything there is to know about this I want to do computer graphics I want to do displays and cameras and it was an incredible experience so now 10 years later or a little bit more than 10 years later you know we think that we're here we think we're living in the future we have had mounted displays and we couldn't create all these experiences but to be honest we're not quite there yet maybe more like here so so this is where we were maybe 150 years ago right this is the stereoscope people in Victorian times and Britain came together they would look at these stereoscopic photographs and hold them up and to be honest you know most of the technology that we have today is still very very similar to this well not quite so over the last couple of decades there has actually been a lot of development so of course Ivan Sutherland at MIT in the late 60s built this incredible system he basically invented optical see-through augmented reality the need for computer generated content he had a tracker on the system like most of the technological components that we now have in consumer products was already in his system and there was quite incredible right then in the 80s there was a big push here in the Bay Area with VPL research and the guys who built the data glove and try to commercialize this entire system and that was really much more focused on the experience so now I think where we are is that cell phone technology has evolved to the point that most of the required technological components of virtual reality are here at low cost but they weren't developed for VR that were developed mostly for cell phones and that includes high-resolution screens low latency I am used we also have fast GPUs that render extremely high resolution content in stereo in real time right so that's great but I think one distinguishing factor between all these components that are just better than what we had before is that we now have the internet also so we can create shared experiences and and and participate in experiences that you know connect us throughout the world so that's something very unique right now and part of the reason why I believe you know we're there almost maybe next year right so in the near future I think all of these will continue to get better screens will be a higher resolution I am used will be lower latency the forum we'll be smaller and so on so the experiences will get better but there are some remaining technological challenges and I'm really just focusing on the technology here because that's what we're working on and and part of that is the Virgin's accommodation conflict I'll go into detail and that what that actually means in a second there's the vestibular visual conflict so motion sickness right all kinds of you know mismatching visual cues or sensory cues that the brain gets that may create discomfort for the user so these are big problems and of course augmented reality has has many more challenges including occlusions form factor of battery life heat I mean you can read that here but I think right now we're almost there for VR but optical see-through AR for wearable displays is still something that's maybe a little bit further out right now so what I want to go into detail a little bit more is Verte the virgin sacrament Asian conflict because that's something where computational optics can really help to mitigate this and I'd like you to really understand what the problem is and for that let's take a step back and just look at the human depth perception there are many different cues that the brain uses to see 3d and many of them are already supported by 2d displays but there there are a few that are a little bit more challenging on the one hand you have these binocular cues stereopsis so every every information that you get from the fact that you have two eyes but on the other hand there's also these focused cues and so even if you just had a single eye there's still a lot of rich 3d information and what we perceive and there are two different cues for each one is a ocular motor cue so a couple of muscles that pull either on the eyes or on the lens inside the eye that send a signal to our brain telling us what the physical state of the eye rotation the virgin's or the focus state of the lens is but that's an ocular motor cue and then you have the visual cues that correspond to that and those include binocular disparity so the fact that you see slightly differently shifted images with each eye and then retinal blur which is like a depth of field effect that you may know from SLR cameras so in the real world these cues all work in harmony our brain gets consistent use from all the different sensors virgins accommodation retinal blurred disparity they're all synchronized together and our brain uses all of these cues to get a sense of you know what's where what's the spatial relationship between different objects in the real world and so these cues match in head-mounted displays today pretty much all head-mounted displays these cues do not match we can create virgins and stereo disparity by showing two different images to these eyes and that allows us to verge to rotate our eyes in their sockets but we cannot create focused cues the eyes are always accommodated focused on a specific plane on this virtual plane that is optically created inside the head mounted display so there's a lens in there there's a physical screen the lens makes the screen bigger and farther away but it is a plane somewhere you cannot ever focus at different distances with your eye and that's a big problem it's not so much a problem for objects that are really far away so when you're in a virtual city you look at things that are very far away and no problem things that are reasonably close like you are to me right now also not a big problem but for this personal space of interaction anything within arm's length things that we can touch it's a huge problem because that's where these accommodation cues the focus cues are most important and if we do not create correct or visually comfortable experiences it may be a huge problem as soon as these consumer products come out especially when you think about games or content where you cannot really control at all times where the content is going to be so earlier this year we presented what we called a light field stereoscope at SIGGRAPH 2015 that's a biggest conference on computer graphics and interactive techniques you can see the demo in the demo area here if you haven't already it's it's a home built display that we you know built from off-the-shelf LCD parts and driver boards that we got from eBay we really printed a housing and my students really worked very hard and putting something together that just looks some somewhat like an oculus rift right and it uses a lot of the same components it has a few extra things in particular we have two LCDs inside and a little bit of a spacing about six millimeters so the form factor doesn't change very much but you basically have two screens inside the head-mounted display and with this simple trick and a lot of advanced computation you can create a light field inside the head-mounted display so what's a light field the light field is a collection of all the light rays that come from the physical world and then impinge on on your sit on on your visual field and in this case it's basically all the light rates that come in into the pupil okay so normally we see a two-dimensional image with each eye and the light field would give us slightly different perspectives over the pupil of the eye so you can control all these different rays coming in a different angle into the pupil so we can see that here a little bit we have these light rays I have slightly different perspectives of the real world they almost look the same to you on the right that's the light field because the amount of parallax and there's very very small there isn't all that much change but there's a little bit of parallax and that's what is it's like a hologram and the real work that allows us to focus our eyes and if we don't not recreate this in a digital display we cannot focus our eyes so what we came up with this is this idea of using multiple stacked LCD layers we developed a pretty complicated mathematical framework around that implemented it on the GPU I'm going to spare you all the mathematical details but the idea is really that we want to generate multiple different perspectives of the same 3d scene and make sure that these enter the pupil at different positions of the pupil and we do not want to use eye tracking so i tracking would be great if we had it but it doesn't really work very well yet so we want to create the same experience without any high tracking we want to get all these different perspectives into different parts of the pupil it's extremely challenging to do that engineering wise but if we can do it then we can focus on our eyes at least that's the that's the promise and we developed this mathematical framework I'm not gonna go into the detail so much here but works reasonably well what what we have to do is we have to render a lot of different views of the scene twenty-five images or so per eye so that's a 50 images instead of two right stereo image player has two images now we have 50 images 25 per eye on a 5x5 grid we run our optimization framework in real time on the GPU and what comes out are two patterns that we show for each eye at these different distances you can see them on the bottom it when they're optically overlaid you can actually focus freely on two objects that are close by objects that are far away and also things that are in-between these physical panels so to convince you that this really works I mean I invite you to come see the demo but what you'll see is something like this on the right where now we take a camera with that simulates the human eye you focus on an object that's close the object in the background will be out of focus if you focus the camera on the back the object in the back will be in focus but the object in the front will be out of focus we did not change anything on the display itself this is something that just optically happens and for a traditional head-mounted display on the left you only have the scene in focus for one particular plane and it seems like a subtle effect but this is what really you know generates these subtle effects that create a much more immersive experience and you may think that accommodation cues are not really that important as we get older you know most of us lose accommodation but you think about the applications for immersive displays it's mostly gaming at least that's what it's being marketed as right now so all these kids who are playing games I mean they can really accommodate it down to like 20 centimeters in front of the eye so that's that's really remarkable and that's something that we can kind of support here so so the promise of light field displays in head mounts is to support virgins and accommodation to mitigate this virgins accommodation conflict create more realistic visual experiences and I predict that at some time in the near future this or some other incarnation of a light field display will be in any head mounted display and it would solve the Virgin's accommodation conflict right now there are a couple of technological challenges in particular we need transparent electronics on the displays to make it get rid of the diffraction artifacts and make it very transparent we don't have displays that have those transparent electronics right now it's a little bit challenging so we thought can we come up with a much simpler solution that is immediately applicable right now and we came up with this idea of monovision so monovision is a technique that's been used for decades in ophthalmology for people who have presbyopia you most of the time they get bifocals so you have little insects for reading right so you look through these insects to read you look up and you can see far but mono vision is a different and alternative technique we have two different prescription lenses for each eye so one eye will see far one eye sees closed and your dominant eye will switch and our brain is incredibly good at suppressing the blurry content so we always pick out the better content and we thought well this could be so easy we can just what we need to evaluate it for applications in VR it's very easy to implement it you just put a secondary lens in front of you head mounted display on the one side but there are potential disadvantages like discomfort do we change the visual acuity is the depth perception impaired or does it even do anything meaningful in general so so what we set out to do is we wanted to test that we built a display prototype that we hacked a dk2 basically we put focus tunable optics into the lenses so we can electronically control where these lenses are focused and we can at any point in time change the accomodation state for each eye separately and we use these liquid lenses that are programmable from up to tune they give us an accommodation range from 10 centimeters to optical infinity and we can either adjust that based on the scene for example if you had a tracking and you know where you're looking at just adjust the focus where you're looking at or we can fix it to a particular object we can set that image different distances at the same distances and so on you could alternatively use actuated screens inside the physical that the head-mounted display as well and we run a couple of user studies one was a user preferences study where we showed people randomly you know scenes that and asked them which one they liked better and the other one was a user performance to study where we tested visual acuity and and depth perception and so people like the national state the best so if we know where they're looking at and we focus the lenses to that plane they like that the best that's the red one the normal mode where you focused on a particular plane is the worst and monovision is somewhere in between so it is visually comfortable we also tested depth perception and visual clarity and showed that it really improves the performance of people unfortunately my time is running out so I'm just going to give you the summary the summary is that for objects at very close distance you can better distinguish between depth between objects it's more visually comfortable the reaction times are shorter you can read very small things more clearly than for the regular display mode so it's really something that can have a tremendous impact on the user performance by simply putting one lens an additional prescription lens in front of one of these lenses and you can see the demo also there we have the light field stereo scope and also the monovision demo I'm I apologize for running a little bit over time here but I'm open for questions if we have any time 