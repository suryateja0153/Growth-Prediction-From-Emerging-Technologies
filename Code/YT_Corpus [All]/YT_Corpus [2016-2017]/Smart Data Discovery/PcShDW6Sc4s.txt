 hi everyone welcome to the talk my name is Chandra and I'm from ebay structured data team so let's talk I will talk about how we are use spark to discover new metadata from ebay listings and this is a joint work with really who's also from a structured data so in this toggle well first I will first introduce our organization and then the problem and the next solution and the conclusions so we're from a structured data team so we are responsible for transforming unstructured listing data to the structure information with semantic meanings so these are the following up like important domains in our team so first one its metadata discovery so what we mean by metadata are basically the important name value pairs on eBay so for example are like our brand is an important name and Apple is a value so those name value pairs are always used for describing those are important information to describing these things so we have this metadata system that's discovered and also a managed the repository and so that we can power the ebay website in a lot of important applications so why one application is like in this search page you can see that on the left-hand side so these are the filters and they are all so they are all powered by the metadata as repositories so these are the important name value pairs that relate to those recalled the listings and also if you go if you want to sell something on ebay you will also have a lot of interactions with a metadata so these are these items specifics are also generated and based on our American system and we also provide those recommendations so the other important domain is listing classification so we have a lot of classification tasks for listings so one important task is listing a category recommendation so given a listing we have this recommendation system automatically recommends the most likely categories for those listings from thousands of categories for user so that a user can only have to go through maybe ten are most likely least there are categories and to choose from and we also have other categories other classification tasks like a taxonomy our classification so basically try to assign predefined taxonomy is to these things like prototypes so the third odd domain is catalog so basically are we creates collect and also manage the catalog system so that given a listing item we can we have the algorithm to map them to the catalog and in that in this case this listing item will be mapped to a certain products and it will enable a lot of important applications such as the fourth one so even sure insights so since we have the other other items certain items map to the same product so we can generate a lot of price related insights for example those are like what's the best selling our products on ebay right now what's the price range and also what's that we will predict the training price for certain products and we also identified deals for them for the product within those products so as you can see that a man so metadata is the kind of one of the fundamental building blocks for all those applications so it's in a so if power suppose are selling and buying and it enables a lot of internal applications so why import ask is to discover a new metadata from from these things we have millions of new listings every day so it's it's very important to keep the repository up-to-date and in the past is highly we rely on human to review the candidates and it is very challenged as you can challenge as you can see in this example so these are all our brands are candidates and many of them are not are not a straight forward at all you have to do a lot of search and to make things even more complicated you can see in this so many in this example many terms they actually are brand candidates for a lot of different categories so the so the reviewers have to go through every category to make sure that this is a brand for this category so basically this brand has Prada in this category so which requires even more efforts um so so therefore we start thinking about using data-driven approach to discover those new our metadata and in this so in this project to our approaches to mine item specifics as you recall so when users are sell something they were they will input a lot of Adam specifics to describe their items and we also suggest them to suggest certain fields to input so in so some so from sometimes they will of course they will make a mistake so if we can basically classify their inputs as as valid or invalid then we're able to discover a new metadata directly from their inputs and we also have the intuition that are actually the supply and demand signals now that we collect from buyers and sellers are directly correlated to how ballads this these are items specifics are for example so if the for forcing for the term that has been used for a thousands of thousands of unique sellers as a brand for this category then it's very likely that it is a valid brand compared to some brand that's where I mean some terms are rarely used um and so also similarly so if the this term has been used has appeared a lot of times in the titles it's also strong indicator and also we have we have the data as we've been doing human or review of for a while so we have been cumulated all those reviewed candidates so basically they are there are decisions whether its approval or rejection or a reason of the region and the reason of the rejections so we can basically utilize those review the candidates as training examples so so we have the hypothesis and we also have the data so basically collect those 35 around 35 south and the human revealed metadata candidates and we associate the supply and demand signals with those candidates so these are the features for those examples so we did prototyping with Python and the results shows that those example methods such as random forests has a fairly good results which supports our supports our hypothesis so we want to move forward to implement them production so in the past so when a common practice of implements those machine learning applications and production is to basically between them out offline and you upload your twin model to the production and also write the prediction components on production partially because so at least for us so the production environments in the resources a very are restricted so it's were not allowed to do a lot of like very intense training with a lot of data and also so and and also like some majority of applications are not on HDFS so people tend to apply those machine learning so just implement on certain clusters and a random on that run those predictions on these clusters but for our our projects so we basically this time our day our aura or HDFS if you still take this route there or will involve a lot of while transferring and configurations and which are very time-consuming so our pipeline will become divided into multiple pieces and and also it's very hard for machine learning applications you always have to run around the model and to see the predictions and the tool error analysis so it's really important that if you can just develop and test on your local machine or on your development machine so then we are thinking start thinking about spark so we've been using spark for data processing for a little while and so we found out it's very useful for data processor transform transformation and especially for those like tabular structured data and so we also found out there's a ml lip which covers a lot of commonly used machine learning packages machine learning algorithms such as a regression classification clustering and some importance of routines like dimension reduction so we we just found out this probably it's an ideal ideal tool for us to implement this production application so and also one important thing is the a melody is very well connected with spark so basically if you can run spark you can run an ellipse you don't have to do any like additional configuration or something and it has very flexible data are access so basically you can run it's on your local machine and you can access a single file or folder or a lot of different are different inputs just want different input formats so and then we just add the way I implement this machine learning system which is really really standard so it's completely based on MLA and so it has three components the first one is featured generation so I basically use our spark to pre-process the data to get the other supply and demand signals and to output them into a into the into the feature vector file and in the training parts we just use the MLS run forest to do the training and the good thing is you can so after training the motto is will be directly saved to the HDFS and you don't have to transfer its at all and for the prediction parts we just reading in the new data and the load the model and do the predictions and also the output we also be a HDFS so this whole project is completely on HDFS and without any a file transferring needed and so here for so much learning part we found it's really are useful to use the pipeline mechanism so basically you can just put your you change your data and the steps into a into a pipeline and then you can define you define your also here the RF is run for that didn't show read it here so and also you define your own evaluator and then you put out define your own parameter grade as well and then you put all those information into a cross validator to let it run and to select the best model so this is essentially the training part of the project so you can see that it is very straightforward so and we did some evaluations and you can see that there are a little bit performance variations across different implementations and settings so but these variations are acceptable and we we found that it's like for it it's hard to design like how many executors we won the same set are not completely correlated with the performance so-and-so ran all we still have limited understanding of the implementation of the ml lab so so still not very sure about this one and also we found that python is slide has slightly better performance than the mo lab so we also in the future we still also need to find out the implementation details and also we are this is the latest rung in production and you can see that the overall the speed we the time it requires is really small so basically so the future generation part we read in like over 1.7 3 billion records so I think it's around 30 gigabytes and to do the like data transformation and the Select training data and the overall just take like six minutes and for training parts you can see that is is also very fast it's under 10 minutes and for ya to do those cross validations and for the prediction it's even faster so it is reading the new data and load to let around the model to run and to make to make the predictions so overall this job is able to run daily on the production even though it's right now it's part of other pipelines so our current schedule is to riots every month so but still it's a speed up the metadata discovery process from like from months two days so previously we spend around the three months to maybe can discover eleven thousand new metadata but right now we can just just let model to do those predictions yeah here are some odds example output so basically in all poodle provides also provide the probabilities so that other so other other teams can also I can rank and decide on like which which prediction they want to use based on different thresholds and also you can see that those newly discovered the printer or not three forward at all you have 24 human reviewers even really have to like do a lot from google search tool to verify them so so to summarize so we found that sparks really are good for data science applications so it's much shorten the development development cycle and it so an MLM on top of it is very comprehensive and it's very well integrated into a spark so you can just write on your local machine on your own you're determining and so the development and testing can be very very efficient and also are we've been using a skullet who are implement this project so the the code is code base is very small it's only around six hundred lines so which is very are very easy to maintain so in the future i think we need a better understanding of those emission learning algorithms that's in how they are implemented in ml lab so that we can know how to better optimize our set setups and so so here i would like to thank our team and and with that i'm going to stop and open for questions any questions it's got to be one question somewhere okay hey thanks so just just to better understand sort of the business use case of your your predictions so you take these predictions of whether a keyword is brand or not and you use that and populate sort of like a like a graph metadata store that used to and from those relationships is that stored sort of as like a ground truth so I get that prediction is like eighty percent that's good enough so anything that's eighty percent like that's definitely a brand inserted or do you still sort of maintain that probability in your in your metadata store yeah so actually we had this quality check before we really load them into the repository so in we have thresholds as ninety percent so you basically we when we do quality check with sample the part of the predictions and send them to the reviewers to review it has to like pass ninety percent precision and so that we can import them so there is such a step so we're not importing everything yeah yeah it's there but our repository have the editorial our tools so if the like many other users found it's incorrect they are able to actually modify them yeah Thanks is doctor how do you deal with like minor variations in the brand like so solve them sometimes we'll have a copyright symbol or this felt slightly different so do you then have some kind of prefabricated ontology to kind of convert these variations to like a root form yeah that's a great question so we actually have this step so we have like we call them synonym list so basically if these brand like yes some brand they have different different form like Ralph Lauren has basil without space or just our RL so we have such a mapping fouls but it's actually not perfect we're still in the process to updating them but yeah we need that otherwise there are a lot of like very sparse the data yeah you mentioned yo yo spark like to develop and test your model so I wonder I wonder when you finish that we were ready for production how to integrate that how do you work with your tag team to use the same code or do they have an onset of code and how do they use your mother to you persist your model and just hand it over to attack I'm just curious about how do you kind of integrate it assigns to work with you know production work yep so actually in this project we are the people who do the do the research and do the like yeah do those prototype development as well as push to production so we don't have this type of like communication going on and for the spark developments are initially we actually give up the Python version to test our to test our hypothesis but we found that since spark can be run locally so as long as your data size is not big so you can basically just use a spark my bro you're even laptop to do the experiment and yeah so for our ship to production basically there's no are much modifications we just around them and test locally and basically just deploy the chart to the production sorry yes yeah but we actually also do the college check after we deploy to production make sure yet they have same behavior alright thanks for the talk but um I have a question about like on the front end like did you have all the data you needed in HDFS wasn't it a challenge to get all the things stitch together in one place so you can run your thing that's like oftentimes one of the major challenges I face in my work can you talk a little bit about that oh well so you mean how I gather the data right so actually we have this pipeline to basically periodically right now it's twice a week to generate to read all those items specifics from our data warehouse so yeah that's how we keep refreshing our candidates and after we get all those candidates we we also have the pipeline to to gather order like supply and demand signal like the title currents like unique seller counts associated with all those items specifics so yeah that's how we yeah I go get the data yeah we actually wrote them I in one of your slides you hit the comparison between site kid and ml lip i think it was accuracy yeah did you use the same data set or yeah how does okay yes no but yeah we found that seems like Emma lab is still a little bit lower when you do that comparison like you can't really use I killin production level so do you even know we just compare its with like different different versions okay yeah because we initially you like it to do the prototype so we just also output the performance to compare it with ml herbs behavior hi so just to kind of piggyback off that question I was curious about your setup I think you did some comparisons relating to performance and I guess when you weren't using spark so I just wanted to know like what does the cluster look like or how many nodes are using and then sort of slightly on a different note going back to the question about probabilities and brands what exactly happens i guess when I just felt like I wasn't sure what the reasoning was like when you say okay the probabilities over point 9 like do you then just start listing that brand is it just like a new branch or trying to adopt and see if it's a good if it will sell good on ebay something like that yep so the first question so about the settings of the environments are we actually I think we our cluster has like tens of thousands of nodes but here we only use like Cabo hundreds of them for the yeah for the for this machine learning application what were you using before stark are before spark actually there are a lot of different ways depends on what your model is so if you're using deep learning we really basically tween them on Threepio machine and upload those matrix to the production and we'll write those matrix multiplication parts in the production to do the computation and if you are using if you're seeing run before now if you're using a logistic regression so we have those ap is in production as a service so can we can directly use them and for others like like other these gbms you so some teams have their own implementation so you really have to take into their code base to try to reuse those so really depends on which model you are using for the graduation parts so basically if we generate those I can the graduate candidates and they are imported into our repositories yes they will be reflected on the ebay website so if you do a search and your records your real recalled items are in this those specific categories then those predicts for example new brands will be reflected on the left-hand side yeah but still I will still need to pass some editorial criteria I believe yeah so after your execution you will like up not if the result is passing ninety percent hope to your score then you were sent to the user to do the review and the user would do some decision may accept or reject the result and how do you incorporate that use the decision into the next run that's a great question so right now we haven't really incorporated them because it's this project just as finished so in the future we really so for those user are like review results we still want to incorporate those decisions into next round of training but actually for our for this evaluation perversa basic just sample small setup the data and send them to the reviewers so I just want to make sure that the the precision is correct yep the amount of data may not be that large by so useful to add there are decisions back to the training data yeah we we are thinking about having this a feedback loop to add them to the production so on the probabilities that you've shown all of them are under ninety percent do you generate a confidence interval and if they're under ninety percent do you then choose a subset where you can't get over ninety percent and actually make a prediction as to identification of a brand oh right now yeah we don't have our confidence interval computed yeah but yeah that's a great suggestion actually we should think about that cool last question I have Kristen about your models so when you test your model are your assigned for any new listing or your assigned probability for each token or your model expect some fries and dinner sign probability Oh could you so first so you have a new listing right and try to identify some metadata so r ER sighing a property to each token two ish word on notice for item specifics for item item specifics so basically the ones that we at least you're asked to like input those things right input those aspects so we basically our model basically are collect those items specifics from users are inputs and use classifier to classify whether this is a violates our Valley Brando ballads model okay yeah awesome well we're about out of time for this presentation but let's give Ching another round of applause 