 okay I think you your start now so a big welcome to everyone who has joined us for big data little data no data who is in charge of data quality which is the 9th in there XE world beta system webinar series and we are delighted to have two speakers with us today we would jointly give us this presentation firstly you have the Christine Goldman a distinguished professor and presidential chair in Information studies at the University of California Los Angeles and who directs the center for knowledge infrastructures in addition to being on numerous panels and boards Christina the author of more than 250 publications in Information studies computer science and communication including the 2015 book Big Data little data no data scholarship in the networked world you know the 2015 American prose award in computing and information sciences and which is undoubtedly inspired the type title of today's talk alongside Christine we have dr. Andrea Andrea Scharnhorst who is head of research and innovation at data archiving and network services Don's this is an institute of nor and no in the netherlands and is a regular member of the exterior data system although andrea has a background in physics and philosophy of science current work is part of the information sciences in particular sharing the cost most no escape action together with christine seems she's involved in a project looking at damned users which I think we will hear a little more about in a few moments time before I hand over to Christine and Andrea a quick reminder that you can add questions to the end of the presentation any time through the question answer panel we use the chat panel for any technical issues so just to me to say thank you again and to hand over to Andrea to start bag so let me do that you alright thank you thank you very much very for the nice introduction and we decided that I kick off our webinar and I'm very pleased that Christine made it she just yesterday arrived from the US so the reason we have set up that is that everybody talks about data as when you go with and a new source for knowledge and it's also an imperative for researchers to reuse data so everybody should share their data reuse their data and it's actually very easy new technologies like Dropbox like Google Drive make it very easy to drop data but it's not so easy to actually find data and then find the right data and it is on the desk of the libraries and the archives network organizations like WDS code data the research data align and try to come in there devote time to sort out actually who is responsible for data creation what would be good practices and as part of that and that is something I see my colleagues at dance and also in European infrastructures also being a bit puzzled as part of that there is an expectation that the archives also take care of the quality of the data similar like the editors of journals duties for the articles now how is the librarian or an archivist or documentalist going to do this this is actually the reason we put them together and again are delighted that Christine is able to share thank you Adria for the wonderful introduction into Rory also lets one more thank you is an order which is to Ingrid Dillo who thought that this would be a good way to kick off John participation in the world greater systems group and it also is a nice way to tie together the work that I've been doing started coming here a couple of years ago as a visiting scholar through the Royal Academy and I started asking questions such as this about data quality and that led to a research project that I back so third or fourth time to work and we'll talk about that a bit more at the end of some the session so here we are we've got the cover slide and I'll just move through them from here let's first talk a little bit about what's meant by data that's probably the hardest part of thinking about data quality is that people don't agree in the first place on what our data one person signal tends to be someone else's noise as we found in the 15 years or so that we've been out of the field studying scientists and people in other disciplines who are trying to make sense of their own data as we followed them around with their data practices this slides with the big data on it that is a definition of big data that's been around for a good number of years now at least 15 and what's useful about this is to show that data can be large in many respects it's not just the volume it's the speed at which they're coming at you the velocity of the speed at which they change and the variety or heterogeneity of the data all of those contribute to complexity and the more complex the harder it is to maintain some kind of data quality another common metaphor you see is this one about the long tail of data and that one is similarly reductionist in that it suggests that some a small number of researchers typically scientists have large volumes of data and many people have much smaller volumes of it but of course any field or any science is much more complex than just those two dimensions the OECD criteria which are nearly a decade that old now 2007 these have been widely disseminated around the world and generally we talk about the data in archives such as those in world data systems or we think about data quality we're concerned with making those data open in some respect or another now open can many things this list of 13 criteria that they put forth at that time suggests how complex it is to make data open you cannot possibly I lose any data set or even the positive data set that meets all of these criteria at once so generally speaking you're going to need to make some priorities and some trade-offs between these things depending on the local circumstances why is it that we want to sustain data or access to research data in the first place well we might want to do so for a number of different reasons first of all if it's observational data you may want to keep it because you cannot reconstruct them again for example you can those Chandra observations in the sky you know certainly you can go back and see things that are stable again but the sky is constantly changing and you can only capture what happened at that time in place by keeping those data you may want them for reference models we found that in the Sloan Digital Sky Survey for example that in addition to using those data as for observational purposes to do other kinds of studies they are considered so accurate and of such high quality that later sky surveys and later projects and people even building their own instruments will go back to the Sloan Digital Sky Survey as a point of reference to calibrate their own particular projects or ways to leave to collect and manage the data reprimand because people don't agree on what's reproduce ability what's repeatability what verifiability and so on and how far back the beginning of project one might have to go to make something reproduce achill most often of all you may want to keep data so you can aggregate multiple sources that would get you to a different kind of way of keeping them to make them more interoperable or ways they could be say combine the next two parts run users in time frame the difficulty of maintaining data quality is going to there we go it would sit still and the lights go off the if you're going to keep them for the investigator that's hard enough but then it's more labor to make them useful to collaborators who may not have been involved in the original collection and making them available third parties is yet more difficult similarly keeping data for some months is of a challenge but keeping them for years for decades much less for centuries is something we don't yet know how to do for digital data so we can simplify the challenge somewhat that's not to say that the challenge is simple in any respect a couple more slides from our own research to show you some of the different kinds of parameters that we're looking at as far as how our scientists make data and use data and evaluate them this particular contrast between big science and little sciences is also reductionist that most sciences fall somewhere in between but rather is the idea that things like astronomy you have projects that take decades to accomplish very large amounts of money and many people are involved in them over large distributed arrays the result is a highly centralized approach data collection the Sloan Digital Sky Survey the large synoptic survey telescope that we're studying you find a decade or more maybe spend on designing the data archive the data collection process to assure that those data can be useful for much longer periods of time conversely if you look at the more salt small science which would be the most of science most of scholarship with small teams and local work those data are collected at individual places in a highly decentralized way where each team or even each individual researcher is collecting his or her data with local protocols multiple methods local instruments and then when we try to put things into data archives later we have to reconcile them at that point that turns out to be a much more difficult challenge that comparison between centralized and decentralized is one of the main themes that we're looking at in our current research at UCLA as we look across a number of different Sciences some of which are more centralized in some more decentralized this next slide is the definition of data that I ended up with in the book and this is probably no 10 or 15 years worth of thinking about how data become data how things become data in different people's work is to say that their representations and something becomes data only when you use it as evidence for your research or for your scholarship what this does is recognized that determining something to be data is itself a scholarly act and that part of why it's so difficult to standardize data production and data coordination across projects how are we going to sustain these data and you see that iceberg on the right where you've got a small amount of data at the top and large amounts of metadata being really sees underneath the real weight of the iceberg if you're getting say a publication a journal article then what you thought what you want to do generally is to read it with your eyes although nowadays people are going to do more mining of those if you're getting a data set it's not enough to have that rows of numbers in your hand you need to be able to identify what they mean identify the form the content more context objects that are related to them code books protocols instrumentation software particularly software we are finding in our research that if you don't have the software you don't have the data generally and there's so much customized software that it's part of the difficulty of data preservation is preserving the software in some distant ways and again that's one of the research questions that were pursuing in the UCLA work from the center for knowledge infrastructures this line Susie is from a project that is currently in process called the stewardship gap and the URL and the site are there on the bottom this is a project funded by the sloan Foundation led by myron gutman of university of colorado and Francine Berman the head of the research day Alliance to look at what are the gaps between the way stewardship is done now and what really needs to be done that for long-term sustainability and this I'm an advisor to the project we just met in New York last week there of this is based on oh say 15 or 20 interviews of the first round but already the interesting model is emerging that the way people judge value has at least these half dozen different dimensions the first one being culture what are the community norms and goals around what data are worth keeping and why how much knowledge of the community have about stewardship we've often find for example that people don't know the difference between backing up data and stewarding it or curating it for the long term responsibility is something that also has come up a great deal in our UCLA research that data often aren't deposited because the team can't decide who's really in charge who should take responsibility for doing so who is the author of a journal article is something that is negotiated but the control over the data less so whether long-term commitments are made you know people say I preserve my data I back it up but that's very different from saying I had a commitment to transfer it to an archive for example one for the world data system the resources vary wildly not just the money but the skill base and what kinds of actions are being taken all of those come together to think about value and data this is going to be a a useful framework as we go forward you can to examine data value and data practices the next slide about when to invest in data is a very library centric view that suggests that we have a virtuous circle around the production in use and reuse of data that people plan projects they implement they publish they preserve and that we used in to go into other projects this is certainly an idealistic model we don't have as much evidence as we might like that this happens consistently what we do know is the earlier in the process that people begin to take some responsibility about long-term investment in their data the more likely it is to survive so if you begin by laying out your code book in your protocol in such a way that you're collecting data consistently and storing them as you go that is more likely to begin to promote a large a longer-term cycle it's an interesting contrast to this next one that I grabbed from a finance group at University of Michigan which is again one of the major research universities and from a finance and an ethics and a compliance perspective the project starts with finding funding and ending ends when the grant ends you close out the project you're done and they go on to the next one so that isn't model that says financially that's all we care about there's nothing in there that supports data stewardship so that also tells you where the value lies depending on the different stakeholders so we would want to negotiate different stakeholders for different roles now let's spend a few minutes talking about the project that we have underway here at done when I came here as a visiting scholar initially of and start asking questions of which i'll give you in a moment it turned into something from an interesting conversation into something that really was worthy of substantial time investment which we're now doing on two continents it also hope that we're building a model that will allow us to send us that other people can use our protocol and make those reusable to compare across different archives so if the two of us vandenberg and Peter Dorn of add-ons malena goals and Ashley sands two of my research team and then two other well-known scholars represent the sample in a neutral or been working with us on some parts of this project also the next by the research question this was really the beginning of the conversation when it came to me in comparing what Don's was doing and the archive but a particularly unusual archive in that it does have a research operation wrapped around that they do a number of interesting projects in addition to the usual roles of collecting data and serving data is to say you know where do these data come from who are these people who contribute their content and their their life's work to this archive and when do they do it it was dates the project why do they do it how do they do it and you know what does it get them one of the effects and the reasons and similarly who acquires or consumes data who are the people at that end and when in their save the project do they do it why do they do it in what becomes the data what power they willing to exploit them we also have questions about the overlap between of those two groups is it the same people who contribute and consumed with a completely different communities and then thirdly and this is one that job Andrea and ended troll or really cold out into the for is to look at the ball the archivist play and we had been looking at the archives of it more as a black box and not as a very active environment which was seeing that it's that it is of how much time is being spent by those archivists in soliciting data in managing them and curating them and then reaching out and matching I data to detention users so the the model and he'll take you to model on the next stuff this diagram here on the next page that is that we're seeing the set of relationships between the arguments as the bridge to both the contributors and consumers Dawn's being the archive and actually particular parts are done in between and that you've got these data sets that would move over time from being contributed to data archives and being selected for reuse at the other end so we want to look at both ends of those processes in much more depth about where those data come from where they end up and how and why they get there i also want to reiterate as part of this study that we are not doing an evaluation of don's rather don's is very graciously agreed to be a site for case study also that we have been working through the web logs and that's the part that had been the birth brings to this mystical er project and we're finding that the web logs were designed for managing system uptime and system security they were not designed for doing really good research on so it's been a considerable amount of light which is coming up with a way to get a sample from this process and we have a couple of posters and a short paper out already talking about some of the difficulty of how you use things like web logs to figure out who who your users are so we're hoping to make contributions that have several stages of this process so far we've interviewed all of the archivists and about half of the contributors and have the consumers that we want to Andre and I are going to get more of our interviews done this week and next week live in the Netherlands and over the summer we hope to do a good bit of writing so you can look up we hope to say next fall or so for fuller discussion but you do have a couple of short piece because two posters at a cyst this past year for simple work in progress this other sub set of slides that I do want to spend a couple of minutes on is to think about the economic so I spent a good bit of time in the book about the economics of dating use and access to data and the data archives in data quality and just plain stewardship the rows and columns come from fairly classic economics the exclusion whether it difficult or easy is you know you can't people keep people out of beautiful sunsets are beautiful beaches very easily but you can certainly keep them out of libraries and you can keep them out of a paid subscription journals the subtract ability or the rivalry is what happens when someone has something does it take away from the other and information goods are generally fairly low a nest of that spectrum because more people can use the same information and it gathers value rather than reducing value common pool resources are those in the top right corner where difficult to exclude and the the rivalry is considered high a countable resources or anything that really needs to be governed and that has a problem with what's called Free Riders that is if you just put them out there at no cost people will will take and they won't have much incentive to give how do you find a way for people to give or contribute on an equitable basis who gets to govern the process who gets to make the decisions and how much money is going to get paid by each group to keep things like individual data archives or the dems or other members of the world data systems or coordinated and that that approach appears to be the most viable for the community in the long run rather than saying going completely public goods where you just throw things out there and they you can have them that we that's not work very well the club good locking them down and paying a subscription to them is doesn't appear it's going to work that well and a whole lot of data are simply a private good there are things that stay inside people's offices on their their spreadsheets on their computers and never really gets get shared it's the common pool resources that were concerned about no data is the third part of the title of the book and that's the case where data are just playing not available for use maybe they were never captured well enough that they could be released maybe they were in fairly good shape they were not released or maybe they're released and out there in our crime but they're not usable the metadata is not good enough the software is not available whatever code books aren't clear enough whatever other reasons and probably the majority of change research data would fall in one of these three categories relatively little appears of all the scientific 8b produce are actually readily available what we really want to be doing is thinking in terms of building knowledge infrastructures and infrastructures are not simple they're not clean they're not neat they're much messier like we have in this image of a workshop that we did a couple of years ago with funding from the National Science Foundation in the alpha P sloan Foundation that shows you've got lots of string and baling wire old technologies and new technologies and a lot of people working together just to make those pieces spin so we're going to have old technologies and new and old standards and new as we go forward lastly here is the outline of the Big Data little data no data book and the first part of it the first four chapters really lay out the framework the provocations why we're having this conversation now was changing the world of scholarship frames the idea of data scholarship and particularly the diversity to show how different data are from one field to the next and one one domain to next how difficult it is to get common standards the middle part has extensive case studies in group by the sciences the social sciences and and the humanities and those range from sensor networks and astronomy through to museum collections and the PISA Griffin in our history and Buddhist Chinese Buddhist swallow G so they sort of something for everyone in there much of the book was written when I was in residence at the University of Oxford at Bailey oh the Oxford internet institute and the you search center and many that the case studies are largely drawn from some of the work going on there in combination with our last decade of work at UCLA the last part is bring it all together and thinking more broadly about behavior issues and economics issues of releasing sharing and reusing data the credit attribution discovery this is a important area and things like software citation in data citations are we concerned with those to give credit to people to attribute the source or to improve the discovery of data and lastly what to keep and why and that's where the real value questions can tend to come in is the the scientists and scholars aren't sure themselves what's worth keeping they often say I don't know why anyone would ever want to use these again they often defer to librarians and archivists and librarians and archivists tend to say that's a domain decision not a professional information for decision it shouldn't be our choice to make it so there's a lot of circular conversations going on we're at a critical stage here and much much talked about so I hope that the book proves useful to a broad audience and very nice reception of multiple communities already so let me stop there and Andre has been taking notes you may have questions you to add and Rory said he was going to combine the the questions from our attendees which apparently is a record-setting numbers delighted to know so all right Rory's it back to you a back to aundrea on well first can get me thank you very much the wonderful and very interesting talk and you're exactly on time so except even better so we have about 15 minutes a laugh for question and answers in fact and we've got only one question at the moment so I assume people are still having a little thing so we'll maybe let them think a little longer and while they do that maybe if andrea has some word she'd like to add then that would be very welcome Thanks what's the other fellows room I had Christine when you were talking is that to compare the centralized approach to the decentralized to decentralized approach and we add dunce we are situated in the social sciences and humanities but in those areas require cover various communities so we are kind of in the middle of that what would be the bottom line was it would be the minimum of things we can do to actually ensure a good description a good documentation of the data we have what you think is the way to go well done definitely falls in the the decentralized range because you are a fairly broad kind of archive insurance the number is the kinds of disciplines and an array of sources of data that that you have and give it given that limit finders over here they'll turn this to you for your attention and given given the breadth of kinds of data and users that are involved you have the difficult problem of having individual data sets and there's also common with things like icpsr the in the University Consortium for political and social research is they are focused on wrapping a whole code book and software around owned the data but they don't expect to really be able to merge different surveys directly whereas the Sloan Digital Sky Survey they spent a decade producing one very great archive but even there they release it a decade at a time sorry of one about one year's data release at a time your question was more about like what's the appropriate lowest common denominator and that's I think one of the biggest challenges that we have is certainly there's some basic cataloging about code books and software and tools that they all have to have but you know at the bottom level you would want something very specific for archaeology very specific for statistics very specific for census and as a higher level you'd want the bill just throw it all in and make sense of it later the sweet spot 20 somewhere in between at the level of the level of abstraction this meeting last week at the sloan foundation headquarters in New York's and stewardship gap Vince served one of the key and God con were the ones that wrote tcp/ip in sort of the basic layer for Internet protocols and dr. Cerf was comparing the digital stewardship problem to what they had done with the original protocols for the internet and he said that we had these four different you know packet switching and radio and others and they decided what they need do was go up a level of abstraction not build on features unique to each of them but a layer that would work across them and what happened several decades later is all four of those who fall in a way and yet the broader abstraction layers they built has lived on and with a very interesting conversation about whether we can think of digital stewardship is something comparable to finding the right layer action okay more question and answers in the chat yes you do so we've got a couple of couple of nice questions here so let's let's have take these so we've got the first one is from David Maroni and the question is with regards to software being essential for data preservation which type of software is being considered data program processing algorithms weed software or both I would say all of all of the above there's some very interesting projects of David if you're interested in that particular problem being led by a group at Carnegie Mellon University I led by Jim herbs lab and some related work at university of texas at austin by jim how James how lessons group where they're looking at how the scientific community works to sustain its own its own software some of the things that we have observed in our studies of science is that researchers will make minor changes in an algorithm with every single run of the instrument and they often don't keep great the track of them so it's very difficult to reproduce or reuse the data because you've got to know what change with each with each delam element as they go the the workflow processing things like taverna and the work that carol vogel at Manchester and Dave de roar at Oxford are doing where you can start to capture them the workflow as you go in a machine-readable way those are important to capture I think it's sort of all of the above this is the problem maybe I can maybe I can add to that David so we are currently in the research infrastructure in the Netherlands called Claudia which is which stands for a large infrastructure research infrastructure for Social Sciences and Humanities the kind of combination of clearing so the computer linguistic part which are pioneers we have an automatic system which things on its own there mbn value so the digital humanities branch in Europe and we are looking into software sustainability as part of or connected to data quality and data sustainability and then it's very useful to look into software sustainability criteria in the software producing software engineering kind of branch and the UK institute for software sustainability is it's a beautiful shores of all kind of guidelines and practices and they very near also work with industry standards but at in practice I think what we also see is that if software is really used there if tooling is used then that's the best guarantee to actually still have the tooling and the data connected to it or produced with it actually active and what we trying to do is to think about selection criteria what kind of software we wanted to have as meticulously documented as possible and what kind of software to say that's experimentation that's kinda lab experiment you build and then you deconstruct the gang let's go on to the next one so it only got a few minutes okay so the next one is from someone I'm sure you will know well jean-bernard lynnster he asked the book only discusses scientific data social sciences and humanities it seems that health data particularly public health data should be part of the mix only two percent of all clinical data ever collected are actually contributed to trusted repositories well um thank you for that John Bernard the I have lumped of health data under the sciences so it's really sort of science technology medicine and health in there and not going into much depth on on health but I do have some things in there from say the structural genomics consortium that I was working with out of Oxford and then the other of course there's a lot of good medical work going down there also I think it's the the two percent of clinical data are actually contributed that's a very worrisome a very worrisome number of course and in a big push I was at the Library of Congress on Thursday and Friday and we had a symposium on some biomedical research there and I asked those clinicians from parts of our National Institutes of Health about their practices on sharing a clinical trial data and I got a surprisingly defensive response in that on the one hand they acknowledge that there now required to release the clinical trial data on the other hand they have one set of systems for intramural data another set for extramural data another step that they fill out for their human subjects protection another that they fill up their grant proposals and they can't even get interoperability and release between the NIH ones that they're working with which really sets that surprised me but but on the other hand given our experience in other kinds of Sciences where people have a hard time even working side by side and getting them to interoperate is not shown so I guess it shouldn't be that surprising it's very it's very difficult to get data in a form to submit and even once you do get it submitted of people may not trust it or be able to make enough sense of it yes certainly from a wbs perspectively we have a lot of interest in the health sciences and health data and it's a very very tricky problem and when we hope to look at and moving to more in the future which again as regard that I would say to the people are online watch this space because we hope to have some some webinars that actually address this in the coming months so hopefully those are you live an interest in that area will have some of your questions answered so you're almost out of time I think so do you have any last words Christine or Andrea from you from your side is actually lori i would like to answer that question from Hank goergen if I might about the the conceptualisation scientific article based on a certain data set and whether that should be linked as the metadata the short answer is yes absolutely the journal article is very important not only metadata but that is the framing of the argument and scholars make arguments using using data journal articles are much more than simply containers of data on the other hand I think the role of archives is much more than just taking data set they're attached to individual articles we find that you've got a many-to-many relationship between a research project and journal articles and data sets people will can collect data for decades and slice them and dice them in many different ways for individual articles so that one-to-one mapping will not will often not get you something that's aggravating anywhere near a true represen so I think we need to be in terms of many different kinds of models could I could I add something to Peter's question because I'm also often I also sometimes think so where are they where are the beautiful building so where is the stability in all of that and I think body what would the challenge we are kind of facing and that's probably also a common sense is that is not so much in the building i mean digital data are not in the building they're not holding the building but there needs to be some stability and that's also in terms of skills and capacities so if you're kind of mixing the people around all the time yeah if you don't have a professional education which gives you gives you a kind of the good a good diploma or degree that you actually know what you are doing if we if we lose this if it becomes L that that let's help on want of our shop then we losing it all if that needs to be in one institution in one building with bricks around that I'm not sure about that but i think is killed we should defend like a fortress what do you think i think that is the knowledge infrastructure the knowledge infrastructure is the people it's the computing networks it's the set of skills and it's a set of social conventions that go with this and so we're very much in a state of rethinking the whole infrastructure and that's the the argument of the 350 page book is really walk us through Howard a stage of rethinking the knowledge infrastructure for scholarship and where data and publications fit into that and what the next generation is going to have to solve so that's probably a good last comment is anything else you would like us to cover all right no I don't think that's about my job and be supportive his question I assume that you were addressing now which came in lots of lights at the last minute we've got that collect yes yes yes and now rod is welcome to mention to the edge because I guys could be people online can actually feel it so the actual question was considering that we live in an increasingly networked world with many distributed systems being established or conceptualized what are your thoughts on the future con of the concept of centralized where will bricks and mortars in bits and mortar institutions fit so yeah that was that was what we were just discussing about so thank you very much for but seeing that one it was that it was a slightly delayed coming up on my list better go maybe that's a nice point to stop there already so very slightly over time so it just remains to thank you both once again so thank you Andrew and thank you Christine for again an incredibly interesting talk and thank you to everybody our audience who joined today I hope you also find it of interest and we will obviously make sure that this is archived and put online afterwards and notifications will go out about that and so yeah it's so i can say thank you so much okay thank you I from Amsterdam haha bye bye everybody thank you you 