 alright good morning everyone thank you I was worried that you're all morning the person we left at the end in Florence or something so this is the last lecture on network programming and the plan is to talk about implementation issues so essentially what we've done is we started out by looking at the kinds of machines that were programming when we're programming network and then we sort of jumped way up and designed this higher-level language for describing network functionality and today we're going to sort of bridge the gap between those and describe compiler and a runtime system that implements network programs on top of those machines so this lectures a little bit sort of disjoint either just sort of these two halves so I'll start by talking about compilation but the way they fit together is if you recall the way that we build applications using netcat is we actually write a sort of driver application in a general-purpose language like Oh camel or Python and what that driver application does is it receives various kinds of network events the kinds of events that open flow has and it generates a stream of netcat programs each of which describe the static configuration of the network at one moment in time and then each of those net programs gets passed to the compiler for the network line for the net net cat language which produces configurations for the tables that the switches implement and then in order to handle dynamic changes between adjacent netcat programs we also need a fairly sophisticated runtime system that will sort of transition us from a state where the switches implement the current net cap program into a state where they implement the next netcat program so but we'll start by focusing on just the compiler so basically we're going to implement one program on top of on top of software-defined Network and and that's it if you recall back to when I introduce netcat i showed you sort of the languages features in sort of three levels so we started by writing what i called local programs these are programs that didn't contain the dupe construct and they described just the input-output behavior of a single switch and then we described global programs which can just wiccan which can specify sort of network-wide behavior in terms of regular expressions and then we want to step further and said well we can even write programs against virtual networks that have some relationship to the physical Network but the program can be expressed in terms of some different structure and so our compiler will sort of follow those three levels as well so the compiler is actually a pipeline a virtual compiler composed with a global compiler composed with a local compiler and I'll build up from that from the bottom so we'll start by describing a strategy for compiling local netcat programs into forwarding tables for the switches and the key feature or the key merit of this local compiler is that it's fast and you might wonder why why is compilation speed so important in general we don't really care how long our compilers take as long as they finish in the reason amount of time well network programs are sort of simple like nut cats not true and complete but they can be very very large if you're describing the configuration of you know a data center network it might have thousands of switches and hundreds of thousands or millions of of rules and the necap programs can be very large and before this work the state of the art for a network of that size was it would sort of take tens of minutes to compile a program and this compiler gets it down to a small number of seconds and for smaller programs that basically completes instantaneously then we'll build up and look at the global compiler and this will actually just take an arbitrary netcat program so one that includes dupe and it will do something I call local ization so it'll take the program and turn it into a more complicated local program that can then be fed into the compiler and the key challenge here is that if you recall from my examples implementing two tunnels on a network in general its implement a path you need to add some extra state that is actually carried by the packets and matched in the tables so the global compiler is that is the component that's responsible for managing all that state has to figure out what state there should be how the state should change as you move between devices and then I won't do this in too much detail but I'll just sort of sketch how virtual compilation works at a more technical level just to give you a preview of where this is going the key idea in our local compiler is to have a sort of smart data structure for representing local programs and we call these f dds or forwarding decision diagrams they're based on a data structure called BDDs or binary decision diagrams but are generalized slightly for the networking domain and the advantage of using these structures is that some of the algorithms that we need for compiling programs have been studied before in the context of BDDs and give us asymptotically better behavior than the other Occam's algorithms you might use if you're using different data structures the global compiler is actually going to use the automata that we develop last time so the way that we're going to keep track of the states of the packet as it goes through the network is we're going to produce the automata representation of our program and then the states of that automaton will become explicit values that are sort of carried throughout the network so we'll sort of compile the automata into the network and then the virtual compiler actually uses a sort of approach you might you might call it synthesis so we sort of formulate the problem of finding an implementation of a virtual program as a kind of two player game so one player is the virtual network program that is moving packets around in the virtual topology and the other player is the physical Network that those moves and then we use some standard algorithms for finding winning strategies to those kinds of two player games to synthesize the paths in the in the physical network that that correctly and let the program okay so that's a high-level view let's start and look at the local compiler so just be precise the input to the local compiler is a local net cap program so a program that doesn't contain dupe and if you remember such programs denote functions from packets two sets of packets we can sort of ignore the histories because there's no dupes so we never construct a history greater than size one and then the output is going to be a collection of flow tables or forwarding tables one for every switch in the network and the main challenges are this has to be fast because the perms are huge and we also want the tables to not be too large in general there can be many tables that implement the same netcat program and we'd like to produce optimized tables because the memory to store these tables is sort of a precious resource on most witches so let me first describe the sort of traditional approach to doing local compilation this is essentially what all of the high-level languages for Sdn programming that have been produced before this work did and that includes our work and also work by people that at some other universities so the basic idea is if you have a program at the top level it has some one of the one of these composition operators maybe it's the union of two smaller programs so here I'm showing the example we talked about last time where we're doing routing and monitoring and so on the the traditional approach would sort of recursively compile the two sub programs into forwarding tables so we can do that here here the routing program is forwarding sorry there's a typo on on destination IP addresses and the monitoring component is taking all SSH traffic in both directions and sending it to the controller and now we need to somehow take the union of these two tables and I showed you some examples two lectures ago of why this operation is a little bit complicated and if programmers have to do this this kind of weaving of tables manually it's it's kind of not very intuitive and tricky to get right but a compiler can do that but the problem is it what it has to do is it basically has to do an all pairs intersection between every rule in the two tables so it has to consider every possible interaction between the two tables and that amounts to basically intersecting the patterns on both sides and then taking the union of their actions and then doing some simplification because of course some rules may express incompatible constraints the intersection is empty and so you read them off so this leads to a sort of algorithm that's easy to formalize easy to prove correct it's a simple recursive traversal of our of our program and you just have to prove that this operator you know correctly implements Union on tables and similarly for the other operators like sequential composition and star the problem is that if you're doing a quadratic operation to compile every note of your ast the complexity sort of blows up very quickly no oh yeah I'm not so yeah let me I briefly mentioned this last time but I was not very precise so netcat allows you to have these kind of we sometimes call them pipes but they're sort of pseudo ports you can give names to ports and you can move a packet to one of those pipes and then there's a channel on the controller that you can then listen to that receives all those packets so on the controller there needs to be some residual code which I'm not showing you here that takes all the packets coming from the controller and figures out it's sort of multiplex as those packets on to all the pipes that might want that packet so on the controller we need to we need to keep track of like console should have been for these four packets matching that predicate on the switch though we don't we don't actually I mean I see what you're saying you could you could tag the packets with some encoding of console and then not have to do the analysis on the controller but it's probably actually a better idea just to use controller to send them up and then do a little bit more computation on the controller you've already you already kind of lost the game in terms of efficiency when you've gone to the controller doing a second classification I think unlikely to increase the latency by too much but yeah this is just the table on the switch okay so but the big point is that you know doing this quadratic operation at every level of the tree and the table is getting bigger and bigger and bigger the complexity is really bad so our idea is to not use tables as an intermediate representation and instead to realize that make sure you go back if you if you think about what a table is really doing its kind of doing two things so first it's classifying all the incoming packets using a set of of predicates and then there are these actions that are sort of attached to each predicate but the actions are are not so interesting there you can just sort of remember that Association kind of the real work that's being done is this boolean classification and so if you look in the literature you know there's been a lot of work on efficient ways of representing boolean functions compactly and one of the most famous data structures out there is is the so called boolean decision diagram or BDD so I think some of you have probably seen this move you haven't the idea of the BDD is that you have this kind of dag and what you're trying to do is to take some boolean expression over some set of variables and then represent some function of those boolean variables so it could be you know you could negate variables you could take conjunctions and disjunctions and generally have some expression so in a BDD what you do is you let the nodes of the diagram stand for each variable so if i had an expression over variables X Y Z I might have X to the top then why then Z and then each node has two children one that represent sort of taking the true value for that variable so the true valuation of one that represents taking false and then at the leaves you have the final output of the function which in the case of the boolean function would just be true or false so that's the basic idea of a BDD and then the reason that they're so elegant and wonderful is that by imposing some constraints on the representation you can end up with really nice properties so for example if you order the variables in the same way always or at least for a given application then the BDDs for for a given function will be canonical and that's that's a really nice property to have and you can play some other tricks like and I think you even see that here rather than sort of just taking the full binary tree that you that you would have sort of naively you can actually share nodes so if you ever have part of the graph that's isomorphic to some other part of the of the dag you can actually just replace them by one copy and and and share them and so that's how you get these sort of compact properties and it turns out that operations like taking unions are actually instances of algorithms for BDDs that have been developed before in this case the so-called apply algorithm and basically what we can do is to extend from BDDs to what we call forwarding decision diagrams we generalize both the internal nodes and the leaves the internal nodes now represent tests and the leaves represent sets of actions so for example the top node here might be you know the test for IP destination being equal to 10 dot 0 dot 0 about 1 and the actions might be drop or port part of the sign to and so now you can do a pretty straightforward sort of recursive traversal of the two trees that you have and because we keep the tests in a particular order just like in BDDs you can sort of efficiently figure out everywhere that the two tests might overlap wherever they don't and produce a new FD the that implements the union of the two and then once you have the fdd representation of a local program it's actually pretty easy to read off a table from that representation the table is a more constrained representation because we only we sort of don't have true and false at every step we just have this behavior where we sort of omit the rules and fall through and all of the tests have to be positive conjunctions of of tests but it's pretty easy to sort of traverse the tree and produce the table and I'll show you how that works in a couple slides okay so i already said some of this but just to be a little more precise so a forwarding decision diagram is just this dag based representation of a local function a local neck i program rather where the nodes have tests like here I'm matching the TCP source port here I matching the TCP destination port and then I'm drawing the solid line to indicate the true value of that test and the dashed line to represent false so you can see that I can if I test that the source is 22 then I fall through to the action that sets the port to console and if I if that's false then it could still be the case that the destination is 22 if that's true I also go to counsel and otherwise I just drop and again I don't have slides to show this but all of the standard neck had operators including Union compositions dollar negation can be efficiently represented as traversals on on this representation in particular one thing that's kind of important is if we want to evaluate cleani star we need to find some fixed point of the composition of a function with itself arbitrarily many times can anyone tell me why fixed points are guaranteed to exists for local programs so if we write if we had dupe then then the output of a program could be arbitrarily large could be infinite but without tube there's a finite number of packets really large number but there's a finite number of packets and so therefore there's a finite number of sets of packets and all a local program is doing is giving us a relation between packets and sets of packets and so therefore there's only a finite number of possible relations so even sort of naively if we it and actually one more thing and because of the constraints that we have on the representations of F td's like we order the variables in a particular way and we share isomorphic sub graphs although we don't have the same canonicity property of speedy DS we are still guaranteed to hit the fixed point even just doing sort of naive unions of compositions and that's not the case with tables so with tables there can be there can be unbounded ly many representations of the same function so if you were to do some naive fixed point calculation you could very easily loop and actually the first time I tried to write a compiler for local neck at it was like oh we're using tables will just iterate to a fixed point and then I one of my unit tests that quick check found for me actually loop the compiler so that wasn't good okay so that's so these activities are are are nice and then again some of the sort of compact representation properties you get for me d DS carry over to the FTD case you might wonder why we don't literally use BDDs this is something that we wondered and in fact because all we're doing is encoding a relation you could you could just use be these to represent local neck I programs but there'd be a pretty big blowup so one source of the blow-up would be just sort of being able to represent tests and we actually have slightly generalized tests can do include things like prefix matches and in general you can have sort of a lattice of possible tests so we can get some compaction by representing the test symbolically and the other reason is that since we're essentially encoding a function which can be viewed as a relation encoding relations as binary functions in general is not is not super cheap especially if your function or relation is the identity almost everywhere and that's the case for neck at programs like here you see that most of the fields are not touched here we're met we're matching on to fields but we're only modifying the port and so if we modeled this as literally a relation encoded as some kind of boolean expression we'd have sort of the input we have some variables to represent the input values and maybe some different variables like you know X and X prime to represent the output variables and then we'd have to constrain that all of the fields that didn't change are in fact the same you know switch equals switch Prime and IP source equals IP source Prime and so we'd end up with actually much larger diagrams because we'd have to sort of explicitly throw in this identity behavior on all the other fields and with these FB B's we don't have to do that basically we're building up this sort of representation of a table and we just interpret the actions after doing the classification and so the trees tend to be smaller the trade-off is that we do lose this property of canonicity that BDD SF ok so let me show you how we can go from one of these diagrams to a table now so assume we have run our compiler and we've gotten some potentially big fdd here's one that matches on five fields and for simplicity I just have true and false it believes essentially we can do is we can go on a traversal of the tree starting at the root all the way to the rightmost path to the leaf and that rule becomes our top rule in the table so basically following all the true branches all the true choices rather all the way down to the leaf we want that rule to be at the top because again we can only express positive conjunctions of tests and that's exactly the case for sort of for all of our tests that's the thing we want to match first and then we can take the next path in a right-to-left reversal so we take you know all the way down to the all but last node and then take false and then now we have a second test here and then we take the true branch there and so that's what our second goal has and then the third path would be down here and then here and that's the third rule and it's pretty easy to prove by induction that this gives us the table that represents the same function as the FTD question yeah I think that's right yeah these examples are also sort of you know baby examples meant to illustrate the the core algorithms I guess one thing to notice is that order does matter so this is off this is the case with BDDs as well different orderings of variables can lead to quite different representations so the candidacy property I mentioned is only having fixed a variable order and it's actually computationally hard to find the optimal ordering for a given expression in practice pretty simple heuristics tend to work well and we have the same experience with with our compiler so doing things like looking for frequencies of tables that of fields that are matched or putting the switch test high up these are often good ideas okay so that's single table compilation I haven't mentioned this yet but a lot of switches actually have internally multiple tables and you can get great improvements in terms of memory usage by exploiting those multiple tables the catch is that the multiple tables that the switches support do not allow you to match on all predicates so you might have a table that can only match on IP addresses and maybe has room for tens of thousands of entries and then you have a another table that can match on mac addresses and maybe again has tens of thousands of entries and then you might have some actual table that can match on all of their fields but only has a few hundred entries so another thing you can do and our jingu I had a undergrad ru student from Grinnell who actually implemented this is you can use this tree representation and start sort of munging the tree to pull out these different types tables in order to shrink the overall representation so if you have just if you have one enormous table this is a fine way to go but you'll notice that sort of as with things like the same issue comes up in databases for example you can often factor this table into some smaller tables that get then composed together so one way you could do this here is I could maybe group these fields into just the Mac fields and then the fields that classify packets as being SSH packets so here you know that's be an IP packet and it has to be a TCP packet and has to have this destination port and now having sort of cut this tree i can actually sever it off and then i need in general i need sort of some extra tag to tell me how to go from the top tree into the bottom tree here i only have two trees so it's kind of you always flown to the second tree but more generally you could have lots of sub trees and so you need to sort of have a go-to instruction that says go to this tree and the hardware supports that and then now we could emit independent tables and if this table was it run in the pipeline before this one then what we can do is basically just match on so that the top table when we sort of delete this bottom part becomes it only really has this these three tests conjoined and if they all hold then it's then it's false in otherwise it's true and so we can just test these and here I'm setting a new tag for ssh being one or zero and then in my next table when i go to ethernet the ethernet table i can test if ssh is one and then test for the rest of the diagram and drop it so and otherwise do import which is basically the encoding true so the same internal representation can be used to do multi table compilation 2 which is important if you want to fit these big programs on two on two switches okay so that's local compilation I now want to jump up and talk about the global compiler so remember the global compiler is is the component that takes an arbitrary netcat program including do poor including links and it produces a local program that's equivalent but doesn't have any doesn't have any links and if you remember the main challenges are this example we saw a couple lectures ago in general global programs can express network-wide behavior and we can need to we often need to add extra state at particular node so that we can implement that global behavior somewhere remote in the network so we have to add these tags the other reason that it's kind of challenging is that netcat semantics is based on sets and if you sort of naively add state so let me actually just give you a strawman algorithm that we tried out and that is incorrect so the first thing we thought is well net cat has these regular expressions as structure maybe we should just do sort of the Thompson construction so every you know plus that occurs in the source program will just sort of generate two states you know for the the two sides of that plus and we'll just transition to those states so it's sort of naively adding tags for every node in the input ast will actually cause you to duplicate packets the problem is you know if you could have a plus in your source program that actually isn't needed so nekkid has this axiom that Union is idempotent so that says that for any program p plus p equals p but if i wrote down this program and my compiler were to add some state which gets added to packets so the pack would actually carry that state then I'd have to somehow recognize that when I get to the end of the paths described by P the two packets that I generated at the input member plus copies the input so the two packets I've generated that are the same except they have different tags to represent the state they should actually deduplicated and there's a number of problems with that you got the smell you know buffer packets you need a deduplication module but these things don't exist so naively sort of compiling the regular expressions into automata and just implementing those automaton the network doesn't work it would be unsound so our solution is to basically build on the automata that we saw last time in the context of verification and then do some transformations on the automata to try to both minimize the amount of state and make sure we're not producing incorrect answers so the sort of overall pipeline is we start with a global program we turn it into a net katadyn FA just like we saw last time and then to avoid this bug it turns out that having a DFA or deterministic automaton is enough to do it and maybe that's easy to see you know basically before if this is a global program before we go to the next top we only want to add different tags to the input packet if it's really going to lead to different behaviors and so that's exactly the difference between NFA in a DFA so as long as we implement this using a DFA this would basically be compiled exactly the same way as just p they have the same deterministic automaton representations then because we want to try to use as few states or as few tags as possible we do some heuristic minimization we don't do full-blown minimization because that's competition expensive and it would make her compiler slow but there are some easy tricks you can play to reduce states and then we take the automaton and localize it to produce a local program and localization is reminiscent of the algorithm that goes from an standard string automaton to a regular expression if you've seen that so kleeneze theorem says that regular expressions in automata have the same expressive power and you can go back and forth so the you know effective when witness that theorem where you go backwards from an automaton to an expression and the way that works as sort of you put regular expressions on edges and you start coalescing States that's very much what we do to go from our minimize deterministic netcat automaton to a logo program so I'll show you this in a little more detail but the again the main challenges here are sort of getting the net cat semantics correct and dealing with with the sort of size of the of the domain working in I'm superior call a nekkid automaton is characterized by these two maps the observation map and the continuation map which I've somehow flip apologize about that this is the observation map so the observation map sort of captures the notion of a final state and it's it's a function from packets two packets sets apologize again and then the observation map goes from a state to a function from packets two sets of other states and other packets we saw how to construct this last time using derivatives but and I kind of glossed over this you know if you were to sort of naively represent this you're basically building automata that work over an alphabet with with pairs of packets for each character sort of a pair of packets and there's lots of passable packets so trying to directly do this would would blow up but the sort of key observation that my PhD student Stefan smoke ahead was that we actually already have something of type packet to pack it set FD DS give us a compact way of representing functions from packets to packet sets and so we can use FD DS to represent both the continuation map and the continuation map just is an FTD and the observation map so as we go in an automaton from a given state to some next date there's basically the processing that happens locally and then the continuation state and so we can just label the edges so the nodes get labeled with diagrams to capture the observations and then the edges get labeled with diagrams or ft DS to represent the transitions okay so after doing that you end up with an automaton that ends up being pretty small that looks like this so here there's four states and kind of all of the states are final because they all have some observation function it might be it might always drop but it has some it has some observation function and so we represent each of those as these ft DS d1 d2 d3 and d4 and then each of the transitions which if operationally corresponds to doing some input output behavior on a switch up to the next Duke or up up to the next link and so here for example we could start out at at this program at sorry this state we could be done in which case D one has to produce some outputs or maybe we go across to some other switch maybe switch to and then there's some local processing that happens on switch one and that's captured by this FTD and then we end up in this state so these data structures kind of fit together in a really elegant way and they let us compactly represent these things okay so next in order to get a local program we need to basically internalize the states so in the network we we have the topology to sort of tell apart different switches and the links carries between different states in the topology but because the functionality that you get sort of end-to-end throughout the network depends on these states we need to remember you know if a pack came in and went on this path we need to remember that and so what we're going to do is basically all of the observation functions we we check which state were in so we assume we have some bits let me call it tag and we just enumerate the states from thank you prefer 0 to n minus 1 but let's say 12 n and then we test before we run that function if we're in the right state and similarly for the links or for the continuations which correspond to go across the links we modify the tag as we go from one state to the next ok so now we have this kind of state machine where there are the states and transitions but we also sort of redundantly in this in each of these programs are keeping track of the states so now we can start getting rid of the states in this graph and it turns out that and you can see our I cfp paper for the details on this but it turns out that this graph will be bipartite essentially because you're always going between switch processing and Link processing every path will go through a state that represents processing on a switch and then a state that represents processing on a link and then a switch and then a link and then a switch or link and the links we don't actually program the links are just there so we don't actually implement those and so we can a little bit of a simplification but essentially collect up all of the FD DS for the switches and because they already test which state they're in we can just take the big union of them and we get this sort of big we call it a jump table where we're testing which state we're in and running some function and then we also have to there's a little bit of extra weaving that happens for for these functions because when you go across a continuation we need to do the update so these kind of get woven together as well ok and at the end we get a local program that's internalized the state of this automaton and we have one big term that captures the sort of implementation of the automaton on the switches and just say one more thing because I this came up last time someone asked you know what happens if you write down a link that sort of isn't in the network well again the compiler checks that all of the length terms that appear in your source program are actual links in the topology and if not it throws an error and that's important because you know we're not we're not implementing the link behavior in our compiled program we're just assuming that the link actually does carry us from one switch to another okay so that's the global compiler and then the virtual compiler which I'll just show you a sketch of it takes as input a program and I'll for simplicity assume it's a local program you could compose another global compiler if you had a global virtual program but it takes a local over program written against an abstract apology so it's a topology that has some relationship to the physical topology but is not literally the physical topology and then it outputs a global program that essentially emulates the virtual behavior so the way that we do this is we think of when we're running a virtual program there's the physical topology the virtual topology and and there's certain locations here this input port and this output port that are related to each other and the program has to specify that relationship they can actually do it at using netcat predicates so we start out processing sort of the real packet is down here in the physical Network it arrived at some ingress and if thats related to some virtual location then we think of there being some sort of phantom virtual packet that's moving up in the virtual world and because these two locations are are related to each other by the association between locations in the two topologies I eat them as being consistent I'm going to write a green check when the virtual packet and the physical packet are consistent so then we start running the virtual program it actually sort of runs on in the table on the switch but conceptually this program does something and so what it might do is it might forward the packet out one of the ports on that switch and we can implement that down here by using some extra tags to keep track of the physical location and basically run this program which modifies the virtual port so now we have a packet that's still down here run this virtual program down to the physical level but the packets at the wrong location the virtual program has forwarded across this switch which is related to this other location the physical Network but the real packet is still sitting there thing grows and so we need to do is forward the packet across to some related location in the physical network that will restore consistency between these two so the way that again Stefan smoke came up with this clever algorithm which I don't want to overplay this it's it's not some like deep result in in games or or synthesis but it's I think an acute way to formulate the problem and it also let him use some off-the-shelf solvers to implement it so as this observation is that you can basically formulate this kind of execution of a program as a kind of two player game so one player is the virtual program which is moving packets around and each move that the virtual program makes whether it's going across a virtual switch or going across a virtual link might break the property of being consistent of being in a location that's related to the physical location and the other player is you think of it sort of the play that has to match the move to the virtual network and that's the physical network so when the virtual network makes a move the physical Network has to make a corresponding move that restores consistency and once we have once we formally things in this way is a game you can think of a winning strategy for the physical player and the physical player is kind of the you know what the compiler has to has to produce the implantation of the physical player a winning strategy for the physical player is essentially a way of matching all possible moves that the virtual player could make so they're always able to restore consistency and that's not always possible right so like if this link was not here in the middle then this would be sort of a bad program or a bad a bad virtual network because the virtual number can move things around between isolated regions of the physical Network and the physical network would have no way to actually restore consistency or no way to implement the virtual program so again by for many things this way and using a no camel graph libraries definitely comp with a a tool that basically finds a winning strategy to the virtual players moves and he actually did this sort of regardless of what the virtual program is this was a bit of a design choice but we didn't want to have to recompile the implementation of the virtual network every time the virtual network changed so we basically try to find a winning strategy that works no matter what the virtual player is doing and the solution corresponds to a set of paths in the physical Network and then using the transformation that showed you last time we can basically sequentially compose the sort of logical phases of physical process of virtual processing rather so going up into the virtual network running the virtual program running the fat coming down running the fabric and then doing that again ok so the virtual compiler outputs a global program because what it's producing our end-to-end physical paths that might require sort of arbitrary regular expressions to realize or to specify ok so that's the compiler I want to briefly tell you about some of our results so i told you that our compiler is fast and i just wanna show you one graph that kind of quantifies this so we took a paper that was published at sigcomm 14 the papers the sdx paper so it's a paper that's trying to do software-defined networking like functionality for internet exchange points I know people know what an exchange point is but these are basically big beefy routers that sit in colocation facilities and and and also so-called exchange points there's like a huge one in Amsterdam and there's others all throughout the US and basically these are the places where lots of big networks both content providers isps even universities come together and they basically appear with each other and so there's basically a big switch there that that can be configured to implement various various peering policies so the sdx paper was exploring how you could use sort of SDN controllers and associated technology they actually used they used netcat to specify the the sdx behavior and here that the size of the input programs is really huge like you might have 400 autonomous systems that are all at the exchange point expressing policy and the policies could each be thousands of lines of code and so this graph I apologize is a little bit small but there's three three inputs with 100 200 and 300 participants and then on the x-axis we scale up the number of IP prefixes that are being discussed in the exchange so this is this was sort of a synthetic benchmark that we didn't produce but some of the people who wrote STX produced where they want to sort of scale up the size of the program running on the exchange and the dashed lines which are up here are their compiler and this is a believable log plot so it goes up to about 10 minutes for 300 300 groups yeah 3 under Bruce is up here with 1,000 prefixes and they were actually using a Python implementation of the traditional approach I showed you with tables and we ported their backend to produce to produce metcalf programs in our representation for our camel compiler and this is hard to see but that's 2 so 4 their biggest input that took them about 20 minutes we got it down to two seconds and as we're saying we didn't do anything special so their compiler used that had a bunch of optimizations that were sort of tuned to the specific kinds of programs that arise in these internet exchange points we did some general optimizations but we didn't kind of look at the policies and come up with sort of handcrafted optimization rules and we were still dig at this amazing performance and in fact as we were trying to replicate their results we got this kind of weird we discover something weird which was that their tables were much smaller than ours so we were like wow maybe our compiler is not so nice it produces these big tables and then we started looking and we realized that one of their optimizations was actually not right it was producing the wrong tables and they were small but they were wrong so you know I think this goes to show that even though these programs again it's not during complete their kind of finite Dada it's easy for people to get these things wrong and also i think if you find the right data structures and you use sort of i mean i really i was this paper i really love i think it's one of my favorite projects I've worked on we're not really innovating on either of our representations you know automata are some of the most well stat well studied structures in computer science and BDDs again are you know Boldin and very well studied but we're just sort of putting those together in in a nice way and I think when you use when you find the right data structures that match your domain and you start to do things like develop optimizations that are justified by you know theorems about those data structures you can really get amazing wins so we showed this to the sdx team and actually before our paper was even published they'd rewritten their compiler to use ft DS and they've since written three other papers I think now for actually that have been published it pldi and STI and sitcom that are based on on these techniques so it was a good idea all right so i think i have about 15 20 minutes left so on a shift gears now and talk about another aspect of implementation of network programs which has to do with updates so again just a review we have this stream of neck at programs that's being produced by some application they get compiled very fast very compactly by the compiler that I showed you but then we have this problem which is if we're sort of running one neck cap program and then some event triggers a change we have to somehow get ourselves into from this red state we're running the initial program into this blue state we're running some new target program and we want that to be fast you know if you think about we might have decided to implement the change because some new hosts arrived and we don't want to wait 10 minutes for that host to you know get connectivity we want it to be to be snappy and we also want to be careful that we don't step the network in two states where it's doing something bad if we've gone to all this trouble to craft NECAP programs maybe we verified them with our tool we check they don't have forwarding loops they satisfy our access control policy and so on it'd be a real shame if whenever we have an update we just throw all those guarantees out the window the problem is that the network's a distributed system and so we can't like stop the world and you know implement our update I mean you could do that you'd have to basically you know tell all the ingress devices to start buffering packets and not letting them through and then you go reconfigure the interior and then you know turn it on again when you're done but in general people don't want to do that you know you want the network to be online and you want the update to happen dynamically even as it continues to process packets and so it's unavoidable that you're going to be going through somehow a series of intermediate States where you know some switches may be still in the new can the old configuration some switches may be in the new configuration some such as maybe on a configuration that's neither neither read nor blue it's um something we've produced to implement the update but we want to find strategies for doing these updates well preserving properties so just an example to show you what can go wrong here's a little toy example where I have this little topology and this is sort of the world and this is my internal network and for the sake of the example suppose that my devices are divided into two layers i have this outer layer with an ingress switch that whose only job is to sort of divide load balance the traffic by dividing it between these second layer of devices and the second layer devices are all firewalls that can do filtering and the external traffic i'm going to divide into so-called black hat traffic and white hat traffic this is a pun that my PhD student mark right black was especially proud of because he's a Texan something really like having cowboy hats and the policy i want is that black hat traffic or untrusted traffic if it's sending web requests identified say by port 80 traffic then will allow it to hit our internal machines but anything else we're going to drop it so based on a firewall non non web traffic but white hat traffic which might be traffic from you know some remote campus of the same organization or traffic that's gone through some scrubber before it hit us or something that traffic's trusted in so we won't allow it and again for the sake of the example i'm gonna assume the traffic's only flowing left to right so there's different ways you can implement this may be okay for some reason i can't forward let me okay so there's different ways you can forward this and so one thing you could do is you could choose to split the filtering switches so that the black hat traffic runs on f1 and the white truck white hat traffic runs through f2 and f3 maybe there's you know a lot more white hat traffic and so you want to send you use two switches to process it but then of course you know if you suddenly got a lot of black hat traffic maybe the links and and the f1 device would get congested and so you might want to move to a new configuration where you use two of the switches to process black hat traffic and one to process white high-traffic so this example was I carefully crafted it to have some sneaky properties so one thing you might do is to think that way I can just like pick some particular order to update these switches and and you know transition to the new policy while preserving that security policy that I had so here's one particular order that you might think can maybe i'll update f1 first then f2 then f3 and then I so these clouds sort of draw the sort of tables that that are installed in each switch so you know we're in the initial configuration we want to move to this new configuration that you see in the clouds my animations are all dying I'm sure why they's okay so right so first we update f1 f1 was actually the same as it was before so that's that's okay then we update f2 and f 2 goes to a policy that lets everything through and then we go to f3 where everything against allowed through and then we update sorry and then we have a malicious packet that comes through our network so in this case the packet comes in it hits the first switch I apologize way back up this example actually goes the other direction it goes from configuration beaded configuration a that's why i was confused let's try this again so we're trying to go to from a situation where f1 and f2 are filtering black hat traffic to a version where f2 allows white hat traffic so we update f1 it stays the same we update f2 it becomes more permissive and then before we update the ingress my computer freezes oh no we have to a three then we have before we have to be ingress the ingress which receives a malicious malicious packet it it sends it it's a black hat pack I haven't seen militias it sends it to f2 which sends it through the network and bad stuff happens so this shows you that although sometimes you can cleverly pick orderings of devices to update such that you preserve your property it doesn't always work another thing you might think and this was actually the first thought that we had was well maybe what I want is atomicity right atomic update sounds good like we're going to have the network be in some state and then it's going to flip over to some new state but actually because the network's processing packets as we go the same kind of example can arise with even with atomic updates so you could have the packet the malicious packet comes in it goes into the ingress which it then is sitting here on the link between f1 two and then maybe we can somehow flip a switch and move to the new configuration and now f2 is allowed to send traffic through and again that stuff happens so atomic updates it turns out actually that you could you probably could implement atomic updates there's a group out of the Technion and marvel that has been exploring how you could use GPS GPS clocks and basically switch update implementations that use synchronized clocks to actually have the whole network transition faster than any packet could distinguish so it's kind of amazing that they can do that but you can actually get these sort of timed updates but i think these examples with these atomic updates failing show what that's not lecture what you want so our idea with some work we call consistent updates was to realize that the failure of the atomic update to go is what we want sort of leads us to the solution you know it's not we want the network to update atomically what we really want is that from the perspective of the traffic going through the network we want some kind of consistency property want those that traffic to experience a consisting configuration and so what we do is we sort of developed a semantics that provides abstractions where it's as if the network updates all at once from the perspective of a single packet and so slightly more formally every packet and you can extend this two sets of packets but every packet flowing through the network sort of sees or experiences a single version of the net cat program so internally the network will be going through a series of intermediate states which are neither the old northern new configuration but logically it will be as if every packet was processed with one version so the network kind of steps in an orderly way through sort of red to blue blue to green and this is again from the perspective of a single packet or flow and it turns out there's lots of ways that you can implement these consistent updates so sometimes you can you know cleverly pick orderings of switches that give you consistency but there's also other kinds of updates that you can run that sort of always give you consistency and two important such kinds of updates are one we call it an observable update so if you install some rules in the network that are unreachable no packet can hit them then it's pretty clear that ensures consistency you might wonder why that's useful but will be clear in a sec and something else is if you make a change such that every path through the network if so if you install a rule for example such that every path of the network hits that rule at most once then that also is consistent and it turns out that compositions of consistent updates with these so called OneTouch updates you can prove semantically are always consistent updates and this suggests an algorithm that we call two-phase updates which I'll show you in the next few slides that's a generalization of sort of two-phase commit or other other two phase kinds of protocols and distributed systems and so it gives you a general way to get consistent updates and I'm so confused why I I travel this my hotel room this morning in the slides all rendered fine and now it's freezing up every five slides so and another theorem you can prove is that we call this the universal property preservation theorem and it it's kind of tautological if you unpack it but it does kind of tell you that it's nailed something very economical so you can actually prove that in one direction every per packet consistent update preserves all packet properties that's kind of obvious right if every packet is seeing one policy version then any properties that hold up the two versions of the policy are going to hold up all packets a little more surprising is that it actually works in the other direction too so any update mechanism whatsoever that preserves all safety properties meaning where properties are over the trajectories that packets take will be perfect consistent ok so again just to show you in pictures what's going on here is we have some initial configuration we have the final configuration we step through a series of intermediate configurations but per pack consistency tells us that properties like our security policy will be invariant across across the whole execution and so that this kind of gives you a way to modularize the reasoning that you do lie network programs you know justifies using tools like our equational axioms or a decision procedure even in the context of a dynamic network because any properties that hold on adjacent configurations will actually hold throughout the execution of the network for both of those ok so just show you a little bit how to phase updates work the the crux of it is to use versions and to actually attach version tags to packets so when we get a new configuration so the network sort of running some old configuration we get some new configuration and we modify all of the rules in the forwarding table so that the rules test if so that they test if the for the version of the new configuration so we actually add a predicate to each rule saying you know version equals n where n is the new configuration number now we have this set of rules that test on this version number no packets carry that version number so we can install them everywhere in the network and this will actually be what I was calling an unobservable update right no packets can actually hit it because I don't have that version number so that puts us into the state where in the interior of the network we sort of have both configurations side by side then we can go around the edge of the network and do a set of do a sequence of one touch updates and at the edge what we do is we change the ingress processing so that all incoming packets are stamped with the new version number so basically here I've updated this switch on the right and now any packets coming in from the right we'll get the new version number they'll hit some the internal switches which all have the new version already and then go through the network and then finally at the end we can garbage collect the old policy once we've done all the ingress rules because now the old configuration rules with the old version are essentially dead code okay so this is a general mechanism I think I'll skip the animation in the interest of time skip over that and jump to correctness so you might wonder how do we prove this correct of course you could do it correctly but sorry you could do it directly but we thought we found it useful to actually explore some of the sort of semantic properties of these updates and so we developed in operational semantics like that one flow we formalized these mechanisms and several classes of mechanisms and then proved once and for all that things like unobservable and 1-touch updates give us consistency and then the actual proof of correctness for two-phase updates follows from this update composition theorem which is that whenever you have observable and a one-touch update its consistent and there's another theorem that says any consistent and 1-touch update is consistent so you can basically string together these theorems inductively to get the correctness of the whole thing you have a question mm-hmm you have to wait for the propagation delay of the network you can find upper bounds yeah and of course if the configuration has a forwarding loop then the delay might be infinite or then I mean in practice packets carry TTL so this you would probably have configurations that wouldn't allow loops but yeah you have to wait till the network is empty of old packets okay so just to jump very quickly I want to show you a couple of results so the main the main cost of these two phase updates is that while you're in that middle state you have both the old configuration and the new configuration in place and I mentioned several times that routing tables are kind of this limited precious resource and in the worst case we're actually doubling the number of extra rules that you need so we did some quantitative experiments to to get this more precisely so we took three classes of topologies we took a standard data center fat tree topology we took a random graph so called small world topology and we took a Waxman graph which is a often used as a model of campus and enterprise networks and then we did a sort of set of canned updates where we added them remove hosts or added and removed links in the topology and then generated shortest path forwarding configurations and then we also did some where we did both and what you see here is the sort of relative overhead of these updates for these networks against in these different scenarios and it's all relative eyes so it's not always a hundred percent sometimes the new configuration is smaller for example and depending on the details is about and the links we added or removed the numbers are different we also implemented some optimizations I haven't shown you the details but there's some cases where you can recognize for example that the same rules are used in both configurations and then by cleverly encoding the tags you can actually get away without replacing those rules and so there's a an optimization that's in our paper we call it the subset optimization that sort of generalized this idea across multiple switches and and multiple predicates and we're able to show that actually using the subset optimization you can often dramatically reduce the overhead so sometimes with with these waxman graphs which have very little structure we still often ended up with more than fifty percent overhead but often it was sort of more more like twenty-five percent ok so that's consistent updates and that's a general way to handle dynamic changes in software-defined networks I want to briefly say spend two minutes talking about another approach that we've been exploring more recently which uses software synthesis to try to generate update programs from configurations and properties so the idea here was basically to try to further reduce the cost of two phase updates so this is joint work with Pavel attorney and and Jed mcclurg at Colorado and so we realized that consistent updates are wonderful because they as I showed nor theorem preserve all properties but if you're willing to go to the trouble of writing down which properties you want to be invariant across the update then that opens up the door to more implementations of the update so for example if you only care about not having forwarding loops and preserving connectivity but you don't care about the paths that are taken by packets during the transition then they're in generally more possible ways to update the network that preserve those properties but maybe don't preserve other properties and so we built a tool that uses a fairly standard counterexample guided inductive synthesis approach to take a specification of the properties we want the network to have across the update the topology and both configurations which are assumed to satisfy property and then we basically sorry basically search through the space of possible updates where an update is the sort of atomic thing you can do in each instruction so a full mod for example and basically we searched through this space and if we're if we succeed then we find you know a particular other than several past year but a particular path that gets us to the target configuration without violating this property and there's the basic idea is I think almost completely vanilla to make this be fast there's a couple of things you need to do so one is as with most counterexample guided inductive synthesis approaches you want to do things like learn from counter examples so if you discover that maybe you know going from here to here violates the property or rather from say from here to here then you want to find sort of extract out from that what's the essence of what led to that property violation and then avoid even ever checking configurations that have that same characteristic so our tool does that something else that is very useful for speeding up synthesis is we're proposing a series of questions to a solver that can check if the property holds of a given configuration and the configurations are huge but the questions are all very similar you know we're kind of taking a topology and configuration and then changing one thing and then asking again so the data structures that the solver uses to represent these things internally will often be very similar between these two and so Jed the PhD student who led this work actually built an incremental model checker where you can as with most model checkers you know ask is this formula satisfied and you can also incrementally make sort of add Delta so you can say you know what happens if I tweak the model by deleting this node or adding this node and then there's an incremental propagation strategy that that we use to Phyllis you're familiar with model checking we're using the approach of based on automata by VAR d and volper and so we can incrementally sort of update those internal data structures to quickly check if the property holds or not and that's again really important for performance one other thing I'll say about this is some of you may've been following some of the recent work on semantics of weak memory and that people like Peter Sewell at Cambridge and Derek dryer at mpi and many other people have been working on and there are deep similarities between sort of this problem and and weak memories essentially we have this sort of distributed memory we can modify it by sending control messages we can synchronize which is like a barrier we synchronize using barriers which is like a fence either by using a barrier instruction or waiting until we know that some update has happened and then we do reads of the cells by basically sending packets through the network so there's some nice connections between this sort of synthesizing these update protocols and some of the synthesis work that targets weak memory systems okay so that's all I'll say about updates they all say one more thing this there's a there's actually a bunch of other people who have been working in this area since we did the first work on consistent updates so the people who have looked at how you could do consistent update like mechanisms for distributed protocols like BGP this work actually predates predates Sdn and there's also people who have looked at a number of extensions perhaps the most important is updates that preserve quantitative properties like congestion so in the same way as consistent updates preserve all sort of packet forwarding properties you might also like to know that if you're doing an update you're not going to cause you know all the traffic to go over one link exceeding its capacity so some really nice work out of Microsoft Research row to alaj on and others showing how you can extend these consistent update techniques to also give you congestion properties okay so I want to very quickly tell you about sort of where things are going I've talked at meals and over beers and stuff with a bunch of about this but I think the sort of broader software-defined networking community is sort of at an inflection point on the one named there's been sort of all these successes sort of getting the networking used ready to open up and the development of low-level machine languages like open flow there's deployments that all the major tech companies and large ISPs like AT&T and Verizon are moving to SD n in a big way but there's also the sense that the sort of current realizations of low-level languages like open flow are really not quite right and also that the current control abstractions that we have even in languages like netcat or systems like I showed you are are limited in certain ways so we've been doing work in sort of two directions to try to address that and the first direction is actually developing languages that can reason about probabilistic and quantitative behaviors so one way to view the motivation here is that if you go talk to the practitioners who are actually running networks they appreciate tools like nut cat and header space analysis and Vera flow that can model the functional behavior of a network and check properties of it but often the things that really puzzle them and cause them trouble involve various forms of uncertainty and quantitative properties like congestion and so you know when they move to st and they'd love to have tools for being able to reason about that kind of stuff precisely so improbable signet cat we've tried to take a step in that direction focusing especially on the sort of uncertainty aspect and the basic idea is that we we add a new operator that represents random choice so and syntactically prepares reasons we decided to co-opt the + operator and then we replace the old + operator with ampersand so random choice just captured exactly what you think so basically with with if we have a packet with probability are run policy one and with probability 1 minus our run policy too so at the syntactic level this prob knockout language is almost trivial but it has required a sort of complete rethinking of the semantics of netcat and in fact you know many of the obvious extensions you might think of don't actually work so you might think that you know maybe we can start from the standard semantics of netcat working on you know as deterministic functions from history's to set up histories and maybe we'll be able to move to you know functions from history's to distributions on sets of histories but there's for those of you have seen probably six emetics before will not surprise you that this doesn't work and there's actually a sort of network specific problem which is that the sort of intuitive meaning of programs that we want on sets of history's is not actually uniquely determined by its action on individual histories so the lifting that we did before in netcat doesn't actually work out and it gets worse we sort of thought originally that we could get away with a relatively simple notion of distribution maybe just discrete distributions maybe even just finite distributions but it turns out that you can easily write a probable signac at program whose natural interpretation would generate a continuous distribution here's a little example just to whet your appetite if you're interested in detail point we saw papers and some pure around time let me write pi for a program that generates a specific packet let's say pack at 0 so pi 0 bang means just generate on any input just generate back at zero and likewise pi 1 bang means generate packet one so i can write this program which you'll be able just a coin flip so it says probability one-half generate packet zero with probably one half generate packet one and then i can iterate this program with a dupe in the middle and the thing about this does it's basically you feat in any input packet it basically flips a coin sets the packet 201 stashes it in the history and then repeats again let's see like what this says and this turns out to correspond to sort of an infinite tree where every possible infinite sequence of zeros and ones is represented and so you need a continuous distribution to represent this and so our solution to both these problems which is again in the resale paper is basically to we have to sort of go and be pretty careful about doing sort of all the measure theory required to sort of formula formulate a space of packets that has the right properties we actually we have to write down at metro space two packets we actually represent these things not just as arbitrary functions but as markov kernels if you've seen that before and then we can actually go and interpret each of the net cut operators over markov kernels and so this is quite a bit harder than for the deterministic case and actually the kind of key challenge is showing that clean East are actually converges this required some new research that Dexter cozen worked up having done this we can prove some nice properties so maybe the the most intuitive and and in some ways most powerful is conservatively so if you don't use the random choice operator then even though we've kind of completely redone the semantics in this more complicated setting the true semantics coincide so if you run the probe netcat semantics on a terminus tick program you get a distribution which is a point mass or a Dirac distribution where all the probabilities on the output that you would have gotten by running the deterministic program using the standard semantics and that's nice because it means that we can actually use so we don't know how to act meet eyes prob netcat yet but we can use the neck at axioms and they're still sounding complete overall deterministic subprogram that might occur in your overall program and we're still working on this we actually found a nicer way to formalize the semantics which again involves some measure theory and so on but it lets us basically connect up the sort of usual way of doing semantics using Scott domains for this language and we're currently working out those details we also looked at a bunch of application some I'm personally very excited by this probabilistic extension because I think it covers a gap that the simple deterministic language is sort of fail to address and that is you know uncertainty arises all over the place in networking and also a lot of network programs use randomization and so you can't you just get model these in in languages like neck head and so our Esau paper has a bunch of examples of this we look at some congestion properties we also look at fault tolerance so you know if you have a network and each of the links might fail with some probability maybe it's only ninety-nine point nine percent reliable then depending on which routing program you're using your forwarding program you're using you'll experience different levels of failures and so you can actually use the semantics to calculate the expectation that packets will be delivered under a given failure model we also looked at modeling some classic load balancing algorithms so there's this beautiful algorithm by valiant and brebner that's been quite widely used in networking and the basic idea is that you're trying to forward in a mesh but rather than taking the shortest path to your destination you go by an intermediate representation intermediate node that's chosen randomly and you can prove that this gives you very nice load balancing and congestion properties and so we're able to read arrive the bounds that that they proved in the early 80s in our calculus and then we also formalized a simple gossip protocol gossip protocols are protocols where each node sort of runs a very simple program that talks to its neighbors and they sort of do a local competition on their state and you can do things like disseminate information through a network very very rapidly even with this simple randomized behavior and so we people often talk about sort of gossiping uh or a node being infected when it sort of learned the gossip and so we encode the gossip protocol and again used our semantics to show that the number of rounds it takes for node to be infected is the same as what some older theory papers show um one other thing that I want to quickly discuss and then I'll wrap up so I've talk to you again at dinner and over beers about the fact that this architecture which i think is about as good as you can do in systems like open flow is not so great we have to you know if the data plane if if the switch is can't keep any state and can't sort of do conditionals depending on that state then the controller has to be in the loop on every single change and also any state that encodes things like a ethernet learning table or a firewall table or a routing table for the whole network has to be kept up in the blue box and then sort of push down here so we'd like to be able to you know take not just the net cat part of our program but also the blue application which might be more complicated might involve collections of data structures and loops and things and somehow compile that into the data plane just as a quick example we've been talking about firewalls and often I've sort of introduced a firewall and then modeled it with a very simple function that just statically filters some some patterns but actually firewalls are often stateful so you can imagine a policy where internal hosts are allowed to initiate communication with external hosts but not vice versa debase the firewall blocks you know request the CIM in from the outside but if an internal host makes a request then the firewall punches a hole in the reverse direction to allow the reverse traffic it basically flips the source and destination addresses imports and then allows those the other direction to come back for at least as long as the flow is so if you want to program this in Sdn you currently you would need to basically use the controller to punch the hole it would basically be in one configuration and then when the outgoing request sorry outgoing request comes out it would push down a new configuration and that would be bad for a number of reasons the controller has to process every single internal request it slows down setting up connections because the first pack of us to go to the controller so we'd like to actually just do this directly in the data plane and so in a paper that was just published at PLD I and we developed a stateful version of neck head where you can write programs like this I won't pronounce the whole thing but essentially what this is doing is it's doing a test for the forwarding peak case it's doing a test to see if flows between this internal and external hosts are allowed and then it's it's setting the sort of allowed flag to true when the internal host initiates communication and the hard thing here is you know once you have these stateful data planes and programs that locally can test and set state now we need to sort of figure out what it means to correctly implement a network-wide state update now I could have state that's distributed across multiple switches and if a right happens on one side I need to worry about what's the semantics of a read somewhere else so we built a stable version of netcat it adds these two operations for testing and setting state model this sort of a single flat vector of ants and then the way we think about network programs is basically that we again have a kind of automata structure although this is a different notion of automaton where each state corresponds to a static netcat program and then the edges represent what happens when state is written so if state is written when a certain packet comes through the program then we might transition to a different state where the configuration can be different so this is kind of the internal representation that our compiler uses and then the really tricky thing we spent most of our time think about is what's the right consistency model for this kind of event driven programming you have packets flowing through the network they're changing the state but we there were two things we didn't want to do we didn't want to allow we didn't want to require some kind of expensive packet buffering so we didn't want it to be the case that if a packets sent from Eugene to Ithaca that the behavior difficult at eugene and so therefore you need a round trip of communication to decide what to do with occur we wanted to allow the network to basically be sort of weight free that they could just ten packets through at the same time we wanted to be sure that packets still get some kind of consistencies we wanted something like perfect consistency and we also wanted to know that when a state change happens eventually it should happen and so the model we come up with is basically the combination of per pack consistency for forwarding and something called causal consistency for the state changes so essentially when a node has heard about a given state change through a standard kind of causal relation then it has to transition to the new state and then to handle a sort of tricky issue where some state changes may be incompatible we use a framework called event structures that when school has been working on for quite a while to rule out these state changes that are incompatible and then we build an implementation that uses a simple sort of replication protocol by passing around digests of the state so as the state changes happen the switches sort of collect up these digests and then disseminate them for the network and this actually isn't possible to an open flow but there are new open flow like languages one in particular is called p for that are either already exposing or are going to expose the ability to collect and modify and disseminate state directly in the data plane i'm a little over time so i will just skip over this but we both a bunch of applications and we did experiments again with our compiler showing that they have the right behavior and we actually showed that some of the sort of open flow in plantations using a controller would have been incorrect because they wouldn't be able to implement the state changes fast enough okay so just to wrap up I hope these lectures have convinced you that there's lots of fun to be had taking the tools of our field programming languages for methods and so on and applying them to problems in other areas and you know the solutions that I'm most fond of they sort of are a little bit less ad-hoc than some of the solutions that you often see in mp systems conferences but they tend to have you know some more mathematical constructs that are really driving the solution so things like using automata using BDDs using classic istry protocols I think the strength of our field is that we don't well we do swept a t-tail we don't sweep things under the rug and so if you're going to the trouble to actually you know build a compiler that handles every possible input and then proving that it's correct and that naturally leads you to a solution that's more elegant more streamlined and like gets every last detail right and I think that's the value that we bring to these areas and when there are systems where the kind of getting things right is subtle I think you really have to use our approach so you know it's wonderful that you're at the summer school learning about you know the basic tools of the field semantics various forms of reasoning and I think there's great promise in applying all those tools to sort of the rest of computing problems okay today we focused on compilation with fds and symbolic automata and these different notions of consistency for update mechanisms there's a bunch of reading for these lectures i'll post it very soon on the website but the the first papers the compiler paper i talked about last time this describes ft DS and automata and then there's a bunch of papers on consistent updates synthesis of updates and probabilistic aniket just to briefly acknowledge my collaborators sort of new collaborators you haven't seen before so the net cat team is kind of involved with all this and then I want to point on a particular Jed mcclurg at Colorado who's been driving the synthesis of updates work and also stateful maquette and his advisor Pavel journey all right so thank you for your attention and lots of questions and fun discussions if anyone has follow-on questions I'm very happy to answer them by email or I'll try and look at the out so I'm taking off today but thank any quick questions all right let's get coffee 