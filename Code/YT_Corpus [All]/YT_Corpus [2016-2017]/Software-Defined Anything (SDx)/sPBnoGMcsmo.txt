 my name is Wei I'm VP of products at trifecta I'm joined by my colleague CTO and co-founder of Chi factor Sean Kendall VP of products from water line data Mohan and then also Director of Product Management mark at caldera we want to jointly talk to you guys a little bit today about how you can expedite Hadoop data lake adoption by combining with an integrated set of solutions that focus on self-service and governance I know that these two concepts seem to be at odds with each other but in reality they have a symbiotic relationship and both are equally important to achieve success so I want to start by talking a little bit about the data Lake so just for curiosity a show of hands how many of you here are eating the process of implementing a Hadoop late data Lake project or going to production with it okay so almost more than half of you are doing that that matches to our own observation with our own customer base that I factor many of the customers are spending a fairly significant amount of dollars in setting up the infrastructure for Hadoop data Lake now for the rest of you you might be wondering what is really driving adoption well there are a few factors the first one is an obvious one data volume is getting larger now I know a lot of you like the joke is big data is actually not that big right but in individual file size that is true but in aggregation Enterprise is collecting a lot more data than ever before and this is driven by the fact that data is more increasingly recognized being recognized as a competitive asset so much more data may be the enterprise doesn't really know what they want to do with the data yet but definitely much more data is being collected and traditional analysis framework like Excel isn't a suited to handle that type of volume of information second driver for data Lake is that the data is getting also more diverse so it's not just about the volume and this is just the data landscape that we living today whether it's JSON logs that you're collecting whether it's clickstream data whether it's social media data the data is becoming increasingly more diverse and at the raw form much of the data cannot be directly ingested or worked with by an end user or by an analytics application but the enterprise are still collecting this data because they don't want to lose anything and more increasingly they want to make sure that they have all this data so that they can compete effectively now I know that many of you have been in a data management space for a long time including myself and this isn't a brand new problem in the sense that Big Data has been around even before the term was coined and there have been attempts at solving this problem initially it started off as departmental data marts where people are just building morning a silo either a data warehouse or data Mart that they need for analysis over time I think IT recognizes that that approach isn't really scalable and isn't really manageable so instead of having siloed data marts data warehouse came along where IT or shared services try to structure or try to bring order to kind of this siloed chaotic approach and then you should have a much more arable environment for everybody to work together the problem with this approach is as earlier I mentioned that both the volume of data is getting larger and also the variety of data is getting bigger a structured environment like a data warehouse is ill-equipped to handle the requirement that are coming from the business perspective so this is where really that the drive for something like a data Lake came about and this is fueled really by technology innovations that have happened in the last several years so with things like a do for HDFS for storage as well as computation frameworks such as MapReduce and SPARC data Lake becomes a lot more attractive to store all the raw data that the business requires and it becomes an environment that everyone across the organization should be able to share and really bring insights now earlier I saw the the show of hands regarding how many of you actually engaged in a data Lake project now I would I would say that from our own customer base even though many dollars are being spent at building the infrastructure for the data Lake very few companies have actually been able to achieve success with the data Lake project from an adoption perspective so this talk is a little bit all of us coming together to share with you some of the key challenges we think that are actually hindering data Lake adoption I will start with the first one and then my co-speaker here Mohan and mark will go through some of the other challenges so the first challenge I'll talk about is really about end user adoption now what is an end user and user is someone within the line of business who's really trying to leverage the data in the data Lake to make critical business decisions this could be a product manager it could be a marketing analysts it could also be an HR analyst again the roles may vary but they're also trying they all trying to access data in the data Lake there's some challenges with this with this picture here in the sense that even though the technology has innovated on the backend to store more data to process the data much faster but on the user perspective there's a chasm that is happening between the line of business and IT or share services and again this metric should be familiar to many of you it's widely reported that 80% of the time that is spent in any data management project is actually trying to munch or trap or transform the data and get them ready for analysis this is definitely true from our perspective as well as we see many business users struggle with the Hadoop data Lake infrastructure they're not able to really access the data but it's not just a divide between the data the raw data and the processed data that they're trying to get to there's also a process chasm as well and again this issue of business and IT exchanging requirements where a line of business user goes to IT and says you provided me with this awesome data Lake and I want to be able to access it here's my requirement and IT looks at it and say what exactly do you need and the business goes and say well I can't tell you what I need unless I actually have my eyeball on the data and be able to do some discovery or be able to do some wrangling so this process chasm is also hindering data like adoption and slow down many projects that are trying to take advantage of this infrastructure there's also a skill gap and this is something that we hear over and over again I know we're all hearing San Jose attending strata Hadoop this is my sixth time here at the show and there are many many programmers and people with technical skills out there but for the vast majority of the business end users this isn't something that they're used to the technical hand coding on the left hand side is still predominantly the way in which people work with data in a Hadoop data Lake today they use scripting languages such as Python to really be able to get the data into the structure and the shape that they need and then on the right hand side you see that there are ETL tools for more of the traditional data management layer where they show more of a technical mapping interface and again this is one abstraction layer away from the code but still based on the conversation we just had about data being more variety and less structured a technical mapping interface is also not ideal for an end user in the line of business now you might be asking what about Excel and we talked about Excel a little bit earlier but Excel is predominantly the tool that the line of business users are familiar with and that's why you hear this a lot we want an excel like interface well what does that really mean they mean that they want to see the data they want to be able to touch the data directly without really understanding or having a need to pre structure or filter the data that they need to analyze but Excel has many many challenges also it doesn't scale very well in terms of Hadoop it's it's really not a good match for the kind of storage and a processing paradigm so that is also a challenge so we believe at try factor from from my perspective one of the key to data Lake adoption is to make sure that there is a good user experience and I'll tell you a little bit about the approach that we took at trifecta it is an innovative approach that we created it really centers around the need for the user to be able to interact with the data directly so when a piece of information is loaded into try factor the user is able to see it visually a few different ways one is the data elements themselves but second also a set of profiles or a set of visual histograms that really describe to the user what type of elements they're looking at and the user can simply go ahead and choose from any one of the data elements whether it be the actual data or the histogram and the visualization that are being shown for the data that then become an input to the system that generates a set of transformation suggestions that aid the user in crafting their transforms this process then leads into a immediate preview and this preview is very important because in instantaneous feedback is desired by the user who's looking at the data to really try to validate if the transformation or the suggestions are correct so this interactive loop really underpins some of the features within try factor from a data prep perspective one of them is the deep visual profiler and again you will see this in a demo later but here's one example of such a user interaction where the user can brush and link across variety of visual histograms and then use that as a way to drive predictive transformations on the bottom here you can see that different suggestions are being generated based on the user input through the user interface so again in summary I believe the end user paradigm is really really critical and if you ask majority of the customers that we're working with today this is remains to be one of the big hurdles to get the end user into the ecosystem and be able to start leveraging the data that they have in the data lake all right so at this point I'm gonna turn over to Mohan and talk a little bit about some of the other challenges with adopting the data leak Thank You Wayne hello everyone most of you have worked with Hadoop probably realized that finding data in Hadoop data lake can I have that thank you it's like scavenging for data in a flea market where your principle strategy really is luck and so the the problems that big data are architects encounter here is that the huge deluge of information that comes in is really hard to keep up is really hard to inventory all these items business analysts and data scientists on the other hand to be able to find data easily in the data likely to be able to select and act on data again is a big challenge data stewards always struggle with staying compliant trying to have their policies defined and implemented and in trying to bring order to all of this is really important consider a stark difference like in amazon.com where you know products are neatly inventoried they're all categorized consumers can find data very easily either by browsing or by searching they find all the information about the products they can select the right product by looking at all this information and with a click of a button have it delivered to your doorstep nowadays by a draw knife here what online data is like amazon.com for Hadoop we inventory all the data assets in the repository automatically we automatically tag categorize classify all this information such that when an end-user such as a product manager or data scientist or a business analyst comes to look for information they can easily find this information and then finally they are able to provision this so that they can use it with their favorite bi or data prep product later on in the demo Shawn will demonstrate how once you choose a data set you can then automatically launch try factor for example to prep your data after you go through a search for your data so in summary deploying a product like deploying a Hadoop deployment bringing it quickly to market is is really important and automatic inventory of assets helps a big data architect achieve that similarly for a big like a data scientist or a business analyst accelerating their time to analytics or to prep is really important and helping them find the right data said choose the right data set using either data quality metrics or lineage or other factors and being able to act on that data is paramount for a data steward the ability to crowdsource all this data curation so that they don't have to do it all by themselves gives them like you know three steps ahead of their job in getting there and finally other metadata like lineage and data quality assists to accelerate governance with this I want to transition to mark who will talk to you more about governance mark come on oh hey thanks very much mark so I just have a couple moments that I'll speak with you about you know we pointed out at the beginning of this how's this going okay I so way pointed out at the beginning of the presentation that I almost you know certainly disputing but you know we making the case that you have to choose between supervision and adoption or at least saying that often when people start looking at Hadoop that you have to make a choice between supervision and adoption I'll be making the case that these are actually not two opposing forces but actually two sides of the same coin and actually investing in one correctly and properly will inevitably cause you to benefit on the other now before we actually go into specifically why supervision and adoption are really two sides of the same coin I want to spend a couple moments explaining why people fall down the path of assuming that you have to choose one or the other so when we look at Hadoop I basically all the benefits of Hadoop are exactly what make it difficult to govern so we look at Hadoop it has support for significantly large datasets it has support for significantly diverse types of data as you can have structured semi-structured unstructured data sets buried inside any of these data sets could potentially be a social security number or a credit card number and because there's so many certainly when you're dealing with Colin or data or structured data it's not terribly difficult to figure out where the sensitive data is located that gets increasingly difficult when when you're dealing with less structured data say a log file or potentially a customer call recording an mp3 where they may be giving their credit card number to the calls the customer service agents on the phone so we have diverse types of data and then layered on top of that you have diverse compute engines so you can have sequel interfaces with hive or Impala or you could have scripting interfaces with say Pig or programmatic interfaces like spark and so on alright so we have all these different ways of accessing the diverse different types of data I and if you aren't watching every single one of those ways of the data accesses again you could potentially have a security breach and then you have to layer on top all the custom applications and all the software applications built on top of Hadoop and all of the different users that are working with all of those applications so it's very different from a typical enterprise application where you just have likely one or two different ways of accessing the application you have a whole diverse set of data compute engines applications and so on all right so really this is exactly what one would think makes a dupe difficult to govern but what I'll be pointing out is actually as you drive the appropriate governance capabilities throughout your Hadoop distribution or your Hadoop deployment you'll actually be able to drive adoption because it'll satisfy all of the stakeholders in the organization in the ways that they need to to be able to trust and use the data effectively I really in this slide each of the pillars points to a different kind of adoption perspective and at the bottom we have governance okay so we could think of governance as if we look at supervision as being one end of the continuum we could think of governance is really being a supervision and if we look at adoption that could potentially be the usage of the system I so then we started thinking about compliance groups you know compliance and supervision that kind of goes hand in hand but there are certainly groups in compliance who are worried about deploying to Duke you want to make sure that like they can block the deployment if the great controls aren't in place so you need to have adoption from the compliance groups that means these compliance groups need to make sure that in the extreme case if you're in a regulated sector like healthcare financial services insurance intelligence I suspect no one from intelligence would admit to being care but in any of those sectors then you need to make sure that you're tracking the sensitive data the AraC set that you have complete controls in place to watch the flow of sensitive data from one data set to the next if someone extracts credit card numbers or social security numbers you need to know exactly where that data set went to and then who types that derived data set that's just part of basically every single regulatory standard when it comes to digital data so the compliance groups certainly need to embrace adoption and they need to have the right controls in place so there's a pretty straightforward mapping between compliance and governance and adoption and then at the other ends there's end-user consumption of the data I so both Mohan and Wei pointed to this in varying degrees and would all really point out is that what end users need to do in order for end-users to adopt the system not only do they have to have the right tools in place and they have to have basically the right tools but more than that and to be able to trust the data that they're working with it's very easy for someone to create a new data set inside a Hadoop cluster that's called q4 sales and how do you know that it's q4 sales how do you know that it's complete how do you know where it came from all those kinds of questions so trusting the data is one important element of adoption and that very easily ties back to governance or supervision I another aspect of that is figuring out how to use the data set so just because the data set is available and let's say it's even wrangled or prepared in a format that you can consume how do you know which column is the right column to link up to your existing data set that's a complete question that's anted that's a question that's directly answered through a governance layer if you can look at historical usage patterns of the data sets you can quickly figure out the likelihood of where you would join it and so on but without the right controls in place users are left on their own to try to figure out how to get that data linked up together so we have two sides or two ends of the spectrum where there's compliance and then there's usage but really in the middle is stewardship and curation which I would say is really the bridge between the two stewardship and curation is making sure they're ensuring that the datasets are available to the right users that you're purging the datasets whenever they whenever you're supposed to let's say it's after seven years and you're regulated or you're classifying the data according to where the regulatory standard may be tagging things as being protected health information or friend user consumption tagging it is key for sales data and adding the right profiling information and so on so these three pillars really are important elements of adoption but they're very related to governance or supervision and then finally the fourth pillar administration there's a whole set of capabilities there as well they're necessary to talk about but I'm gonna leave that to the Q&A if they're questions I so this leads into you why I'm speaking here which is to quickly talk about clutter and navigator right Navigator is an important element of clutter Enterprise our Hadoop distribution and it delivers that governance foundation ok so comprehensive lineage comprehensive auditing unified meditate and a platform for metadata so that all of our partners including water line and try Factor can collaborate and share on metadata so that one when one product puts in metadata available for all the other products I understand a whole workflow engine or a policy engine on top of that so you can trigger partner applications and trigger workflows as its needed based on changes to the metadata I certainly you'll see a little bit of navigator during the demo from Sean and we have this at our booth if you want to see more about it but a few quick points that I'll point out is that we at Cloudera we've been focused on governance and laying out governance of the foundation for a broader set of data management capabilities for several years now and we have several hundred customers who have deployed navigator in production I and I would say that I've worked with nearly all of those customers and have made sure that across various sectors whether it's regulated or not regulated whether it's compliance or end user adoption or data discovery all the various aspects that we've spoken about today making sure that we've incorporated those use cases and challenges and solve those challenges in the product so I'd be happy to take some questions about that during the Q&A as well so with that I'm gonna hand you back to you Wei and she's gonna have a few comments thanks Mohan and Mark that was great so now before the demo I just wanted to quickly summarize kind of what we've been talking about so earlier I talked about data Lake becoming an increasing enterprise standard and many of you are actually in the process of implementing one which is great but there are some challenges with the adoption of the data Lake and we talked about three distinct challenges one is around end user adoption and end user being a line of business user really able to get to the data be able to work with the data and wrangle the data second as Mohammad out the ability for them to discover and actually locate the data that they need from the data Lake and then last from a governance perspective self-service is great but imagine hundreds of users in organization all self-serving themselves from the lake there has to be a manageability and governance layer to bring it all together this brings me to a solution slide here and again as much as talking about these challenges is also a bit of my perspective on what is really going on in the marketplace so there are full-stack solutions out there there will be vendor that clean that they can do it all everything from ingestion to discovery to wrangling to governance to analysis even in some cases all one full stack and to end my belief is that that's just simply just too much for one vendor to chew on and as you can see every one of the vendor has very differentiated solution and my belief is best debris is the way to go especially in earlier stage of a technology platform like Hadoop so you heard from three vendors here today we're definitely not the only vendors that are out there my main point for this slide really is if you are evaluating an end-to-end data solution for the lake you really need to be thinking about all these components there are some components we didn't even get into today like ingestion and analytics but even with wrangling discovery and governance there's already a lot of components in here and the only way that you can have a successful solution is through a open set of api's and integration and that brings to that the metadata piece is a critical foundational piece to make sure that all the vendors have an open ecosystem and they can really store the data in a central repository and be able to really govern and manage that so with that I'm actually going to turn over to Sean and to show you a demo of the three solutions working together end to end thank you all right so for this demo we go into a joint solution we've done with some of our customers looking at compliance and the cost of non-compliance so you know why is this an important problem to solve for those of us who work in finance or financial institutions the cost of compliance is actually estimated to be around twelve percent of the operating cost so just making sure that you comply with regulations is this this huge cost to the company not to mention the cost of non-compliance where you can get hit with you know massive fines reputational damage and so on so in this demo I'm going to show you how using water line trifecta and cloud here Navigator together if your compliance analyst you can quickly kind of take data from many different systems clean it prepare it and ultimately do an analysis to look for any compliance issues within the bank so just to give you a sense of the the workflow we're gonna start off in trifecta where we'll wrangle some unstructured data so in this use case imagine you're looking for say insider trading within your own bank so you want to make sure none of your traders within the bank are doing anything suspicious so the idea is we're gonna look for important news events and then correlate that back with our own internal trading systems to see if there's any kind of large suspicious trades that happen you know a long time before the news breaks not necessarily insider trading but potentially a signal that we can then use to maybe correlate back with email or phone records or other records to do that investigation so in try facto we'll work with both these structured and unstructured data sets I'll show you how while you're in the middle of an analysis if you need additional data you can quickly kind of call out to a cataloging service like water line to find those data assets and then finally I'll show you how by operating on top of cloud air and on top of navigator all of the activity that you're doing throughout the analysis is recorded and really audited in navigator and I'll show you how you can kind of search for that after the fact in that product so before I get started I want to show you actually what the raw data looks like so if we start say with the news feed many of you might be familiar with data that looks like this coming in from news services like Reuters or Bloomberg you know there's some structure to it but it's actually highly unstructured most of the information you care about is unstructured embedded in the news articles themselves so I'll show you how using trifecta you can kind of quickly extract the features you want to perform your analysis and then we're going to correlate this unstructured data with some structured data so this is just you can imagine a database of all the transactions that your traders have done on your internal systems so let's pull up the data so first I just kind of pre loaded data that was on HDFS into trifecta so this is the news data and you knowwe talked about earlier kind of encouraging in user adoption so I'll point out a couple features of trifecta here I'm gonna go through each of the products fairly quickly because of time constraints but if you have questions see any of us afterwards or I'm sure all of us are happy to do kind of a deeper demo up on the floor but as soon as you kind of pull any data into trifecta there's a few things we do first the product will try to infer any structure off the bat in the data set so as I mentioned earlier there's actually some structure in this data you can see that there's the news or the the time of the news and one column the title of the news the content of the news and then it links back to the source of the original article and the URL here so try factor kind of automatically spots out this structure additionally it'll profile the data so way mentioned kind of interactive exploration or profiling the idea is that trifecta will detect datatypes and the data so for instance here we recognize that it's a date time and using that type information we can do a few things one is we can perform or provide summary visualizations so you can quickly kind of get a sense of how your data is distributed are there any weird outliers in the data is there any seasonality and when these articles are coming in so here we can see kind of a histogram of the amount of articles over time additionally we can perform data quality checks on top of the data so we can spot out things that aren't valid dates so here we see a red bar at the top indicating that there is some mismatch values in the data set and when we select those we can kind of zoom in on them if we like and take a look at just kind of the the rows that are you know either missing dates or have kind of a weird formatting string in there similarly we could do the same thing for missing values and zoom it on those now all this kind of profiling is also tied back to what we mentioned earlier the the interactive or predictive transformation the idea of being that as you interact with data elements on screen the product will suggest transformations for helping you clean those values or you know restructure those values as necessary so here if we click again on the mismatch values we see at the bottom of the screen here there's a few suggestions for dealing with those dirty dates in the data set here we'll choose a delete suggestion and we can see this visual preview here indicating that it's going to delete these records out of the data set and if we like that suggestion we can say add to add to your steps or out to your script and those rows are now taken out of the data we can see in the profile it's kind of automatically updated so now there's no more red data inside the quality bar similarly if we want to get rid of the missing values we can select the gray bar at the top here choose a delete suggestion and add that to our steps and now we have all valid dates in this data set so there's a couple important things to point out from from those interactions one is that we're not actually modifying any of the source data right here we're working on a sample of the data that we've pulled into the product but instead of modifying the source data instead what we do is we build up a reusable kind of set of steps or recipe or script here on the far right of all the transformations that the end user is performing on the data and I'll show later how we can use this so then run the data or run this type of transformation on a schedule you can use it to kind of repeatedly run these steps to run it in different execution environments and also importantly for this demo all of these steps can then be published out to governance tools like navigator so you can then trace back any of the results that are published on the cluster you can see not only the source data but how that source data was transformed throughout the analysis so here we see that we added a couple of steps here to delete the mismatch values we can hide that for now and let's do some additional structuring so as I pointed out earlier most of the interesting information is actually in the content of the news articles so if we're looking for insider trading we want to find articles that are going to mention stocks inside the news and then also maybe some important keywords about the stocks such as profit warrant or some big news event that might have occurred so to start here if we look at the news articles we can see that a lot of them are are mentioning stock ticker symbols in here and we want to pull these out into their own column so that later we can correlate or join those columns back with the actual internal trading data so if you think about how this would be done in traditional systems we actually pointed out a couple earlier when he when she was presenting one is the probably the most common way of doing this is actually through hand coding so writing regular expressions other kind of pattern extraction logic that if you're not very technical is actually extremely difficult to write so in trifecta instead what you can do as an analyst is select the data you care about so in this case we'll select an individual stock symbol and try factor will suggest transformations to help you kind of pull that information out into its own column so here we see that the first suggestion is to pull out BBY into it's its own column over here so that that's a start but not exactly what we want if we like we can kind of scan through alternative suggestions that the engine is offering we can also give the engine or the product more examples so here if we select say a second stock symbol in the second row the prediction will update based on the new examples so now we have more examples it'll generalize its suggestion and here in the visual preview we can see now that it's actually pulling out a stock for each news article into its own column so you can very quickly kind of just create new features of interest without having to write any code so this is actually fairly close to what we want but you know an article might mention multiple stock symbols so instead of extracting a single element we'll scan through here at some of the other suggested transformations we can see that we could you know maybe count the number of times transformations appear in the article we might want to do some other operations and as we scan through here we can see in the visual car down here a transformation that's actually taking out a list of all the stocks I mentioned so to pull that out all into one column for us and we can see here again the visual preview all of the stocks mentioned in each article and again we can add this to our steps so that it's recorded in a reusable history so this looks pretty good we now have all of the the stocks mentioned in each article the problem of course is if we try to join this back with our trading data will want one stock in each record instead of having all of them in a single single cell like this so what we'll do again try factor recognizes that this is a list or an array of nested data when we select it will get operations for dealing with that type of data including transformations for doing stuff like uh nesting deriving features from that array and in this case what we actually want to do is kind of flatten this out into individual items so if we choose the flatten transformation we can now see that column gets exploded out and reshaped so that we have now a single stock on each line and the other information for that stock is repeated along with it and we'll add that to our set of steps and then finally we can just clean up this stock very quickly we can select these parentheses that we we won't need when we join back with our stock database and if I select another part to see again we get suggestions for cleaning that up and as we clean the data up you can see now that we have individual stocks the system again is doing some automatic validation and profiling of this data for us so we can see here if we kind of look up at the top that there's actually a ticker data type so the product comes with a bunch of common data types if we click in here we can kind of see the the common data type social security number date times and so on URLs and in others but if you like you can kind of embed your own custom types in the system in this case we've already pre-loaded a dictionary of known stock symbols and that allows us to quickly validate and find anything that's not a valid simple so in this case if we click here on the mismatch and zoom in we can see things that aren't valid symbols and we can quickly kind of delete those as well as any missing values in the data set all right so we've kind of structured the data fairly quickly we've got the stock symbols out one other thing we might want to add just quickly for the analysis is any kind of key words that might be alarming here we see profit warning so I'll extract that and again we can see quickly by selecting it you get this extraction instead what I'll do is just I'll just count the number of times that that appears in the article and we can use that to again correlate later with the the amount of volume happening on the trades and I'll add that to the script and then quickly since I've already extracted the information I need from the news article I'll just drop that column and I'll drop the title column so we've gone from kind of completely unstructured data to fairly structured data here we have the stock symbols we need we have the number of times that it's mentioned a profit warning in the article and we can use that to correlate back with with the trades so now as the next step what I'll need to do is find that trading data somewhere on the cluster wherever it is make sure I find the correct trading data and then join that in with the news article so to do that let's go back into the trifecta workspace and we'll create a new data set and we're going to load in data off the cluster from within trifecta you could you know upload your own data you could search over HDFS or hive and we have some basic search but really when you're when you're searching over something like an entire cluster you really want to rely on something a little bit more powerful so in this case we'll search over a catalogue to find the trading data we care about so here I'm interested in trading data so I can search for trades and I can choose my catalogue provider so we'll go waterline here and I'll hit search and what this will do is automatically populate water line in query water line for trading data within water line so we can see here we've searched on a keyword of trades here at the top and there's actually many different data sets throughout the the lake or the cluster that referenced trades in some way so now the challenge is how do we really figure out which data set is appropriate for this analysis so within water line what water line does in Mohana mentioned this earlier is that any data that's created or modified on the cluster is kind of continuously profiled and that metadata can be used to power power search so if we look here in the far left we can see things like the the size of the data set that we're looking for when was last accessed so we may have done an analysis recently so maybe we know it was done sometime this week we can use that to filter out the filter out the example set and as we scroll through we see some additional information we can search over data quality metrics like the the the density of the data set the cardinality of the data set and in this case what's actually gonna be really important for this analysis is that we know we want to join on ticker alright we have our data set where we extracted the stock ticker from the news articles we're going to want that and the other data set that so that we can join on it so water line actually automatically can profile and find data types and tags across these data sets so in this case we know we want a data set that has ticker we can choose that and filter down the data set in that way so once this loads we can see now we have kind of a single data set that we can inspect to make sure it's it's the right one when we zoom in on it we get additional information again some data quality metrics an indication of the types of data that are in here we can see again the ticker and it looks like the right types of example values that we're looking for and so on we can also see some some other tags in here that you can go in and as you kind of explore in here you get some additional options because this is all done automatically for you in the background you can kind of confirm and annotate these fields with additional tags but in this case this looks like the data set we want so we can then this back into tri factor to wrangle it so to do that within water line you can go to this button wrangle here and when we select that it'll take us back into our create dataset flow where we were searching before but now it's kind of pre-populated the results set with this trading data and you could imagine adding additional data as well but in this case I'll just add this back to our product or our project here the fraud demo and say create and transform and this will bring us back into trifecta again now with the trading data loaded into trifecta once it's in here again you can profile the data we see here on the far left the stock ticker if we want more information about any of these columns you can kind of zoom in on column details and get additional information say about kind of the top values which stock tickers are appearing most frequently if we're interested in suspicious volumes we can kind of zoom in here and look at a histogram of the volumes looking at statistics and other outliers and so on but just to kind of quickly complete the analysis let's instead go back to our news data and let's join in the trade that we just brought in to the project so we'll choose the trade data we can see a preview of the data set and when we go to join we see here that we're getting some inference again using predictive transformation guessing at which types of columns should act as join keys so we see the column we extracted before being joined here with ticker we can go in and edit that if necessary but it looks good instead we'll just choose to join in all the columns and we can create one kind of final result set that we can then use to kind of correlate across attributes from here I want to show you how you can then publish all this information that's been collected both the navigator and trifecta inter sorry from water line and try factor back into navigator so that you can use it for auditing so here again remember that all the steps are being recorded and in this script we can run this on a schedule if we like or we can just run it kind of on an ad-hoc basis within the product so in this case we can kick off a job to run on the Hadoop cluster instead of waiting for that to finish I've pre-populated this already with what that would look like once you've run this over the entire data set here we see the profiling results so again summary of the data quality and for all your attributes and then you can export these results into downstream products in this case what we're most interested is in publishing this out to navigator so we can we can publish the script out you can set this up to either run automatically or to do it on demand so let's publish this out to navigator and then we can show you once we load up navigator so this is starting from navigator again it provides the search interface so you can search over all the operations that have been performed on the cluster and all the data assets that are on the cluster so in this case since we've run a job from trifecta oops logged out here since we've run a job from trifecta I'll search for a trifecta we see a bunch of assets that have been created by users using trifecta in this case we're most interested in operations that were executed on the platform so we can zoom in on operations we see there was a number of different operations that were run here in this case I'll pull up one of one of the news demo ones that I've just performed and when I select that we can see a bunch of different types of metadata that can be recorded within navigator so the first I'll point out is custom metadata so navigator allows all of its partners and third-party applications to actually publish custom added metadata that the application is generating so in this case this is the same script we saw previously in trifecta now being recorded in navigator so you can search over these scripts you can find all the scripts that use things like joins with the trading data and so on or when you're looking at outputs of a job kind of use this as a as kind of an audit trail of where the data was created additionally if we want to jump into the job instance that was submitted actually to the cluster on to yarn we can see that as well so because the job is submitted through clutter all of that technical metadata is automatically tracked so anytime any job or any data is accessed on the cluster it'll appear in navigator and here we can see a job that actually started with two data sets so these are the two data sets we joined in together and then run through a script to create an output if we're interested in zooming in on the actual input data that came in we can do that so here we see that it was the the trades file that we had loaded in earlier and when we select the trade data again we can pull up more details about that file and here again seeing the the custom metadata that water line is publishing into the system we can see all of the tags that water lion is generating for the data assets also being published into navigator so you can search over that metadata in addition to the the metadata that navigator would automatically capture on its own so with that I'd like to conclude the demo and like it's hand it back over the way yeah great thank you so much Shawn I know we're a little bit over time I just have one closing slide and I'll let you guys go to go to lunch ok so you've seen just kind of as a summary that Dana lakes are really really powerful they are an enterprise standard that's getting adopted but the challenges with both end-user access as well as discovery and governance is real our perspective is that an integrated stack with open API is and metadata is the way to go and from that perspective because metadata is so important I want to make a mention that there are actually a lot of development that is happening in the metadata space there many vendors out there today you've heard from caldera and water line but there are other vendors in the ecosystem that are trying to do really really cool things and innovate on top of either Big Data or on top of big data and relational systems also on the open source front there's a bunch of frameworks that have been happening also on the data management and metadata management perspective LinkedIn recently open sourced something called warehouse and you probably have heard from Apache Atlas which is also an open source metadata catalog and again my point here really is if you're thinking about data Lake as an initiative you owe it to yourself to think about how you're really gonna manage this end-to-end I do want to make a plug for a framework in open source development cycle that's called ground and ground is a new dear to my heart because the person that's working on it is Joe Hellerstein who is the co-founder of Tribeca and also professor in University of California at Berkeley so there is a session this afternoon at 1:50 he's gonna be talking about ground with Vikram and ground is an open source metadata framework that relates to a lot of the things that we've been discussing here and then if you have any questions and I know that we're running short on time water line data try factoring caldera we're all here at the show have our booth so you're welcome to come by and and ask questions or see more with that thank you so much and it was a long session 