 hi I'm Kevin dearling the vp of marketing with mellanox technologies it's a great pleasure to be here and the mellow nice guys in the front row here so I'm going to talk about the power of 100 gig and a perfect storm for open networking and also will have netflix here to talk about what they're doing with hundred gig so the perfect storm that we see with open networking is really a combination of three different changes that are happening and we heard about many of them this morning in the keynote sessions and the first one is a technology transition for Ethernet and there's a thing called the 25 gig and 50 gig consortium I'm actually going to be talking about a hundred giggle it'll give a little background on that technology transition at the same time that that is happening we see an open platform transition occurring we call this open Ethernet OCP is really the class a group that's driving that so with the open compute project we've been involved since the earliest days I'll talk about what we've done with ocp but this whole concept of open platforms is really disrupting the traditional players that have closed proprietary platforms and changing the way technology is being delivered today and the third piece that we see is really all these cloud and telco applications embracing both the new speeds and these new open platforms and together with new software to find infrastructure so that instead of running on custom-built hardware you can run on a standard server with standard interconnects together all three of these are combining to create a perfect storm that's going to disrupt the way that we've seen traditional markets with the incumbents there that are building closed platforms so I'll talk about each of these three so first of all you may be hearing about 25 and 50 gig and 100 gig and it's kind of interesting to understand where this came from the on the your left there it's Bob Metcalfe's ether napkin so it was the first Ethernet sketch that he made back in 1973 that said hey here's this idea for Ethernet and that was at the lightening fast speeds of 1 megabit per second when that was first introduced with coax I see some of Nathan's nodding his head here he remembers it so do I actually ran this when I was at Berkeley into the professor's office but over the course of 30 years we got to the point where we had 10 Gigabit Ethernet and so we advanced very quickly over those years and even that wasn't enough and if you go look at some of the servers today that Facebook showing their booth they're using 40 gigabit and if you look at it the way they got the 40 gigabit is you striped for 10 gigabit solutions together you took 4 10 gig links and you got to this 48 it's for by 10 gigabits per second and even that wasn't fast enough so we said hey let's go to a hundred gig and so now we have four x 25 gig last year at ocp we introduced our connect x4 based platform that went into the ocp cards at a hundred gig and it seemed really obvious that we should also just the way we have 10 gig we should have a 25 gig link and we should have a 50 gig link and it turned out that it didn't happen ok it turned out it didn't happen the I Triple E actually looked at it a bunch of people said hey should we have this 25 gig lank and 50 gig links and the I Triple E said no we're not interested it turns out the I Triple E is not actually it is a technical organization but it's also a political organization that there's a lot of companies in that didn't want 25 and 50 gig to happen and so together a bunch of companies there were five companies that formed this 25 gig consortium was mellanox Microsoft Google broadcom and Arista that warmed this consortium and we said we're going to do it ourselves and this is an example of the disruptive technologies that's happening it's no longer these big companies that are controlling that I Triple E that are saying hey you can't do that we just went and did it ok the industry can go do this OCP is an example of things that we can just go do and really disrupt the industry so that's where this 25 and 50 gig is coming from today we say that 25 is the new tent we don't see why people would deploy 10 gigabit today when you can get 25 gig slightly more expensive but a small price premium two and a half X the performance so why wouldn't you future proof your network and go with 25 gig we're also seeing 50 gig now people moving from 40 to 50 and of course 100 so one of the reasons that's driving the adoption of these higher speed networks like 25 50 and 100 is storage faster storage needs faster networks and if you look at this graph here it's showing what's called nvme this is the state-of-the-art non-volatile memory over pci express and a single nvme drive can actually saturate a 25 gig link so we can saturate 25 gigabit ethernet with a single nvme drive so if you're putting 10 gig into a system and you've got an nvme flash drive in that system then effectively you're throwing away two-thirds of the available bandwidth and performance of that nbme drive so really we see with this is the ocp card that we introduced this week the connect x4 LX based we made the contribution to the ocp organization so the design both for our single host and our multi host version of that that goes into the leopard those platforms have been contributed we've also contributed a 50 gig version of this as well for multi hosts and single list so this is a key capability we believe that this faster storage is driving the requirements for faster networks you can see with just three nvme drives we can saturate 100 gig link okay so just three nvme drives in a server and you'll see later that some of these servers can certainly support a lot more than that and need the bandwidth so these are the contributions we made here this week these are some of the new things that we're introducing at ocp so I talked about the 25 gigabit ethernet nix the 50 gig and the 50 gig multi host all of those have been contributed the design specs they're available there in consideration now for being accepted by the ocp organization and on top of that we also were showcasing this week our spectrum switch so we have an end-to-end portfolio products adapters switches cables copper cables silicon photonics cable nixle based multimode cables and the other key thing that we announced yesterday was a CDN reference architecture that you'll hear a lot more from our friends here at netflix and so that's a content distribution network where we did the reference architecture we publish that it's not exactly what Netflix is used in their data center because I think that's very customized for their use case but it's a representative of what they're doing here the other key thing that we introduced was something called open composable network and open composable network is something important i'll talk about here in a second the other key things that we did here was show all of these platforms in our booth invite you to come upstairs and go look at that we're on booth before we have the Yosemite which is a multi host platform they're also showing that in the Facebook booth and this is where we can actually share a single Nick between four servers so each of the server's looks at one neck and it thinks it's it's own Nick it doesn't realize that it's actually being shared by four other servers and obviously there's lots of cost and cabling expenses that you save when you can actually share a neck between four separate host CPUs and servers the other thing we're showing is the leopard which is the dual processor version of that and then the barreleye which is a rackspace based server that's actually running the open power architecture so this is a PowerPC IBM powerpc-based CPU again running on the open power platform so we're showing all of those in our booth exciting times but probably the biggest announcement we made here this week is called open composable networks and I was really excited in the keynote this morning when the telco guys were talking and they said what we need is composability ok so we're talking about disaggregation of the servers and really you have to put it back together if you take it all apart you also need to put it back together and that's the whole idea of open composability and there's really three different elements to that one of them is these open platforms so we call it open Ethernet platforms and they're the real key is is that you need your choice of network operating systems so the other key announcement that we heard this morning from Microsoft is that they contributed sonic into open source so now we have the microsoft network operating system which is a debian based linux operating system that's a switch operating system now that's been contributed to open source through the ocp organization but on top of that we were also showing the cumulus networks which we announced this week is supporting our spectrum switch so cumulus is a linux-based network operating system that's running and our booth sonic is there open switch so open switch is the network operating system open source from HPE so hewlett-packard is also entered this market we have our own network operating system that's running on our platform and Metis which is another company that their network operating system is running on our spectrum switches so really this is the the key first element is these open platforms and open network operating systems the other thing is to make this easy we need standard api's and again you heard Microsoft this morning talking about sigh so sigh is the switch abstraction interface and what that means is that they write their operating system in this case sonic 2 sigh and the underlying hardware is abstracted away so they don't care whether it's a mellanox piece of silicon inside of that or somebody else's silicon they use the same commands and they don't have to rewrite their network operating system so what this means is you get very nimble in terms of not being locked into a single vendor and if one vendor execute and another one doesn't you very quickly move to the best solution that's out there the best hardware that's out there the last piece of course is network automation and we heard people talking about automation as a key and again we have a platform called neo which is our network orchestration package and that interfaces as well with a whole bunch of different third-party tools for doing Network orchestration and network automation we published a blog today about our open composable networks I encourage you to go to our website and you can read that the other thing that I talked about was that we're going to compete at the hardware level not by locking you into the vendor and really we think we have the best performance here and I talked earlier today and gave some details about things like packet forwarding microburst resilience and fairness in the underlying hardware itself in the ASIC that we're doing and we develop our own asics we get 10 to 15 times better microbrews performance than our competition and similarly with bandwidth allocation but most importantly we don't drop packets okay we do not drop packets when we say we're full wire speed that means we're phobar speed across all packet sizes 64 bytes up to 9k jumbo frames and so it's really important that when you're writing an application and you're running with switching infrastructure that if something is wrong it's not in the network you know if you have a congestion problem or there's something in your application that you're sending the data too fast those are things that you should maybe have to think about and worry about but not that somehow on the way through the network some packets got dropped for unknown reasons so we're really all about zero packet loss and predictable networks here's a comparison that we did we published this today what's called the tally report this is showing us on the right that no matter what the packet size is we don't draw packets so this is our zero packet loss performance whereas this is a broadcom based silicon you can see that you know even at 200 bytes they're dropping twenty percent of the packets that are going through the network so we published this today on the thali report you can go tally report com you can read this there's lots more of interesting things in that report but ultimately 32 ports at 100 gigabits per second needs 4.76 billion packets per second anything less and you're going to drop packets and that's not something that your network should have to worry about so the third element that I talked about that's really disrupting this market is now the adoption by the telco players of these open platforms and we saw that this morning and here are some of the companies that are involved here now with open compute platform again we did an announcement last last week or I guess it's two weeks ago now at Mobile World Congress in Barcelona where we showed with our 25 gig adapter we were doing 33 million packets per second again this is line rate or near line rate at 25 gigabit per second using dpd k previously we did 100 gig last year at that conference but really I think the interesting thing here is there's already already an early adopter that's showcasing how in this environment 100 gigabit ethernet can be very important and in a second-tier honor it is netflix and they'll talk about their open connect appliance the OCA running at 100 gig so with that I put together a slide just to kind of show where this is positioned you know it's really we published a reference architecture or reference architecture naturally is based on an mellanox products what Scott's going to be talking about is primarily on the adapter side where they're using us obviously it plays in with all of the different telco players here sanmina was who we based the reference architecture on so it's actually a new Isis box come to our booth and see that and with that I'll letter just got long from netflix thanks a lot Kevin so i want i'm here today to talk about just how great of a partnership we've had with mellanox and how it's really allowed us to build a great product with Netflix Netflix is about entertainment it's about a great user experiences about great programming and it's about helping you find great things to make yourself happy the technology side of it is there because we need the technology in order to make that all happen so um you know you might have heard a few weeks ago that blog post about how everything was being transitioned to AWS that was actually true for all of our server infrastructure but when it comes to content delivery that's part that we that we need to control ourselves that's why we've created our own content delivery network we've been in that business now for for several years we started to grow worldwide lots of infrastructure in in both established and developing countries but with that growth in geographic area we've also had growth and demand higher bitrate movies more customers just more viewing hours per customer so we've had to grow both horizontally and vertically to do that and for a long time we we were deploying 40 gigabit solutions in our data centers trying to get 40 gig of us out to some of our partners but a year ago we we SAT asking ourselves you know we need more than 40 gigabits it's going to be to buy 40 what's it going to be and then suddenly 100 Gigot 100 gigabit came along and it solved the problem that we needed right when we needed it um you know I I'm thrilled to say as an engineer by training I'm thrilled to say that with a single compute node and a mellanox connect explorer we can do 100 gigabits line rate from a single node single socket Intel platform and that's thanks to the status a collaboration we've had with mellanox there they're great driver set and the collaboration we've had with all of our other partners new Isis for building chasse ease and all of our other component vendors um so what does that mean for everyone else we like to lead the way we like to help direct technology to our benefit we also want to have everyone else come along with us because it helps with scales of economy it helps it's not us not operate in a bubble that leads us in the wrong direction we like to work with other people in the industry so what we like to talk about is by building these these these compute nodes are doing 100 gigabits we're really about helping to maximize our footprint get down the cost of serving bits to get down the cost of power get down the cost of data center space but still being able to grow and we want other people to be looking that looking at that too because we don't want to be in competition for data center space we want everyone to be able to come together and everyone to be able to liver a great product the other great thing about this architecture with necklace for and the fact that we can do hunter gigabits and single note is that we don't have to do a higher gigabits in a single node we can start out and we can we can start with with our box that has nvme like kevin said you know a bunch of nvme drives they can saturate hundred gigabits but we can actually start out with 25 gigabytes or 40 gigabits because that's the only interconnect that we have but the fact that we've created scalable solution with mellanox means that when the infrastructure catches up and we're able to do more than 40 gigs we can do fifth year we can do 100 it's a matter of rule relatively simple hardware change out we don't have to fork lift entire racks of equipment out and change them back to to meet future demand so it's really helped us create this not only vertically scalable product but also this future proof a platform so um both of that really fast I always tend to talk really fast but but yeah so it's it's been a great partnership you know we're not technically part part of the ocp platform but we do watch it very closely and you know we're very happy that that L Knox and new Isis have have come to get to make this this reference platform that they have on display upstairs and if there's any questions about it i'll be around afterwards and I can talk to both the hardware on the software side of it so thank you my closing will be just to come see our demos you can come see the new Isis box and our booth Scott didn't mention it but we're actually running freebsd drivers on this and we announced that as part of that and i think there's a good partnership there with netflix and that's part of the secret sauce that's enabled us to deliver hunter gigabits per second on the single core cpu or single socket cpu so encourage you to come out and look at all the different things we talked about the open composable networks and all these new ocp cards and then also the new Isis platforms happy to take any questions as well so do I have any comment about infiniband finna band is fantastic we love infiniband it's still a core business for us it drives a lot of our revenues certainly in the HPC space also in the enterprise space you're using I guarantee if you surf the web today you're using InfiniBand there's data centers that are based entirely on InfiniBand there's cloud data centers that are based on InfiniBand it's still a great business this business happens to be Ethernet related there was we made a strategic decision as mellanox five years ago six years ago to enter the ethernet space today we have ninety percent market share in the ethernet adapters above 10 gigabit per second and so so we love Ethernet it's growing very fast for us but we also love them finna bed stay tuned so the question was how about 200 Gigabit Ethernet stay tuned you know I think there's been a lot of discussions and the I Triple E I kind of bashed that I Triple E it's still a very important organization we are very much involved I hope there's nobody in I Triple E here but 200 gigabit has been defined in I Triple E there's different things we participate at many levels electrical optical single mode you know we have lots of things there's pam-4 has been the technology which is amplitude modulation where you get to bits so it's still sending at 25 gigabyte per second but you're actually taking four of those lanes and you're sending two bits on every symbol so you get to 200 gig and state-owned it's coming Nathan awf work that you've done and the content delivery network yeah so everything that you see with Netflix before you hit play on a movie comes from EWS all our back-end billing and business logic the website the authentication comes from EWS it's it's when you actually hit play that you go over to our open connect network and everything streams off of that AWS is fantastic for us because it's very elastic platform sometimes we need a lot of compute resources because we're transcoding a new release of a TV series and then we're done after after a period of time and we can collapse that back down so to make a comparison of magnitude it's really hard because it's so elastic absolutely absolutely the way we'll get it bring video cash yeah we're just a big web server really sure no spoilers no no spoilers the other question watch it yet yeah we probably have slightly different answers we today I said we have 90 plus percent market share of adapters that are greater than 10 gig that's being driven by the hyperscale guys that were the first to adopt 40 gig Ethernet almost all that 40 gig today 25 50 100 just taking off in terms of volume most of those vendors are consuming our Ethernet devices because of a technology called Rocky I talked about in an earlier session it's called RDMA over converged Ethernet it's reliable transport that doesn't use the CPU Microsoft showed that they were able to get that 100 gigabits per second on on a single server with rocky and when they turned rocky off they only got half the throughput I got about 50 gigabits per second and the reason was is they consumed all the CPU cores running software protocol stacks so from our perspective there's a lot of benefits to running Rocky if you're going across the internet and you're not in the data center then all of a sudden you need to start thinking about tcp/ip because of congestion management there's some magic happening there as well there's a thing called ecn which is explicit congestion notification we wrote a paper co-wrote a paper with Microsoft last year at siggraph and so there's some interesting things that tcp is going to get a lot smarter so you're going to stop dropping packets as a way of congestion notification we're going to tell the network early that hey we're going to experience I always liken it to rearing rear-ending somebody to figure out that there's traffic on the highway it's better to use brake lights so so I think we're out of time but thank you very much 