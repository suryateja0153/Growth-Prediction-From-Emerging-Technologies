 on behalf of the Natural Sciences and Engineering Research Council along with the Royal Canadian Institute for science I want to welcome you to this year's foundation lecture featuring the current Jhansi plan II Award winner dr. Chris Elias Smith so on behalf of Ryerson thank you to n circ and our CIS for choosing Ryerson again as the host for this prestigious event this is now Ryerson's fifth consecutive year hosting the foundation lecture and we're honored to be part of it and I'd also like to thank of course the lee kai-shing knowledge Institute at st. Mike's Hospital our partner in ìbest the Institute for biomedical engineering science and technology and the new biomedical Zone located here in the business building it's a innovative partnership between Ryerson and st. Mike's to advance scientifical scientific and clinical work I also want to congratulate dr. Chris Elias Smith on his prestigious award very much looking forward to his presentation and learning more about disruptive technology and artificial intelligence which I definitely need more of in addition to honoring each year's award winner the foundation lecture is an opportunity to share advances in science and to foster potential collaborations and partnerships across institutions and disciplines I hope you'll join us after the lecture for refreshments and networking and I'm sure dr. Elias Smith will give us much to ponder and debate this evening now I'd like to introduce miss Helen to seen the hundred and 13th president of the Royal Canadian Institute thank you very much Wendy so as Wendy has said on the hundred and thirteenth president doesn't mean that we switch presidents every couple of weeks the organization is a hundred and sixty-five years old and our mission is very simple it's to bring science to the public through venues like this so we provide a platform for scientists to engage with Canadians across Canada we do that through public lectures events pub nights through webcasts and so on so you can go online later and you can actually see Chris's talk where there's many programs out there that introduce science to children and youth we're unique in that we reach out to the public we believe that a higher level of understanding by science about science by Canadians makes us a better Canada so I'm very proud to be the president of such a scientifically important organization we were established in 1851 if you saw some of the graphics going through you can see there's a big Royal Charter given to us by Queen Victoria in 1851 so we're the oldest scientific society in Canada we believe in the power of partnerships and together we can accomplish more than alone therefore over the past many years we've been partnering with Ryerson and other key organisations across Canada who shared the vision of bringing science to the public so we're very pleased to be here today to partner with Ryerson University and within Sirk to bring our foundation lecture in honor of the 2015 Natural Sciences and Engineering Research Council Jhansi Palani Award so it's my great pleasure to introduce dr. Pere sureiy and Sirk's vice president of research grants and scholarships who will introduce our speaker dr. sureiy joined and Cirque in August 2011 he previously held the position of associate vice president of corporate planning and policy at M cirque prior to that he worked at Health Canada where he was director general of science policy Directorate he's also held positions of associate vice-president of the sign branch of the Canadian Food Inspection Agency and director-general of Health Canada's pilot biologics and genetic therapies Directorate an office of biotechnology and science he started his career in public service in 1989 as a research scientist at the Canadian forest service of Natural Resources Canada prior to his current position he also served as chair and member of several peer review committees forensic for the Canadian Institutes of Health Research the Canada Foundation for innovation and genome Canada so welcome Pierre well thank you for those kind words ladies and gentlemen good evening bonsoir it's a real pleasure to be here tonight at the Royal Canadian Institute for the Advancement of Sciences foundation lecture first of all I would like to thank the Royal Canadian Institute for the continual collaboration within Cirque and this lecture is a wonderful way to communicate the work of Canada's top researchers tonight of the the opportunity to introduce you all to the 2015 winner of the John C Palani award dr. Chris Ilya Smith this award recognizes an individual whose research has led to an outstanding recent advance in the Natural Sciences and engineering dr. Leah Smith is a prime example of the excellence the tensor supports and promotes Chris research is helping us to understand as he's put it before our most intimate on the brain Chris and his team develop a model of the brain that is the largest functioning model of its kind in the world it's called spawn hopefully I've got the right pronunciation it promises to revolutionize how we understand the brain and how we treat brain disorders it's an incredible feat for Chris and his team now zoom is usually up to 0-2 bouquets at a conference it's as well so please join me in welcoming our speaker this evening dr. Chris yes thank you very much I feel like I should actually introduce somebody it's everybody else got it you do somebody but no such luck so if we can switch to my slides I'm gonna start by thanking everybody who made this possible thanks very much for the invitation from RCI thanks to Ryerson for providing this facility and thanks of course to insert or the prize and support throughout my research career and I'd also like to offer my special thanks to Professor Paul ng for coming himself it sort of puts the pressure on to some extent extra nervous here but I really appreciate we had a wonderful conversation over dinner so yes it's been fantastic to have all this support and of course you know I wouldn't be here for everybody showing up and coming to listen to a little bit of science right so thank you all very much for being interested enough to show up it's a you know always exciting to talk about basically the my favorite thing to talk about other than my kids perhaps which and that is the brain right the brain this is what Woody Allen once referred to as man's second favorite organ and in fact recently it's become the favorite organ of a lot of very large governments around the world and for this reason there's been a huge amounts of money being poured into brain science and so I'm just gonna take a little quick tour around the world here to give you some sense of what different countries are doing and you know the brain is such a multi-faceted and complex thing that everybody can do something different so in the European Union they announced a 1 billion euro fund which has been focused on building a very large scale model of the brain so trying to build software using developing computers which don't exist right now called exascale computers so this is sort of you know thousands of times more memory and storage and computational power than what's currently available because we need those kinds of resources in order to simulate something as complex as the brain there are countries like Japan who have decided that instead of going the route of trying to build a model they're going to try to understand one particular creature a lot of detail and they picked a marmoset and they faked the marmoset because it is closely related to humans it's sort of good in captivity it's easy to do lots of different sorts of experiments on marmosets and they have a history of being able to collect a lot of data about the marmoset both the anatomy so with the structure of the brain is like and physiology what happens well the marmoset is doing various kinds of tasks like vision or audition or what-have-you in China they've taken a more technological kind of approach so they've been developing a lot of hardware that is taking the sort of massive amounts of expertise that they have in building computer chips and seeing if they can build computer chips that work more like the brain in order to run algorithms which are more like the brain because you know the brain is still much better at certain things then standard computers are if you look to the United States right after the European Union sort of announced their big project not to be outdone the Americans about two months later announced a 1 billion dollar investment into brain sciences this has become known as the Obama brain initiative they have focused mostly on just getting more and better quality data than what anybody else can so they have a lead in lots of areas and being able to record my new details about what's going on in the brain and they're trying to turn this more towards a functional map they call it which means looking at what happens at a very small level while the animals actually doing something so we have a lot of anatomical data a lot of genetic data that you can sort of extract from the brain when the animals not doing anything ie did but you also want to have an understanding of what's going on during activity and so they've launched this initiative which is over a 10-year period to try to get that information at a level of detail which is just not yet available and of course Canada we've also announced a hundred million dollar fund or it's called what is it called 'rain Canada and they have decided to focus mostly on diseases so trying to understand the sorts of things that go wrong and start to get certain kinds of clinical applications in order to sort of improve the future of brain health right so let me so all of the different things going on around the world and I've been lucky enough to actually sit in on a meetings and conferences that all of these different initiatives except for the one in Japan actually and it's quite clear that you know when you're sitting in these rooms there's industry leaders there's politicians there's scientists everyone has come to the realization for reasons which one that don't take long to figure out when you stop and think about it that if we deepen our understanding of the brain we are going to improve Society in all kinds of ways and we can just stop and think about what those sorts of ways are right so in Canada alone we spend about 51 billion dollars a year dealing with mental health issues if we could come to understand how the brain works better we could presumably test drugs right or other kinds of brain interventions while harming far fewer humans or other animals we could maybe come to understand kinds of algorithms for building machines which are more helpful to us right they could become partners in researching deep and difficult questions and maybe in some ways with behind all of these projects is just the realization that if we understand the brain better we're really going to understand ourselves better because it's quite clear that all of the sorts of complexities we see in human behavior in some sense have a root in the brain both the good and the bad so we'd like to understand this because it's quite clear that the implications are enormous right there's huge industrial implications social implications scientific implications but how are we going to come to understand something as complicated as the brain so the human brain has about 80 billion neurons in it billion with a beef that's about as many stars as you can see a huge number of individual cells these cells are connected such like on average a neuron has 10,000 inputs and 10,000 outputs they're about 72 kilometers of fiber connecting all of those cells together this is probably the most complicated device that we've ever tried to understand so how are we going to come to understand something that's complicated so you can if you look at those examples that I throw up there of brain projects you can get some idea right so one thing we're gonna do is call a lot of data and buy a lot of data I mean more data than we've ever collected before about anything else so radio telescopes generate huge amounts of data but recording from a single mouse brain while performing a task is expected to make us need to record about three times more data than what we're collecting for a second from all of the radio telescopes so we're gonna have huge amounts of data another thing we're gonna have to do is we're gonna have to organize that data right just recording information isn't sufficient to do science you really need to organize it and what that means is that we're going to need theories so this is something that's familiar from physics right the way that we've made some huge leaps in physics is by coming up with fairly simple theories simple in the sense that they're short to write down that explain huge amounts of data and ideally we would like to do exactly that same kind of thing in neuroscience but there are some important differences between physics and neuroscience in neuroscience you are essentially dealing with a nonlinear system to start so in physics there are lots of instances where we could sort of simplify things to get to a point where we can write down equations and do very sophisticated analyses on them but when you turn to biology off and that's just not possible because of the complexity of the system so instead of doing the same sort of analyses what we often do is we build simulations so instead of expressing our understanding merely through equations that is just by writing equations down and analyzing them we write down equations but we need to simulate them to see what's really gonna happen right so essentially we're trying to understand how the brain works by building one but in this case we're building a software simulation of one so let's let me sort of try to step you through what it is that we're trying to simulate so this is a single neuron right so we tend to start with a single cell and many of you probably have some idea how neurons work but I'm just gonna sort of run through this quickly so on the left hand side we've got inputs to the cell so those processes are called dendrites the big blob in the middle is the cell body and then the long extension going over to the right this is an axon that's kind of the output of the cell and then it branches into many parts which are going to connect to the dendrites of substance cells so information couldn't keep on flowing so if we go back to the left hand side here what happens is a lot of information is coming in from presynaptic neurons so neurons that are connected to this one if there's enough input that it's such that it goes over a threshold then we see a very brief action potential get emitted and you can see these yellow circles that go flying down the axon every once in a while so that's sort of like the cell making a decision that it has gotten enough information to this point where it's willing to send a signal to the neurons that it's connected to and wants to pass that message on and when it gets to the far right hand end that same electrical spike goes to all of the neurons that it's connected to now before it actually gets for those subsequent neurons it turns into chemicals so it actually goes from electrical to chemical the chemic chemists the chemicals go across a very brief or small space and then induce another electrical change in the receiving cells and when that happens when it goes across that what's called a synaptic cleft so this little space be actually potential that spike can either have a big influence or a small influence or medium influence on the cell that receives it and so we call that a weight all right so we get the same spike going to all of those connections at the end but then they have different weights depending on who's listening so cell number one might have a small weight cell number two you might have a big weight and so on and surprisingly in order to exploit explore neural computation the thing that you really need to pay the most attention to are those weights if you can figure out how to set those weights in the right way then you can build systems that do things like do things like recognize images or control arms or store a memory right that really seems to be the place where computation is essentially happening in the brain so being able to set those ways is basically how we do information processing all right so we begin with a single cell and essentially we can write down enough a few equations to express that process that I just described for you one of gathering information and the dendrites making a decision but whether that we've got enough information and then sending a spike down the axon so most models in theoretical neuroscience that is in people who are you know built by people who are interested in understanding the brain really begin with a single single cell and then they'll typically put many many cells together right so the European project that I spoke about they have built a very large model which they call the blue brain this is kind of the centerpiece of the human brain project and this is a small slice from a big model that they built and in that model they have about 30,000 neurons and they have about 37 million connections between all of those neurons and you can see each of the neurons is really quite complicated it's patterned after specific recordings that have been found in rodent brains so they describe the functioning of one of these cells they use hundreds of equations because they're very complicated machines there's all kinds of ion channels and so on and once you have something that's complicated you can connect it all together and then you can run a simulation and what was being shown in this simulation is the flow of voltage or change of voltage across all of these neurons so if you see any neurons that are blue those neurons are basically doing nothing if you see a lot of red that's very high voltage yellow sort of in the middle and so on and the white ones are the most active they're the most voltage when this sort of snapshot was taken so it actually takes a large supercomputers days to run a simulation of this very small volume of a rodent cortex so this is one example of how people build models another example is from the synapse project this is something that's been rolled into the Obama brain initiative from the United States so they spend about a hundred million dollars on this project and a couple of years ago they had this exact slide up in their PowerPoint presentation and they said you know we built this really big brain model in this particular simulation they had a billion neurons that were being simulated and so cats have about a billion neurons in them so they said we have a cat scale simulation since then they've actually simulated five hundred billion neurons all right so this is five times more than your in the human brain and again you know they take supercomputers quite a long period of time to run these extensive simulations but they're able to say you know we've now simulated something on a scale in fact past the scale of a human brain now there's some important differences between these models that I've shown you so as I mentioned the one with about thirty thousand neurons had hundreds of equations per cell this one only has about two equations per cell it's a very different ways of using computational resources in both case you need a whole lot of them and so of course as you can imagine scientist being where they are there's all kinds of competition about whose models the best and so this project the synapse project is headed by Dharmendra Mota and he said that his project was a tremendous historic milestone the other projects I mentioned before from the European Union is headed by a guy by the name of Henry Markram and he disagreed in fact he went on to say that it's highly unethical of Moda to mislead the public and making people believe they have actually simulated a cat's brain absolutely shocking and in fact he went on for pages and pages about how he should be strung up by his toes and all kinds of things and then he sent it to all kinds of journalists and so you can find it still on the web it's a very interesting example of social connections in science but I actually think that both of these projects are sort of limited what they're trying to do right so what they've done is they've built models of individual neurons and they've connected many of them together sometimes billions of them together but in my estimation they've missed probably the most important thing about a brain why do we all have a brain we do not have brains to make complicated voltage patterns right or to say that we can run 80 billion of them at once we have a brain to control behavior right and you'll notice that I didn't say anything about behavior I mentioned cats but that's just because they also have a billion neurons so there's nothing about behavior in any of these models which means that they connected all the neurons together which I mentioned was very important but they connected them essentially randomly and so when you connect them randomly it's not clear what your what information you're gathering about how real brains actually work because real brains are not randomly connected they're connected in such a way that they realize behavior right they let us remember things they let us see things they let us stand up in front of audiences and give talks on all kinds of stuff so this has really been the focus of the work in my group right is try to say can we write down mathematical theories that let us express ways of connecting neurons together to realize particular kinds of functions right to try to make neurons which we will model in the same sorts of ways that we saw these other neurons be modeled to make those neurons actually do information processing and one thing is pretty clear right if we if we actually do this correctly so if we connect these neurons in the right kind of way we can get interesting things to happen so like everybody we start with one neuron and then we put thousands or millions or lots and lots this is two and a half million neurons together to try to see can we get them to actually do something interesting and this is an example of that model that was mentioned before it called spawn which stands for semantic pointer architecture a unified network it's not just missing and when we do this we can see that we can build things that in this case is performing a fairly simple task right it's a simple task but it is going all the way from input to output so it's got an eye where someone get an image just like we would show to you and we're basically asking it to classify that number and so it uses a 10 to write out what it thinks it saw okay so it's going all the way through and it's actually determining what the tensions and the muscles on the arm are so it can move that arm so the arm has weight and length and is a nonlinear system just like your arm but of course because this is a model right we cannot just look only at the behavior it's important for us to get the behavior but we can actually look inside right so for instance we can look at all of the neurons and how what they're spiking activity is like in the visual cortex that's what I'm showing at the back of the brain now in order to solve this task you can't just see a number right you actually have to remember the number at least long enough to write it down so we can look at the frontal cortices where we see memories being stored so here you can store it see 1/8 is sitting there for a period of time before it is written out and goes away and lastly we can look at what the activity in the motor cortex is and what I'm showing in that bubble is basically the set of points that the motor cortex is trying to make the arm go through in order to express its answer so this is nice because when you have a model now you've got all the data right there's nothing that's being hidden from you in the case of a model that wasn't supposed to happen so there's nothing that's being hidden from you and what this means is that you can go and begin to do analyses on the model of any kind that you've seen people do in experiments so this is showing an example of what happens if we look at the neurons in the visual cortex of the model and we perform exactly the same analysis on the neural activity there that was performed on the data coming from a monkey so here you can see on the bottom row you've got data coming from a monkey performing a visual task where it's basically just being shown natural images and then we took our model and we showed it natural images and then we collected spikes so you can actually record those that neural activity in the form of very brief action potentials in the monkey we can do the same thing in our model we process them in exactly the same sorts of ways and then we see a lot of similarities in the kind of responses that individual neurons have so these are showing basically features and images that those neurons like and you see you get you know some that are sort of slanted in a particular way some that go white black white some that are like circular blobs and some that are more elongated and these are all the kinds of features that seem important for explaining how vision happens in monkeys so it was nice that we had this sort of simple perceptual task but it is a simple task we really wanted to go for cognition right we wanted to understand how can you put neurons together to do something cognitive and so I'm going to show you an example of a cognitive task that you can perform so you can see if you can solve this this is a puzzle and what your job is is to figure out what goes in where that question mark is at the end so when you're doing this this is actually sort of trivial for people sometimes it's very easy for you to look at that and see a particular pattern and figure out how to complete that pattern but in fact only people are very good at performing this kind of task right because what you've really done is you've noticed that there is one thing and then two things and then three things and the thing part doesn't matter right it's an abstraction it's a variable in the first row it's ones in the second row it's fours and in the last spies and your ability to make that kind of generalization just looking at the structure let's call the syntactic structure and so it's called syntactic generalization your ability to do that is something that is uniquely human or at least humans are far far better at it than other animals so we can watch spawn doing the same sort of things now spawn can't look around its thigh it's basically fixed in one place so we're showing it digits right in front of it and I should comment that it's never seen this particular pattern of digits before so it's trying to figure out based on what is being shown how it's gonna answer at the end once it's shown the question mark and presumably just like you got it's gonna figure out that the next thing that comes is three fives and the nice thing is that we could actually then ask you but what would come after that right so if you had to go after three thighs we wanted to go the next in the row what would you say four fives right so you're sort of figuring out that it's just yeah add one that's what happened across the rows and that's how it extended and we can see that spawn makes those same sorts of inferences so it'll generalize in the same sorts of way that people do right so it's looking at those for those structural patterns in the input and it's finding them and this is really interesting because if you actually do this across lots of of these sorts of tests this sort of cognitive test we can show that it scores about the same as people in fact this particular kind of problem is one that's used on fluid intelligence tests called the Ravens progressive matrices and so spine scores about the same as college-age students so it's nice that's one is able to perform these tasks but hopefully it's clear that I'm not just interested in that it can perform these tasks but in how it performs these tasks right we want it to perform them in a human-like way and if that's the case then we should expect human-like mistakes as well and so we wanted to test the model to see if it failed in the same ways that people fail so here's an example of remembering a list of digits just like you might remember a telephone number so we're going to give it a bunch of digits and it has to repeat them back to us and you'll notice that in the middle of that list it's sort of beginning to forget some of the information and so it's beginning to write its answers down and when it comes to that eight it's going to draw a horizontal line which means it's not sure what's there but then it will continue to complete the list right and so just like you remember the beginning and the ends of lists better than the middle and in fact you can show it lists of any length that you want and we can see statistically how similar it is to humans and if we do that then this is an example just showing four five and six item lists but the data on the right which is from humans and the model performance on the left are statistically indistinguishable so you can see obviously slight differences but those differences as far as we know are just statistical differences so this means now that it looks like spawn is able to not only perform tasks which humans are able to perform but it actually is making the same kinds of errors right so it's usually the same kinds of resources and it seems to have some of the same kinds of cognitive limitations and last I want to show an example where it's fun sort of goes into the task not really knowing what to do so this is going to be a sort of gambling task so you'll notice that the very first thing that spawn is going to do is going to guess a number so it's gonna be shown a question mark says what do you want to do and then it has to guess and this task is patterned after something called banded tasks after the one-armed bandit sort of like gambling because you know and what you do how you get an animal to perform a bandit test so I wrote it for instance you put it in a teammate and it will run up to a choice point and then have to either go right or left and it makes a decision to go right or left and then you give it reward or you don't with some probability right so rights gonna get some probability of a ward left is gonna get some probability of a ward and if you leave the rodent in there after a while it will figure out what the most rewarded Direction is and start going that way for a long time and of course being a scientist happens if I change the reward probabilities and so then you can see that the rodent will start basically getting disappointed when it's not getting any more reward and start exploring more and then eventually to figure out where the most reward is and so on and so we can see the same thing happening in the model so the first thing it does it's being told which task to do says guess what would you like to do and it drew the number one and it got a reward it got a one then what would you like to do draws a one and get to zero meaning no reward so it guesses one again zero no reward so now it's begins to change its guess right so it's now beginning to explore the space to some extent begins to guess it too and in this particular instance two is the one that has the most highest probability of a word and so as it guesses the two over and over again it keeps getting reward and so it keeps guessing it too but eventually the scientists that's us in this instance change the probability of reward so now it's much less likely to get reward it still keeps guessing for awhile and they can occasionally just by luck get some reward because it's a probability it's not 0 or 1 but eventually it gets very little reward and again begins to switch strategies and say okay I better try something else and so on and so on and so you can actually begin to gather this information and compare how often does the model choose to go left or right under which probability changes will it make that switch when will it not switch and so on so you can compare the behavior fairly precisely to rodents in the same sort of situation you can also look at the spikes so the individual neuron action potentials in the area that's really responsible for this kind of reward behavior this is in part of the brain called the basal ganglia in the ventral striatum in particular and so what I've done here is just plot the activity of all the spikes in the model at the bottom and then spikes from an experiment that was run in a rodent and then we can just filter them in the same sort of way which gives us the black and gray line on top so that's basically just doing a simple analysis and failed to compare them and you can see that a lot of the dynamics so how fast it's firing when it fires a lot when it fires less and so on it's fairly well we replicated by the model all right so this is fairly you know rewarding to us a scientist to be able to build a model and make matches not only at the level of behavior but also at the level of sort of fine neural detail where when individual spikes occur in the brain while performing a particular behavior but in some ways perhaps the most important thing for us about this model was that it's actually exactly the same model that's doing all of this stuff right so instead of building one model for one task another model for another task we were able to begin to explore how we could have the same model perform lots of tasks right so for instance you can be driving a car and then you can stop your car and decide to go check email because we all know you don't do that at the same time right but you're using the same brain when you do these different things you could then go play a game of chess or a game of hockey and it's the same brain your brain is not changing by enormous amounts as you switching through all these different sorts of tasks and that kind of flexibility is something that you really don't see in machines these days right so we have some very good chess playing machines but they can't then go and answer an email or drive a car right so that particular kind of flexibility something that we were very interested in right so hopefully you get the idea right the spawn in some senses we believe is working kind of like a brain and so you know we were fairly happy with ourselves at this point all right we had built the model that seemed to work like a brain and got lots of different tasks it got some cognitive tasks and some not so cognitive tasks it also helping us understand when brains could switch between tasks what kinds of mechanisms can make that happen it got us some publication in science which was a good thing if one is Paul any award which was a pretty amazing thing so all this stuff was you know making us feel pretty good and we thought you know we're pretty good but not everybody agreed right so Henry Markram came back and he had a different opinion and I shouldn't comment that actually everything I've talked to up to this point was basically what the paulínia prize was recognizing and everything from this point on his more recent work so Henry said it wasn't a brain model so why did he say that well he said that because the neurons that we're using in our model are kind of like the synapse project neurons so there's a couple of equations per cell and in Henry Martin's model he had more equations per cell than we did and so he said that our model basically didn't have enough detail there wasn't enough biological detail in our model to count as a brain level and I think that's not a very good way to evaluate brain models for lots of reasons but another way to think about it rather than just being sort of sad is to say this is a challenge right this is somebody telling us you know you said that your mathematical theory doesn't care about how many equations used for self which is true we did say that so can you actually rise to the challenge and introduce something as complex as my neurons in my model and so we said yes we could we should try that right so here's a really complicated neuron model so we borrow this model unfortunately we couldn't get there exact neurons because they didn't release them publicly so we took something which is very similar and it turns out has pretty much exactly the same number of equations per cell as what they do the same number of sort of compartments and so on and we took this and we actually borrowed it from another group which had made the model and they showed that it had all kinds of really nice bits to the data a lot of sort of sophisticated response curves which you don't get out of the simple model or as you can see these effects of what the resistance is in the soma as a so that's so much the cell body it's a function of the frequency of the input stimulus all kinds of neat things so we can take this really complicated single cell model and then we could go back to our spawn model and we could say okay let's take out that part of the brain and replace it with these really complicated neurons so I want to emphasize that we're not replacing all of the brain with these really complicated neurons because we do not have a computer big enough not very many people do that could actually run two and a half million of those cells as I mentioned even the Blue Brain Project they only around about 30,000 of those cells so we just took one small part of our model we left all the rest as sort of simple neurons but we made that one part really complicated neurons and so of course the first thing we want to do is to make sure that our model still works so I've got spot in the background and then spawn plus in the front that's the one with the complicated neurons and so here it is performing the same recognition tasks that you saw it performed before and you can see that the front and the back kind of look the same the same thing so that's good so we did not break our model when we introduced these complex neurons but of course you don't want to just make your bottle more complicated for the sake of complexity right there's no point if you're going to make a model complicated it should be for a reason right you should be trying to explain something you couldn't explain before so what did we get with these with this addition additional complexity essentially what we got was the ability to manipulate some of the mechanisms at the neural level that we didn't have before so I won't go into the details too much but each of these neurons has ion channels there's many different kinds of ion channels in our simple model we basically had no ion channels or maybe one and the complicated model we have ten different ion channels and their Maps specifically two ion channels you can measure in real neurons and so what we could do is we could look for drugs that would influence some of those ion channels and see what would happen if we apply that drug in simulation to our brain model right so now we're able to actually test the effects of drugs on our brain model the particular drug we chose is called TTX this is a drug which is derived from I believe it is sea urchin poison so it's basically a kind of poison that you find and the reason you feel bad if you get stabbed by one of those is because they basically make your neurons shut down right they're affecting us a particular ion channel called the sodium ion channel and so now we can look at how does that model perform how well does it classify digits that's all that was doing as we change the amount of TTX drug that we're injecting and the amount that you inject directly controls how many sodium channels are blocked okay so here we have the number of the percentage of sodium channels that are blocked and you can see for a long time you do okay right so it's still doing very good recognition up around 96% I didn't mention the model gets about 96 percent and humans get about 98% on this test and then as you begin to crank it up you begin to see degradation and then here we're now down around chance so after about 60 percent of the sodium channels are blocked basically that part of the brain is really not doing anything very useful so this was gonna need because now we had a good answer to Henry Markram saying well look if we want we could use a complicated neurons and B we had a real reasons for introducing them because it gave us more insight into the low-level biological mechanisms but this really was very specific to one part of the brain which is really influenced this one particular task performance on this one task so we were interested in going back to our brain model and saying oh well let's take out another part of the brain and replace that with complicated neurons and then see what happens and so this part that we picked OFC orbital frontal cortex seems to be important for keeping track of what you're doing during a task and the very first test that we did this too was a counting task where basically a you give the model a digit to start at and then you tell it how long to count so you could say start at a three and then count for four and they would start at three and then after a little while it would write down a second cuz that's how far to count but we noticed then this counting past one as soon as we sort of got the drug concentration high enough it didn't do anything so that was really hard to judge whether it just completely broken or you know whether something interesting was going on so with exactly that same part of the brain model affected we tried a different task in this case we're trying a that working memory task but has to remember a list of digits so here we can watch what's happening so in the back again is the regular spawn model the one in front has these complicated neurons but we've now introduced some of that drug and so here is just getting the list of digits that it has to remember and you see that it can sort of remember those digits fine so it's a very short list only four digits but you can see as it's responding it's sort of going kind of slow and eventually it just forgets what it's doing and it stops right so it's basically lost track of where it was in the task and so the nice thing about this set of experiments this coupled with the counting past is we can look actually at the differential effects of drugs across tasks and so this again gives us a little bit more insight into how the same drug in one place can have lots of different sorts of cognitive effects that might show up in different kinds of ways all right so this you know in some ways is a very initial demonstration it's a fairly sort of heavy-handed kind of drug to use one that is well characterized and so on um but you know your imagination can make you think well if you have a drug that we know something about the mechanisms we can begin to look at what sorts of cognitive effects we expect we might be able to get a sense of the kinds of side effects that would happen we can try this across lots of different models when we every time we build a new model we kind of get a new individual that's gonna behave slightly differently and so on right so this is kind of nice it gives us one way of trying to understand how biology relates to high level behavior but we also want to look at sort of other ways that biology relates to high level behavior so I'm going to turn into a slightly different way of understanding this interaction between biology and behavior or biology and cognition and I'm gonna look at the issue of aging right so I'm gonna turn back to that cognitive task that I had I mentioned that it's it's used often in a general intelligence test and in fact we also built another model so it's not spawn it's sort of game after spawn it's a little bit bigger in some senses but doesn't have all the parts it's fun headed it but we were specifically interested in looking at that Ravens progressive matrices and so what we did with this model is we actually got it to do the entire intelligence test so we can do all of the different sorts of tasks and I'm going to start by just giving you a sense of what the kinds of questions that will show up on these intelligence tests are so one is a sequence kind of task so this is sort of three examples one in each row all right so this is a matrix just like before you can see that like before it's just increasing by one right so that's kind of the example that we saw before and so there's a bunch of these sorts of sequence tasks there's also what we call set tasks so this one to figure out what goes in that empty square at the end you sort of have to notice that each row you can also solve these all by columns by the way but each row has one square one triangle and one circle and then they sort of show up in different orders as you go down alright so each row has to have all of them each row also has to have something shaded from the left shaded from the top and shaded from the bottom right and so if you put all of that together you can figure out that what would go in the last square is a triangle that we shaded from the left okay so that's kind of a set task but there's actually sort of two patterns going on at the same time and then we have this last kind of question which is sort of sort of pattern that you see in these intelligence tests this is the operation pattern can anyone figure out what would go in that last square there so something's happening this one is slightly different than the other two yeah right so the first two sports are kind of superimposed to give you the third square okay so unlike the other ones where it was kind of a sequence right or a set here you kind of added the two together right and so at the very end you'd basically take the diamond and the X on the triangle MoDOT and you'd put that all together to fill in the last circle or the last box sorry there and in fact on the real tests they often match a bunch of these different things at the same time so you might have one Square which has both this kind of operation and the set sort of patterns going through it at the same time so they do get quite challenging these these tests and the other thing I should mention is that I can't actually show you any of the real test contents right it's a I was showing you intelligence test questions then that would kind of ruin the intelligence test for you and so it's illegal to publish any of this sort of thing so these are just kind of examples that give you a general sense of the sorts of questions that show up but for the Model T could actually get a copy of the real test and run the model on the real test okay so when you went after we built the model and we can compare it to the data we can see that the model is kind of an average performer right so it's in the middle there it's the one that has the error bars on it and the stuff on the left-hand side these are just different sorts of errors that people would tend to make and so you see that the model also matches the kinds of errors reasonably well but the most important thing for us is that you know it seems to be an average performer right which means that it's doing fairly well and average performer is about as what college age students will do on this test now one of the reasons we were looking at this particular test as I mentioned before is we wanted to understand how biology would affect this and in particular how aging related to performance on these cognitive tests and as you can imagine there's bad news for those of us over 40 anyways right as you get older you get dumber or well that's not the right way to say it I guess you do worse at these tests of cognitive performance okay so let's look at that data so here we can see right younger people score the highest and then as you progressively get older your scores go and you can see that the errors are kind of distributed equally across so we wanted to make our model do that same sort of thing have that same sort of pattern and so what we're really interested in doing is testing two hypotheses about you know why do we do worse on these tests as we get older does anybody have any guesses why we would do worse as we get older what happens to your brain as you get older I think I heard someone say it neurons just died right you get older there's a myth around that sometimes you get bump to the head you just love so much a neuron so you drink a beer and you'll have so much in Iran so it's not clear that it's that simple but it is clear that as you get older you do have fewer neurons in your cortex so neurons will just die off so we can test that model or test that hypothesis now right this is something which is entirely unethical to test in human subjects right you can't take people kill off their neurons and see if they get worse sometimes but conveniently we can do that with our model right so there's another hypothesis around that people who are older might think of before they think of this neuron dying one because you know even me I'm only 45 but still I noticed already that I'm over 40 and this means that you sort of you know things just aren't as crisp and clear as they once were right and there's actually a word for this it's called D differentiation it means that mental representations that you have become more similar as you get older so they're more difficult for you to tell apart two representations and we tested that hypothesis as well as well so we can we know what the right mental representations are across a group of neurons and we can make them those representations more similar or less similar depending on how we set up the model and what we found is that actually both neuron neurons dying and the differentiation could account for some of the loss that you see in performance but in fact to account for as much as you see as best as we could you actually a to both sources of sort of decrease in performance and this is interesting because there was a long debate as you can imagine about people who thought it was all neuron loss and people who thought it was all d differentiation and when it comes down to it it seems like really you need both of those things this is something that you could not have done without a model as I mentioned you can't perform at least one of those experiments without a moral I don't know how you'd confuse people either so when we put both of those in we get this sort of pattern that I'm showing and you'll notice that it does a very good job of replicating the difference between younger performers middle-aged and older performers of this test so this was nice right he gave us a way of really pulling apart these factors that it wasn't clear how you'd be able to pull apart so to this point I've really talked a lot about the sorts of health applications of this kind of research but as was mentioned in the introduction and else is something that we're very interested in is there are other kinds of technological applications as well so for the Ranger of the talk the next five minutes or so I'm just going to go over a couple of the sorts of applications that we've been exploring with these kinds of algorithms that is algorithms all of which have spiking neurons which are coupled together with weights in some particular way to perform a function and for the most part the way that we coupled them together is inspired by what we've seen in biology and how we've built models like spawn so this very first line is an example of just trying to go into the real world right everything I've shown you has been in simulation so far so here we took a very simple robot right you'll notice this is Lego right lay-oh arm we do have an interesting visual system we have a special camera which actually generates spikes instead of images it's extremely fast which is really nice and works much more like a real retina it's called a neuromorphic camera so the neuromorphic means it's kind of like a brain and so we put these two things together and then we showed that we could get this thing to basically write a name writes name oh and one thing that's nice about this is actually how fast it's going so if you've seen robots right before you've probably seen robots right it's not that complicated typically they are nowhere near this quick they're not quite that messy either right but this is because we have both that camera and also because processing in neurons the way we've set it up here actually can be much much faster than what it can be in computers for various reasons I'm happy to talk about later on so now I've shown you how we can sort of move into the real world but we're also interested in not only applying these algorithms to understanding sort of biological kinds of systems but you know maybe if we can control a robot arm we can control something even less like a biological system so something like a quadcopter so this again is going to be in simulation again on the left-hand side this is a bunch of the neural activity that's actually controlling how this quadcopter is flying around and what it's basically doing is it's just trying to fly to that green sphere that you'll see this image on the very top here that's the camera so that's what it's seeing from its point of view and so the first couple of demonstrations are just showing that it can sort of go very rapidly to the circle the other thing I should mention is it doesn't know anything about gravity so it's actually learned that there's gravity and it has to act against the gravity a particular way right there we put a box on it so it didn't know about the mass of the box and here we're gonna drop the box and so again this changes the dynamics of the system a lot when you begin to add masses and remove masses and so on very quickly and so it's adapting to all of these changes on the fly so it's being able to you know account for unexpected states that it's in and then have a controller which can actually still kick the system stable and fly to the targets as required and I haven't shown this part but you know we've got all these other weird parts in the environment these are all kinds of weird perturbations and things that you can send it into and it will very quickly adapt to different kinds of perturbations things like wind and so on so one of the things that becomes clear as you build these sorts of models right is that you need to be able to run them quickly as I mentioned so if you want to fly a quadcopter you need to be able to solve the control problem very quickly if you have a really big model you also need a lot of compute and what's happened is people have began to take this question very seriously at the hardware level and begin to design chips and computers which are called neural Morphin computers just like that neuromorphic camera I talked about where they're structured in order to work more like a brain than like a standard digital computer and the easiest way to understand the difference is that the brain is computing in parallel so there's you know all these billions of neurons are active at the same time and sending messages at the same time in your computer instead and actually they do it kind of slowly in your computer instead you have very very fast communication right so the the sort of clock speeds are very high but it's bottlenecked at some width of a bus like 64 bits in most modern computers now or less and so it's a very different way of sending information around the system and so people have been building hardware like so this is one of our collaborators from Stanford and they're building chips where they're actually organizing the individual transistors in order to simulate the effects of a neuron right so they're taking transistors and they're building a model of a neuron by organizing those transistors in a particular way right so this is no longer digital computation in fact it's analog computation and that's nice too because then you can get very very low-power computers and if you can you know make them massively parallel and so on then you can begin to a not pay a lot of power and B have a lot of computation so here's a very it's a simple preliminary demonstration of this particular kind of neuromorphic computer but it basically is showing that you can compute rotation so you'll see on the far right-hand side that the neural activity which is the wiggly one is kind of out of phase by 90 degrees so it's like 90 degrees behind the input but that means it's actually computed a function from the input and these are all the spikes that are being generated on the chip in the first population which is just getting this input and then in the second population which is computing this rotation so it's simple and preliminary and so on but right now we're working with that group to build the next generation of these chips which should be able to model that's to run that spawn model so we're hoping to have that done in the next couple of years a very different kind of neuromorphic computer is this one this is called spinnaker and this one's from Manchester which is also where Professor pioneers from in case you didn't know and what's being done here is actually a lot of digital chips have been taken so these are the chips that are in your cell phone and they put thousands and thousands of chips on one board so each one of those black squares that you see as about 48 chips or has about 20 chips in it and there's 48 chips on that boards we have about 2,000 processors all working together and each one of those processors can run about 200 neurons 200 to 2,000 it really depends what you're doing neurons in real time so what we can do in this case right so this is digital right but it's still a very different kind of architecture than what you'd find in your laptop or your cell phone and so because it's digital it's a little bit easier to work with than some of the analog computation so we've been able to do things slightly more complicated so here we can see this is that board actually stuck on a little robot it's got two of those neuromorphic cameras right here and it's driving itself around you'll notice it's fully autonomous and what it's trying to do is that guy has a light in its hand in his hand and is trying to go towards the light if the light gets too close then it backs up if it goes one way or the other then it tries to keep the light in the very center so it's a very simple sort of computation but it's a nice example where we've now got sir input and output and a fully autonomous system all of which is running and spiking neurons using the methods that we devised so this kind of just gives a sense of some of the ways that we're sort of pushing forward in the more technological domain as opposed to looking at health applications and in order to move both of those ways of applying this research forward we thought it's very important in order for us to teach other people how to use these methods right so we've developed these methods kind of in our lab and we've used them a lot but in order to you know begin to explore the brain and anything like it's full complexity you need more than just one lab working on these sorts of problems and so what we've done for the last two years have we invited everybody to Waterloo everybody being like people from all over the world to Waterloo to come and learn the sorts of methods that we're using we give them access to these neuromorphic computers we ask them to bring their problems and then try to get them to solve their problem over the course of two weeks or at least get a really good start on their problem and so what I'm going to show you at the very end here is an example just a very quick video showing a bunch of different sorts of examples of things that people did many of which we didn't at all expect so we call this summer school brain camp for fun all right and basically people come as I mentioned from all over the world that came from South Africa China the UK the United States and this is an example of people sort of in this instance they're doing a inference that is using language like representations here's a different terms give me a little kind of robot that's a finger that's a neuromorphic finger so it actually senses touch and something like the way that people and other animals do here you can see some of the hardware that people are getting access to and so we for the first couple of days we just teach them the methods and introduce them to the software platform which we've developed in order to make this sort of large-scale modeling much simpler and then we let them add it right so people get together in groups of two or three and they begin to develop models of all kinds and so begin to show that see them showing up on the Oh that wouldn't disappear too quickly so this is that 48 chipboard this is one of those cameras that camera was actually being used to look down at ping pong tables that was trying to hit a ball as it came flying in it's always nice to see people working together a lot so if there's a lot of collaboration involved you see engineers and neuroscientists computer scientists and so on all show up and try to solve problems and they bring very different skill sets together this is a leach robot that somebody brought with them and then built a programmer or a controller floor that's the finger I mentioned and they got it actually cents different kinds of textures and be able to classify textures this is an experiment that was being run at Cambridge University where they're getting monkeys to perform particular kinds of decision-making tasks and they had it running on the spinnaker board here and that's the model itself which is quite complicated but they were able to use it in order to understand what kind of neural activity to expect in the monkeys this is the same model that was being used and spawned to control an arm but in 3d this is controlling a wheel using the same sort of control algorithm against something very non-biological and these last few pictures are just showing that it's not all work all right so people do come and have fun and hang out and go canoeing on all kinds of stuff but it's always great to be really impressed and surprised by the collaborations that occur for many years since people have been publishing projects which started up at this brain camp and so you know we're very excited to be running it again this year and with that I'm going to stop and thank you all very much for your time and attention okay so in the interest of time we have opportunity for three questions and then maybe you can capture Chris as well at the refreshments time so questions yes sir right yes so the question was how do we I mentioned that we you know the important part is coming up with the connection weights between all the neurons and the question was well how do you learn those weights I guess the sort of shortest answer is that we do not do learning the same way that people typically do learning in order to generate those weights so we have developed a set of methods which don't rely on learning so the typical thing people have done in the past when building artificial neural networks is to take a lot of neurons connect them all together randomly and then have a learning rule which will tune each of those weights slowly over time so you can get your network to perform a particular function we've developed another method which is much more efficient so it can solve the this optimization problem a lot quicker but it doesn't in any way attempt to sort of replicate the process of learning until you actually run them up so when you run the model I showed an example of the model learning you know what reward are what sort of writing down which digit was most rewarding and so you can introduce learning into the model after the fact if you want you can use learning as a way of developing the model if you want but one of the reasons we've been able to build such big complicated models or other people have into schools we also have an awning method of finding those weights so I I mean that's as much as I can take as useful and this yes so the question was do we use any 3d printing not until recently but as soon as we began to go into the world so we wanted to build robots that were actually the kinds of things that could be controlled by these controllers we realized that most industrial robots are not the right kind of robot so they expect you to give them a position to go to and then they'll go to that position this is not how your body works your body is forced controlled so you determine what tensions and torques to put on your muscle or on your joints by using your muscles right and so you actually have to figure out exactly what kinds of forces you're applying it also is the reason that we don't you know crush everything when we pick it up or that we can slow down if something gets in a way that we weren't expecting and so these kinds of robots are generally not available so we had to turn to methods of manufacturing our own robots in order to test out these kinds of controllers and as soon as you begin to manufacture any kind of hardware like that you have to use lots of 3d printing ok so I'd like to invite dr. Paul Yanni down to thank Chris please I believe that well what can you hear me it's no great loss if you can't I was actually invited to introduce Chris this evening only there was like dysfunction in the collective brain and yeah I will now introduce him there he is actually I can tell you what we did before we came into the lecture theater we had a lovely dinner and discussed my own history in Manchester where I was befriended by Alan Turing who perhaps came into your attention through Cumberbatch's film breaking the code anyway Alan Turing told me that I should join him in doing something rather reminiscent of what you've been hearing about trying to build a bridge from mathematics to some achievement of millions of years of evolution what Alan Turing wanted me to work on was the mathematics underlying a pine cone why you have to go certain number of turns before you find another pine which registers with the first one and I told him quite correctly that I wasn't clever enough and went and did something else but what you've heard tonight is an even more daring undertaking because you've been watching somebody in the process of building a bridge from mathematics to the supreme achievement of evolution which is the human brain and I think last time that was undertaken was by Mary Shelley who wrote Frankenstein it was a relief to see some human beings I must say in that film at the end they looked awfully nice but it has been a memorable experience and when I heard that the spawn or spawn plus or double plus responded to rewards there was no mention of how you reward a computer but there could be a reward and that is in 20 years time from now the foundation lecture will be given by spawn with many pluses after it and we all look forward to that meanwhile it's been a lovely evening and thank you for it thank you thank you again for for coming this evening it really was it was absolutely fabulous and I have to confess this is the fifth time that we've done it and this may be among the best because at least I understood more than 50% of what was being discussed I'm not a scientist I also have to say in part because this is the last time I will be hosting this event that one of the big thrills about being here as being in the same room as dr. plan and not just because many of you know he is a distinguished scientist and has won the Nobel Prize but because he shows all of us that we can excel in the lab and also make a real difference in the world and so I'd like around of applause for I was a history major than a business professor so that's part of the reason why my grasp of some of the underlying science is weak but I I did take a few points out of what you said so the first point was that dr. Eliza Smith's research really confirms my own which is I am getting older I am getting dumber and you have much to look forward to and I'm very sad that spawn is not going to solve my problems in the short term the second point is we don't like Henry Markham I made note of that but I did think it was really interesting to hear about how the research is being and the approach is being opened up in a very different way than perhaps was the tradition in the last century so the idea of open science and bringing people together to build on each other's work in a collaborative way I think his is fascinating and really inspiring and I think the last thing for me is the power of interdisciplinarity I mean your background is is astonishing bringing together philosophy on the one hand with neuroscience on the other and I think too often we dichotomize science technology engineering and math and the social sciences and humanities so for me this was really exciting really inspiring and a great way to to frankly in my time at Ryerson I also wanted to thank of course answer Canarsie is as well as the staff in the office of the vice president of research and innovation I hope you'll all stay to enjoy the light refreshments and perhaps delve further into the great brains in the room thank you very much 