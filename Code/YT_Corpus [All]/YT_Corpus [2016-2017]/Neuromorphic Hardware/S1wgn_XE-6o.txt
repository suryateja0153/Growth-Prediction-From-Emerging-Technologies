 so welcome to my talk about accelerated analog neuromorphic hardware it is a short summary of the basket system and the future developments in this area between hide it back collins already showed two days before the outline of the project and it's embedment and how it's in bed in the human brain road chip and i will now go a little bit more into the technical details the motivation i will make this shot because we've heard it so many times and i think it's the same here it's that we want to learn about mm how the biological brain processes information so we want to understand biological information processing and which is more or less the other side of the same coin we want to use it to build future computing devices and for both things we need models to test our ideas and as far as I know there are only two modeling possibilities at the time you can build a numerical model that means a computer simulation or you can build a physical model and that is what neuromorphic hardware or analog no morphic hardware is all about and this physical model represents the model parameters as physical quantities that means as in nature its voltage current or charge and we will see this later you can combine both methods to form a so called hybrid system this is an example the most simple example of a physical model it is a basic model for the membrane of a biological cell you can put this in a simple equation and modulus equation by a simple electrical circuit and here is I think the one key feature which sets our work apart from what we have seen so far is that we do not try to get the time constants in this model closer virology we do contrary we use the fact that the physical properties of the resistors the capacitors are different in vlsi compared to biology and so if we put this all together our time constants here are much smaller than in biology we have higher currents we have lower capacitances and that leads at the end to a continuous-time model for this kind of equations a physical model but at an accelerate time so in our case it can go up to 10 to the power of 6 times the speed of the yeah of biology but due to the problems miscommunication they also use a little bit smaller acceleration factors but our models are always accelerated so they are not comparable to real time systems you should keep this in mind if you see the details of the system later and now if you do build such an urn we can put it in a chip and then you measure some activity so here this is such as an example of a postsynaptic potential with a different number of synapses at different rates projecting on it and you see this is an analog measurement you have noise here on it this is noise here from some feats roof woman from another signal but you might want a little bit more complexity so we use the more differ yeah a little bit more complex model the adaptive exponential integrated fire model which also supports the second variable besides the charge on the membrane for some kind of adaptation and then the circuit just becomes a little bit more complicated we still if the capacitor here in the center but now we have several conductances connected to this capacitance and also we have to add some nonlinear things like a reset term and so on an urn this way then it's layout it and put on the chip this is just an example of the layout of this kind of adaptive exponential equation fire on just to show you that at the end all these circuits add up to a lot of components so this is two hundred micrometers x 20 micrometers and these are two neuronal circuits so I don't think you got the area right sorry in the last slide because it's not what was on the slide in the previous talk it's only this 200 square micrometer spanner and then this norm can have much more complex firing modes because it has now two variables so you kind of simple tonic spike and these are all measurements you've seen the noise again you can always see the measurements as we identify them by the noise and it also supports adaptation you see the spike great changes or bursting modes or the combination of person education which gives you initial bursting and then tonics biking for example this is these are all our firing modes you can record from real neurons and I think what we think it's necessary to be able to replicate them in this kind of physical model system the next step is that you want to compare the behavior of this kind of neurons with the model and to do this you have to resort to numerical simulations because it's the only way to prepare a reference here this is a numerical simulator and then we use our analog circuit and compare the response and try to help rate the response to the model so that and the user of the system gets the response from the known he expects sorry okay so this is what a single neuron does the next step is you want to have seven neurons acting together and here is a simple example you hear only the spike times plotted of now a network of neurons connected in a chain like fashion so this neuron see excite their neighbors and so you excitation going through the network this is now transferred to ball time so in reality this happens here in one millisecond 19,000 in one it's a factor of ten thousands of 100 microseconds it's a real time here with this kind of networks now you can also do more complex things this is just an example of a simple attractor network here the small external input switches the whole system's system between two attractor States or this is what me high shot yesterday we can try to prepare a kind of a Boltzmann machine this in this kind of network so the neurons here learn to represent a certain distribution we have seen this yesterday so this was just a shot over you what you can do with this kind of loons now let's go through all the aspects of modeling neurobiology to make such a model really useful to understand the biological system or to test ideas of the biological system so you have to be able to capture the important aspects of the biological system but I've shown so far is things about the neurons and known behavior so in biology we have a multitude of non morphologies an electrophysiologist and we try to cope with this that these nerds I've shown a heavily parameterised circuits so we have a lot of parameter storages we have 24 analogue parameters for each of these neurons but this needs a complicated calibration before you can start to perform experiments you also in Heidelberg data I'm not going to show this multi-compartment no inspector obligating action potentials dendritic spikes and kept junctions this is all very easy in analog and can be cheaply implemented without increasing the power budget that's what your plan to do is to integrate the slow NMDA plateau potentials and calcium spikes so that you can really build a realistic model of a pyramidal neuron as far as V understand is known today a very difficult problem for our kind of modeling and I'm not sure how you can ever put this in our electrical synapses because here you have then coupled equations and you cannot use a digital spy current spot anymore i have not seen also in the last days i have not seen any idea how to do this in Rome avocado maybe this will still be the reality of numerical simulations for the foreseeable future because this is the end of digital spy current spot if you have electrical synapses you really need an analog connection people of all the neurons so gap junctions is easy because this is only connections from one so mattoon another but for electrical synapses you really have to have a network of analog connection so this is very complicated to build and calibrate connectivity is another thing we have heard a lot of power connectivity in two talks before and we do not aim for this numbers like we have seen let's say human sized mouth sighs I don't care about what my knife only neurons which model some equations this has nothing to do with animals so I can only count them and they are large and I don't get many so I think it's there still more than enough to to useful stuff and learn a lot about how to train neural networks to do things but we need approximately we would need approximately 2.5 million wafers too but then to reach in numbers of the human brain so this is unfeasible but our neurons are realistic from their fan in and fan out so we can easily support 14,000 inputs pronounce you have already done this so this is working so you can build small networks which run very fast to test your ideas of how the training can work in this kind of assemblies and I think small if you look at the numbers few hundred million soon up this is still a lot and the problem is really to that all these synapses need some value items parameter so I'm not sure how the larger networks will be really trained in foreseeable future so I think at the moment this is enough for our purposes this is how it looks like if you put this all together this is 20 centimeter wafer and what you've seen before was all recorded from this kind of circuits or other measurements we're done with this kind of circuit and if you look at the enlargements you see that here the individual chips this is the structure you see below they are interconnected by this small structure here's an enlargement these are small connections they they are put on top of the wafer later we call it post processing and to interconnect the individual chips on the wafer this is how an individual chip looks like you can see we're at the edge where the post-processing was not applied you can see we needs to post processing these small network chips and each Network chip has five hundred neurons and 114,000 synopsis and this way first then have to be mounted and they are mounted in this kind of big system is an aluminium frame and they need some cooling here this is the backside of the wafer where we have some cooling and we have here you can see a lot of fpga boards to handle the communication the reason why we need so many FPGA box here is the acceleration factor we have 10,000 times the data then a real time system is the same number of neurons and synapses so we need 10 times 10,000 times the bandwidth and this means we need 50 context fpgas from stylings to cope with the data rates here each of them has I think one gigabyte of memory to record the spikes traces and to send spikes maybe make an external cygnus into the system spikes can also be sent through ethernet into the system so we can build a hybrid system where we have for example a retina running on the classical computer sending spikes to the wafer the wafer represents some parts of envy one for example does I'm doing something this these spikes and you can also sense by expect to the computer to change for example DD remove the I the virtual I things like this and this modules then and up here in our machine room at the moment we will have 20 of these systems in operation and there's this big opening of the platforms Collins already told you about and so visit the website if you want to get access to this system then you can play around with these networks okay so that's two for scaling the size but there are other very important aspects and this is the topic of the second half of my talk so one thing we have continuous time operation in biology this is also in our modeling mats and analog continuous air model and in the biology we have time scales ranging 4 milliseconds two years and here's the big advantage of the accelerated model and maybe the main reason why we do this is that to catch the biological time scales of years you have to have as fast system or you will stay in the lab forever so the accelerated model compresses this time scales so that they are accessible in the laboratory so to say what we do not have and also this is a big problem i have not a easy solution for the future only an approximate solution it's the control delays you know all the external connections if they are really needed they are very precisely controlled as plasticity in the delay in the in the brain and this is very resource-intensive to build this in an electron circuit so I'm not sure how would this will enter the circuits in ya in the newest near future but and this is very important plasticity I think this is a more main reason for the existence of this platform to test learning to develop algorithms for learning in biology we have led several levels of plasticity the system the most important one I think which is also mostly neglected in is that it grows from a single precursor cell so that it's not there and then you Trey it rain soon upses but it grows and curing growth the network comes into existence and this is something you can mm you'll emulate in an accelerated system because you have the time to really emulate development and you can grow synapses by changing the topology you can grow cells by changing the policy but you need large amounts of memory we already have some provisions for this so in principle you can try development and the most important thing is that the genome codes for complex and diverse plasticity rules it is not that we have one rule and then it learns from the data we have thousands of rules vary Brian over the place of the in the nerves in the brain and only if they all work together correctly then the animal lives and does what it's supposed to do so we need a very flexible synaptic plasticity model and we call it a hybrid model and this is something we are now implementing for the next generation of our systems and I will show a little bit about this soon first to the complexity I was laughing because exactly the same slide shown independently from for me this morning and this just shows the level of detail we know so far about as an optic precision what's happening in the postsynaptic side here this only the postsynaptic side of the sunnah of the presynaptic side assess complex and would give us complex there are a lot of popular publications I just elected to summarizing this so it is much more than a simple rule and I must personally say I don't think any device physics can catch this this is absurd so we will never have a single device doing this not a memory star know the transistors all we need more if you want to capture this and I think this out capturing this we will never have anything like intelligence because this is the basis of biological intelligence this is how we're learning is coded so the odds like everybody man this is too complex so we started very simple we have seen this also a few times in started with spike time-dependent plasticity the famous measurements I think we have seen same data already we tested this in a single in the ER in the chip before the lib a fan scale people before the reference care system this is also remotely accessible so who's interested in doing SCDP experiments in accelerated hard work and also contact us and this works yeah you see the white development here and you see that from distribution it sharpens to to the CCD i won't go into detail so this is spike I'm dependent plasticity and then the question is if you have all this amount how I will going from there from the idea to an experiment and this is something to group the number of parameters you have to fix and in our case we have a pine n script so it's in description of the experiment you are going to do in a derivative of the programming language pizen maybe a few of them of you know this so you describe your network and then the software generates a lot of configuration information to configure all the routing and topology switches in the chip and also if you do not use plasticity it has to calculate a weight for each synapse so it's the tremendous amount of information you need and here's where the problem comes in even if our system is not that large as much smaller than the proposed systems from a lot of colleagues here it still has already has a million of parameters and they have to be selected and fixed for an experiment so we have network topology knowing size and parameter synaptic strength and that the current status is that everything has a bo is pre computed on the host computer and this is not really feasible in the long run because it requires a precise calibration of the hardware so that what you calculating the computer really does the same thing on the real hub and that's difficult for an elopement Giacomo can has some opinion to this and it takes a long time because this is a lot of parameters complicated a floating-point equations on a computer so usually it takes much longer than running much much longer than running the experiment on the accelerated hardware system so we have to do something more to solve this problem and this was recalled hard in the hybrid plasticity this is enhanced way to do spike time-dependent plasticity which will give us the arbitrary relievers from the need to calibrate all the synopsis we will have plastic topology and also delays sorry i just wanted to and but the ico accord is that we want to really replace calibration with learning so this is what we change from the current system so don't look at the details but in basic you have an analog network or that where your synapses are and what we added our to kind of micro processors called plasticity processing units here's a photograph of a prototype you also demoed two days ago so we have synapse re and here you it's the plasticity processor directly next to it and the neurons are below and this plasticity processor is as i am the parallel processing unit it for each column of synapses off in the final trip we will have 256 we will happen an ADC and then the digital value of the measured correlation in the synapse is sent to this processor so for each student's you have a lot of circuits but important is here that we have this correlation sensor and the weight the weight projects to the neuron the dendritic input and produces a post synaptic potential and the correlation sensor measures the temporal difference between pre and post signal but this signal is then start in the syrup soon ops sent soon ADC because its analog to the digital plasticity processing unit which then runs software calculating the update of the weight and sending the updated weight back and the important thing is that the correlation measurement happens on a much shorter time scale than this update loop because the correlation center has analog starch locally and can store the results for several herbs for several correlations so this was a signal that my time is over but I've only took two additional slides so I think it's okay and these are measurement results from this hybrid plasticity using a simple multiplicative SCDP rule I'm the same we already had fully integrated in a previous trip but now this is this measurement just shows that we can replicate with this hybrid scheme also the plasticity here is the time difference and the weight change button this is important thing now we kind of all shapes we want so we don't we can have symmetric ones we kind of a symmetric once we can have non exponential ones so the advantage of this process allows us is that it allows us to use arbitrary curves for update of the weight and not only this you can do much more we have not yet tested this all but you can include additional variables you can use supervised plasticity reinforcement learning you can add in principle anything you can fit in the memory of this process and this is how we together with the way for scale integration this is how we want to proceed in the human brain project with our Harbor development so that we really have a system which can train all these millions of synapses even with this complicated learning rules and non-standard learning rules without external yeah without using an external host computer for this so thank you 