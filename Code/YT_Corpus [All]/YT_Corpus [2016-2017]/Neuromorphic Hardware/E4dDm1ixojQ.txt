  Good afternoon. On behalf of the Networking and Information Technology Research and Development program, including our director, Bryan Biegel, it's my privilege to welcome you all this afternoon to joining us in the hearing, the perspectives of Dr. Katherine Yelick, the Associate Director for Computing Services at the Lawrence Berkeley National Laboratory. The key piece of this really is the opportunity for us to learn about the ongoing interests and the developments and the perspectives in a superfacility for data intensive science. So I think across the NITRD program, in computation and data and networking and a lot of the areas that the NITRD program is most closely focused, this is a wonderful opportunity for us to hear about Kathy's perspectives on current developments. It's also my privilege-- on behalf of the National Science Foundation, I'm Bob Chadduck. I'm a program officer in the Office of Advanced Cyber Infrastructure. So it's my privilege to join you all [INAUDIBLE]. And I am remiss, those of you who are just joining us via the net, to also encourage you all to listen to our next hour or so intensely. But also should you be interested in [INAUDIBLE] and have not otherwise exercised your privilege to contribute to guide the future directions of our country, so also for you all to consider exercising your privilege of voting. Even on the East Coast, I think many of the polls will be available for some time after 4:30. So I think there will be ample time for many of you to go to the polls-- whatever your interests, whatever your perspectives are. But it is a privilege for all of us. So with that, Dr. Yelick, it's my privilege to welcome you. Thank you. Thank you very much. And I'm honored to have been asked to come here and talk a little bit about a very high level vision of what it means to serve data intensive science. And we call this a superfacility. The ideas here are really an amalgamation of things that we've been talking about at Lawrence Berkeley National Lab, but more broadly within the Office of Science. And it's not really just a vision for DOE or for our lab, but really a perspective on data intensive science more broadly. So, the first point I want to start with is that science is really poised for a transformation. And we really want to be part of this transformation. And I think our goal here really is to help to really fuel this kind of a transformation. So, for some perspective, I'm going to go back in time and look at what people think of as scientists. And I think this kind of perception of what a scientist is still persists today. And it's the idea of a lone scientist. And here is this-- so, this is sort of the old school version of what scientists look like. Something that we do within the Department of Energy especially, and at the labs, is really think about team science. And of course this has also become something very important to the university system. But this is a picture of Ernest Orlando Lawrence, who was the founder of Lawrence Berkeley National Lab-- he's down there on the bottom in the center-- and his scientific team. And you will see that in addition himself, there are other people who will later receive Nobel laureates, become Nobel laureates. But also his entire team of administrative assistants and engineers, and that in order to solve these large scientific problems that he was looking at, he needed a huge team. And I think we see this today across the national lab complex. And also as we see large facilities and large projects within the National Science Foundation. Now, what is my view of a new scientist? And this is kind of to disrupt a little bit our thinking about what it means to be a scientist and how they operate. This is a picture of a high school student at the time, Brittany Wenger, who you may have heard about. She was 17 years old and working on a science project. I don't know what your science fair projects were like, but mine was nothing like this. And she created a breast cancer detection tool that is much more accurate, so 99% accurate than previous tools, used a minimally invasive-- what was considered an inaccurate test before. So, it was an existing test that had been used, but it wasn't considered accurate enough really to use at least solely. And what she did was, first of all, teach herself how to program. And secondly teach herself-- and this was, I'm sure, years before-- teach herself enough about machine learning that she could write a program that would analyze the data. She got access to all the breast cancer data that's available on the web, and by signing a fairly minimal form that says you won't de-anonymize things, you can actually get access to this data. And then she presumably used her parents' credit card, or maybe she raised money to buy some time in a computing club in order to run this machine learning algorithm. So, I think what's really interesting about this is certainly a very impressive feat in and of itself. But it's the idea that having access to the data, having access to the computing, and then having special algorithms and understanding of the latest kinds of algorithmic technology and data analysis, can allow you to solve previously unsolved problems, even if you're not necessarily a scientist that's been working in the area for many years. So, the other idea that has changed my perspective a little bit was something I heard from Atul Butte, who's at the University of California and also at UCSF, looking at the computational medicine problem. And what he pointed out to me is that if you're running an experimental trial for biomedical research, you can actually outsource the experimental part of it as well. So we have the idea of being able to outsource the computing, and in Brittany Wenger's case it was into the cloud-- similarly into HPC centers within NSF or DOE. But you can also outsource the experiments. So you can basically go to a website-- and there are multiple sites like this-- and you can buy a set of mice and have them given a particular treatment, for example, and have the results sent to you. So you no longer have to necessarily have your own lab in order to run laboratory experiments. And I think this idea of being able to remotely perform experiments, while I have to say, when I talk with experimental scientists, it makes them very unhappy and nervous about the future of their discipline, I think it is something that we need to think and how that will disrupt the scientific process across a number of different disciplines, not just biomedical research. So, here's a version of how we use our large experimental facilities. And the picture here of the facility is the Advanced Light Source at Lawrence Berkeley Lab. The synchrotron is one of the foundations of the ideas at Berkeley Lab. And so this is known as the ALS. The little picture there is one of the beamlines, the GISAXS beamline, that is used for looking at things like materials. The idea of how-- the old-school, at least, idea of how you use the Advanced Light Source is you apply for time, you get a reservation of time, you come in with your data or your samples or whatever it is you want to study, and you run it through this particular beamline at the Advanced Light Source, and you collect the data. Maybe you brought in a memory stick if you're at one of the new very powerful beamlines or light sources around the world. Maybe you're bringing in a whole stack of disks because the data has gotten quite a bit larger. You take that data home to your own university, and you analyze it. And you write your science publication, and all is well. But your data probably gets thrown away. Now, that's not necessarily because you actively throw away your data. It's because-- and I learned this many years ago from one of my senior colleagues at UC Berkeley when I said, what do you do with all these paper memos that the administration keeps sending you, because it seems like my office, even after a year or two, just became piles of paper? And he said, well, you know, years ago I realized that when I get these papers, I can do one of two things. I can file it someplace, in which case I'll probably never be able to find it again, or I can just throw it away. So you might as well just throw it away, because you're not going to be able to find it again anyway. And I think that's what happens oftentimes to our scientific data. The graduate student leaves, the data cannot be found, or even if you can find it, you're not quite sure what to do with it, what format it's in, or how to analyze it, or whatever. And of course, there are many stories of that across scientific disciplines. So, what's really the essence of the idea of the superfacility? And there's one demo that we did at the Supercomputing Conference in 2014, actually, that I think kind of captures this idea better than anything, and because it ties together a number of different facilities it has been named superfacility, and really gets to this idea of being able to analyze but also preserve the scientific data. So once again, I'm using the Advanced Light Source as the example of an experimental facility. And you'll see a little picture up there on the upper right of a little robot. We do see increasing use at these facilities such as the ALS of robotic support. So that will increase the throughput of these experimental facilities because you can run a much larger number of experiments, or perhaps a more detailed kind of experiment. So robotics, I think, is a piece of the superfacility story about how things are likely to evolve-- not necessarily for every single experimental facility but from many of them. The specific beamline in this case is also this GISAXS beamline, which is looking at X-ray scattering. The particular experiment was to look at organic photovoltaics. The problem with these organic photovoltaics-- well, they're very promising. They're very energy efficient photovoltaics. They look like they would be great to use in mass production. But the problem was when you tried to scale up the manufacturing process, so print them in large quantities, that something happened to the energy efficiency. And so what they did literally was to take a printer for the organic photovoltaics up to the ALS at the beamline, they installed it nearby, and they started printing. And they were watching paint dry, or actually watching organic photovoltaics dry, with the Advanced Light Source. Now, the data that's coming out of this was coming out at such a high rate, they streamed it across the energy science's network, ESnet, which is the DOE network, the wide area network that is run similarly to Internet2. And it was streamed to the NERSC Computing Facility, which is also at Berkeley Lab. But nevertheless at the time, actually, the computers at NERSC were in Oakland. So whereas the ALS was in Berkeley, so it was streaming over ESnet to the NERSC facility there in Oakland. There was a software tool called the SPOT Suite that was built by Craig Tull and his group. And it is used for doing remote visualization and analysis of data that's running at one of these beamlines, such as the GISAXS beamline. So you see a little picture there of what you might be looking at. And in fact, there was an example of a different experiment where a scientist was in Europe on a train, looking at a smartphone. And they could see their experiment running at the ALS. And so this is not to do your data intensive analysis, your sophisticated analysis of the data. But it is really so you can get that kind of first cut to say, is this experiment actually running correctly? And so you use the SPOT Suite for a number of different kinds of analyses, but that's the first web interface to your data in the experiment, which helps you execute this idea of a remote access to experimental facilities. We have a new center of applied math at Berkeley, which is looking at the applied math problems that come up in these types of experimental facilities. And that's led by Jamie Sethian. And I will say a little bit more about it later. But there is a lot of mathematics behind the analysis problem for this data. What you see there in the little green and orange picture, those stripes, are some examples of the types of output that you get from this GISAXS beamline. There's a high performance code called HipGISAXS, which is used for doing analysis of this. But interestingly, this is not just about analysis but also about simulation. So roughly speaking, what you're doing is you're running simulations of a material and then shooting in the simulation an X-ray at that material, trying to figure out what does the scattering pattern look like, and then fit that to the pattern that you're seeing coming out of the experiment. And this is a theme that we've seen across scientific areas that simulation and analysis and experiments are all going to fit together in a lot of future scientific endeavors. It turns out that the simulations that were done here, the HipGISAXS was optimized for GPUs. And it was run, therefore, on the Titan System at Oakridge, which is a GPU based supercomputer. And that system-- actually at the time, that computation required-- they did it with a reservation that was done manually. That is, we call our friend Buddy Bland and Jim Hack at Oakridge and say, can we have your system for a day? And they said, for the purpose of the demo, yes. And the data streams there, and all these simulations are run to try to fit in real time against the data. It turns out they were off by about a factor of 50 in terms of how much computing time they would need. And so that was actually-- say that if we had an exascale system there, we would have actually been able to keep up with it in real time. Now, the applied mathematicians have weighed in, and they think they can come up with better algorithms so it won't take quite as much computing time. But the point of this is that analysis of these data sets when combined with simulation will have very large scale computational challenges as well as the networking and other sort of facility challenges that come up. This is a picture I put together years ago to try to kind of explain to people outside the DOE or outside of the high-performance computing area sort of how I see HPC changing. And I think that traditionally there's been an emphasis, certainly within DOE, but I think more broadly even within NSF and the broader community, that high-performance computing is really synonymous with modeling and simulation, that HPC means that I'm doing simulation. And so it's really supporting the theoretical side of science. And it requires a lot of computing. I think the experimental side of science was not really getting that much attention from the HPC community. So what we're seeing today, because of the growth in the data that's coming from genome sequencers, from CPDs that are inside of detectors and all kinds of experimental devices, from sensors that are embedded throughout-- weather it's in our devices or IOT or in other sorts of sensor systems is an increase in the requirements of computing for experiments, for data analysis about experimental and observational data. And so that is not to say that simulation is no longer important. But as I mentioned earlier, there's actually going to be a tie between doing simulation and doing sort of direct data analysis, to the point that you may not even be able to distinguish between the two. It is really all part of what the scientific process involves. But there is a big increase in importance of high-end computing for data analysis because of these growing data sets. So another example of the integration of simulation and observational science is from cosmology. So this is looking at the Intermediate Palomar Transient Factory, where data is streamed nightly from these different telescopes. There's a certain automatic computation that's done-- so image subtraction to look for transience, to look for things that are different across different nights, for example, in the night sky. You're looking for, say, a supernova explosion to try to find places where you can measure and observe something that you haven't seen before, such as a nearby supernova. And machine learning algorithms are also used to analyze that data. Some of the big problems they have in the future analysis is trying to remove the systematic bias that may come from, for example, the placement of a telescope in a particular hemisphere or the lens that's used or other things like that that can propagate across the data set. So there continue to be interesting mathematical and computational challenges, in addition to just dealing with the size of the data set. The data is already put into these candidate databases automatically. And once again, simulations are really key to helping to interpret the data, because you can't see some parts of the night sky behind the Milky Way. And so you're using simulations that match what you can observe in the sky and then use that to interpret what you cannot directly observe. And both of these examples have led to important publications in the case of photovoltaics and in this case in supernova. Now, my next example of old school science is searching for scientific data. And the example I put in here was-- this is a sort of a silly experiment, by the way. But it was to say, well, if I do Google Image Search, and I'm looking for something like a blue blouse, size 8, I can put that into Google Search, and I can get within a few seconds a whole bunch of blue blouses of size 8 that are either nearby me in Berkeley or here in the Washington DC area, depending on where Google thinks I am. I can figure out how to order it online. And I can put in an image of something I'm interested and get similar examples of that data, of that image. Well, I put in some scientific data. And this is an image of an antineutrinos graph-- it's a little JPEG that you see there at the top-- to see, well, what would Google Image Search do? And of course, I didn't really expect anything very interesting to come out. And I probably could have improved it by putting in the word neutrino into the search field there. But the point is that in scientific data, I get nothing like the kind of information that I can get for, you know, shopping online or various other kinds of image search or video search or whatever I might want to do. So scientific data is very difficult to find, even if it becomes highly available. And just the internet became much more powerful with the search engines than it was when it was just lots and lots of web pages with links between them, and you had to kind of know where to look for things. We need to do the same thing for science. And I think we also need to take a lesson from the internet community, that kind of commercial internet community, that not all of the data is explicitly labeled on the internet for these images and so on. Some of it is labeled and some of it is inferred. And so I hear people often talking about the need to put metadata on all of your scientific data. That is a really great idea, to the extent that we can come up with standards that help scientists who label their data. It's a very important thing to do. But I think we also have to recognize that we may need to do automatic metadata and annotation and analysis of the scientific data. We can determine where it's from, what time it was measured, what type of a beamline, or what other type of experiment was being done. And that will at least help us to find data that may not have been perfectly labeled at the time. So, that's actually one of the ideas here on this slide-- automated search and automated metadata analysis of scientific data sets, as well as, in this case, on demand simulation. So I think many of you are familiar with the Materials Genome Initiative. And one of the projects that we have at Berkeley Lab is called the Materials Project, which is one of the earliest instances of one of the Materials Genome Projects. And this one is specifically looking at, or started with the problem of looking at materials for batteries, alternative batteries, and looking for things that are very energy efficient but also have other properties that are required of batteries. And this just is a picture of the web portal that you could use to access this. And what's important about this example is not the specifics, necessarily, of the interface, but the idea that you could put in a material structure. You can describe, for example, an atomic structure of a material. And from that, it can either look things up in a database, or at least in the vision of the Materials Project, you could at least run simulations automatically based on some kind of an atomic query. And so I think it really changes the way we think about using our high-performance computing centers. Our model for the most part is that a computing facility, whether it's an NSF or DOE, is an allocation of time is given to a principal investigator. That principal investigator gives sub-allocations and accounts access to other graduate students, their postdocs, and the other collaborators in the project. And those people log in and run jobs in response to their own scientific inquiry. But this is really saying that the people who are using the high-end computing systems may not have any familiarity with high-end computing. They're just coming in and querying a database, and it's really these kind of bots behind the scenes that may be running the HPC jobs for them and then running algorithms, machine learning algorithms, for example, that extract information about the interesting properties of materials that come from either the pre-existing simulations in the database or the new simulations that have been run, and perhaps also experimental data for those materials as well. So, those of us who are in the computer science area, both on the facility side and the research side, need to think about what this means if the broader science community moves to other models of discovery. And the first place I'll start with is the computing facilities. Very broadly, the computing and networking facilities are the kinds of things that are done in ACI at NSF. In DOE, as I mentioned, ESnet is the wide area network. This is looking at some historical data that has been-- in terms of the number of petabytes per month that are transferred across ESnet. And you can see the historical growth over that last year and a half. And if you project this out to 2024, it says that ESnet will be sending about 100 exabytes a year of data. But what is at least as important as the growing need for bandwidth within the scientific community is the different types of models that you need for analyzing data. I mean, when you are at home watching a Netflix video or something from one of the other kind of services, you may want to have some extra bandwidth in order to download that kind of stream of data. But these scientific data sets are much larger. And therefore they require a network that can really handle these enormous, say, hundreds of terabytes of data that will be growing to petabytes of data and moving those around, especially if we're going to implement this superfacility model, where you need to be able to stream these very large terabit per second flows of data from one of the experimental facilities into a computing facility. So, two of the innovations that ESnet has had in this space, one of them is the Science DMZ model, which NSF has also adopted, or embraced, which is to look at how you-- a model, for example, how you can structure your network at a local university in order to allow scientific data to flow in and out of that university without all of the usual bottlenecks that occur for administrative and perhaps hardware reasons. And then OSCARS, which is a software facility on the network for bandwidth reservations. Also very important if you want to be able to run these very fast births, and you have real time requirements, such that if you can't capture, for example, a video of a supernova explosion, it's not something you can ever recover again. So you want to make sure that the bandwidth is available when you need it. Question? Yes. So I'm trying to understand how you-- so what are the modalities that lead to that network traffic? I think what you are saying is-- well, let me ask a question. Is it the case today that-- so, the instrument is at one location and then there are computing platforms somewhere else. That's right. So the instrument [INAUDIBLE] will be streaming, or will you dump data on some kind of a storage thing and then [INAUDIBLE]. That's one question. Then the second question, in the future, are you projecting-- because you also said something about real time streaming. And in the future, is that what is going to drive? Yeah. So we are looking at real time streaming from things like the Advanced Light Source at SLAC as well as the ALS at Berkeley Labs, so LCLS and SLAC at Stanford, DOE lab, and some of those-- those have close proximity to the Bay Area. So you can imagine that we can have more bandwidth between those facilities than we would have to, say, Oakridge or Oregon or one of the other DOE experimental facilities across the complex. It is an open question. I think one of the most interesting questions is, what is the architecture of this overall superfacility? And how much computing and storage do you have to put on site? I think what we all recognize from everything we heard about cloud computing-- and I think it's equally true of high-performance computing-- centralization is cost effective. You will save money if you can centralize the computing resources somehow. So, in some cases, we may have to put computing right next to the experiment because you can't do it fast enough. My sort of position is, if you can get the data off the detectors, we should try to move it to a centralized facility because if you're going to store it on a disk, you're going to be limited by disk bandwidth, so maybe you should just send it across the network. It is just a kind of working hypothesis, not a statement that we can necessarily do that. But it's sort of-- I think the model should be, let's see if we can move the data to a centralized facility where we know there are cost advantages of sharing and so on of the infrastructure. And if we can't do that-- and I will say a little bit more about this in a minute-- then we might think about exactly what kind of computing do we need on site? And some of that may be happening down in the detectors themselves. The Large Hadron Collider, this already happens. There's a computation that happens in the detectors. Thank you, Kathy. Thank you, [INAUDIBLE]. As a concession to the limitations of the network that we're using, those of you that are joining us over the web, please hold your questions. At the end of Kathy's remarks, what we will do is we open the voice channels up to you all. So we know you're there, and we haven't forgotten you. So please hold your questions. So, for all of you who are objecting to my answer to the last question, we will certainly hear from you at the end. I realize it's a very controversial perspective on this. So, another example of this superfacility model that was really something interesting done in which the ESnet was central was an example from SLAC at Stanford, the LCLS light source. This was a particular experiment looking at Photosystem II. And the data, once again, streamed over ESnet to the NERSC facility, NERSC at Berkeley. And the interesting thing about this is it did roughly triple the network bandwidth during that period of time when this experiment was running. So this was a limited demo that was done. And over that period of time, the graph here is showing the network to and from the site, and into NERSC, actually. The blue is the network traffic into NERSC. And it basically tripled that network traffic. ESnet is looking at the future of networking and what these kinds of superfacility requirements will put on NERSC, should they need software to find networking to give them more flexibility and reprogrammability of the network. Should they combine the packet optical layers together or use something that's more of a current architecture. So these are all, I think, interesting open questions for the networking facility community, and to some extent the networking research community to make sure that we can keep up with the types of demands that will come from science. The mantra here, by the way, is that this network, ESnet, provides discovery that's unconstrained by geography. So it doesn't matter where the scientist is, it doesn't matter where the computing facility is or where the experimental facility is. But you should be able to do your science anywhere. We need to look at systems that are configured for data intensive science. This is a photo of the latest supercomputer at NERSC. It is named after Gertrude Cori, the first American woman to win the Nobel Prize. And the first phase of it-- and that's really what you're seeing here in this picture, although you can't tell how many racks are behind it-- is a data-- it was configured specifically with the idea of data intensive computing in mind. It has Intel Haswell processors, so very high-performance individual processors. An interesting question is whether [INAUDIBLE] processors, which are in the phase two system that is currently pre-production, how some of the data workload will run on those very lightweight cores. And I think there's a lot of preconceived notions about what it means to be a data intensive architecture. I think we need to really be scientists about this and go back and think about what runs well on different kinds of architectures, both the processor architectures, the network architecture, the storage architecture, and so on. So there is a [INAUDIBLE] to Cori infrastructure for networking so that you can stream data directly into the HPC system. And that was, you can see the speed up there, about 100 times faster from the LCLS facility at SLAC into the Cori system, and also speed up from Globus-- it also connects you through Globus to the CERN facility. One of the things that I think is somewhat broken about our kind of traditional HPC models for these experimental and observational workloads is the idea of batch scheduling. Batch scheduling with high utilization, which today means usually jobs that wait in long queues in order to run. So we run our systems at NERSC, as I think NSF does, at over 90% utilization. They are always running jobs, and that means in order to ensure that there are always jobs waiting in order to run. And NERSC, the head of the data initiative there, Katie Antypas, along with the NERSC director Sudip Dosanjh, looked at-- you know, is this a fundamental restriction about how we run our systems? And so they set up a partition of this Cori system that is dedicated to real time workload, and went out for a small little call for proposals asking who actually needs real time access. And of course, some of the responses they got back were, I'm really sick and tired of waiting in your queue, so I need real time response in my queue. So they didn't look at those. But they did take some that had some very interesting scientific reasons that they needed-- they needed that real time turnaround. One was a cryoEM facility, Eva Nogales' lab at UC Berkeley and Lawrence Berkeley Lab, which is doing real time image classification. There is the Palomar Transient Factory that I mentioned before. And at the Advanced Light Source 3D reconstruction algorithms that are used in this SPOT Suite web portal. So there's other examples there. It's a small piece of the allocation right now. It's really a pilot program. But I think the long term question is, can we run our system still at 90% utilization, have enough workload that is, say, killable? You know, jobs that are doing enough checkpointing that we can kill them off in order to run the real time workload without using a cloud computing model which says, I therefore run my system at a much lower utilization, say, under 50%, in order to guarantee that you can always run-- anybody can always run their job. So we've got to balance those kinds of considerations. One of the other things that has come up when you start talking about data intensive computing is certain communities-- the biology community, is one good example of this-- that say, well, we really like the cloud model because we like to release our software as a whole package system as a container, containerized software. And so at NERSC, they've been looking at the issue of containerized software on the HPC systems, since that kind of Docker technology, for example-- one of the container models was really developed for a cloud computing environment. And what they did is built something called Shifter. And Shifter is a version of Docker containers that runs on HPC systems specifically. This is work that was done with Cray. The current systems at NERSC are both Cray systems. And these are very popular also in higher energy and nuclear physics projects. And you can see some improvements in startup time, where Shifter is that bottom, the orangish reddish line, showing that you do need to really think about how you build this kind of software to take advantage of the HPC system. Now, superfacility, although it sounds like it's just an addition for the facility, it really is intended to encompass the entire research ecosystem that needs to be part of this data intensive science vision. And there are lots of interesting math and computer science research problems, computational and data intensive, data science problems, that come up that are very interesting research questions. The first example I'll mention is the CAMERA Center, which I did mention before, is run by Jamie Sethian at Berkeley Lab. This is the Center for Applied Math for Energy Research Applications and advanced mathematics. This is looking at a number of different sorts of problems, so designing mathematical algorithms for real time analysis, algorithms that transform manual sorts of analyses into automated analyses, algorithms that are used to match new kinds of devices that come up in the world, and so on. And as I mentioned before, algorithms that integrate other analysis tools together and combine simulation with observation. So, just to give you an idea of the complexity of this center, it's funded by the Department of Energy's Office of Science, specifically tied to Basic Energy Sciences, which is the program that runs their light sources [INAUDIBLE] of neutron sources and so on. And what are the connections here? Well, across the left-hand side of this circle, you see some of the different mathematical techniques that are used within the different CAMERA projects in collaboration with the different types of the ESnet user facilities, which also include things like the Molecular Foundry, where simulation is done. And so I won't read through all these, but you see machine learning algorithms, you see a lot of traditional PDE, kinds of things that come up. You see a lot of image analysis, and so on. This gives you an idea of some of the different types of data that they're dealing with-- so [? tachography ?] data, GISAXS data, single particle data, and so on. And the connections, then, between the different mathematical techniques and the types of data that they're looking at. In terms of facilities that they work with, it's all over the DOE complex. There are also all the different labs. There are also remote facilities, such as the Diamond facility in the UK and other facilities in Europe. And there are also collaborators from industry as well as use of third party codes and university collaborations. And here, you can just see the connections between all of these different facilities and the different types of data that they have. So this is a way of kind of getting at the order N squared problem that comes up. If you tried to have the researchers within CAMERA all talking individually to every one of the experimental facilities. Instead, they're kind of grouped by the different categories of data. Now, one of the things that comes up a lot is the question of whether you need different types of architectures, different types of programming models, different types of algorithms in order to work on data problems than to work on simulation problems. And I'd like to look at the analysis that was done in an NRC panel report, looking at what were called the 7 Giants of Data. Phil Collela originally coined the phrase the Seven Dwarfs of Modeling and Simulation, which are shown there on the right. And this has been expanded by Berkeley's [INAUDIBLE] paper to the 13 Motifs, which are not just about simulation, so combines some of these things together. But I think going back to these original seven is useful. The NRC panel obviously decided they wanted to have a complementary set of seven of these scientific kernels or motifs. But because it was big data, they decided they were the 7 Giants. So, some of these things, there's differences certainly between the left and right hand lists, although if you look at them a little bit more closely, you'll see a lot of commonality. There's a lot of linear algebra, both on the left hand and the right hand side that comes up. There are things that look like-- those are called structured meshes on the right. You'll see things like convolutions that come up in image analysis in the data analysis problems and basic statistics, which tend to be kind of embarrassingly parallel, reflecting the types of things that happen in certain parts of the Monte Carlo codes. The one that I've highlighted there is graph theory. And I think that while you see unstructured meshes in scientific stimulation, you tend to not see the kinds of network flow algorithms and things like that that come up in graph theory and data analytics, or graph analytics in particular. But of course we see today with all the work on deep learning algorithms, not only is there linear algebra but there's dense linear algebra that looks perhaps more like the LINPACK benchmarks than a lot of the simulation codes that we currently run today. So there is a lot of commonality between these, and I'll say a little bit more about the graph analytics question in a minute. [INAUDIBLE], one of our junior scientists at Berkeley Lab, and working with Sang Oh, one of our former postdocs now at UC Santa Barbara, and John Gilbert of Santa Barbara, put together this slide looking at what are the kind of linear algebra kernels that come up in a number of machine learning algorithms? And once again, this is trying to understand from the outside-- I'm not an expert on machine learning by any means, and trying to understand, well, really, what do these turn into when you try to implement them on a high-performance computer of some kind? And so it's divided into different categories, such as logistic regression, dimensionality reduction, clustering, and so on. And then the linear algebra kernels that come up inside of them. So there's sparse times sparse. There's sparse times dense. There's vectors. There's matrices. But I think this is a really useful way to think about the types of computational problems that come up in machine learning. And you know, the point is, these don't look all that different than a lot of the kernels that come up in scientific simulations. There's a lot of linear algebra everywhere. What I would say is different is the structure of the matrices. So sparse matrices that come up in data analysis problems may be quite unstructured because there's not an underlying physical domain. And so that will cause issues for how we want to execute those, the kinds of architectures and network bandwidth that you need, and so on, and latency. One of the things that we need to do for these data intensive problems is really scale up the types of analysis pipelines that we have. This is looking at an example from Wes Bethel's group in the visualization pipeline, which is both visualization and data analysis, and looking at-- this is actually some of Dani Ushizima's work, looking at image segmentation, looking at porous structures, for example, and images, and then constructing from that a graph problem that tells you whether or not a fluid can flow within this, so if you're looking at something like carbon sequestration. So lots of interesting algorithms in this data space. Of course, making the scientific tools more interactive, more usable, is one of the other things that I think comes up from data intensive science and has always been there. But I think it needs to be part of the model of the superfacility. And so this is looking at the work of Fernando Perez and others on the Jupyter system. Some of you may be more familiar with the previous name, the IPython system, the more Python specific version. But it's really broadened to other languages. It's widely used across science, and in collaboration with NERSC, they have deployed it on the NERSC systems. There are over 100 users in pre-production use of this. This really gives you these scientific notebooks that you can share with a collaborative community. It looks kind of like a Google Doc, except it allows you to embed code and execute that code. And the question then in what they're looking at is being able to execute code on the HPC systems in addition to just executing code on, say, your laptop. A lot of discussion that comes up in the data world assumes that you want the programming models such as Hadoop and Spark that have really come from the data intensive computing community. And for Spark is-- for those of you familiar with it-- it was developed at UC Berkeley and has been commercialized and is-- they advertise up to about 100 times faster than the original Hadoop. This is a map reduced model, so a fairly simple parallelism model where you can farm off a bunch of independent tasks and then perform reduction, sort of local [? alt-all ?] types of communications operations on them. But what we see is that there's still huge speed up possible, even relative to Spark, and comparing to some of the kinds of programming models that we use in the modeling and simulation community. And when I looked at this a little bit more closely, what I found was a really interesting self-consistent set of assumptions about what the programming model needs to look like, and why Spark and Hadoop make a lot of sense for those kinds of communities. And we may need to rethink them when we look at running them on HPC systems. So, one of the assumptions behind Hadoop and Spark is that there's a very high node failure rate. By the way, one of the reasons for the high node failure rate-- I mean, John [? Schaaf ?] explained to me at Berkeley Lab-- is that there's a local disk associated with each node. So disks fail at a fairly high rate relative to the processors. So going into that with the assumption that you want a huge system, and therefore you need to assume there's a high failure rate on the nodes somewhere in the system. And there's a relatively slow network that was also part of the going in assumption from the cloud community. But there was fast local disk. So that fast local disk kind of perpetuated this problem of failures, or increased the problem of failures, but also gave them an opportunity to solve the problem by storing the data on the local disk. And so that was the original design of Hadoop. Spark took the idea of, well, we don't have to sort everything all the time on disks, and you can get huge speed ups if you avoid that. But you can see this kind of set of assumptions are all internally consistent, but not really consistent with the assumptions that we have in the HPC community around modeling and simulation, partly because we don't have local disks, we have very high speed networks, and we therefore don't have very high failure rates. I mean, we are worried about failure rates on the big systems, but a different set of assumptions can lead to very different programming models. So Spark is still, at least in order of magnitude, slower than Message Task. A number of people have measured this on various algorithms, including algorithms of interest for data science. The network is very important. This is some work done by [INAUDIBLE] and others. And I won't go into details, but this is looking at what happens if you run Spark on a cluster with a fairly slow network, or you run it on a higher speed network. You really do need a good software stack that is a TCP implementation. It's not just about the physical network. One of the other questions that often comes up-- I thought I had an animation on this slide, but I guess I don't-- is the difference in architectural requirements between data and simulation. And kind of going into this, when I first started talking to people a few years ago about big data and what this meant in science, and I would hear two very different things. The first one is represented on the left hand side, which is, oh, the data stuff is really, really boring. It doesn't deserve to run on an HPC system, because it's a massive number of independent jobs, and therefore they shouldn't be running, for example, at NERSC or any of the other NFS facilities. Well, first of all, we don't spend half of our budget buying the network and the HPC facility, but nevertheless, the point is that for things like analyzing the data from the Large Hadron Collider, there are problems which are humiliatingly parallel, if you will. Just massive numbers of independent jobs. The other perspective I heard on big data was no no, you need the best possible network. And actually, you can't even get by with a cluster, because on the right hand side of this, you've got random access into a big large data set. And therefore you need a shared memory machine. And the biologists, for example, are reflective of that perspective, that some algorithms are predominantly run on big shared memory machines. And if they can't put their data into the shared memory machine, they just don't run that problem. They come up with some other way of doing an analysis, or they don't analyze a particular data set. In the middle here, I've tried to sort of capture different degrees of irregularity in modeling and simulation, from kind of compute intensive things to more communication intensive things. But the point is that data-- and by the way, deep learning is in there in the compute intensive things-- so not embarrassingly parallel, but certainly very computationally intensive when you're doing a lot of dense matrix multiplies and convolution. So, I think the spectrum is much more complicated than people often think when they kind of come into it from the outside in terms of what data looks like. It very much depends on what type of data problem you're solving. Just as in simulation it depends on what type of simulation problem you're solving. One of the examples of random access is a problem I've been working on with my graduate student, [INAUDIBLE], and a number of other people, including experts in biology. And this is the genome assembly problem. And I won't go into the details of what this problem looks like, except that it is a problem if you don't have a reference genome. So you're not looking at the human genome. You're looking at some new genome that's been sequenced from, say, a soil sample or whatever. And you're trying to figure out, what are the microbial communities that live in that. It's like putting together a puzzle where you don't have the cover to look at. So you don't know what the reference genome is. You're just trying to figure out how all the pieces fit together. And you read it multiple times so that you have some overlap so that you can manage to put the pieces together because the sequencers produce little fragments of the DNA, not the entire genome all encapsulated. It turns out at least one of the most popular algorithms for this uses a graph on analytics. It's a De Bruijin graph specifically. That graph is stored in the hash table. And so the question was, well, what's the biggest machine that I can get that will hold my hash table so that I can solve this kind of genome analysis problem? And what we've discovered is that-- and those of you familiar with my research will recognize that I've been working for many years on this idea of partitioned global addressed based languages, you don't necessarily need shared memory, hardware shared memory. You do need the ability to directly read and write remote data so that you can, for example, look something up in a hash table without having the other processor, say, receive at the point that you need to look up something in the hash table. So you also need a very high speed, low overhead communication network and things like remote atomics. So these are some things that come up in terms of the architectural requirements. This code can then run, for example, the human genome in about four minutes on the HPC, the Edison system at NERSC, and it scales to actually over 20,000 cores at this point. So something that a thought was really limited to shared memory architectures with the right kind of programming instruction and the right kind of network can really scale much more broadly. Another kind of theme that requires this sort of irregular access is what I call here data fusion. That is combined together observation simulation. This was worked on by Scott French and his thesis advisor at the time, Barbara Romanowicz, and looked at one of our other global address based models, UPC++. The problem is, they're measuring data from seismic event and then trying to fit that to a simulation model. So it turns out that in the middle of this simulation, he had a large FORTRAN code. It was [INAUDIBLE]. It was written in MPI. But he had this one piece of it where he needed to do this sort of random access to assemble a matrix from data that was coming in from these remote sensor devices. And that turned into this very irregular problem that was-- certainly not impossible to implement an MPI, but very inconvenient. And he hadn't really kind of figured out what was a reasonable way to write that, but it was a very natural thing to do in a global address based model where you could just build a big distributed array and then write into some piece of that array. And so that was used and was the first ever simulation of the [INAUDIBLE] at this kind of scale. Going back to the question I had earlier about whether you need to put-- can you centralize all the computation. And a related-- well, maybe at least related in my mind-- question is, what kind of computing do you need, and can use more specialized computing? Can you use FPGAs? Can you use deep learning processors for image analysis? Can you use some kinds of neuromorphic chips or whatever? We're looking at and using neuromorphic for analyzing particle tracking data, for example, which seems to work quite well. And I think that what I see is these special purpose architectures first coming into the superfacility model perhaps at the experimental facility, where the problem is simply the data volume and dealing with that very high speed data volume and trying to do something like extract features or compress the data or filter the data or something like that. And so I think those kind of very regular uniform tasks that need to be performed at very high rates may be the first place where we'll want to look at some of these more specialized architectures. And I think that makes the overall architecture question of the superfacility even more interesting, which is where do you put some of the computation, and what kind of computing is it? And is it something where you want to put more specialized computing at some place? Centralization, by the way, also supports specialized architectures, because you may not have the need for a neuromorphic chip for particle tracking at every university in the US. But there may be a physicist at every one of those universities that wants to access some of those a little bit of the time. And so centralization, just as it's more cost effective for general purpose high-performance computing, may also make it possible to then install some more specialized hardware throughout the system. So, a pretty wide open question here, I would say. So that's really what I wanted to say about extreme data science. And I'll just leave with the statement, I really believe that the scientific process is poised to undergo this radical transformation that's based on the access to data. It's based on our ability to analyze data with more sophisticated algorithms to simulate data and to combine very large complex multi-modal data sets together. And I think there are really exciting opportunities on the computer science side, on the math and statistics side, and also in the facility side, in both networking and computing facilities. And I think it's exciting for the rest of the scientific community more broadly, in physics and biology and chemistry and material science, and so on. And we need to figure out how to work together in order to execute some kind of vision like this. Thanks for-- 