 for the past year we've been working Manny for DARPA but Google's are supporting us on hardware acceleration of cortical algorithms and this is the team members down here I talked about early working that we've done in this area last year what we've been doing is taking a set of algorithms of enjoying success and applying and building accelerators for them so far we've worked on new mentor HTM which you'll hear about this afternoon and spazzy which you'll hear about this afternoon and and building accelerators for those we've also taken on tasks in that with a cogent confabulation which is a bayesian algorithm and long short term memory which or sun look at now as well and what we've been doing for each of these is were tasked by dan harmon strom at DARPA to to work out the potential for the improvement in the power per unit of performance so we've actually designed and one of these built our something to demonstrate that and we've built both programmable versions of accelerators and also non programmable versions a basic architecture is relatively straightforward we have an array of processing elements which in one case of programmable another case are just somewhat configurable we rely very heavily on processor and memory which is basically logic that's connected directly to the memory that read and writes to and from the memory directly in doing its operation thus never uses intermediate storage and this actually has turned out to be very powerful moment is connected to an array of sram's those interment connected to a network which then go to a global memory over a larger network so first the programmable version this relies on a simdi compute tile that was designed in a previous project the model modified here simdi means single instruction multiple data so you operate on data in parallel in parallel processing lines as they're called are key to getting the power efficiency improved include shallow use cello pipelines and the use of scratch pad sram's rather than caches are for intermediate data storage and this is this was simulated in thirty and sixty five nanometers a 32 gig of watts per watt in contrast and modern GPU is about 122 gigaflops per watt however very key to improving the performance efficiency of these out for these algorithms was the processor in memory what we did is we profile these algorithms on suitable benchmarks and GPU from that worked out the hot spots worked out additional hot spots based on our own evaluation of major steps in these algorithms and then made specialized instructions that operate directly on memory in order to speed these algorithms up so for example be some sums across the vector so and what these and all together these give a about a 250 x speed up over GPU in fact without these we only get better 1x speed up over the GPU and the simdi processor this is a key to the speed up though both of these implementations of much dramatically lower power reduction 3d implementation is a key to what we've been doing in November we take tout a to chip stack the two projects on it one was a regular a heterogeneous processor the other one was this simdi engine stacked with SRAM and that is turn going to be integrated with the Taser on diagram for which is a very high bandwidth deep dram about four to eight times the bandwidth of ddr and key to actually building these is a very high density face-to-face 3d interconnect technology at an eight micron pitch called direct dbi for direct bonding interface this was done in 1 30 nanometer because we had to use multi-project wafers and that was the most advanced note though they'll let us get whole way for access to and and that and the wafers are actually coming back very soon as I said we also implemented a sick versions these are designed to just to map the algorithms to the hardware pretty much as is there's some flexibility but only limited flexibility is compared with the program solution so we did one of these force posi which can you hear about this afternoon and this is implemented with a mesh network of our processing elements the map onto the columns within spazzy we also did one of these for new mentor hierarchal temporal memory which is a basically a logic design this we found in this case a ring network was more suitable than a mesh network for the local connectivity the global connectivity is a different network again we've also been playing exploring these other algorithms cosian tabulation and lstm with cogent we actually had the bandwidth to look at the robotic application which is object avoidance that is learning the space around you as you as you as you move around the space around you what we do is we have a lidar that does the 360 degree scan that forms a sentences of information which is used to update the network within cogent which is a bayesian algorithm and using this we can identify route suitable routes through cluttered spaces for navigation of the robot this brings together what actually our conclusions for Cape for potential for improvement over a GPU baseline this was run on the kth video action recognition benchmark these implementations are 65 nanometer we implemented a full GPU baseline this isn't a server class GPU this was in a desktop class GPU the simdi with processor memory gets a 10,000 improvement in speed per unit of power the ASIC surprisingly is a lower improvement but that's because it didn't use the pim enhancements when those added back into the ASIC it will be probably a several orders makes you better than the ice the sim d-unit HTM is a little harder to get a bigger as big improvement sparsely is more locality to it but we saw over get many orders of magnitude movement over the baseline in the mentor as well and key to all these are course heavy use of the the processor and memory as you've heard many times in this forum tight integration with memory is important in these applications finally have two other slides on two different other aspects that we seek collaboration and feedback on one is with other universities we're starting the Center for application of machine learning including neuromorphic algorithms to solve problems in electronic design there's one such problem presented yesterday that we're not working on that the problems we're working on include better macro modeling of i/o IP translation between nodes and and reliability issues and what we're doing is applying machine learning techniques including neuromorphic ones to actually solve problems in EDA and then in this Center is starting in August 2016 with a multiple sources of support finally along with Matt who's back of the room there from sandia i'm on the ITRs emerging research architectures committee and we just finished a chapter on emerging research architectures this is the key figure which includes things done by a lot of people in this room and if you interested in giving us feedback on what we wrote for this chapter will be happy to distributor a preliminary version and and get feedback on it we can see here of course is no the interests here in this room which was the right of this chart for the computation paradigms where you either train offline in the case of conventional machine learning or train in line which caused has been discussed several times today so thank you very much for attention at that point I can hand it over thank 