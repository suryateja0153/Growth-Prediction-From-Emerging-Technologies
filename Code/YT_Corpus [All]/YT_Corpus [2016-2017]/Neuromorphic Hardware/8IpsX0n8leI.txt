 okay so when he was Muslim women Osito and one of the cofounders of bit fusion we are we're a company that just started a little over a year ago so we're relatively new we're based in Austin Texas if you guys are ever in town please feel free to stop by we like visitors so I wanted to talk about uh you know this this curve are these two curves and it's the reason probably most of you are here and it's a reason why we start a big fusion and this is what I believe is you know the problem in computing today right so on the upper curve we have software right that is becoming increasingly abstract and it's also slow it's more demanding right even with exponential increase in transistors exponential increase in computing power people are demanding an exponential increase the number of instances and exponential increase in that course so something is is quite demanding and only that people you know don't write assembly anymore and they are not writing as much C as they used to most graduates are using high-level languages and increasingly a I you know metamodeling right the second curve is the hardware curve and the hardware curve is something is happening you know things are becoming more heterogeneous and this is the pursuit of speed right something's got to give we can't do with modern architectures we need to do something architectural about it and so this I think is the problem that we're facing and the gap is widening software guys are becoming increasingly unaware of what the hardware is underneath and the hardware guys are increasingly having a hard time catching up with software right where you think you've sort of solved the problem with see you know the next year someone's doing something it with a brand new language now what do you do so um really delivering performance delivery efficiency is just becoming harder and harder this is another you know set of graphs that we talk about the softer problem on the right is a survey of the most popular languages and you see that you know more and more there are more scripting languages you know JavaScript Ruby we're not having languages that are specific to statistics like are you're you're coming up with new and new new languages to take care of or to come to actually extrapolate or extract parallelism for the machine and on the on the Left you have you know the you know the advent of machine learning everywhere like literally everywhere that any one analyzes anything there's some sort of machine learning thing every new startup has a machine learning sauce and next to it right it's all about you know instead of automating the machine or physical labor it's automating thought right so where does a human go at that point right we're all you know physical and you know mental thought is automated than what does a human do right so things are happening very disruptive lis I know the on the other hand you guys know this already a right transition scale seems to be ending we're starting to see a lot of cracks with Moore's Law right there's a technology curve which says that you know every 18 months and now it's two years and now it's two and a half years there's a there's a double number in the actual transistor density there's an economic point of view which says that you know the the cost per transistor is is supposed to decrease there's a power argument with with leakage and there's there's also an engineering effort right when I when I first started until at Intel you know what was on the market was 19 nanometer processors right and I used to work in the factory and so I would go to you know these process engineers and they showed me you know and the latest you know the research fabs what kind of transistors are they're actually developing it was nineteen ninety meters from ninety to nineteen that's how far in the future that to look to actually make you know 22 and 14 happen you know this year and last right so what people don't see or what they think is sort of tick tock tick tock they think that oh it just takes a couple years or takes a year to advance the process from now it actually takes more like 10 years right people have been probably working on 14 for a really really long time so this is becoming increasingly hard and if it's Intel is sort of the the Moore's Law tick tock tick tock thing right even their own architectures are increasingly heterogeneous you're seeing so much more fixed functions being embedded directly into the pipeline AVX instructions are heterogeneous right so and so what does this mean you guys all know this as well the way I like to looked at this is you know we started at sort of like the frequency Wars you know battles from the single megahertz to the single gigahertz single-digit gigahertz the era of multi-core where I hear people in that in that time frame say I literally heard this no one could possibly use more than two or four cores so it's no there's you know there's no point in anymore right we'd hear things like that there are of many core many many cores you know virtualization became very popular there I think you're not having more than one user on machine people are becoming especially in HPC much more savvy on parallel programming methods and then we sort of the situation that we're sort of getting into as sort of the era of special Corps where things like many cores are not good enough there's a limited power envelope there's limited sort of bandwidth there's integration problems something has to give you know does the process technology actually give the economic value that it used to I don't know but what we see right now is a lot of people are creating the remote rips right Google Amazon they're creating their own ships right mediatek china is is 22 how to create an indigenous chip on them by themselves right the cost of technically doing business for a technology company in China is to do some sort of technology transfer right like you can't you can't sell into the Chinese market unless you are actually building you know Chinese capability right there so they're also looking into developing their own ship for the next their next supercomputer it's roughly like a V li w a DSP type thing at least that's what the rumors are you know Google Amazon Apple these guys are thinking about vertical integration and people think about vertical integration why not go all the way to the silicon right see Altera Xilinx micron is coming over the top of the processor we have also other tile based architectures that you know you can you can find there's no neuromorphic architectures so there's probably a about ten logos I you know missed on this craft but you get the gist right what I see is a lot of complexity here right and this is sort of unwieldy so in short you know error frequency was easy for everyone except for you know the semiconductor company but now you know the complexity is going to more and more people so this is in my view the way I look at things and there's a big help exclamation work here I believe that you know all of us probably in this room are trying to solve some of this problem right and this problem is so big and it's Whiting widening so much that there's just a lot of opportunity to collaborate right so um there's like a famous quote right Henry Kissinger says you know when there's so little at stake there's so much competition right like with the university professors this is like the opposite problem right there's so much at stake right and that's where collaboration needs to happen right so Solutions at least the way I see it is you need to basically make these two curves for to converge against each other on the hardware side specialized hardware is is is really hard to deal with right you have to use it but you have to make it accessible at least allow developers to use them right so price especially for new architectures right if you're thinking about quantum computing like how like how many millions of dollars do you need to get access quantum computer maybe like 20 million dollars I don't know right there's are the architectures that haven't been you know delivered at volume you may need 10 to 20 K right you know FPGA is you need you know what are there like a high-end one what is it for no wonder like 2 to 5 K right a GPU like maybe a thousand maybe up to 5,000 for the K 80 right so as you as you go sort of in the future the the more heterogenous the more specialized they become really expensive so we need to encourage accessibility somehow in terms of software it's really about at least what I believe is its libraries API is really the tool the tool chain right so that's why we're really excited about SD excel and you know the work that we're has also been doing in supporting OpenCL getting something that looks like C all the way to gates is really a profound thing if you think about it right so supporting actually developing a tool chain that goes all the way to a compiler level I are I think that is like minimum because if you can go to a compiler level IR that means that you can now support a lot more languages use translations wherever possible this is an important one oftentimes when you when you do performance optimization um think about who like who has developed with cooter OpenCL chef hands ok very few has anyone done like any sort of parallelism like OMP like multi-threading right ok so anyone who's done any sort of sort of parallelism work right what you end up doing is you're you're injecting pragmas you're changin you're changin the the architecture of the of the beautiful algorithm that is the is the fundamental thing that you're trying to accomplish right so the guys from from Team T right they show the matrix multiplication right and how it's very simple in their syntax but if you were to implement it in in in a very optimal way you're polluting like the essence of this algorithm with what the architecture underlying architecture is right so imagine if you had a really optimal matrix multiplication than for CUDA that does not look like matrix multiplication anymore it might be like ten times longer a lot of complexity you have to squint really hard to see that I said that's actually a matrix multiplication that's one of the problems developing for a high-performance hardware is the essence of the algorithm gets lost then it becomes very very hard to move that algorithm to another architecture right becomes very sticky so any sort of translations that are possible please do that an ecosystem this takes a lot of money right learning materials conferences a lot of marketing university engagement this is all sort of like boring stuff but you know a new hardware vendor you know actually have to do all this stuff and really the engine that I believe you know drives all this is developers right enabling developers and so developers have to find some sort of compelling reason to try new hardware they also have to be supported so so focus on this talk is really about developers and I'm just going to touch on I guess four or five different architectures and sort of talk about them this is sort of what we think is the developer experience very unhappy so you have issues like this so bender tools only support a specific operating system I have a Mac I want to I wanna you know work on a DSP how do I do that well that DSP vendors are going to support some old version of Linux that Note has used in past five years right you know the tool flows make them as open and accessible as possible setting you up environment licenses right like literally if you wanted to in a lot of cases create a custom like solution you have to talk to like five different people get licenses from everyone and then integrate it somehow and figure out if it actually works right licensing is really is really a big big hurdle there's hardware installation people who the scientists right imagine if they had to install harder themselves it's just right numerous pages of documentation do you really want to read 50 pages of documentation to to you know learn how to use your hardware so the developer experience especially new hardware is really really really hard right no good so I start with integrated GPU is because most of us here probably has an integrated GPU how many how many here have used their integrated GPU do you know that you have an integrated GPU okay like how about like a discrete GPU like CUDA OpenCL okay cool FPGAs cool feels like the same people raising your hands anything else a table no okay cool alright so so the way I'm going to explain these like examples is so integrated GPUs right so these are these are GPUs that are integrated tightly with in the CPU complex so if you look here you look at an Intel processor you have a whole bunch of normal CPU cores and attached to it is you know several execution units right so this is kind of like CUDA cores kind of thing right and what's cool about integrated architectures is you can reuse a lot of the architecture right you don't have to have your own memory controller you don't have to have your own interconnect right you don't have to have your own PCIe interface you don't have to have you know any of these you don't have to have page workers you don't to deal with you know I own them use or like you know virtualmin you know virtual memory right page walking all that all that stuff can be really reused in in in a CPU complex right scratchpad memory you could use caches right so integration saves a lot of gates right it's a it's a really an efficient efficient way of reusing a lot of the architecture the downside of course is the real estate on on this die is really really expensive right so that's why these these things are never that big right it's classic Sindhi architectural single instruction local data this is typically medium sized offload maybe small-to-medium-sized right you're probably not gonna have a great experience gaming very latency sensitive workloads right so if you want to get something done really really quick some small amount of computation really quick you know it's very latency sensitive this is a great architecture try that it looks like this this kind of architecture is pretty popular in media media transcoding and programming models are you know these are the main ones I believe OpenCL direct compute people supposed to amp speer HSA have you guys has anyone heard of any of these and okay so open seal is probably the you know the most common one I think that you know if your developer in general that's looking at source indeed kind of architectures you guys should read up on spear spear is a sort of it's an IR intermediate representation that is compiled from a wide variety of other languages so open Co will compile the spear any new like C++ like 17 will also compile the spear so and you can create your own language intra positive sphere it's a it's a it's a great sort of standardization layer a few if you will I'd say ecosystem maturity is pretty high and right AMD's is very interesting so this heterogeneous integrated approach next one is direct discrete GPU as you guys are probably familiar with this right this is not integrated it's still 70 it's in a discrete coprocessor configuration what that means is there's a host your host machine is connected you know through PCIe to this GPU the GPU is not a master it's a slave right the host tells it what to do your software tells it what to do this is geared towards throughput sensitive workloads you have to have a lot of computation for this to make sense a lot more computation for byte program models the new one is CUDA another one to look out for is sickle it's another Chronos standard I think it's probably going to take a lot of like May years for that to be you know very popular but is essentially changes to C++ for you know for it to be sort of it you know in one file you have code for the GPU code for your CPU all in one you know quickly it's kind of like CUDA the CUDA runtime api ecosystem maturity I think is uh like probably medium to medium high there's a lot of innovation happening from the chromis group so again I would look at spear and I think you know CUDA obviously most widely used GPU API mix so mix is this is many core architecture so this is also in a coprocessor configuration so this would be like a card that you install in your server and it is it's also for a large offloads throughput sensitive and it's it's not really a Sindhi kind of thing although it has a lot of Sindhi hardware on it it's mainly gears for generic HPC you see these a lot in supercomputing so it's a you know pretty popular it supports you know multi-threading with OpenMP MPI and just general general x86 so when you saw this in your server uh basically you SSH to it so like in this in this card is like a other mixed distribution right you say top and you see a whole bunch of course right and you can generally use you can run almost any x86 code the cores in here are based on the Atom processor at low power not highly out of order but there's a lot of them right I think as a developer one thing to keep in mind is as I said before integration is like name of the game and semiconductors right integration like makes like a lot of very interesting things happen right what I mean by that is if you if in this processor here where you have a whole bunch of like low power cores you put in a Xeon like a server class zealand cpu this whole thing becomes bootable as like an independent node and that's really powerful right you can run Linux you can run your database app and it's all running of the school processor that changes everything right because now you're not talking about data transfer between the host and the coprocessor this becomes like a first-class citizen so if I didn't if you had like eight it using your in your machine right now they're talking to each other in PCIe I believe that you know I'm sure IBM in media and Mellanox are thinking maybe around around that area I don't know but that would also be very interesting right let's see the other thing that's being integrated tonight's landing is a fabric so in this coprocessor you might start seeing a network port on this right so imagine 200 Giga bits a second coming to this coprocessor that becomes very interesting right now you can do who uploads like you couldn't before so this is one to watch out for obviously this is all like you know theory it's all executions and conductors right if we could easily see something very interesting from you know IBM in video at PJ's where I talked about the Fernando's the expert he's right there so FPGA is very cool reconfigurable obviously a lot of very cool things I've happened with you know enabling of OpenCL so definitely something to watch for it looks like it could become mainstream in cloud in sometime in you know next five years definitely something to look out for this is what I want to talk about so automata is a if you guys heard about a tongue and a processor yeah from micron so this is like a way way way out there kind of architectures remember I was saying that expensive it is the less mainstream it is well one of these cars is $20,000 so it's way out there I don't expect you know high-volume anytime soon so okay so there's this theory that every computer language whether it's a job C++ or whatever is can be represented something called a context-free grammar right you could write a set of rules and you could explain all of that language in a simple grammar just like you have a you know grammar for different languages so just you know in computation you guys have heard of Turing complete that's our like a fundamental thing of completeness of an architecture context-free grammar is sort of the completeness of expressiveness of a language right so a context-free grammar can always be represented in something called an NFA a non-deterministic finite automata right and it's just it's it's a kind of state machine with certain properties right and every NFA could be represented by a DFA which is discrete finite automata and it's also a finite state machine that that will always and deterministically with a deterministic input so an NFA think of this as a state machine and the state machine is composed of these elements called STS there are state transition elements right and as you get an input you decide if you transition to the next state or not right so you just had matching characters and you have these rules and you can describe whatever your algorithm is based on based on certain rules based on your your state transition right and this fabric has basic like bullying logic it has some counters to do some mathematical operation counting and if you look on micro site you can actually see a nice little simulator where you can wecan or you can actually you know create your own finite state machine so what's cool but this is originally they were thinking of putting these state machines in a dim and a memory like I'm like a dim like you could put this in your like instead of six four gigs you take out one dim and put one of these in it and it becomes a programmable processor well because the Jade expect doesn't allow that there's no way to have interrupts in your in your slot this was really not possible so they went with the PCI config and sort of the same sort of coprocessor configuration so this is an interesting one the kind of workloads that are interesting are basically regular expressions regular expressions that's another thing that could be represented in NFA very efficiently it's great for pattern matching so do you know this guy is he here Dave isn't it okay so this this kind of thing is great for fuzzy matching which is computationally hard for you know standard architectures so that's sort of touching on what I think are sort of the up-and-coming architectures potentially they stay interesting ones what does this mean right you have all these architectures you can go into the websites you can get some getting started guides really the accessibility is a problem imagine yourself wanting to try any of these right these things are pretty expensive it'll take quite a bit of time that you can set up so so when I was talking about sort of that widening gap we want to be part of the solution and so we stick ourselves in this widening gap and the problem W want to solve is we want to make we want to develop software that can translate the value of high-performance hardware to your software right make it as easy as possible basically it you know add layers of abstraction and this is what we mean so while you one of the fundamental problems that we wanted to solve is that one can spend a lot of time to port your code to a another architecture but getting performance from that getting additional scaling from that becomes becomes difficult right you have an application that you've coded for a single GPU you want to now use all the GPUs in your cluster how do you do that well it's it's a it's a basically iterative approach of doing some performance optimizations so here are three three different kinds of scaling that we try to enable one is sort of the heterogeneous system automatically going from a CPU basis into a GPU system Bertil scaling making a single device application to a multi device application right so you can scale within the machine I could take advantage of you know for GPS or HCP using the same machine and then horizontal scaling or scale out taking advantage of all the GPUs and all remote systems so so we've enabled all of this and we tried this across different applications and we've had pretty good success across a rendering computational science computer vision image processing machine learning and and rendering going from like one to four local GPUs right so so depending on with the kind of skin that you're interested in we tried across you know different you know scenarios where you're in the cloud and you're on the low-cost instance how can you scale and get that access to GPU but the question is you know so all this how is this possible I can know which time we have but the way this works is this is the view that we want this is the view that we want the application the user to see which is just their application and all they see is one abstract view of their hardware right this is the application that you wanna see under the curves is a whole bunch of complexity right you have to make things performance we intercept certain api's we split you know the both the data the computation across both the local resources and also across remote resources we have a mechanism to do server discovery so basically you try to make this as plug-and-play as possible and also have the ability to introduce libraries that are specifically optimized for your architectures so let's say you're using that pga and you know it takes four hours to create a bit stream while you're creating that bit stream you could target the the local cpu that's running OpenCL when that mystery becomes available you plug it you plug this in you don't have to turn on the the application and you just run faster so we talked about scale-up that's one of the problems they were trying to solve why don't you scale in which is have multiple micro clients that share an expensive device right so imagine you know you had a HPC hardware that cost ten thousand dollars obviously you can't have that on you know multiple machines you're not talking about performance you're really trying to enable people start using it putting their applications and so what you could do is you could have these really inexpensive small maybe one core to core tech machines and you can access these devices remotely so this this has an economic you know advantage right you can now you know support multiple users enable them on an architecture and help to solve the accessibility problem and so this is this is basically that the announcement of you know Olympics and the fusion working together really providing low-cost instances for people who want to work on GPUs so yeah hopefully you guys try it out and give us some feedback any questions yep thank you in the case of scale in when you're talking about sharing resources I'm just wondering in the case of say kudos specifically how you deal with competing kernels of different sizes that are executing on the on the card because obviously some kernels can be quite long-running and some be shorter there's obviously a time where they didn't have a interfere with each other so so that's a good question so there's two levels of scheduling you can have multiple users target the same server in that case we do for scheduling to the to the actual devices in the number of requests so if you have one request to request you know we basically you know it's a fair scheduler the cuda runtime is really the one that does this micro level scheduling right so and I think CUDA summed up five they do much finer grained scheduling where you could use some you know some part of some cute of course for some application another crude of course without the application it figures out how to in diamond dynamic provisioning so one of the new features in some top five I I would assume it becomes better as time goes on that's one of the fundamental things is that we're not solving a vendor problem right so using you know the multi tendency at a micro level that has to be you know solved by the vendor that's the really the most efficient way to do it so one thing sorry I don't want to say one thing about the the new machine types is that we also and we haven't actually tested any code this way but we also enable OpenCL scale in right with the same exact machine type so we're you know we're kind of leading with CUDA as as that seems to be you know very popular for machine learning and so forth but you can also do OpenCL on the client side and get the same with the same basic economics right exactly thanks Mazz are how do you envision kind of you know you mentioned the kind of the application view in the abstraction and i kind of have the relative understanding with with cuda for example or OpenCL and maybe that's that's the answer but tell me i guess why would this work in the context of Ivette PGA's for example and and what how does the application you know relative to the other things we heard today about SD excel how can we how can we leverage that you know jointly so we so we we've done a lot of tests actually before we enable of GPUs we enabled PGA's actually we started with alternate ones but we also enabled with SD excel it's the same concept so what would the applications use OpenCL we intercept open CL and we all those calls to the to the server now if I look at like this picture here on this side this is where SD Excel lives this is where all the FPGA tools live this is where the compiler the offline compiler lives none of that is on the client machines this is only this is where the application lives right so the applications completely segregated on the client side and the the vendor tools SD Excel is all on the server so that's one way of you know really simplifying things because on the left hand side we could be a Windows machine we could be a Mac right you could be like whatever whatever architecture because OpenCL is a standard right and as long as we remote that you you don't have to expose for example CentOS or you know a Linux distribution that you know what developers not familiar with right pending other questions all right well thank you very much Messer 