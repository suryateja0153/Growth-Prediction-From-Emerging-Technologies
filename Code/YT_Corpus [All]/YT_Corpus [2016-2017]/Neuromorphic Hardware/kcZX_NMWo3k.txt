 my name is Conrad James and I'm from sandia national labs and what I'll be talking about with you today is a project that I've been running for about a year and a half now it's called the hardware acceleration of adaptive neural algorithms project and it's a we've got a very extensive team actually a lot of our team members are actually here today and so if you see someone from sandia national labs more than likely they're affiliated with the project in some form or fashion so please feel free to get in touch with them and to ask any questions we also have some of our partnership folks here with us today I don't know what this is David follett from Lewis roads Labs is here as well as one of our other collaborators tarik Tahoe from the University of Dayton so I think I don't need to go on to this slide too much for this crowd I think everyone understands the importance of data inspired compare data driven computing with regard to the types of problems that are of interest to many of the people in this room in terms of conventional numerical computing obviously there are cases where closed-form equations or very rigid boundaries just don't make sense and so in terms of data during computing looking at things where the boundaries are fuzzy where the connections between data are much more tenuous it's much more important to have a data-driven or as we term here a neural inspired approach towards computing and so in terms of the the types of applications that were interested in I think a lot of people understand the importance of imaging and video data processing with regard to neural inspired computing and obviously you have situations where you have a world that you then need to collect data from and then to take that data and actually find salience in other words what's important data and what's not important data and we think that this problem set can also be extended to the cyber security issue and that's one of our primary focuses for the project is to take this formalism which is very very well tuned for imaging and to apply it to the cybersecurity scenario where obviously you have a world you need to collect data and then again you need to extract features from that data in order to understand what's important and what's not important and obviously the end goal is to have this sort of interpretation or behavioral analytics where you take this this enriched data set and then try to come up with what's important about it and in the case of this toy example shown here there's a new manufacturing plant that's being put up to two operation or in the case of the cyber security scenario where you now have a potential security violation that's occurring so for the project in terms of how we're formalizing this this approach what we want to do is to leverage neural algorithms in concert with existing hardware that has been reformatted in reshaping and nor morphic architecture and then also to develop new ways of actually processing new types of data using new hardware that's actually commensurate in better shape for learning applications and again with the with the ultimate goal of having this sort of a system where you could make a small card that you would put into the back of a satellite or maybe into the back of your desktop computer to do this type of detection so when it comes to thinking about again this this idea of neural inspired or David data-driven computing I think we all understand how much that this technology is emerged as developed and evolved over the last four or five decades and so again we used to start off with a very simple network such as this working on handwritten digit data and now you see that the the problem sets have become much more complex the approaches have been come become much more sophisticated and now we're solving very complicated problems both in industry and academia and looking at things such as pattern recognition and also caption providing for for image data and video data and again one of the things that we wanted to stress in this project is that when we again think about neural inspired we typically think of data-driven but the question is can we make the algorithms approaches much more similar or better connected to actual neurosci it's so if we if we think about how a lot of these different algorithms work whether it's deep learning or other methods you typically talk about lots and lots of training and we know that the brain doesn't quite work the way that some of these algorithms work and so in looking at some of the more recent demonstrations we do think that there is becoming better connections between algorithm development and actual neuroscience and so this is just an example from the new sim lab from a couple of years ago where they're actually took a liquid state machine approach to better understand how the prefrontal cortex integrates data selects inputs and actually makes decisions in terms of target tracking in this particular application and so these sort of temporal type algorithms that better handle temporal data and these kinds of data sets is something that's much more similar to to what the actual brain performs and just another example from the Tenenbom lab this is a publication from a couple of months ago in this case what they did was they developed a Beijing program learning where you essentially have one shot learning now this is much more similar to what the brain actually does where the brain doesn't usually and we could probably debate this but you typically don't require 10 million different types of data sets for training a brain and how to recognize particular objects and so in this case this is something that's much more similar to what the actual brain does and also a secondary example from this publication where they take concepts that have actually been learned and by breaking these concepts up into individual components that can actually start to generate new ideas for what a new concept might be and so again that's something that's very much more similar to how actual brains operate so we're encouraged by a lot of these more recent developments in trying to bring neuroscience back to neuro algorithm development and so this is just an example of one of the ways that we're trying to develop this approach to formalize new algorithms from theoretical neuroscience and so in this case we're looking at inspiration from the hippocampus and we have a couple of different examples only one of which I can actually go into today but what you can essentially see here in this this small vignette is what we're trying to deal with is this issue of drift so if you start off with the situation where your data is collected and is in place what happens as new data comes into place or as the system achieves new representations that are coming into the world and so in this case with these black dots representing this in this vignette is existing data and what these purple spheres represent are sharply two neurons that are able to actually encode for that new data and to provide representations for the system at large in the second case here what you have are more broadly tuned neurons that in which you can see is essentially the sphere is much larger so these neurons are less finely tuned and they're more broad in terms of their ability to represent and encode information and what Brad and some of the members of his team are doing is coming up with a way of using mixed coding where you have both of these sets of neurons that better allow your system to represent new data that's coming in which is shown by these yellow circle shown here and again this idea of concept drift is very difficult for these types of problems because as you'll see in some of the slides coming next dealing with new data while maintaining earlier representations is a very difficult problem and so there will be a lightning talk and poster on this by will severa I think tomorrow here at the nice workshop so in terms of what we're trying to do what we wanted to develop is a a set of vignettes that will demonstrate our approach towards solving this problem so again we have our two applications that are really driving the project at this time cybersecurity and imaging and what this particular set of three set of example shows here are how we're breaking up the algorithms problem and it essentially goes in degree of increasing complexity from the left to the right and so I'll walk through these examples somewhat quickly here so when we're dealing with the first set of problems in terms of feature extraction our exemplar problem is sparse coding so in this case the toy example that I'm showing here some vehicle that's trying to be tracked and in terms of sparse coding just think of this as a an abstraction of a dense set of data into more sparse representation where you now have these features that are being pulled out that are more easily compared contrasted transformed in order to help with the classification of this particular object in this case and so this is one of the approaches that we're taking for the algorithms development now the next step in complexity is to then take these these features these static features that have been extracted from your data and then to start thinking about well what are some of the temporal components of that data so in this particular case we have an example of a vehicle that now has a continuous pattern that it's that's going through that is unique to that particular object in comparison to other objects the next step of complexity is where your again getting back to that concept I mentioned on the earlier slide this concept drift now in this case it could be changes in the actual object itself these features these static features may be changing in the case of the toy example that I'm showing here it's now a change in the actual trajectory of that object and so the the degree of complexity is increasing as we go left to right and again in terms of the algorithm development that we're doing we're trying to focus in on all three of these different components to better develop approaches and hardware that will help us solve very very complex problems like this so just filling in some of the some of the ideas here I'll walk through a couple of these vignettes to show how this overall project comes together and again I'll highlight the different components in terms of the architectures that were developing and also in terms of the new microelectronic devices that we're using and trying to leverage so the first step I'll go through is shown here in terms of the theoretical computing advantages that we have for neural systems and begin we're working on a number of publications for this again getting back to this issue of concept drift and trying to have algorithms that are continuously adapting to new information that's coming in and just as an example from a complexity analysis we all understand that it's much easier to develop an algorithm that can adapt to new data than to completely construct de novo a brand new model every time that you have changes in data and so we think that there's definitely an advantage in the amortized cost of training and running algorithms as long as that algorithm is flexible to handle new data coming in so again that's just one of the approaches that were that we're trying to take and this is a good vignette here and this was some work that was submitted to I clr that we're still waiting on hearing the review back for but what we're essentially doing here is discussing the the sort of the elephant in the room and that is with data-driven computing methods they're obviously limited by the data that you have and in some cases that's not always optimum and in some cases you don't have the entire spread and depth of data that you need in order to represent or to solve the problem that you're trying to look at so in this case what we have here shown on the left is an auto encoder structure and we're using M nest handwritten digit data and what we've done is we've set up this problem so that we start off with a representation where we only have ones and sevens so we're starting off training our network only on those two digits and what we wanted to do is to try to come up with a way that would allow our system and allow our network to represent and to be able to recognize new digits that are coming in over the course of time again dealing with this issue issue of concept drift and so in this particular case we start off training this network with ones and sevens we then challenged it with our test set of zeros through nines and what you can sort of see here is that with our very first example of training it essentially represents every digit as a one or seven now again that makes sense because it's only been trained on ones and 7s at this point so obviously zeros and threes and fours will be represented by things that look like ones and sevens now as we progressively introduce new data into this network and train it what you can see is that the new digits are initially very well represented and reconstructed but over the course of time as we challenged this network with new data sequentially here we have tues here we have 3s here we have fours what you can see is that some of the earlier representations start to degrade over the course of time so by the time you're up to your 9 digit you now can't even really recognize zeros anymore and you can see that some of the other digits also degrade in terms of their ability to be recognized so what we wanted to do is to develop an approach that would allow for this network to actually change with and adapt to this new set of data that it's being subjected to so in this case what we're doing is instead of having a static network what we now allow for these individual layers in the network to do is to actually grow so we have new nodes that are being added to these layers essentially mimicking the idea of biological neurogenesis which we know happens in certain locations of the brain and so in this case we're able to actually introduce new neurons into these different layers which will do two things one it will allow us to maintain the representation of earlier train data in the network and then to it will also allow the network to adapt to and to be able to recognize new data that it's being subjected to and so what you can see here again with the same procedure shown in the last slide as we train this network on our ones and sevens initially it only recognizes ones and sevens and then as we start to go through and subject it to zeros twos threes fours what you can see now is that in this particular instance of the zeros we're well maintained and we're still able to recognize those zeros even as it's being subjected to new data and so this is again some some preliminary work that we've been looking at but we're very much encouraged by what we see so far in terms of the ability of this network to maintain earlier representations and also to be able to represent new data that's being subjected to so in terms of that the next step one the the progression of the project we wanted to focus in not just on concept drift and things of that nature but also how do we actually process more data as a function of time how do we speed up our ability to recognize patterns in data in this case may be streaming data in a cybersecurity scenario so in terms of I don't think I need to go into this slide too much everyone here pretty much understand some of the limitations of anointment architectures where you have your memory your control units in your your ALU separated the the primary thing to ten note here is that the processors themselves are very complex it's to allow for that flexibility that we're all used to with traditional anointment architectures while the memory is fairly simple and that's that's really comparatively speaking and so when we start talking about neuromorphic processing units and again this is one instantiation that we've worked on with David follett from Lewis Rhodes labs we're flipping that complexity so what we're doing is we have a simple processor which is in this case a simple integrator just think of it as a leaking integrate and fire neuron or whatever formalism you'd like to think of but what we're doing is that they're massively parallel in terms of their ability to handle the amount of data that's coming into these integrators and now what we've done is we have very complex memory so now instead of just having strengths of connections between input and output or input and integrators we now also are able to have multi-dimensional data so we can have images we can have video feeds because we have multiple layers of memory that can handle both spatial temporal issues as well as strengths connections between them and so again what we're simply doing with this neuromorphic processing unit architecture is flipping this complexity on its head so that we can have more complex memory and actually have more simple processing and some of the advantages for this particular architecture will be shown in this slide and in this case again we wanted to go back to our original cybersecurity scenario where we're talking about looking for patterns within data so in this case we're talking about streaming data Gigabit Ethernet speeds and in this case you can think of different types of applications one of the systems that we've talked about is perl-compatible regular expression rules where you essentially have these these little rules that you can compare your data stream to and look for these patterns and pull out hits and try to look for things that are of interest so in this case are our neural processing unit architecture is a little fpga card that would go in the back of a server we would essentially tap onto this network data stream and look for instance for patterns such as this again these these can be just generated for whatever's of interest to to the particular application and so when we start to think about how this neural processing architecture compares to more conventional CPUs if you think about how a system like this would work with a conventional von Norman architecture as you increase the number of expressions that you're looking for the execution time just dramatically increases because of the the serial nature of how conventional von Norman architectures work but when we started using our our second version of our neural processing unit what we can see is that we get dramatically increased speeds in terms of the number of rules that we can actually look for in data streams so a 100x increase in our ability to look for these patterns in streaming data was something that was a very nice demonstration and just looking at how this would compare it to a conventional system here we coupled the FPGA with snort which is a open-source intrusion detection system and if you can think about how this would actually scale up thinking about a conventional system where you have lots of these intrusion detection servers and the the amount of power that would be required the amount of cost that would be there going towards this this FPGA technology we have a pretty dramatic increase in the power reduction as well as the weight and the cost we're currently thinking about trying to instantiate this in an actual ASIC chip where we can actually get even in higher increase in the speed with which we can process these rules and again continued reductions in the costume in the price so we were very much encouraged by this and David followed us here he had a demo in the room yesterday and I think they'll also be doing a demo today i think i'm not sure but please definitely stop by and talk with david follett he's doing some amazing work with our team and we're really proud of this work so in terms of the third and final vignette and let me try to tie all of these together at this point in terms of the algorithm development this issue of responding to concept drift and trying to handle new data in a complex world where things are very dynamic is really the first vignette that I showed we're then tying that into this second thread that I just went through in the behavioral trajectory analysis think of that as just temporal coding things in time that really gets us to the point where we're now able to handle larger amounts of data over the course of time and to deal with changing patterns over the course of time the third vignette that I'll go through here is dealing with their learning component how do we improve our ability to speed up the training of our system our putative system at this point and that's really what we're going to deal with here in terms of the resistive memory architecture that we're really focusing on to drop the power and to drop the size of our overall system so in terms of one of the things that we've really focused on for the project is making sure that we walk through a complexity analysis of how we think that the system will perform and one of the first things that we walk through was in dealing with how these neural inspired more analog type computational systems can actually achieve significant energy savings so in this particular case if you think about a conventional crossbar made out of SRAM one of the concerns are one of the issues with SRAM is that you essentially have to charge up an entire row to to provide a readout for any particular location in that in that crossbar whereas with the resistive memory crossbar where we're actually operating an analog we can essentially do a parallel right or a parallel read and do everything at the same time and so there's a significant energy savings on the order in of the number of processing units that you have that you can get by moving from a conventional SRAM architecture into a more analog rear am architecture for the crossbar learning memory system and so this is a publication that we actually just had come out of the frontiers in neuroscience sup and agra walls actually here you have more questions about how this particular theoretical analysis actually works but again we're encouraged by the fact that by moving towards analog computation we can actually improve the energy savings of our system which is again one of the big metrics so one of the things that we've also done is to provide some S&T or science and technology development with regard to the types of microelectronic devices that would go into a crossbar architecture such as what was shown in the last slide so one of the things that we've been doing is really focusing on device development and so this is a couple of publications that we put together a couple of years ago focusing really on the power budget of these devices because again power is typically a big concern when it comes to cyber security applications where you're talking about mobile devices or in the case of remote imaging UAVs and things of that nature and so these are some examples of a couple of the publications that we have and one of the things that we wanted to do was to provide a more broad formalism to the theoretical understanding of how these devices switch and so we actually applied this surface temperature approach towards different material systems in this case we primarily focused on tantalum oxide type devices we also showed that this formalism can actually work for hafnium oxide and some other device materials so it's a fairly broad approach and a couple of the publications are listed here if you're interested and going and looking into those and in terms of how we're actually tying the device development into the overall system performance again one of the things that we're doing is actually fabricating devices and then doing the electrical characterization ourselves and so in this case what we want to have is essentially analog type devices where we can move from one state to another just by applying voltage pulses to these in dual devices and so once we have the electrical characterization of the devices we feed that directly into the overall system simulation so again in this case what we have here is a crossbar architecture where we're now taking data from our electrical characterization putting it into this model taking into account things such as strike capacitance and all of the issues that go into actually building devices we've got a python wrapper around it so that we can actually perform training and performance on in this data and things of that nature we're collaborating with the University of Rochester for some of this work and one of the things that I did want to highlight here is that in building these realistic models of algorithm performance it's very important to actually look at the real amateurs that go into these devices and so what's shown here on the right is an example of a set of data from a number of devices were on the y-axis here this is the number of devices and on the x-axis here it's the actual resistance value of the device and so what this essentially does is provides a probability distribution of how these devices actually switch from one state to another and so in this case we're talking about the right noise in other words how how easy is it to switch a device from one state to the next state so when you're thinking about running your algorithms as these weights change over the course of time you need to be able to accurately tie in your device to whatever resistance state is important for that performance and so in this case one of the things that we're doing is actually going in and tying into how the accuracy is affected by this right noise again how accurately can we actually apply a voltage pulse to these devices to get it to switch its weight to the proper weight that's needed for the algorithm performance and so in this case as our noise increases for for these devices and moving from one state to another obviously there's no noise the the training accuracy is very very high and as you start to see more noise in the right noise you can see that the performance does start to degrade over the number of epochs for the system in terms of how we'd like to continue making that connection to the the cybersecurity application we didn't want to come up with a number of data sets that would be important for for a cybersecurity application so this is a publication from a couple of months ago where we used a neural network to actually identify different file types not by looking at the suffix of the the actual file itself but actually looking at things such as entropy bite distribution as well as the power spectral density of those files and so we were able to demonstrate that in this particular publication and what we wanted to do was to again look at how the crossbar architecture and how the variability in these devices would actually affect the performance and so in this case there are two issues that are shown here again going back to this cumulative probability distribution for the devices so the linearity is essentially how repeatable is the device State changed with a given voltage pulse in other words does it change a large amount of resistance with the same voltage pulse and at other times a small amount and so this linearity how easy it is to change device state is something that's very important to track for the performance in terms of the asymmetry if we're now again talking about these these training algorithms where the weights change they go up they go down over the course of epochs how easy or how asymmetric is that change in going from up or down in resistance and so this is another parameter that we need to check for our device simulations so this is just a plot here to show what this looks like in this case for this file categorization accuracy where again we're categorizing different types of files so you can obviously see that as the device becomes more and more asymmetric your accuracy your performance degrades very very strongly and as the device becomes less asymmetric and more linear you get better better performance so again this gives us a very nice goal in terms of how these actual devices are being fabricated what are some of the electrical characteristics that we're really looking for and so these are just two of the examples of the variables that we're really focusing on and so just to summarize here I'll just highlight a couple of the things from from the talk there's obviously demonstrable gains from moving towards neural inspired another words data-driven computing and we walk through a couple of the publications or near publications that we focused in on that there's obviously commercial viability for the neuromorphic hardware that we're developing again this idea of data streaming and being able to look for patterns and very very rapidly moving data streams we think that there's a very good commercial application there and then tying it all together once we start having these these data stream analytic neuromorphic systems combining that with the learning hardware that we're developing independently at this point we'll start to have more integrated system solutions for for very difficult problems and again this complex design space of how do you actually tune your hardware to be optimized for performance is something that obviously neuro simulations and other types of simulations can really help with and so again I apologize for delay but really shape the opportunity to be here so um I'm sorry I think that what Nvidia is doing is just great stuff the what you call conceptual drift okay if you study the literature you're not in that area but you could have your chips speak English or French or German at the same cost you just don't know it okay the so imagine a chip that's doing this that's describing real video to a game player in his language what you think is complex is not complex you don't understand it and you need to understand it and then you're this to a linguist when you said a conceptual drift okay i just went mad okay i think you're crazy you're dumb and you need to understand how you can actually have the chips talk it's not that expensive you entered from universal peace / one question regarding the impact out of variability and the symmetry so we will talk about this result which are treating master are you using the Alpha and training or online training outline or flight thank you 