 so our next speaker is geoff hinton geoff is a scientist who essentially needs I think almost no introduction he's a absolute pillar of the field of machine learning and he's made more contributions than I could sort of plausibly enumerate here you know david has mentioned the profound impact that Geoff has had on his his scientific career and this is something that that I would echo and I think actually quite a few people in the room would would also say the same thing in fact I would say that with the revival of interest in neural networks that we've seen over the last several years and also Geoff's contributions to to online learning he's inspired I think an entire generation of machine learning researchers today I expect as is Geoff's custom he's going to explain to us how the brain works take away Geoff okay since Ryan has gone silent I assume I'm talking now before I start I'd just like to say a couple of words about David kid Ryan can you hear me can you nod if you can hear me yes okay good so I first met David at a nips poster this kid came up to the poster and looked at it for one minute and said why didn't you do it this way and I thought damn why didn't we do it that way and so I am me I immediately thought I'd better get him as a postdoc and I tried to get him as a postdoc but he said he didn't like America later on however um he visited Toronto a lot in the 1990s and at that time I was working on variational methods and I had this crazy bits like argument and most people when I tried to explain the bits like argument got very confused David immediately understood it and liked it and so that was very nice and we talked a lot about variational methods the last thing I want to say about David is that he's not just a brilliant academic but he's also somebody who made a difference so both his book and his work as a government chief scientist actually may have helped to save the world and you can't say that about many academics okay onto the talk I'm going to talk about joint work with Timothy Luther crap I've talked about some of this before in Cambridge and at the end I'm gonna have some new stuff next slide can we get the next slide okay so neuroscientists have claimed for a long time the backpropagation is ridiculous as a model of cortex and they've I don't think they really understand the importance of adapting early feature detectors so they provide what things later on in the system need they don't really understand how computationally important that is they need to understand it because we now know that this supervised learning works really well and so we should ask why do they think it's impossible next slide you got it um and I think there's four reasons four main reasons one is there's no obvious source for a supervision signal a second is that the second reason was what Francis Crick believes and it's the reason why Francis Crick thought the backpropagation was ridiculous is the cortical neurons can't communicate real-valued activities which appears to be necessary for back rub the third is that neurons in backdrop need to send an arrow derivative backwards and send a signal about what feature they detective forwards and so the output of a neuron needs to convey two kinds of information now many people say well we'll have two different neurons to do that but it'd be really nice if you could do it with one neuron and the main idea of this talk is that you can do it with one neuron if you commit yourself to a certain way of representing error derivatives and the last problem is that although if one cortical area has connections to another there's always backwards connections the backwards connections aren't actually to the same neurons has sent the forwards connections and they're certainly not symmetrical in their weights so you don't have this kind of point-to-point symmetry that you need if you're going to use the transpose of the forward matrix next slide next slide okay um so there are many sources possible sources of supervision I spent a long time in sore 80s and 90s thinking about this the obvious one is something like an autoencoder where you reconstruct all or part of the current frame you can also have an auto encoder that predicts the future that is you try and reconstruct the next frame and that's a very simple way of turning unsupervised learning into a formal supervised learning um something that I think cortex is probably using is to make locally extracted features agree with features predicted from the local context or from another modality so if I show you the sentence she strung him with the frying pan the context of the word scrawled gives you a sense of what it might mean and so in one trial you get a pretty good idea of what strong means I mean you helped some uncertainty but it probably means she bashed him in some way with a frying pan okay next slide so I think that all those sorts of supervision are readily available and so I think we should stop worrying about that problem the next problem is that neurons don't send analog signals and this worried me for a long time and I think it's actually easily understood once you realize that we operate in the domain of small data so if you've got a big brain you can afford to throw a lot of synapses at training examples and you can ask how many synapses do we throw at each training example and it's about 10 to the 5 it might be as few as 10 to the 4 or as many as 10 to 10 to the 6 but we throw thousands and thousands of synapses at each training example if you talk to a statistician they'll tell you you shouldn't have more parameters than training examples luckily the brain didn't talk to a statistician and there's hugely more now we all know a way of using many more parameters than there are training examples and that's to build an ensemble of models so use a few parameters in each model in the ensemble and we know that as you get more and more synapses you can make your model better and better if you just keep adding things to the ensemble you get diminishing returns so we know there is a way to always make use of more synapses and a few years ago I noticed that you could get the same effect as an ensemble but be much more efficient by having a technique called dropout where you randomly leave out neurons in your network so each time you run the network is a different member of the ensemble and it's going to be a exponentially big ensemble but the nice property is that all the members of the ensemble share parameters now and that means that you can learn the parameters even if you never see an example one member of the ensemble and so you get a nice property so basically this shows that adding noise and having ensembles is very very similar dropout you can think of as a way of adding Bernoulli noise to the activities of units but at least we've one hidden layer it's exactly equivalent having a big ensemble so in dropout each neuron is either gonna send the normal output actually twice a normal life but or is going to send zero you get a very similar effect to drop out if you send random spikes proportional process again you get this press on noise and it looks as though plus all noise is going to be bad but actually plus all noise is very good it's a way of in effect getting the same property as you get from an ensemble which is that you can have hugely more parameters than data and still not over fit so it's actually better to send a spike from a Poisson process than to send the real value underlying the Poisson process next slide so what I'm going to conclude from that can we have the next slide what I'm going to conclude from that is that we don't need to worry about the fact that neurons don't send analog values let's assume that they do and we know that at the end we can replace these analog values by spikes from referral process and everything will be fine except it will be better regularize we also know that stochastic gradient descent is very robust to added noise as long as the noise is unbiased so we're fine having personal spikes instead of real numbers okay next slide so this is where we get to the new idea on the next slide we need to be able to represent error derivatives in feed-forward pathways in the sensory cortex which is what I'm mainly concerned with him and the normal idea of neuroscientist is you'd have some separate neuron for sending error derivatives backwards this is actually quite problematic because that separate neuron would need to send things that are both positive sometimes positive and sometimes negative and there's a basic rule in neuroscience which is neurons can't reverse the sign of their effect so it violates Dale's law but there's a much simpler way to send error derivatives which is to make a fundamental representational decision that we're going to make the rate of change of the output of a neuron represent the error derivative so the value of the output represents what the neurons representing and the rate of change of that value represents a narrow derivative at least for a limited time so now the same you're on the same axon can simultaneously carry the signal forwards and the error derivative backwards and that makes life much simpler if you can now figure out how to do back prop with that so next slide please sorry there's a big delay in getting the slides okay I'm going to show you a way of using temporal to Reuters error derivatives the J McLelland and I played around with in 1987 and it was an early attempt to make an autoencoder that didn't require the backwards connections to be the same as of always connections and the idea was you take the input you send it once around a loop and the loop could have several stages to it so you sending one strand to loop using the green connections and then you send it around the loop again and the red connections are the same as the green connections is just showing you it goes around a second time and then you look at the activity in each neuron and you say the change in activity is bad we would like the activities to stay the same because if the activities stay the same in all the neurons when we go around the loop again that means we've reconstructed the input well and so we just can treat the change in the activity of a neuron as the postsynaptic term in the learning rule so the change in a weight between from your own eye to neuron J is now minus the output of neuron I times the rate of change of the output of neuron J and that is spike time dependent plasticity but with the wrong sign what's interesting about this was I think in 1987 when we um we actually published his model in nips it was an old nips paper that with very poor experiments I think at that time people didn't know about spike timing-dependent plasticity so you could think of it as a prediction but it has the wrong sign but this finds any one bit let's go to the next slide okay so I realized later on that you combine this kind of stdp with the wrong sign with s GDP with the right side and I realized this when I was doing pre training with stacks of restrictive Boltzmann machines and I actually have a talked about it in 2007 and the slides and an updatable sides are on the web so the idea is we're going to first learn a stack of auto-encoders we shall involve using reverse HTTP once we learned a stack of autoencoders we're then going to put the signal through this stack of auto-encoders we're to get some error signal at the top and then we're going to drive the neurons top down in two different phases in the first top-down pass we drive the neurons from the representation that we predicted from the input and in the second top-down pass we drive them from the correct representation or at least from what we predicted regressed in the direction of the correct representation that is in the direction of the target and so you get a difference between the activities of the neurons in these two time top-down passes and that difference will actually allow you to do back prop that is you just use the stvp learning rule where you make the change in a way to be proportional to the input activity times the rate of change of the activity of the postsynaptic neuron and this will approximate back prop and we've been running this now it actually does work so Tim lilyc rap has simulated learning a stack of auto-encoders with this kind of loopy reverse stdp and then running this to pass procedure and we can actually now in somewhat more plausible neural nets do an approximation to back problem works okay next slide please so I just want to go over the fact that spike timing-dependent plasticity is just making the change in a weight be proportional to the rate of change of the postsynaptic activity the normal way spike time dependent plasticity is presented is at the time of a presynaptic spike that's the zero there you look to see whether a postsynaptic spiking just before or just after this presynaptic spike and if it occurred just after you increase the weight and if occurred just before you decrease weight and remarkably is a period of just a few milliseconds around the time of the presynaptic spike that determines whether the weight is decreased or increased and most people interpret that as you're looking for a causal relationship between the incoming spike in the outgoing spike but you can also interpret it by saying that the SCDP is just a derivative filter applied to the postsynaptic spike train and the weight change depends on the derivative of the postsynaptic spike train it's a very different interpretation and the derivative interpretation depends of thinking about many examples with stochastic noise in them and averaged over many examples it behaves exactly like a derivative filter okay next slide please so there's one problem with all this which is if the top time waits for the transpose of the bottom up weights as they are in an RBM then everything works fine and when you do these two top time passes you get to simulate back propagation it's important that when you do the top time passes you can actually reconstruct what's below so that means at the top of your system you don't just want a class label coming out you want a representation that contains the class label and that's the bit you're going to correct with the target but also contains enough of other stuff so you can reconstruct the input so your top level representation needs to be the label plus the other information needed to reconstruct the input like if it's an object like the pose of the object okay with that without provision you can do back prop like this but he needs symmetric weights or it appears to need symmetric weight so that the backward pass is using the transpose of the weights used in the forward pass next slide please so when I thought about all this in 2007 that seemed to be an insuperable obstacle oh I'm gonna skip this one next slide please this is just showing how you turn the target into a rate of change and this is showing that it really does do back prop so if you think about look about neuron I the black arrows are what you use on the first forward pass and then the green arrows a what you start using top down and if you have auto-encoders that are working then when you replace the black input Y with the green input UI the green input Y is going to be Y K times W ki plus y J times w ji and if Y can YJ changing then the green input is going to have some rate of change which depends on the rate of change of Y J and YK and you can now ask well what's the rate of change of the output of I going to be and the rate of change of the put of AI is going to be the rate of change of its input times the derivative of the output with respect to the input fry and that is exactly the back problem that's what's interesting if you reconstruct using an auto encoder and the things you're reconstructing with are changing then the rate of change of the output of the reconstruction is exactly what you need so that you can back propagate the error derivatives in the form of the rate of change of the outputs okay next slide please I'm near the end now so the problem is the symmetric weights that's the one thing I couldn't figure out how to solve how will it work if the weights aren't symmetric and on the face of it it's just not going to work if the weights on so make sure you didn't get the wrong derivative and I tried it and it didn't work I talked a bit about functional symmetry but I couldn't get it to work and then a year or two ago Tim Miller crap and some his co-workers in Oxford noticed something amazing next slide please what they noticed was that the top down weights don't have to be the same as the bottom up weights if you run back prop and you don't use the transpose of the feed-forward weights you just use fixed random top-down weights initially nothing happens obviously initially what's back propagated doesn't encode the error at all but then something very interesting happens the forward weights adapt so that the backward weights will be carrying the error basically machine learning there's two kinds of um behavior that ideas have some ideas want to work and some ideas want not to work and this is an idea that wants to work that is you given these random backwards ways and by magic the food's weights adapt so the backwards weights are now doing more or less the right thing they're closer to the pseudo-inverse and the transpose but if you run back prop in this kind of system it takes a little while to get off the ground then it works just fine and when I say it works just fine you can run a big net like this it's important that be quite big narrow bottlenecks this doesn't work well with but with big wide nets it gives performance very similar to stand a backdrop and it only takes about twice as long sometimes less than twice as long sometimes almost the fastest under backdrop so it's not exactly backdrop but it works it does do supervised learning very nicely so that solves the last problem next slide please so I've got an analogy for people who do machine learning which is in variational inference we make up how how we're going to do the inference well we make up a restriction on the distribution we're going to use and it looks like that might be a very bad thing to do because the true distribution the true posterior distribution over the latent variables or over the weights might be very different from the one that we've supposed that was a nice simple one but the magic of variational inference is the generative model will then adapt to make our way of doing inference more corrects and so in feedback alignment which is what Tim the other crap calls using random feedback weights what happens is the same thing the forwards weights adapt to make these backwards weights work now of course the backwards weights don't have to be fixed you can they can be weights in one of these loopy auto-encoders so they can learn to and the forwards weights will keep trying to track them so that they are carrying back something that looks very like the derivatives we don't fully understand this yet I have lots and lots of different explanations of why it works and having lots and lots of explanations may seem good if you want to integrate a crisp posteriors but actually it's a sign that you really understand what's going on um okay so it works but we don't really fully understand why feedback on line one works so well next slide please so I just want to summarize it's easy to get a supervision signal in a neural net there's all sorts of possibilities for that it doesn't have to sort of be injected into the middle of the cortex by a teacher the fact the neuron sent spikes isn't a problem in fact it's just a great regularizer if we represent arrow derivatives as temporal derivatives then we can get a neuron to send in the same axon the error derivative backwards and the signal forwards and if we do that we ought to see spike time-dependent lasticity that's really the signature the back propagation is using temporal derivative zero derivative and the problem that you'd have thought each bottom-up connection needs to have a corresponding top-down connection with the same weight turns out to be a non problem for reasons we don't fully understand but it works just fine if you don't have that the forwards weights will adapt to make the backwards weights work nicely so I've got one last comment on the final slide which is that this funny representational idea of using error derivatives represent temporal derivative has a huge consequence it means you can't use the temporal derivative of a neuron to represent the temporal derivative of what the neuron represents so if you've got a neuron that represents the position of something you can't use the rate of change of the output of that neuron to represent the velocity of something that's the obvious way to represent velocity but it's not going to work if you've already decided that temporal derivatives are error derivatives and if you look at the right kind of brain damage you find people who can see the positions of things and they can see that the positions change but they don't see them moving they have no sense of velocity so the idea of temporal derivatives as coding herre derivatives explains a couple of things like why you can't use position neurons to represent velocity by their rate of change it explains why you get spiked and event bus DISA T and explains how you might be able to do back propping the brain now it's early days yet to make this really stick in neuroscience terms but what I want to do is cast doubt on what neuroscientists say which is that you couldn't possibly do back prop in the brain I think you actually could and evolution must have figured out a way to do it and I'm finished we have time for a couple of questions so can you make any predictions on the graph structure you might expect to see in a brain so look as simple some like C elegans is there some loops in there that you could predict and then find okay I've been thinking about basically the human brain or the brain of primates oops I think we're both talking at the same time um I've been thinking about the human brain and sensory pathways in the human brain I'm not convinced that the same kind of learning goes on in much lower organisms so I think in insects for example it may be a much dumber system than it is in our brains so I sort of want to remain open on the question whether something like C elegans might be able to do back up I'm not sure I believe that Jeff I have a question okay so is there an opportunity by of in a situation where you don't have to update the sort of the matrices that you're using on the reverse pass is there a sort of computational opportunity there for say distributed training of neural networks less communication cost letters um I haven't actually thought about that I haven't thought about that issue um I'd sorry the line isn't full duplex so I think it's very interesting question it would be very nice if you could use fixed backwards weights and you pay a bit in that it's not quite as fast as backdrop but you win a lot in that you don't have to be updating the backwards weights but since you still have to be updating the forwards weights I'm not convinced but I'd have to think about I don't see a way of making a lot of use of that to parallel I think hi Jeff you you suggested that your autoencoders have to be lossless that you annoyingly encode the label but enough information to reconstruct the image that seems like quite an expensive requirement for the brain to be able to be able to do that all the way up do you see any way that you could be more lossy and still get back up yeah I think I've realized what's happening there's a big delay between the visual input I'm getting from John Wynne speaking and my sound for John wins okay so assuming I got the whole question it turns out that this method is actually very robust and if you make the top level not be sufficient to reconstruct the level below it still works pretty well so it helps to have extra outputs in the top level that are representing things like pose but if you just use the label the top level it still works just not quite as well so I think actually it can tolerate lossy auto encoding pretty well we haven't done the experiments to figure out exactly how well it can tolerate it and I completely agree with the sentiment that you don't want to have to encode all that sort of noise in lower levels all the way up the system but I think it's going to be fairly robust for that Jeff Crowe Richardson here as you know the connections from the higher levels are very diffuse they're in the upper layers of the cortex and they spread out very far and wide what kind of precision do you require in the signal that comes down from the top levels okay so I can say a little bit about that which is but if you take fully connected Nets which of course isn't very realistic and you make the board connections and the backward connections completely not overlap so you put in for example one quarter of the possible forward connections and then you put in one quarter of the possible backward connections but making sure there's no overlap between four connections and backwards connections so you never have a pair of neurons which are connected forwards and connected backwards the algorithm works just fine that doesn't place it at all obviously it hinges on the fact that redundancy in the representations in each layer so you can get the information you need backwards by looking at other neurons not not just the neuron that you would have thought you had to look at in the layer above to figure out our derivatives so as long as there's redundancy in the representation so as long as you have quite wide layers it works with no overlap between the feed-forward and feedback connections that's not a proper answer because you'd want to know what happens locally and whether it pays to make things more diffuse I think the reason for making things more diffuse is probably related to where the supervision signal comes from because I really believe in this idea that the supervision signal which is going to be happening at every level is the agreement between the bottom-up activity you extract for a neuron and the top-down prediction from a wider context and so I think that's why you want the feedback connections to be more diffuse because they have to carry things from a wider context so you're going to compare with the bottom-up I guess that's all I have to say about that now it requires a lot more experiments to know what's the ideal rate diffuseness and also we haven't done the experiments to show that this supervised learning of agreement between bottom-up and top-down predictions is a good way to undo supervision at all levels of the system thanks Jeff let's also thank Jeff again you 