 so we've been hearing a lot about neuromorphic computing in this workshop and there's a lot of nice examples that we've sought we've seen already today and I just wanted to point out there are some differences between the original approach that as was mentioned today by Srinivasa and started out in the 80s with Carver and in today so today there are several goals that are motivating that the research and development which is for example building computing systems for neuroscientists building high-performance computing platforms for neuroscientists or building spiking neurons chips for making money like TrueNorth the idea that came out of the 80s with Carver and max delbruck and john hopfield and Fineman was really to try to understand how to exploit the physics of the physi of natural systems including silicon to reproduce the physics of neurons of cells it was really deeply rooted in biology in fact Carver's first student was a biologist who had never seen a transistor before and an Misha together with Carver developed the silicon retina the silicon neuron and many of the things that we are actually using today to build these types of systems and in zurich at the Institute of Noor informatics we follow this approach we listen to the silicon like Carver used to say and also we follow the approach of synthetic biology this is what a dig Fineman used to say what I cannot create I do not understand and the type of things that we actually build are using sub-threshold analog circuits because these set threshold analog circuits reproduce the biophysics of real neurons they the mechanism of carrier transport in transistors is diffusion so you have Boltzmann statistics and you get Exponential's and logarithms just like you you have in ions crossing channels in in protic channels in neurons so the type of the neuromorphic processors that my group is building in Zurich is essentially a large arrays of these massively parallel arrays of these analog circuits that implement neurons and synapses they really emulate the physics of real neural systems is not just multiplying by some number it's really implementing dynamical systems there there is a lot of parallelism because all of these circuits act in parallel if you stimulate many of them they just integrate currents in parallel we we do use rates to compute so there is the these analog signals going across but there is here in this case there is a need for spikes spikes are actually very useful to represent signals and to reduce power when you're doing on this type of computation and to communicate these signals across long distances across the chip or between ships so it's it's the optimal communication method that also biology found so we have analog continuous time streaming circuits for doing the computation and then whenever neurons spike we send these digital if events long distances either on ship across cores or across chips these are all of the circuits or most of the circuits are completely passive if there is no data if there is no signal there's just leakage currents they are extremely low power there is no active amplifiers or anything in any of these circuits up here and the digital ones are asynchronous they're self clocked there isn't a clock that's burning power continuously so we're really talking about ultra-low power not low power like you know tens of watts or hundreds of watts like some of the things that we've seen really a Pico joules microwatts nano what's even if you if you go at the single neuron but the price that you have to pay for this low power is that they are very imprecise and noisy inhomogeneous and slow so you might think of these as bugs or features if you're trying to reproduce the physics of real neurons these are actually features if you want to do you know high precision convolutional net works these are bugs so this is the point of discussion of where the worst is technology useful but this is the price that we have to pay for this ultra-low power on the other hand because we can use these neurons and put them in architectures that have learning and adaptation and feedback at the architecture level we can have resilience we can have fault tolerant mismatch incident insensitive robust type of computation so you your we're really looking at this paradigm shift that we've heard also about yesterday in this morning where we have these circuits that are doing memory and computation in the same place we are not using this a standard way of doing computation where you transfer data back and forth between a processing unit and a memory external memory bank we are not trying to do the type of applications that Google is doing where you're trying to do recognition of store data really this type of technology is optimal for streaming continuous time dynamical data that's coming in your act on the data as it's coming and you take a decision you produce a motor output as as you have the data coming in so there is no there is no clock it's continuous time and because of that it's really really important to have time constants which are matched well matched to the type of signals that you want to process if you want to interact with the environment if you want to recognize speech recognizer gestures your dynamical circuits should have time cost and constants that are well matched to the type of time constants that we produce and so we try to slow down silicon we really go through a lot of efforts to have time constants of the order of ten fifty hundred five hundred milliseconds in our circuit so that they are inherently synchronized with the real world natural events that we want to process this has the back this is an additional advantage of reducing power and and bandwidth so we at the system level we also have ultra-low power because of this because we slow down silicon and we've been doing these types of chips since you know since the 80s I was at Caltech with Carver and Christophe I was a student of Christophe we didn't start in 2005 we started much much earlier than that but a recent chip that now has enough neurons and synapses to do something interesting is the one that I call the reconfigurable online learning spiking Norfolk processor essentially if you see what it is it's a big memory chip in which the memory elements are these dynamical circuits these synapses that can have short-term plasticity long-term plasticity different types of dynamics all of these neurons are connected sorry all of these sinuses are connected to neurons on the side and the chip this particular chip has 256 of these neurons if you look at the block diagram it's more like you had a block of synopsis that is programmable with short-term plasticity another block of synapses that has on chip learning they're really dynamical systems that depending on the timing of the spikes and under statistics of the outputs they change their weights there is so this could be like the distal synapses that Jeff was mentioning these could be proximal silences you can think of these as compartments in a multi compartmental model and finally you have the analog circuits that implement the neurons all of this is embedded with logic so you have mixed signal analog and digital it's fully reconfigurable so you can connect any sign-ups to any neuron and any neuron to any other neuron so you could have multi layer liquid states recurrent combinations of liquid states and multi layers and so on so it's really flexible and reconfigurable online and once the the neurons as I said generate action potentials those get converted into digital address and they get sent out right away asynchronously there is no clock so if we zoom in and we look at what's actually inside this is just to give you an idea a single neuron here is about 30 transistors you have these sub-threshold analog transistors they represent channels you have calcium channels sodium potassium channels sodium activation sodium in activation positive feedback loops leak circuits so you really can get a lot of interesting behaviors in fact if you take data from the chip you'll see that you get behaviors that resemble real neurons this is a three variable system it has the membrane potential variable the calcium adaptive spike frequency adaptation variable which is lower and reset potential and henry abhart bunnell tells me that if you have the three variable you can you can really get chaotic behavior like you can without full hodgkin-huxley models this is a variant of the adaptive exponential integrated fire model so you can as I said you can change the reset potential the refractory period you can turn on spike frequency adaptation depending on the parameters that you put in you can get you know bursting or different types of spiking behaviors the sinuses are the same you have maybe five ten transistors and you implement really the same type of synaptic dynamics that you met in that you measure in real but in real cells this is a real measurement from a real synapse this is data from the chip you can change the weight words that here the weight of the synapse you change the efficacy you can change the time constants you have a lot of parameters that allow you to play and get different types of behaviors of course you can put many many transistors in in parallel and then you can integrate all of these currents spatially so you can get your weighted sum of synaptic currents if all of these elements share the same dynamics you don't have to have a capacitor per synapse you can do the you know if it's a linear filter spatial and temporal superposition so you only have one large capacitor on the side of the chip in a very high dense array of synapses in inside your chip and in fact now we are replacing these transistors with memory Civ devices you can even use these emerging nanotechnologies to have state holding elements not something that leaks away so you have a capacitor you have your synaptic currents and you have your wait voltages here if you add extra circuits to the wait voltage you can have learning on chip learning online on chip continuous learning to these weights and that's that's what we also did we've been following the literature there's a huge number of papers on stdp a very nice set of learning algorithms was proposed by stefano fuji and colleagues from daniel Amit's group Nicola Brunel there's a whole series of papers from the from the computational neuroscientists theoretical physicists that show that you can go beyond plain a CDP and the nice thing about these learning rules is that they require things that are very very nice for VLSI they require by stable synopsis so one bit you don't need to have 32-bit floating-point resolution redundancy is a requirement but that's easy to get because we have very large scale integration we can put many many synapses and they are compatible they are they require at least compatible with variability and so we don't have to worry about minimizing mismatch we can use small circuits and still be able to do computation and learning this is an example of what's happening if you have presynaptic spikes postsynaptic sand your weight is changing and this is also nice because also Jeff was saying you have internal variables that are changing the weight but then you're essentially you have two states either you have the synapse or you don't so it's also some a way of having like structural plasticity where you either have a synapse or not very long time skills on short time skills you have your analog weights going up and down that on long timescales you have a drift that's pushing you up or down depending on where you are and using the circuits we show that you can do supervised learning using mean rates so you can do basically perceptrons we have papers that show that you can actually because we have spikes we might as well take advantage of that and if you order the timing of the spikes in the precise way you can have one-shot learning you can you can learn sequences of phonemes so by with this unsupervised precise spike timing mechanisms we can actually and we showed that we can learn fine spatial temporal structures all of these are as I told you in homogeneous variable and precise circuits so if you use one single neuron to do like a binary classification task binary perceptrons it's going to be very weak or I would say crappy classifier but we have 256 neurons and you can do ensemble learning and you can take advantage of all of the bagging and boosting theories that are there and there you can really become competitive with the state of the art if you collect many of these perceptrons together and you and you use your bagging techniques and we showed that you can compete with standard machine learning approaches of course as I told you you can implement recurrent networks and we try to do hopfield attractor networks there is a nice nice example in the Proceedings of IEEE Trapani paper for that and you can do liquid state machines following some of the work that was inspired by Wolfgang X work we show that you can do liquid state of perceptrons in brain machine interfaces and you can really decode online spikes that are being measured from from the brains of zebra finch of birds in that case so that's what the learning but you can even do more interesting things by connecting neurons either hard wiring or having learning algorithms that learn the structure of soft winner-take-all networks this is also work that's inspired mainly by rodney douglas and kevin martin in our institute but also by both con mass and let me show you here this is some data that actually Emirate took from one of our old ships where he was applying some stimuli with maybe 190 Hertz and 210 Hertz on two different regions of the chip so that would be the light-blue input these are just spikes over time the address of the neuron in time and then if you look at the output of the neuron there is some initial dynamics initial transient where the neuron the chip responds to both inputs but then through competition through a population of inhibitory neurons the winning the population that receives the strongest stimulus suppresses the population that receives the weakest stimulus so it's a classical soft winner-take-all Network and we have chips that can do this in real time there was a video that I can show you if there is time later so we started what you can do now is you can combine these winner-take-all networks that's where interesting stuff happens if you combine together these sort of cortical micro columns and one of the things that we classical thing to do is to look at decision-making if you have different populations of winner-take-all networks that are competing you can have them do different things and one classical experiment is you if you present the same input to the same pot to two different populations for example if you present an image on one retina and another image on the other with the same sort of strength then your perception will alternate between these two inputs and the same thing happens in real-time with the hardware and you get this gamma gamma band distributed switching which is exactly what is measured also in in primates another example is if you connect multi this is again work that was done with M ray if you connect multiple winner tickles together through another winner-take-all Network you can model inference you can clamp one input and you can see what is the output of the of the whole network you can do Q integration function approximation all of these things have can be done directly in hardware with no computer in the loop you just apply inputs to your physical system your sub stress your chips and you measure the raster plots in output something which is very nice that Steve mentioned this morning is the pseudo coup we just connected neurons together in this pseudocode type of arrangement yes it was explained earlier today so if you just make one or and inhibit all of one neuron that represents a digit inhibit all of the neurons that represent the other digits in the cell then you can start solve these constraint satisfaction problems and this is stuff that and Venus is doing which is with using our nerves we just connected them with this inhibitory connections without really trying to match the precise equations that come up keeps happening we just connected them and we observe that the network explores the state until it finds the optimal solution and it tends to stay there more we did not need to inject any noise we just use the thermal noise that's present in the chip and just try to get the refractory period to match with the inhibitory period and if there is any fluctuation then you get out of this valley like liquids shown earlier today by me hi this is with thermal noise in the chip you can even connect winner-take-all networks in this way where you have if if one node is representing a variable a in one network and another node is representing variable B in the other and they excite each other then this variable B will impose constraint on the other network with variable a and this can happen if you have these networks oscillating at different frequencies so then you can implement constraint satisfaction and travelling salesman problem in 3sat problems using these coupled networks without requiring explicit noise it's just the fact that they are oscillating and they're oscillating at slightly different frequencies because of mismatch that they can explore different states all of the details are in this Nature communications paper and narrow computation paper if you're interested or you can come talk to me later finally the other thing that is really interesting that you can do is you if you connect multiple winner-take-all circuits together you can implement soft state machines if you give me any description of a finite state machine in XML or any any other description I can convert that into a coupled winner-take-all networks actually amaura can convert that into a couple we don't take on networks and then from that you can really program a network of spiking neurons to carry out a procedure if I see the SKU and I see this input then take this action so in doing that in this PNAS paper we show that you can actually connect these synthesize the these networks to carry out the same type of tasks that are being measured with the monkeys or non-human primates in neurophysiology labs to probe cognition so you show a stimulus a qu you tell the monkey there is a rule and then if this rule is satisfied make us a car to the left or to the right we did exactly the same thing with our hardware chips whatever chips were available in the lab at that time we needed to collect all the chips that we had in the lab and connect them together so we realize it's important to do these large-scale projects where you have multiple core you have enough neurons on a single chip so that you don't have to have all these wires hanging around and so the next type set of slides I want to show you is how to efficiently put together many computational functions on a single chip and so this is clearly a multi-core approach when you have multiple cores you can contain you need to transmit signals between one core and the other and we've seen different ways that were proposed today and yesterday the classical one is to use a 2d mesh or a Toro its structure this gives you most the most flexibility you can connect any one neuron to any other neuron anywhere on the network but it's also very expensive in terms of resources of memory if you have a fan-out of F if you have any neurons in your in your system you need F log and bits per neuron to do that it's very flexible it allows you to do anything but in if we if we really want to build cortical models we don't need to have the total flexibility we can just try to analyze how is cortex organized and try to take into account those constraints to try to optimize for memory resources and this is an example of some of a neuron that was measured in in the Institute it's a pyramidal cell of layer 3 that shows how the axon branches out and then connects to local clusters it doesn't connect with everyone around homogeneously you have some structure in the connectivity scheme so we can take advantage of this and try to optimize rather than having a wasteful but powerful connectivity scheme we have a constraint but optimized connectivity scheme and this is what we did with the logit manoa who is also one of the main architects of the TrueNorth chip and we came up with a two-stage routing that first does branching so if your neuron has an axon out it branches to n possible clusters and then broadcast so we have a multicast scheme that is combining source address and in target address routing so the stood state routing really minimizes memory requirements we get a square root factor there which is really a big improvement and because we did not start our design in 2005 like many of the of the things that we saw recently we started actually later we could take advantage of all of the things that have been proposed in the in the field and get the best features off of all of them so we we designed a new chip that is taking this these advantages it's fully asynchronous it's event-driven it has this two stage routing scheme that minimizes memory requirements multicast hierarchical it's using both mesh and tree routing and it has heterogeneous memory structures it actually uses kin SRAM and capacitors analog to distribute the memory around across the chip so this is an example if you have a four core chip you can send any one spike from any other chip around the board to any one neuron on any one of these cores using these combination and routing schemes the chip that we did a prototype chip using a very old technology it's a hundred and eighty nanometer we put four cord because we wanted to have a multi-core of course it's the smallest we're still trying to explore principles we want to pay the least amount of money so we make the smallest chips available possible still this is a quite large chip as 40 square millimeters each core has 256 neurons and then there are all these routing memories and different memory hierarchies embedded in with analog and digital circuits in each core it has 1,000 neurons because it's four cores of 256 each neuron has programmable synopsis so there are 64 programmable so here you have really structural plasticity that can change the connectivity scheme and each neuron can fan out to 4,000 different neurons so it's it can have this clustered connectivity it's massively parallel continuous time like the others it's using sub-threshold analog and asynchronous digital extremely low latency and if we if we this is measured data from the chip if we get if we just inject current into all the neurons make them all fire this would be the worst case scenario we get about 276 micro amps on a 1.3 power power supply so we're talking about micro watts here now because Ning is a really good designer he designed things such that you're you only need to connect with direct wires multiple chips to scale this up so he did a board where he put nine of these chips together and I want to show you an example application of of this board where we only use three actually chips and because convolutional networks are so popular we thought well that's even though that's not our main motivation let's try to see if we can use this board to implement a convolutional network this is just to remind you convolutional network is typically composed of multiple layers do you have your input layer your convolution layer where you convolve the data with some kernel a pooling layer and then a classification layer so what we did is we connected a silicon retina a dynamic vision sensor that's developed by my office Manitoba Tobruk directly with wires so events coming out of the chip here go directly into the first ship of the of this board with subsampling so that we have 32 by 32 input pixels then the convolution layer was actually convolving these events with oriented bar come very simple oriented bar this is just you know toy example that we tried and then the pooling layer was used and then we trained the chip to decode the output let me show you what we did is we took some data that burnable in addis barranco produced by showing to the retina deck of cards and flipping the cards you know over two seconds flipping 50 cards so you go through and you show this to the retina if you see what what the retina sees in slowed down version this is the suits of the deck of cards that are being presented to the retina these are the output of the for convolution layers the oriented bars and this is the output of the pooling and then the classification output this is lowdown so if you actually see what's going on within a few milliseconds when you start to show this to the retina the neurons in the retina start to spike even before the image has finished the convolution layer starts to produce an output the pooling layer starts to let's look at here the pooling layer starts to produce an output and then the classification layer tells you what suit it is so by the end of the two seconds of the that in which you flip all of the cards you can know exactly what was the sequence of suits that that you were showing to the to the retina let me see if this is would be the live demo so here's it's really hard to see what's going on but you see that the chip is producing in real time these raster plots that tell you what the suit is this is just to show you the very simple convolution layer convolution network can be done without requiring any computer in the loop let me let me conclude by saying that I we saw some really nice challenges that were put up by Bruce yesterday morning and I hope I convinced you that our chips can are addressing this power challenge really well because of our ultra-low power neurons synapse and communication circuits we go from peak to peak or joules per spike to microwatts at the chip level to milli watts at the board level resonance is something that we are addressing because of the adaptation and on chip learning circuits that we have complex memory hierarchies is something that we actually implemented we have cam SRAM and analog capacitors all living in the same substrate high-performance networking is something that we managed to achieve things also to the collaboration with rigid manual at Cornell and and so this is the current state of the art that we have but we are already thinking of the next steps and so I'm really excited about the possibility of using the STMicroelectronics fully depleted silicon on insulator process it's an ultra-low power process and we are going to put this type of architecture on this Tunney nanometer it's going to be really challenged to see if these analog circuits were work well and scale on these more advanced processes the idea there's a new project which we'll try to put in addition this 28 nanometer advanced circuits resistive memories and 3d technology so we're gonna try to put everything it's a really high risk project but we're going to try to do this hopefully three years from now I'll be able to show you some nice demos using this new technology the next thing that you might think I want to do is implement deep learning systems I don't want to do that this is not the type of technology that's useful for that I don't think I would be able to compete with GPUs or Nirvana's type of systems what I do want to do is I want to interface like like we started to do these chips to neuromorphic sensors silicon retinal silicon Cochlear's I am new units temperature measurement units and build examples of cognitive agents these robots that will go to Mars at one day but I'm not thinking so big as Jeff is thinking I'm thinking of cognitive agents as any small system these tiny brains that can do interesting things so of course autonomous robots is one thing but you can think of a cognitive agent also as for example a micro device that's living in your body that's measuring metabolites in your bloodstream and deciding on its internal state and the context and on the sensors output whether to release a drug or not that would be a cognitive agent in my mind or if you think of Internet of Things and wireless sensor networks the same thing if you're measuring the traffic flow or the cars in a parking lot and you're measuring some temperatures and something you decide to take an action on it so it doesn't have to be a full-blown humanoid robot it could be really a micro system that's acting on the data that it senses continuously with ultra low power circuits and slow it down silicon minimizes this power consumption I argue that it's important to do slow slow scale small scale systems to try to understand how the brain works so all of this work actually was done in collaboration well as I said Rodney and Kevin inspired much of this at Instituto Turner formatic the students and the postdocs were really really great in getting all these chips to work and also their collaborations with Rajat Manoa and the next student of mine who is now at Cornell Sabarmati good Calvin Berg and Emory stephannie Fujian Fabio stefanini also ex-student well Stefan as a professor at Columbia fabulous the next student of mine is there Elisabetta is a professor at Bielefeld also next student of ini christine Myers and up or exposed talk of mine that's now a professor at Dresden and timís is working on memory stirs at Southampton finally the last slide is the I'm not part of the human brain project everybody asked me that I'm actually benefiting from the other sources of funding that Col Heinz was showing earlier yesterday actually and also from the Swiss National Science Foundation and finally the last slide when I thank you for your attention is an advertisement slide where another workshop very similar to this one is going to take place in a few months in Italy in capocaccia the deadline for registration is actually March 13 many of the people in this room are actually going to be there but I hope more and more can come because it's exactly the type of community that we are addressing in this workshop and it's a workshop for two weeks we bring chips robots and we actually try to get these things to work together we discuss with neuroscientists about these primate experiments and then put the result of those discussions on at work in these chips so look at the capo caccia webs if you just google for a cup of chai neuromorphic you'll find the link and I hope to see you there in end of April beginning of May thank you for your attention one quick question or we can have discussion during the break that if we have a quick question okay thank you very much for some very nice examples of what you can do with spiking neural networks my question is are there any plans or ideas on how to extend this approach to larger networks which are arguably computationally more powerful well this as I told you the CX quad is a prototype chip we just tried with the cheapest technology that we could afford and the smallest size that we could build and it's a thousand neurons now we have nine thousand neurons already with nine thousand neurons we don't know what to do with them so we are already we don't require more to explore computational principles and try to use these chips to understand how the brain works as soon as we will find the need for example with this 20 nanometer process in the same area we can put you know many many more neurons so it's the size and scale is not a problem at this point unless you want to build a exabyte computer but we don't want to build exabyte scale computers hey thank you 