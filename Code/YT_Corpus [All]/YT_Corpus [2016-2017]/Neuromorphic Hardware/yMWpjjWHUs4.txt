 thank you and and also so in the time between what I sent them my resume and in now I actually am co-leading the hardware for the do ii exascale program so I'm stepping down as CTO for nurse but but hey alright so I'm gonna go through the initial part pretty quickly but I still wanted to put in context I I see students and so I always like to leave in the the necessary slides that a lot of us have already seen too many times but you know the first is of course that if you just follow the top 500 list over the past two and a half decades we've come used to improvements in performance that approach about 1,000 times improvement in terms of well according Linpack which people can debate about but a thousand X improvement performance every eleven years like clockwork for about yeah twenty thirty years now and and we've come to rely on this for not just improvements in resolution for our our our scientific codes but but in fact transformational changes in our ability to do modeling we haven't just improved resolution we've added complexity new species and in the kinds of physics breakthroughs that that we expect to have in the future require continued scaling of performance improvements but you know kind of the this was the shot across the bow and this has been shown so many times I think the first time I showed it was at an ISC in 2006 and and we had an earlier version of the graph that was hand drawn by herb Sutter but the issue is that we still have Moore's law scaling for now I know the Satoshi told you you're not going to have it for much longer but in the meantime we we still see scaling in the number of transistors can't borrow pointer from somebody I forgot to grab a pointer it's alright let me try to use the mouse here can you see that I also have to move very carefully because this is your mic thing alright so you know we still have this exponential improvements and I'm just gonna yeah okay thank you thank you yeah so we're seeing these exponential improvements you know what oblique angle I'm just gonna go back to this guy here all right exponential improvements in the number of transistors we can cram onto the chip but we hit the Dennard scaling essentially ended in 2004 and Dennard scaling was our ability to improve the energy efficiency of individual transistors at a rate faster than we were cramming them onto the chip and so we took up the slack by cranking up the clock frequency at an exponential rate every generation and of course we can no longer do that anymore because of the end of ginardi and but the the industry found a different way to continue technology scaling so instead of scaling up the clock frequency we doubled the number of cores every generation I was involved in a study with David Patterson's folks that we started in 2004 2003 and in 2003 we're going to skewer industry we're going to say this idea of going with scaling doubling the number of cores every generation it's just stupid everybody's run out of ideas and by the end of four year study we determined actually it wasn't such a bad idea after all and moreover maybe you should go to lighter weight cores which we'll get into and of course Satoshi has already covered this but you know coming around 2025 we're not might not even have the Moore's law anymore and this this will make things even more intriguing the the net result though is that we have an expectation gap we've come to expect these exponential improvements in performance and yet were probably not going to we if we continue on our current path we're not going to be able to get the kind of performance improvements we need to advance science so the challenge here and now we're done with all the contact stuff and everybody's also managed to filter in so worked out perfectly the challenge is that you know the nature of digital competing systems is changing rapidly due to the end of Dennard scaling in towards the close of silicon lithography as we know it and our existing programming environments were designed with a machine in mind that is 30-plus years old in all of the things that we had in mind the design constraints for which our languages our software environment our applications have in mind a machine abstraction that's nearly 30 years old and it does not reflect what is emerging as we move towards the exascale and it's also important that as we migrate our software environment and our applications in algorithms taxes scale that we extrapolate to 2025 and beyond in order to come up with durable design because you could find yourself porting everything to you know whatever the architecture is it's currently on the floor and then constantly having to change over and over again you'd really like to think in terms of extrapolating what the longer-term trends are that go out 15 years so that when you do your redesign you come up with a durable set of abstractions something that you can actually continue with over the next 15 years instead of making it work for each so you know we we have we've talked about the the do a strategy we've decided to adopt a holistic strategy for these challenges which is involves Co designing the hardware the applications and the programming environments together and the reason why is that we don't actually think that we can ask the hardware architects to accommodate all of these changes and preserve our current software environment you could do it but you will not see particular gains and perform that we need you also cannot expect the software the application programmers to eat it all and program and exotic or strange in rapidly changing programming environments the it really requires a constant trade-off evaluation together in order to come up with a compromise that minimizes disruption of the applications and maximizes the effectiveness of the hardware and and so you know before I get into the hardware changes and that's going to be the next phase of the talk here is to really go through the hardware changes and talk with them in terms of not what is the next chip that's coming out but extrapolating them out to where we will be in 2025 because we have our crystal ball has gotten a lot clearer since over the past decade and the first thing is that to not conflate physics challenges which is like the end of Dennard scaling with computer architecture challenges a lot of my colleagues over the years have said oh you can't get the computer architecture guys to change anything you know they got their gamers in the consumer market and then he said well would you want to change like well I want you know more capacity and I wanted it a terabyte per second you like well that's kind of limited by physics you know and these guys are trying you have to understand that the computer architects are trying the best that they can with the physics limitations that they have so you know physics is technological constraints there is a cost of data movement I can't magically make the resistance of copper wire improve that that is a physical constraint the capacity of DRAM cells clock frequencies at the end of Dennard scaling speed of light the melting point of silicon our physics constraints and the computer architects have to work within that box computer architecture is the design of the machine and so you can put in power management you can make changes to the instruction set architecture you can make design choices about Sindhi widths these are the things that in fact if you talk to the architects they can MIT if you talked with NVIDIA No make the change within eighteen months we convinced them to double their the number of registers per thread and they got it in within 18 months of us having that debate with them computer architecture is a data-driven process if you can come to the table with quantitative evidence that justifies a hardware architecture choice then you actually have you you can actually move things and Fred Brooks said computer architecture is like all other architecture like architecture of buildings it's the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible but within economic and technological constraints and there's a lot that you can change there's a lot that you can still do now to effect you know the future of architecture but within the constraints of physics and so that's why we really believe in Coe design if you believe that both the hardware architecture choices can be influenced and worked together with making changes to the software then we can really make major create a more productive environment the long term so you know changing hardware is very expensive and it takes a long lead time but also rewriting your software is extremely expensive and also takes a long lead time co.design is basically making quantitative trade-off analysis using trustworthy models or simulation in order to evaluate the cost and the PI you know the cost of these changes in an environment that is constrained by physics and by cost in and so in this you know architecture modeling and simulation are really important for about making creating credible information to evaluate those cost trade-offs so that's my stick about you know Co design and in in in background so let's let's move into the meat here so we're going to talk about architecture trends over the next 15 to 20 years and we're what I think the exascale note is going to look like and then we're going to turn it over to the implications for our programming environment and how we can do much better so so I get together with Peter Kogi ever since the 2008 DARPA report we get together and reevaluate did all those really depressing diagrams that he put together continue and and so we collect new data and discover that yes it continues and so you know we can't crank up the power dissipation for these these chips it's not very practical so clock frequency growth has tailed off and the number of cores is going up this is a really old diagram that I created in 2005 when I was with the Patterson study it was at the view from Berkeley as we called it to explain really why this isn't such a completely stupid idea to increase you know double the number of cores and for that matter to go to lightweight cores and the way it works is this is that you know V squared F so you know with Dennard scaling you could drop the voltage and you'd get this cubic effect where I could reduce the energy required by the voltage squared but if like you no longer dropped the voltage then I only have the frequency term to work with so you used to be I had a cubic you know so I lower the clock rate and I can actually operate it a slightly lower voltage so I can get a little bit of that cubic improvement back and so we start from like a power v here really big chip if you just look at this chip here that guy up here in the corner that's a floating-point unit all the rest of the surface area in that chip is to hide the latency in to handle the high clock frequencies it's the pipelining its caches all sorts of stuff to feed that little teeny floating-point unit if you slow down your clock rates modestly like you do with let's say an Intel Core 2 which was contemporary at the time that I made this slide then you you not only recover some energy efficiency because of the capacity of the transistors you also get to reduce the depth of the pipelines and now I reduced the surface area of the chip so I've reduced cost I've also reduced leakage current which is another source of power loss and so that's pretty good but I'm also I have a quad issue processor core but my CPI my instructor is is still very low I very rarely actually use for instructions per cycle so if I go to an atom core and at the time an atom core which is now like KL used to be a dual issue core or even a single issue core same is a is an Intel Core 2 but I've simplified I've reduced the complexity of the chip and modest decrease in sight in in net performance but huge reduction in area and then if I go to a specialized processor I use 10 silic as a stand-in for GPU core then I can have further improvements in fact the way it plays out as I go from 120 watts at this 90 nanometer so 120 watts at nearly 2 gigahertz I take all these modest productions I get down to like a GPU core point 0 9 watts at 600 megahertz that is a peak improvement of 400 X improvement in peak efficiency if I ignore the memory subsystem and things like that when I did sustained efficiency I get an 80 200 X 120 X sustained efficiency improvement but I'm still hampered by my DRAM technology but regardless when it comes to the chip this is the right choice because I each simple cores a quarter as computationally efficient is the the complex core but you can fit hundreds of them into the same chip in the same price point in in and be a hundred times more a peak of a hundred times more energy efficient net it ends up being like 10x but with all the burden of other subsystems but there's a peak that you could extract out of that that might be closer to 100 X and the result is that this is you know the classic graph from the DARPA report we're trying to cross that 20 PI kajol per flop finish their op finish line here in the only way that you're going to get to that is either lightweight core or accelerator cores like GPUs and that's the of all the scenario to study that's the only thing that crosses the finish line you can continue with the conventional x86 or large core architecture they will still make them in 2025 but you will consign yourself to this gently tailing off improvements in performance this graph by the way it looks almost identical to a graph I saw when I started my career 25 years ago when we're showing parallel cluster type computing versus vector computing that you could continue with vector computing but you would consign yourself to that shallower curve of performance improvement so NVIDIA has term locks and talks and I actually kind of like it because if it doesn't say one core is better than the other it says okay I have a latency optimized so I still have my fat cores my Zeon's and stuff in and this is the most energy efficient solution if you don't have a lot of parallelism these these lightweight cores or even GPU cores that I'm talking about that is by far the most energy efficient solution if you have sufficient parallelism to exploit it if you if you do not have sufficient parallelism that in fact the latency optimized for is the most energy efficient path forward and in and so this gets past you know core shaming you know that oh it's a fact that you know I hate fat or thin core you know it's it's like you know the oh this core is you know needs to go on a diet or something it's this is not it's it really is the large complex cores like power eight are actually the most energy efficient if you don't have parallelism but that's that's not everything that's wrong as we move forward so okay we have by going to scaling the number of cores and furthermore going beyond just multi-core we've go into many core which is to go with even lighter weight cores that we used to rather than merely replicating cores of the same size now we push the problem to a new area and the problem is this the problem is with wires copper which is what we're currently using for our materials is about as low resistance material as you can expect at room temperature basically for these machines so if you think of this little hunk of metal here this is a copper wire and the power dissipated by copper wire is equal to the frequency multiplied by the length of that hunk of metal and divided by the cross-section area well this is just your plain old resistive capacitive wire circuit that you're taunting in undergraduate physics so what's the deal well the issue is that when I use a lithography process and this is where I get my technology scaling III with lithography this wire shrinks in every dimension equally if that happens you look at this equation my energy efficiency for that wire is not improving when I shrink it that's a problem because when I take this transistor as described by our speaker yesterday now it looks like a capacitor well it is a capacitor it's a capacitor with an active material underneath but it's capacitor and when you shrink that guy down the area of the capacitor goes down and therefore the energy efficiency of the transistor improves I don't have Dennard scaling I can't drop the voltage but I still get an energy efficient efficiency improvement with technology scaling when I scale that guy down and so if I have my compute which is the transistors is improving every generation and my wires are not improving it gets to the point that the compute part is not dominant anymore data movement now becomes the dominant cost in this inverts everything that we're used to when we calculate things we think of flops as being the most expensive component of a computer and we try to minimize the number of flops that we consume when in fact it is now going to be data movement we've only just begun to develop order of complexity theory and communication avoiding you know algorithms theory that takes into account reducing data movement rather than reducing the number of flops you know photonics could break through this bandwidth distance limit but thus far it's photonics converts a energy efficiency problem into a cost problem and so we haven't been able to quite you know deal with that yet but it is a dark horse candidate the the other aspect is pin limits so you know we have the issue of data movement to the distance the other is that Moore's Law does not work for pin packages Moore's law has been nominally thirty percent per year compounded annual rate of growth of the number of transistors you can cram onto a chip pins at best or one point five percent closer to one percent growth rate on a year per year basis so it's hard to get no no Moore's law for pins so if you imagine four thousand pins in a chip which is a pretty aggressive pin package but the GPU guys do this kind of thing half of those pins are going to be power and ground because the thing sucks a lot of watts of the remaining two thousand pins you run those guys as differential pairs to reject noise so if you get beyond about and you're also limited if you go beyond about 15 gigabits per second per pin then you go from a resistive capacitive circuit to a inductive in the losses go way up you have to have complicated surgeries all sorts of bad this happens so you try to keep it below 15 gigabits per second so if we just multiply this out that's you know 10 gigabits per second times a thousand pins that's about a peak of 1.2 terabytes per second if we get aggressive and do single ended signaling and some other tricks we might be able to crank that up to 4 terabytes per second but that's there now if you know that we're reaching an upper limit and we're already within striking distance of this upper limit of the rate at which you can move data on and off chip with electrical connections a two and a half d integration boosts in density but it's a one-shot deal the way it works here is that you know I got a silicon carrier and I can actually use with lithographic processes at like 130 nanometer to increase my wiring density of my PIN pad density by a factor of four to eight but that's about it and so so then I'd bond I take my CPU and I put my memory stacks and you know I kind of bond them ionically to the surface of this thing I increased my PIN pad density so now the CPU can speak to these memory cubes at very high rates so the individual pins are still operating at ten gigabits per second peak but I have a lot more pins down and if you look at hpm it's actually just for ddr4 channels conventional ddr4 channels but I've crammed them into a single package so that's great but one time boost and also there's limited surface area and they sing silicon carriers so I don't get my memory capacity improvements I do get memory bandwidth improvements but not capacity and yet it's expensive so maybe there's an inn for photonics here but you know this is a diagram I created years ago to try to communicate this but this is you know cost a data movement relative to ops and in the real point here is that we're improving the energy efficiency of each flop but moving a small distance in the chip is not improving and also getting off the chip is only improving at a modest rate and this this really impacts our programming environment I still despite showing that diagram had people saying well are you sure it's a problem how big of a problem is it you know did you really calculated it out and in the sadly I started saying well the answer depends and and then everybody started to doubt whether or not this is a real problem so so let's just let's just do this let's calculate it out I'm a EE and I know how to use Excel so you know this is a die photo we've got Intel big cores small core so our said you know in Intel so I talked to Algar at Intel and he said oh yeah you know the the risk five core that you guys are working on it's got just about the same power dissipation characteristics as the KL core it's like great so can I use the KL core it's like no we can't use a kml core so in a user risk five core which just happens to have the same power in area characteristics of the KL core just by coincidence and the same goes for the ten silica core I use this as a stand-in for the GPU I had exactly the same conversation with it Bill Daley in 2009 where he gave me preliminary permission to use a GPU core and then Jensen saw it and I removed it and so the ten silica core just happens to have the same characteristics it's just not a GP core so you know and we have to do some area adjustments so by the magic of PowerPoint I actually proportionally scaled these down to what their actual relative sizes are on-die which you wouldn't make as interesting a die photo when you see that so point two millimeters by 0.2 millimeters tiny tiny little things so if we look at the air energy area cost for these guys it's the same as that 2005 photo thing I had where we go from 12 millimeters squared at 651 Piko joules per operation cycle so this is the fully burdened not with memory but with cache and all the other stuff how many ops I can graduate per cycles so this isn't even benchmarked it's really the peak we're going to have the point six or point zero four six millimeters squared at 22 Piko joules pour per op graduated per cycle so so huge energy efficiency improvements when you go from this fact course thin core but the key thing here is that the wire energy for all of these guys is the same 150 joules per bit per millimeter traveled so how does this play out then so when I get the fat core you want to have two metrics that are interesting I have what at what point does a compute operation equal the cost of moving data move the data movement so for Intel Haswell core we're talking about you'd have to move data 108 millimeters across the silicon the surface of a piece of silicon in order for the data movement costs to equal the 651 PI K jewel operation cost so when I move data 20 millimeters across the surface of silicon it consumes only 0.2 times the amount of energy that a flop does and that's why people are like what are you talking about you know to see on I I never see these effects that you're talking about this is crazy but now if we go to a lighter weight core I only need to move data 12 millimeters across the surface of the chip before the cost of compute is equal to the cost of data movement energy and you'll notice that the Xeon Phi the KL offers a quadrant mode where you can actually divide the surface of the chip up into four Numa nodes and it's because that's actually 12 millimeters is half the distance across the surface of the KL chip and now we go to an even lighter weight core I only need to move three point six millimeters before the energy cost moving the data is equal to the energy cost of performing an operation and these metrics will get worse with each generation up until we get to the limits of silicon lithography and that is why were worried so you know we have our locks and tox and another important thing to understand is that as we move to chips that have hundreds of cores on board the chip each hop that you take across the surface of that chip also has latency I have to take at least a couple of cycles per hop if there's buffering it might take ten cycles per hop so there's a latency cost and there's an extra energy cost I need to pay attention to apology and this gets to this you know everybody said well data locality that's always been a problem I've been cache blocking since caches were invented this is true that's vertical locality management this is a packaging issue I do have to do loop blocking to move stuff vertical locality is moving stuff from my main memory into the CPU back out again through cache hierarchy the thing that is different as we move forward is that topological locality even within a single chip so these little squares here meant to be this is a 2d planar surface of a silicon chip and these little squares here are your cores on that chip and now suddenly I need to know something about or maybe my runtime system hopefully my runtime system needs to know something about how to map processes on to this to maximize the topological locality of data movement and that horizontal data locality management is the thing that our the current programming environments stink at and it's and this is something that that we need to address but that's not all that's wrong so we're almost done with all the stuff that's going wrong and we're going to get to the programming environment part so you know the memory technology is also evolved to the point I talked about pin density and I can go to silicon carrier technology in order to continue to crank up bandwidth for memory technology but there's limited surface area on these caret silicon carriers that they have for that so I can't get the capacity anymore because I can't fit enough of those little memory cubes in this in the little ring around the processor chip and and so in the past we'd always been used to for your main memory I get target I wouldn't always get it but I'd target one byte per flop of memory bandwidth and also with the very same memory technology one byte per flop of memory capacity and we kind of gotten used to that over the time the new paradigm though is I can get you one byte per flop of capacity but I'm going to be off by a factor of 100 in terms of the bandwidth that I could deliver with that technology but I could go to stack memory and in package memory I get one byte per flop of bandwidth but I am off by a factor of a hundred in terms of the capacity that I can offer you know one technology gives me both anymore and in fact this is from samsung documentation there this as they said this is what the future looks like two tiers of memory nobody likes us I can't imagine who would like it but you know we asked the vendors we said we want one byte per flop of capacity one byte per flop of bandwidth they said that's great here it is like well we are really hoping with one technology but they just split it and called it a day and so we're gonna see if we can try to migrate stuff between these two tiers of memory but this is kind of an unsatisfying solution and in in in general what we're seeing is increasing depth of the memory hierarchy not only are we seeing the inclusion of HBM HMC type memories but we're also seeing additional tiers of memory in the storage hierarchy and and yeah so this this is the future and in the end it's it's going to impact our programming environments so so quickly I'm going to go through with all that in mind you know what are we seeing is the strawman architecture that we're going to be targeting so the first is that when we did procurement the the Cori procurement 2012 well we saw kind of four different kinds of nodes bid for this we had heterogeneous many core homogeneous many core you know traditional type nodes heterogeneous accelerated and attached accelerator type architectures like they have a Titan first thing that we noticed was that they offered us a choice of integrated accelerator where they had something like the Numa link technology that they offered for quarrel where the bus the instead of PCI bus you have a bus that's so fast that you almost don't see it anymore I can actually the GPU can actually see the CPU memory and almost the same bandwidth of the CPU sees the memory that's a fantastic not only that it they have unified memory architecture with the GPU so it's no longer an error if I did - to copy the stuff into into my fast memory well that's the case I don't I don't want this thing anymore I want the Numa link thing so there's still a bus there but from a logical perspective it really looks like I've got an integrated accelerators integrated together with my big course I like that better I'm never gonna buy something that doesn't have unified memory again so the next thing is accelerators versus lightweight course you know people have said oh the ISA is different security is different I got to use CUDA with one and openmp with the other somebody with thread divergence cache coherence but if you actually look at the natural evolution of GPUs even for your cell phone they are no longer for graphics anymore it used to be that it was crazy wild eyed people like us that we're trying to use GPUs for compute but now it's actually mainstream a lot of the regular computing functions even your cell phone are actually invoking the GPU core and this is general-purpose computing not just graphics and this has pushed the GPU manufacturers to actually generalize the architecture to tolerate more thread divergence to have coherence albeit its release consistency model but they have a coherence model in in the ISAs they're getting better with the compilers virtualizing that I claimed that over time actually the differences between these things this really is heavyweight versus lightweight cores and so now we really only have two swimlanes moving forward and and the question is do I try to do it all with one type of core like we're doing with KL we're hoping Intel changes its mind on that and goes with something that's more the phantom cores and in fact if you just look at Quarry you look at you look at the GPU thing that they're doing with coral architecture this is kind of what things are evolving towards I've got heavyweight cores more or less operating system accelerators maybe run my MPI protocol stack but then I have those lightweight cores and these are my real compute engines I have in package memory which is low capacity high bandwidth and I have my off package memory where I get my capacity but I can't deliver the bandwidth and maybe envir a man that he is that with the heavy weight cores data locality actually isn't as much of an issue there might be some Numa effects of the subsystem but for all practical purposes you can logically think of them as a pool of course it's the lightweight pores the ones that you have to worry about where you need to worry about what is nearby and what isn't furthermore they actually can scale cache coherence across a chip that has thousands of cores in it that it can be scaled but the architects do also warn you if you take too much advantage of it you will suffer because cache coherence have this has this way of obfuscating data locality you have to probe everybody's caches to see who's got that cache line so if you use it you can you can use it but it'll be bad and if there's any way that you can avoid relying on the cache coherence it'll give you a performance boost and then the other thing is that I got integrated NICs so Knicks are moving onboard the chip once the NIC is TLB coherent and cache coherent with the CPUs comes appear now suddenly I don't have to do page pinning now suddenly global address space looks like a really attractive thing to do so so we just made it to implications for future programming models so this might actually work I can't believe it so so implications if you put all this together you know when when I wrote I was involved I was one of the original developers of this code called cactus which was a gr code that I worked on for many years and you know I think about how we designed it we used to have these arguments over well what's really gonna happen with the future of architecture I mean you can't really design a code with all possible combinations of what could happen the architecture in mind you actually have to make choices say this is what's important and I'm not going to make it infinitely scalable I'm gonna make it scalable enough and so some of the design choices I had when designing the code and also what people who design programming environments had in mind was that peak clock frequency was the original limiter for performance improvement but now it's power is the limiter flops so we used to think in terms of algorithm development the flop was the biggest cost for a system and I need to optimize my order complexity everything is about reducing the number of flops but now actually the flop isn't the thing it's data movement dominates I need to optimize to minimize data movement so complete reversal concurrency I was expecting when I was designing cactus that we'd see a 10x increase in currency every decade and so we didn't design for infinite scalability we designed for what was anticipated to be a slow kick pace I know that the MPI developers had the same idea in mind but concurrency is going from linear growth to exponential growth and and we didn't we didn't plan for that when we're designing this stuff memory scaling maintained by precocity and bandwidth now we're moving to compete growing twice as fast as either capacity or bandwidth the memory technology data locality MPI plus X model assumes within a node every CPU is equidistant to memory and every every CPU is equidistant to each other likewise the MPI model when I send a message to rank two I think it's you know logically the same distance is to rank 1000 you know in the system this is no longer the case topology is important we are tapering bandwidth at every level and so you now need to have a way of reasoning about locality maybe not the programmer again maybe we need to have the runtime system involved we used to assume uniformity of core types we used to assume uniformity of performance but architecture is diverged we have that fat and thin codes we have aggressive clock speed throttling we can no longer assume homogeneity we have to Tsun heterogeneity and then of course reliability hardware's problem versus a software problem so we created this document this abstract machine models you know document to attempt to communicate well you know the first thing is that with this it really inverts everything that we thought was important is not important everything that's important for the future is was not in on the radar screen when we designed our programming environment so it's oppositely and everything is the opposite of what we designed it for we need to address that so and I think that the path to addressing it is coming up with reexamining abstract machine models so I'm going to skip from this thing and just talk really about what is an abstract machine model there's a really good document from Martha Kim this is really old concept a MEMS date back 40 years maybe 50 years and the original parallel abstract machine model was called the P RAM model and it is the underlying model that we had for the early SMPS that I worked on so when the first you know Cray XMP s were coming out in then SGI power challenges later when I was at NCSA the model and this is the model that gave birth to OpenMP was that all those processor cores are equidistant to the memory so I can have all the details about it's a ymp it's a SGI power challenge it's an HP SMP but underlying that is a common abstract model of how things are connected together logically and we developed a the original openmp model was developed with this equidistant model in mind where all the processor you listed then we got to the SGI origin 2000 where they continued to attempt to scale up the parallelism of the system but they could no longer maintain this illusion that the processors were equidistant to each other at every scale and we're equidistant to the memory and if you programmed with this abstraction in mind in RAM on an SGI you would suffer greatly for that and so you know at that point we just determined the and you know it's better to just program for locality as a default and at the end of the day the SGI ended up being a very good MPI machine for us and and once we cracked you know decided to go and with this new the CSP model the communicating sequential processes model then also MPI became palatable CSP basically says I have a processor in memory is local in them for non-local memories and processes I have to send messages to between them and it it sounded like your crazy idea at the time but once you get over the hump and redesign your algorithms for it it's great so CSP is the abstract machine model MPI is the concrete and body meant of a solution a programming environment solution to that model so we've had that model there's a another model which is what we call MPI plus X there's actually a abstract machine model for that it's called candidate type architecture terrible name for an architecture but you know the question is is that is that is that what's next is MPI plus X sufficient and in I claim it isn't and all the stuff that I described before I think that the candidate type architecture is the incorrect abstract machine model for programming these machines yeah and why do I think that MPI plus X is insufficient well first thing is lightweight cores aren't fast enough to process the complex protocol stacks at speed when you're doing tag matching when you're doing stuff like that the thread match and despatch operations are very inefficient even the interrupt management is inefficient it is much better and the architects have even told us that it's much better to use something lighter-weight like using the memory address to match endpoints because that capability is already baked into the hardware we can't ignore locality anymore especially inside of the node you know we have the Numa issues aren't just between chips anymore it's even within and within a chip we have new memory classes and memory management MPI plus X doesn't really address this and there's a synchrony heterogeneity and again bulk synchronous model does not address this challenge so you know what's wrong with MPI 30 plus OpenMP well you know I I know that it'll work it's it's already working now the issue that I have is it doesn't offer much of an abstraction I have to create my own layered abstraction x' on top of it and I in that sense I think it'll work but I also think we can do a lot better in here's why so if you look at like you know the old model parallel do I got a loop I slap on an open MP parallel do now it's parallel that's great if my model was the peer and model but my model isn't the peer and model anymore so the old model was I equally divide compute work and spread it out amongst the processor cores and data just magically moves to the right place but if data movement is the dominant cost then this is the incorrect abstraction for parceling out parallel work I need to have a way of minimizing data movement so I want to describe logically how the data is partitioned across the machine and then have the computer follow where the data is located so this is a data centric model this model actually by the way is you know MapReduce and Hadoop do this model but MapReduce and Hadoop are wholly inappropriate for loop oriented codes but you know the notion is I have this is a statement from UPC where I've already partitioned the array a and when I do a parallel do loop the loop iterations execute where a is local I and so this is a data centric programming model and that's inverted from the way that openmp was fundamentally designed to work but that's alright standards evolve so they have open ACC which was inspired by open a MP and it includes things to manage data locality well this is from John Levesque who is a great programmer and this is what it looks like these are the embedded comments for open ACC for the code that's below it I'm not so crazy about that in Levesque is a conserved c'mon you know quantity I can't I can't replicate him very easily so I'm not sure it'll work in this code by the way works phenomenally well but I'm not sure if I want to do that open mp4 isn't much better this is in the open mp4 documentation I have three times as many lines of code in comments than I do actual lines of code for a very very simple loop I have a lot of stuff and I also have to change all that stuff if I work in the GPU the stuff in this comments will be different set of directives so it's portable but not really that portable I it doesn't look much better and see I I just it'll work but do I want to do that no no let's also look at messaging overhead so this is the cost of doing nothing with I send and I receive on a crisis very highly optimized implementation of MPI I do a nice end and I don't even post a weight so it hasn't actually transferred any data yet and this is how many cycles it takes off node and even on node I'm spending a thousand cycles just to do nothing and this is because I have to navigate the MPI protocol stack well you know that's on the Haswell core and that's taking you know 1.6 microseconds in order just to navigate this stack now imagine I have a kml core how many cycles is going to take for a core that's lightweight to do that well it takes three times as many cycles for each core to navigate that stack and he'll what if I had a GPU core had to actually send an MPI message do tag matching and execute that stack I only actually get an execution slot every once every eight cycles so now we're like 3,000 nanoseconds so what does 30,000 in a second says 30 milliseconds spent just to send nothing if I need the strong scale that is a problem I shall skip this one here we're starting to run out of time here so so you think you can do better well I know MPI and openmpi work but OpenMP and MPI need to evolve so whatever OpenMP and MPI is when we get to exascale it isn't going to be the same openmpi in a you know OpenMP and MPI we all have to change I think it's a live perpetrator in our community that just because we're going to continue OpenMP and MPI that somehow that means we don't have to change even if you use openmp and MPI it's going to be different otherwise your codes going to stink so so you know let's look at the alternative so we got data locality parallelism overhead performance portability there are abstractions such as for example data locality and parallelism tiling abstractions are an excellent way to overcome those data locality problems and we have examples with Teta hierarchical tiled arrays Rajah Coco's that people are actually implementing these abstractions as useable api's that programmers could use for communication overhead we've always had PDS it's just not been ideal with the changes in computer architecture though the hardware architects are saying please just allow us to use addresses for sending messages rather than tag matching and type matching and all the other matching now I my Hardware can give you zero overhead data transfers if you would just use memory addresses so why don't we use memory addresses these are things that have been around for ages a UPC UPC plus plus MPI 3rm a co a Fortran which is now Fortran 2008 and open Shem they're already there performance portability DSL is auto-tuning heterogeneity there's a lot of people studying dataflow is back in vogue again there's a number of different runtimes légion charm plus plus by the way charm plus plus predates MPI hpx there are a lot of alternatives available and there's of course containment domains and our Excel verse for resilience and so I'm gonna just take the last ten minutes or so to close this out and then I'll take some questions here so I'm gonna just give you a few vignettes of what these things look like that I was I was just telling you about I think I already gave you the introduction to a data centric computing model we actually examined a number of different people that are doing this we even had a workshop about this that I'll plug in just a second but the common theme that we've seen people adopt is programmers would like to when they move their code to new architectures they just want to minimize the number of lines of code that they need to change in order to migrate to a new machine so how do you make that work with the data centric programming model because right now when it comes to managing data locality I have to change my loop blocking so I end up changing the loop blocking for every single loop in my code and so I have to touch a lot of code in order to manage structure of arrays or array of structures or any of the data locality optimization tricks so the idea here is that instead I describe my data layout my domain decomposition it becomes an intrinsic part of the data type so when I declare a multi-dimensional array I also include additional metadata that describes how that array should be laid out now when I move to a new machine I have to change that description but I only have to change it in one place and then when I hit my loops I use either C++ 11 lambda functions in order to abstract the iteration space and separate it from the data layout or you know in Teta we have our own construct which is essentially a lambda function in doing this I can change my code in one place to describe abstractly what the logical domain decomposition is and then when I hit each of my loops then the lambda function allows me to actually visit the data in the correct order and then the runtime system makes theological - physical mapping so that I don't have to manually do the thread pinning which is what I have to do now with freaking openmp is do manual thread pinning so this is a data centric programming model is actually representing information that the compiler would otherwise destroy about data locality and actually preserve it as metadata which can be understood by the runtime system it also understood by runtime constructs when you visit your loops I'm gonna skip this this is abstraction this is what it looks like the key is that after I make these transformations to my code moving to new architectures I go from naive OpenMP performance to having naively written code that performs as well as code that was hand tuned by one of our experts this is what you get if you change and adopt an abstraction a durable abstraction that's parameterize Abul it enables you to you right now Eve code again and expect to get performance that you'd get from manually loop blocking and generating ridiculous amounts of code simulations as we scale out to thousands of cores so we had a workshop in this and what we determined was that there's many people have adopted the same thing we all are actually using essentially the same recipe Roger Coco's what they're doing in the European Union for these things we actually think this might be a huge opportunity for standardisation programming environments and so we had our first workshop at Logano we had a second one in 2015 in Berkeley we're trying to finish that report but you can see the initial report in the 2014 workshop and it scopes out the design space what's happening in these this space and and so I think it's really promising the other is domain-specific languages this is I raise the level of abstraction so that I right now when you write Fortran code or C code it over specifies the solution when I have a for loop it has to assume the compiler has to assume I have to execute in exactly the order that that for-loop implies 1 through n in order to be correct and I have to use Hercule improves or polyhedral methods in order to prove that I could do anything otherwise way over constrained if I want to have flexible parallelism I raise a level of abstraction I can do it by using a higher-level metal language and that DSL then can write let's see if it comes out of the screen so you have a very simplified high-level description of what your intent was a PDE solver block structure grid and it generates the optimal code for different programming targets and this is in fact used in practice right now for a lot of image processing applications the question is can I apply it to scientific codes and certainly the tensor contraction engine is a good example of this but you know there seems to be opportunities in particle codes and PDE solver codes for applying these kind of dsls so I can I can target its use the same piece of reference code and create a target GPU and create optimal code and target a CPU and create optimal code so low overhead messaging I told you about this already where the challenges we have been depending on weak scaling to hide the the overheads of sending messages which haven't really improved each generation but great thing is that weak scaling has enabled us to to move forward but when I need strong scaling I have to and this is where I cut my I don't increase my memory size each generation then that means I need to take up the slack by doing twice as many iterations per time steps so I need to have a performance improvement in order to take up the fact that I have to take steps that are quarter the size so that means if I have no clock frequency increase and I'm getting everything from doubling the number of cores I really need to focus on strong scaling and this was this this is hard this means I have to reduce overheads and the only place that I could think to reduce overheads and these machines is actually by improving reducing the overhead of sending messages in 3000 cycles to send nothing that's just ridiculous the the architects are telling us this the architects are saying I have now had to bite the bullet and I have to use a packet-switched Network just to move cache lines to the correct location on a chip that has hundreds of cores and the packet the packet header the address is the memory of this so I already have the hardware to do zero overhead messaging it's already on the chip what it takes for me to make that your main messaging layer for moving messages everywhere in the machine is with no overhead is miniscule in terms of hardware design so why do we look a gift horse in the mouth why don't we just do that so you know this is MPI 3 RMA has puts and gets it's even better if you go with P gas language I have mem copy suddenly mem copy works is well within the node as it works between nodes it becomes the same I don't have to have MPI plus X I can have X so I'm gonna you know and this is I have a lot of vignettes on different programming environments issues the the the rise of dataflow its back functional programming and data flow the the point is that we have been doing SPMD it's become a dominant approach to parallelism but he used to be vectors were our execution model for parallelism we also have successful things with dynamic threads of fork/join in the venture of an execution in the business area why why don't we do diligence examine other execution models just because we've been doing bulk synchronous doesn't mean we have to continue to do it and it's incumbent on us to examine the alternatives so alright post-war Satoshi are I told you about this and we're just gonna get to the end which is the emerging hardware constraints are increasingly mismatched with a current programming paradigm current emphasis preserves flops real costs are not flops it's data movement it requires a shift towards data locality centric programming paradigms and I think that openmp and MPI we will evolve them into the future but they weren't really designed with this in mind so it'll work but I think that there are alternatives that exists that don't look as clunky and we should study those alternatives you know the biggest opportunities for energy efficiency and performance improvement or where you really examine what is the easy path and the hardware and work together with the hardware architects to come up with a compromise solution that minimizes this disruption to the software environment but leverages things that are natural design trends that are going out for the next 20 years and in that way you can develop durable abstractions that will last that so you don't have to rewrite your code for another 15 years or so and then the end of Moore's Law and you're screwed so all right that's it Thanks you you 