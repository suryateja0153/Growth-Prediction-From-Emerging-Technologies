 thank you very much so I'm going to take you one of them a tour of what I think the new seismic landscape is here someone else is in control all right okay and and so this is a bit of truth through a number of very interesting and interacting developments around neural networks around cognitive layering really around IOT and cloud computing and the increasing role of energy-driven computation and I think one of the things that we're going to do here is is start from these four fundamental forces that I think are changing the system on chip world and then delve into an important area things that see talk about power talk about vision talk about convolutional neural networks which is a very important new computational model and really to then finish up by summarizing some of the tectonic shifts that are taking place in SOC design so let's think about these these courses the first course I would say is the force of proliferation that we are moving in each fundamental generation their technology to an order of magnitude higher volume for the electronic objects in our universe and of course we both have only experienced the mobile internet revolution all of us have at least one cell phone on us now some of us more and we are we are really anticipating this Internet of Things revolution where it certainly is technical technically feasible it may be commercially feasible to populate the world with tens to hundreds of billions of new things there are no longer really personal devices in the sense that we have a conscious interaction with them but we interact with the world they interact implicitly on our behalf to collect data or deliberate information or deliver experiences to us the same fundamental force is really the force of distance and energy in fact if you look at computing you can think of computing at many different scales you can think of it down here as the dimensions and the energy associated with a 16 fifth at NAND gate and then we can step up to larger and larger scale computing to luton ALU operation what is the power associated with accessing a register file how about a level one cache remember to capture level three cache losing take to access an FD ramp what does it take to go out across some SATA cable to get to a disk look is it ultimately take to get the cloud in terms of energy and what you find is there's actually a very close to linear relationship between the distance that you are bits much straddle and the energy here normalized to joules / 64 bits to get out work done and it reflects not only the physical distance of moving a bit over over some piece of wire or optical cable or PCB trace the fact that typically of those specific all layers along the way which will consume some energy and doing some computation associated with validating that request serving and vowing that request and implementing some API that makes it possible to build systems with a degree about traction but the implication of this is that then you can keep things local if your data is available to you you have a huge energy benefit from keeping it as close to you as you can that is going to be male any more than necessary because of the energy cost is so much greater than making an on-chip reference to a pack and you know go to disk if you could do the same thing in RAM and you don't go to the cloud if you could do the same thing in their own disk array and this automatically sort of sorts out here's the late face to do different kinds of computation so manly are they available to you is available to you literally then you should be computing there because the energy is very likely to be significantly lower if you use the little computation resources to reduce that data but where you fundamentally depend on the aggregation of data that is traffic information is not very interesting unless I have everybody's traffic information is that the only way I'm really going to find out what them to take to work that's what it has to go to the cloud and any of them though it costs a lot of energy to get the data there the benefit is also very large and so there is a natural gravitational force for some problems to be more local in some problems to be more global based on their usage of data and there is a natural tendency to say oh I had better figure out what is the right compromise between consuming energy locally to compress my head to reduce my data to summarize my data so that I don't spend as many as much energy communicating it or do I use less energy and I said more raw data over that communication channel so i'm going to use that energy versus distance invisibly in lots of architectural decisions and it really is the thing that is creating i think the structure of the internet as we see it today we're sometimes you're dealing on a really monumental scale you know I sit here in San Jose and I do anything with Google I'm literally sending my bets thousand kilometers only to the dalles oregon whether they have this big compute site and those bits you coming back now that's that price in energy whereas if I can do something just on my phone for example i'm mostly saying in the bottom half of the bottom third of this diagram so the energy characteristics are pretty attractive the third fundamental force is what I could call layered cognition in fact as I talked about this what kinds of layering become obvious one kind of layer cognition is sort of what I talked about this directly derived from that distance versus energy and the compute locally share globally principle is that there are sunset of things you're going to want to do local to your device things that are going to happen in your phone in your Fitbit in your smart toaster and the level of abstraction the level of energy the level of computer would be defined around that local task but some things I'm then going to escalate to a gateway where I have some little aggregation of data and some things will be escalated further to the cloud where I have the enormous amounts of available compute I'll get perhaps at a fairly high energy expense we also have enormous amounts of data avail and so I will take things up through that staff based on sort of how much data and how much smartness I need to have you look at any one of those levels you can see another kind of hierarchy taking place and this is really emerging in part as a consequence of Moore's law of density that we used to think of just a main processor on one of these chips but we find then you have a general purpose processor yet you have a lot of the time spent in certain specific tasks and a significant opportunity to offload that into more efficient either faster or more energy-efficient some systems that can do that task maybe three five ten times more efficiently and so we have seen the emergence of various kinds of offloads GPUs are offload network processors are offload wireless subsystem processors are offload audio off flow is routine in most of the systems that you buy because they can do a better job less energy and they have a less broad scope of activity they have a kind of a narrower cognitive responsibility and now we even see offload from offload so even when you say have an audio processor there's a growing possibility that there's also a voice triggered processor in there something which performs the wake up function that when you say a trigger phrase allows you to bring the whole system to life and then in some of these these layers you have the emergence of things like neural networks and it's really sagging how rapidly neural network ideas are propagating through all different kinds of computation image recognition speech recognition the data analysis network traffic analysis and it reflects the fact that while it's in some sense a brute-force approach to pattern recognition it's extremely general you can train neural networks to do almost anything I'm gonna talk more about them but they too have this sense of a succession of recognition layers that move from primitive features to sophisticated features as you move up through computational hierarchy so we have radios on layers on layers as under the mobility and the fourth course before I get to the fourth course I wanted to talk about some details of later cognition the fourth course we'll get to in a couple of slides it's let me look at it at a macro scale you see all these new kinds of devices and this is probably the biggest new hardware opportunity lies where you have very compelling and strengths on cost and energy in these new form factor devices dividing lots of sense because people are using them because now they want to essentially sense everything about the environment the people are talking actively about smart eight that will listen and watch and measure temperature and humidity everywhere and be able to communicate and share that but one of the things that they share in common is the illusion of being always alert that as you expect them to be always doing something on your behalf now of course from an energy standpoint they actually need to be almost always off and so part of the trip in the design of these things is how do you create something which is doing implicit communication implicit service it is physically mostly off is apparently mostly on and which is providing some useful kind of activity and able to share effectively over the internet and this is sort of what's driving all kinds of IOT ideas get into the question of what drives different kinds of IOT in a moment the second kind of layering I want to talk about this is chip scale here is just kind of a quick example that sort of says when you have one of these enemies alert yet physically almost always off you usually have some quite as instead and there is some circuit level trigger that at least can wake up some first layer of cognition that might be of waste processor we can recognize a small vocabulary of phrases so in our little scenario here it can recognize hello audio when it among never speaks the Tate and price and when it hears hello audio it knows it needs to start waking up the rest of the system but while it's in that state of listening because there is some some noise it's probably dissipating only a few tens of microwatts fully active you may then alert a higher level audio processor may be reading the whole audio subsystem and you may say turn up the volume and that subsystem now has to do some higher-level processing not only opiate of the wider vocabulary of speech but the actual activity associated with turning up the volume and that may be dissipating a few milliwatts of power they may ask for something which is beyond the cognitive scope of that you may be asking the harder question like play like a virgin that's going to require database access at least into my lyrical music library and it's going to have to open the files are streaming and initiate some activities after you started it that that level of processing which is probably tens of middle ones or hundreds of milliwatts on an application CPU they don't look at to sleep because we've offloaded it down to the lower levels but we may ask a really hard question you may say you know let's Madonna's biggest did your cell phone does not know by itself it's going to happen go ask Google in the sense and it's going to find it and probably offer to sell you Madonna's biggest hit but there's a short period of time that you satisfying that particular request we're probably dissipating many watts of power and so that you've got is a system whose power characteristics overall look like this is level these lower levels at least whose cake intelligence and pink power dissipation looks like the intelligence of the cloud and this illusion is really created by the opportunity to have processors at different levels and their associated software so you have distinctly different categories of process for some being routinely now see systems where really the application is running it across at least four different kinds of processors something like a voice trigger DSP media subsystem DST an application CPU and cloud cpu and then a quick introduction to deep neural networks in the context of this this layered cognition where we really have as the fundamental computational structure is a series of filters really convolutional filters to me and preening convolution filters we have typically some subsampling and some trigger function which is happening at each layer so you may filter the input when you use a series of filters which for example might represent finding diagonal line segments or bright blue patches or circles on this very small state and then those become the input to another filter which is looking for features of features where hundreds of teachers I'm going to find a diagonal up and to the left line next to a diagonal up into the right line and I'm going to use that as a higher level feature for many layers and sometimes it's as few as three or four or five layers and sometimes it's 30 or 40 layers but the process of using this is really centered on finding the right coefficients for that and those coefficients are not programmed they're trained and you train them by providing millions of labor examples into this network and then there's sort of a computationally exhaustive process of steepest descent error propagation that allows you to come up with great approximations to the weight coefficients will effectively find the common elements of all things that are labeled puppy and all things say that are labeled patent examiner such that when you put in a picture of various funny pictures of various things it means of those common elements to find the most appropriate label or labels to apply to that input pattern or input image and the most sophisticated neural networks can simultaneously recognize thousands of different classes of objects and can operate in Millicent and some rental networks have become this automated method not only for vision but many other kinds of pattern recognition and interestingly they have routinely now then trained to do better than humans in accuracy of recognizing all kinds of things and in a sense we think of what once was programming and little replaced it with its automated training that is if you have a large enough labeled data set you can get this kind of atom recognition to do all sorts of amazing things it's computationally intensive but it's very general really really easily adopted as part of computational systems so those are three kinds of layering that are centrally important to the evolution of kind of the new generation about of electronics the last verse I wanted to talk about is really the the impact of our expanded ability to sense the real world and in fact if you look at the market statistics you see all these different kinds of sensors microphones sensors of course the gyroscopes accelerometers pressure sensors you get any sensors all of that and so on and you can see that the total volume of sensors is going up dramatically as we equip more of our our world with a sensor so that we can get this rich data experience and one of the things that's interesting to ask is what's the computational load associated with that and we can look at this in terms of well what's the sample rate of these and what you find is that you get environmental sensors motion sensors audio sensors and then the big boy image sensors and each has sort of a range of associated computation for it because typically when you take a sample you're going to take do at least tens and more likely hundreds of thousands of operations per sample on that ADA as you compress it analyzes it communicate it and use it as part of some application so they deliver the sensor sample a sample rate drives total computational load and we're going to cover back around in this question so let's focus in a little bit on things that see because they are so central to the human understanding of the world and also really important from the data rate and computational load so let's look at that same data again I'm going to highlight the the image sensor data and say okay well there are all these different sensors let's let's adjust the volume by the bitrate so we're really this going to take the volume tons of the bit rate to see an adjusted sense of which sensors matter from a strict bandwidth and computation standpoint and so this is that same chart with that factor or pity hmm image sensors matter if that other data is there but it is absolutely invisible image sensing data is so big compared to everything else that it is the factor which is likely to drive the limited and then with limited dimensions of electronic design for a very real extent already today some cisco reports that like eighty percent of all internet traffic is images in video so through all the hood infrastructure today really building computers to manipulate the images we're building disk drives for building flash for building the ramp it's not all about imaging anyone ironic sense and and this just reinforces the sense that this is is a dirige problem and therefore a computationally hard problem i'm not saying that that computing is fundamentally about manipulating images some things become fairly far removed from it but it is a an aspect of the computational challenge and it certainly is one of the things that drives a heck of a lot of system on chip design because he asked where why do people design more new chips it's simply because the bandwidth or the computing capability of the last generation wasn't nearly enough i had learned more capacity I wanted more storage I wanted more compute them to do it and this is one way of looking at the driver so now let's look at what's happening a little bit in visual systems certainly there are things happening at the application though there are people who are saying oh my god there are all these opportunities to make my mobile experience my car experience security surveillance IOT devices consumer devices more interesting by by embracing vision aren't they driven by the fact that the image sensors are so good and so cheap and so fast and it was so high solution and I cannot weird to stick a camera even almost anything and the cost of the the sensor the lens structure the processing behind it is really only a few bucks and so but something that's likely to drop independence and something which means that I can afford to put sensors in all kinds of devices that I would not have dreamed of before the other thing that's happening is rapid progress in image processing algorithms that people are working very hard on better and better object detection tracking identification classification and all networks are playing central role and especially in the last three or four years they have taken over the vision processing world because they are so generally people are getting such good results on different kinds of image recognition pattern recognition but there are also new things happening in the immigration of systems isn't just the compute or just the storage we have to figure out how we're going to deal simultaneous with different kinds of sensors new kinds of image sensors integration of radar or Oh infrared or ultraviolet with conventional sensors we're integrating electromechanical sensors with vision much more closely adding some of these consumer drones are great examples they have a multiple camera systems in them because they had both the end system which is sort of what's used to capture the video that you want but there is also an integrated vision which is used just for things like stabilization and an obstacle avoidance or the for the drone and lots more integration with cloud of distributed software so it's easier than ever to build the application some of which is in the cloud and some of which is local to device let's take advanced driver assistance systems everybody's talking of course about autonomous and semi autonomous driving and if you look at these systems they actually are the layering up of many different types of sentencing for many different purposes but it have to cruise control for a main following for pedestrian and bicycle detection and collision avoidance cross traffic alerts blind spot detection near collision warning parking assistance and overall surround view to make the driver safer or to make the driverless experience safer and when you start building up some of those functions are audio there they're following what's happening in the cabin there they're doing ultrasound for very short distance collision avoidance or their radar radar is nice because it has very long range and and works well under nighttime and adverse weather conditions but really the one that's most ubiquitous is vision because there are so many different functions that people are doing in vision only because of the accuracy the ability to use a color as well as shape and its low cost and finally you often need to fuse those different data streams you're going to find objects or events or points of interest from all these different kinds of sensors but it has to be put together into a safe team managed environment they have to be correlated they have to then be used to actively control the rest of the vehicle and so you find many different types of computational opportunities but vision does tend to be a single biggest one and we see companies like Nvidia and mobile I putting enormous amounts of resources into this space because of the growing electronic content of cars that automotive ADA's is move the single fastest growing segments of the whole semiconductor and invented electronics world we also see fundamental changes happening in the base vision pipeline where historically you had a fairly stable set of RGB sensors and I'm hardwired image signal processing pipeline was kind of clean it up and converted from that bear mosaic pattern into something that was usable at a higher level like y UV representation but what you see now is is to fundamental changes one be the image processing is getting a lot more sophisticated that is you're doing much more heavy duty algorithms or 3d noise reduction or high dynamic range or electronic video stabilization enhance resolution and some of that is actually creeping all the way back into what used to be that simple hardwired pipeline and so we've seen the emergence of soft ISP functions and of course the other big things is video analytics or people are now doing face recognition and people detection an object classification and gesture recognition you really doing vision and all of these are becoming the hotbeds of new albert design and therefore is pushing of new kinds of computational engines I'm going to talk just about a few developments that I've been working on most actively sort of the only place we're really going to talk about things that remotely resemble products here but I think it's important to put this in perspective because we have simultaneously in this push to very high compute levels associated with these these vision algorithms and at the same time a push or lower and lower power because energy at the end of the day is such so closely correlated with cost little energy inevitably leads to lower costs when you consider the cooling the packaging and the power supply itself so I'll briefly introduce this idea of automatic generation of processors because it's a booming block for some of the other thoughts there so what we have in the 10 silica team really pioneer is this idea that anybody out there who's building an electronic system and particularly anybody who's building a chip has the opportunity to have exactly the right processor for their problem and the wave processor can be significantly smaller and faster and lower power and easier to deploy than trying to take a general-purpose one-size-fits-all processor and put it into what may be a very tight budget so the idea here is that what we do is produced at an ace processor a set of pre-verified options and templates that cover a little of the key instruction set features memory hierarchy features and peripherals and interfaces and the customer Dinah's their application know how to choose a template example and to choose different options for customization which could be either from a vast menu of pre-verified options hundreds of options or to specify their own instruction set features or their own interfaces which altogether flow into the processor generator which in the matter of a few minutes creates both the complete RTL as serfs delivered pre-verified design with all of the scripts and test environment for the combination of these features and whatever you the customer thought of in the shower this morning and the software tools that exactly match that same thing so that you have compilers d-mail gotov simulators are bosses which are already pointed to the unique processor architecture that you created it has still this common base instruction set so in fact you can write a piece of code that moves everywhere on every possible 10 silica processor but only your personal one would have the 128 way parallel DES encryption or parallel DSP that allows your class of applications to run blindingly fast compared to what you can do on a conventional process so with that as a as a foundation we're able to push in a few interesting directions what the interesting directions we've been exploring is this area of near threshold operation and they do but can you do without a completely design of the processor and the answer turns out to be that CMOS gate libraries are actually pretty good still down to near the threshold voltage of the process but those libraries are not typically characterized or qualified at those levels and your vendor often does it do it so what do you do one of the things that turns out to be surprisingly easy with these processors because if you take the base processor it's so small it's a complete 32-bit processor which in you know a 16 nanometer technology is like 5,000 square my clocks it is a tiny little thing with only some tens of thousands of transistors in it is that you can actually learn for spice simulation you don't need a characterised say library you don't need to know what the performance of an and gate is you can go all the way down to the slice definition which you sort of have to have characterized across its full range and you can determine power and performance fairly exactly just by running slice simulation on whole thing you may say that's a crazy brute forced at it but now it only takes on the order of 24 hours to bun some reasonable basic function diagnostic on one of these small processors and then get these much more accurate pictures for example of how does the processor behave as an unction of supply voltage on the horizontal axis and you get some interesting results that in fact when you take these things down into the point five or twenty six bolt domain you're talking about active power on the order of a couple of microwatts per megahertz and leakage power which is in sub micro watt oh and therefore you have some of the booming block here which isn't that suitable for these ultralow low-power configurations it's not and by the way the same thing at that point five holes that thing will run under typical conditions with this lightweight verification up to the order of about a well over 100 meters so you have quite a bit of performance to play with even half very low voltage so that's fun you have very low energy now let's talk about some very height of a fantastic so with the other end of the spectrum when you're talking about a huge computational load that's created by these inner processes you find things like vision p5 we just announced this last month and I won't you know take you through a detailed assessment of it but some of the characteristics you see here are first of all its data memory is for independent bank's of 512 is wide so you're really able to access up to 2000 48 bits per cycle we also have a substantial instruction bandwidth coming out of it you have very wide vector operations multiple vector processing so that you can do up to four ALU operations each of which is doing up to 64 8-bit operations in parallel so 256 ALU operations per cycle as well as being able to do four scalar operations multiple loads and stores scattered gather operations of 30-point and fixed point data reorganization on the fly there was a huge amount of data shuffling data computation and data movement in and out of memory that is the necessary foundation for doing some of these very high-end image processing tasks so all together on a standard process this thing is producing about a peak of 300 billion ALU operations per second and if you compare it to its previous generation it's about 4x the performance and actually even better from an energy efficiency standpoint for the same level of performance you can go about at one-fifth the power dissipation none of the keys to doing this is this idea of scale Gavin now some of you are intimately familiar with this concept of gather but it's a very important capability which has not actin been seen in embedded processors today everyone sort of understands that in many of these computationally intense problems you can operate on vectors that is I can take a whole sequence of adjacent data and we'll the same operation on each one and that do it in parallel so I could learn about a whole wide word of data and deal with it as as a set of 32 or 64 pixels and do the same add or multiply or logical operation but if that happens if the data i care about is not sitting continue this in member what if it's all over the place well in that case most normal machines most moment vector machines would say oh well instead of loading 64 elements together I'm going to have to savor my cycle load an object the only pics olympicsolympics they may take me up to 64 times as long to do the same work that's pretty dissatisfying if I've got this error function but my memory is it parallel and so what stagger gather does is it reconstructs memory as a series of smaller sub banks and builds a hood arbitration network between the wide vector access that would be in the heart of the logic of the processor on top and the memory interface where I generate an individual address the address of some individual pixel for example within each cindy way and i present in a single cycle 16 or maybe 32 addresses at a time and those that set of addresses is then distributed out to the individual memory banks the memory banks satisfy those requests as fast as possible and ultimately return the whole set of 16 or 32 or 16 or 60 for results together now of course make it away it could be that allowed of those different simulates a t32 different ways Oh happen to come up with addresses that all into a single bag and that case you're going to have to wait it's going to take the 32 cycles because that's as fast as this thing is in cycle but the statistics are pretty advantageous if it's randomly distributed it turns out it's only going to take you a few cycles on average and moreover you can have multiples of these requests taking place in an overlap fashion and so the total time for a sequence of AB overlap requests can be smaller sale look at that you sort of say okay here is the number of banks which is configurable in this machine and here's the number of cycles that it takes to complete ello of the accesses it improved the number of cycle that it takes to do say 16 or 32 requests that may come and go and we can look at what happens if I make 16 32 bit wide requests and as you might expect the more banks you have the less contention there isn't any one day and the lower the average number of cycles is to complete all the accesses or if I'm making 32 requests at a time you know the statistics are a little bit worse at 16 banks is taking me on the order of about five cycles to complete all those requests that's the random statistics if I take actual vision applications ng detection or cascade pre classification I find actually that you know I'm well bounded within that sort of revenge on average I'm actually doing a little bit better than random statistics so essentially what I've done is now made each of those silly ways not only going to do independent computation it can have as predication so in the individual local control flow decision and that is essentially independently addressing them it's essentially a food processor by itself and you know that some GPUs have essentially the same kind of stay out of gather and they end up calling one of those similar units of GPU so you really get when you're talking about thirty two GPUs to some extent you're really talking about 30 to see many ways in a single engine it's it's that's a work of GPUs so we have now new kinds of highly parallel processors we have the ability to go to very low energy it have this mad for more new kinds of vision what do people do with it well convolutional neural networks have been kind of this hot sad over the line couple of years and I think it's worth asking the question well is it more or is it just something that's going to be forgotten next year and and my conclusion after a great deal of initial skepticism is that in fact this is seriously taking hold that if you find real world deployments of these neural networks in automotive systems in speech recognition you know every picture that gets uploaded to facebook today in the first couple of seconds on arrival it goes through a neural network that identifies as much as possible all of the individuals in those pictures they get labeled by this kind of training network and they're all kinds of applications in security and social media and data in medical research you know there are thousands of projects underway using neural networks around the world so it's worth understanding and I'm particularly interested in the embedded applications the real-time applications the mass deployment applications so let's look inside a little bit that's made it so hot is not that it was just in bed in fact the neural networks even convolutional neural networks have really been around for 20 or 30 years they got sort of lost in the shuffle of disappointment over artificial intelligence but then Along Came Big Data people actually had large data sets and Along Came very high-performance server-based compute often in GPUs and that combination of things a lot of people do we examine this and try much bigger networks train with much more data and suddenly the recognition rates really started to get significant that if you look at the trajectory for example that people were on on a classic public challenge the image that challenge what you found is CNN started to be used and just in the last year have surpassed recognition rights this is or recognition of hundreds of different object classes of tens of thousands or hundreds of thousands of images and so this crossing of the human recognition threshold really now is matically increase in the number of places where people are either to deploy these neural networks because they can play a very significant role not only in implementing functions that were you know some convenient rough recognition will be good but where you have a technical or a moral obligation to do the best possible job when that when the human is surpassed by the machine then the machine is the preferred choice in a safety-critical or a mission critical situation so what's the basic change that took place in these vision algorithms in the old days of vision so you know say nite 2013 people people tended to build vision algorithms where the day to pass through a series of rather different types of algorithms you did things like sift or histogram of gradients and then you did an algorithm like paintings are coding and then you designed some cooling algorithm and then he designed some custom classifier that is some human looking at that data said I think that this is the right kind of feature to identify and I think we're going to organize features in this specific way and they program in program in program trying lots of different options with the advent of neural networks you apply pretty much the same network on every problem that you can use his that same structure of computation whether you're recognizing cats or recognizing data patterns in network traffic or recognizing traffic signs what a difference is how this is trained and where the training is an automatic process and so by having sort of this this me hammer that can be applied to lots of different problems then people have been able to really refine their understanding of how do you start then applying this how does this work so the basic idea here is that when you are training you're going to take a large labeled data set that maybe tens of thousands to millions of images where the objects within it are related you just now here's an image and here is a label or a set of labels label being the name of a person the name of the type of object you select what kind of an app that is you choose what is that basic structure of the number of layers and how fat layers are and then you automatically derive the filter coefficients are this stochastic steepest descent error minimization or error propagation algorithm I mean you keep running the same images pass it until it comes up with a set of rotations which appears to minimize the error between what the is triggered by that filter at set of filters and the associated label for that and then you take those coefficients and you take a copy of that same network and you implement it just that you're just going to go one way through that set of filters and you're going to do it boots for each input once for each image for example and so now you've rotate all those tens of thousands of images an able to show one image at a time unlabeled and what you hope you get out is the most probable date now the many variations on this you may have graving of many different things you may use a first network to say well I'm going to look at the whole image i'm just going to find the areas of interest within so if I'm driving I'm not going to find to recognize the traffic signs in the same the best I'm going to first find the things that look like they might be traffic signs and then in a further minute we'll detailed neural network which will then identify which traffic sign if any is there so there are some variations on the structure but this basic idea of train and then do forward inference is common to all of them where the number of coefficients is significant sometimes the number of coefficients is far larger than the size of any one data info and that is really reflects the the computational challenge associated with these things another little bit more closely none of these layers in that filtering is really made out typically of a convolution that is a multiplicative convolution followed by some nonlinear function that implements essentially a trigger or a saturation or an activation it can be at NH a sigmoid sometimes there are simple simpler trigger functions that are used and there is typically a subsample so as your subsample your image gets smaller and then they threw the you apply of applies essentially to a larger portion of the original input image such that at the last stage you will be looking in a single further across the entire image so that you can recognize global objects within and so you may have multiple layers of this convolution non-linearity subsampling and sort of do that rinse and repeat so how hard is this computation well typically you may be operating on an image pyramid so that you can look at objects of different scale and then we'll each of those scaled images you're going to do a apply a convolution have every possible offset within that that is your kind of surging scanning across this competing this filter at every point and in fact that filter is usually a three-dimensional filter so some region in x and y of that image and then some depth which represents the you the input channels are GD or imma previously all of the results of a previous filter and so you multiply up these different factors and you get some big numbers you know the number of scales the size of the patch the size of the images the size of the filter the depth of the number of features you're trying to detect and the number of layers and even get some very big numbers it's really usable for people to come out least academics come out and say oh yes on a per frame basis this only takes three trillion multiply ads to do so we get some big big numbers so when we've sort of planned this in real time you get some interesting things going this is what you think traffic signs look like this is what traffic signs actually look like here fuzzy they're in shadow there's glitter on them they have stickers on them if this were the us-german size they would have bullet holes in them but they're really kind of hard to pick out and you know most of us could pick out most of those if you had only the amount of time that a driver really has on the highway now you've been doing pretty well if you could recognize 99 percent and that's in fact in this test of the now famous German traffic sign recognition that that's about how well humans do the average human is like 98.8 and the best human gets a little over ninety-nine percent of these he's right so this is hard problems look at and say well how do you do a better job and so we put a bit of focus on figuring out how to optimize the structure of the network how to do a better job of training so that we can explore and expose a wider range of trade-offs between computational load and accuracy so the two axes here are the recognition error how often did you get that traffic sign law what really went to be as low as possible and on the horizontal axis the number of multiplies person and these signs of just small patches so they may be only a 32 by 32 pixels patch by the time they go into this and so here we've got a hundred million 10 million 1 million and one of the best previous results was up here that is it was about I think 60 million multiplies and it got an error rate of about point eight percent and made been grinding away on this problem in coming up with these new techniques and just last week we had driven our error rate down to less than point two percent and and actually slightly lower compute that in the past there are some methods of using you know many different slightly differently train networks to sort of operate in parallel and vote so there is a better score than this there's one down around here but its way out over here it's about 80 times more compute because it's really kind of doing a whole bunch of CNN it's in terrible so the variable course is to be here you know he's no compute has no errors so you really want to make this straight up that most of the neural network community is just thinking about this axis I want to get ignition error rates down that in fact in real-world systems you also care a lot about this because that really determines the energy so where does that leave us I think that we can take some of these things that are happening in neural networks in a focus on vision in this whole idea of distributed and systems liver is different kinds of intelligence at different layers in the system and put them together into a few teams to finish on first of all we're going to live in a world in which so much of our surroundings are going to be always connected always alert well where connectedness has a cost even from a technical sense and so that we're going to be leaving me out ways to do as much computing as we can locally because its energy efficient to do it locally but we want the data experience of all of the world's data being available to us so we want to share globally that notion of being always connected always alert is going to drive us towards ultra low energy architectures particularly for the local nodes for the IOT devices and cloud-based aggregation of data where we're going to have larger and larger data storage and one more mechanisms to correlate and distribute that data and to bring appropriate parts of it back down to the individual device or to nearby points of aggregation we're going to see the rapid growth of intelligence in both sensing and recognition the types of sensors more sensor fusion type applications for the big where I think the long time is going to be vision because it drives so much of the compute and bandwidth and memory requirements and there will be the driver for computing and again with and memory innovation and idea of convolutional neural networks and the things into which ability of all are completely changing the whole world of recognition it's creating a set of of being scientists and people who used to be programmers who are now running training networks that have trainers rather the programmers because in particular all of this distributed computation but very low-energy pieces we're going to see more numbers and what kinds of processors because even in that extra edge relative of Madonna you know there are four layers of processors and their active power differed by about five orders of magnitude in the level of power dissipation you expected some of it a difference in veterans of the running and some of it in the power / members the bottom of the second you have very simple processors which were you know two orders of magnitude smaller and simpler than the processors at the top of the stack get a paper bowl of running arbitrary C code and finally Silicon integration remains critically important that the benefits because of energy because you can make things more local the benefits have never been higher it is challenging to do of intuitiveness absolute complexity the the entry costs are very significant that people will do it and they will do it partly for cost but most especially because of the energy efficiency that comes from high levels of integration and that will become important that energy efficiency is the key going to mobility so that you can afford the to carry around the battery that powers it and it's important to scalability to be able to get up to the thousands or millions or billions of compute notes that you need to operate on some of these grand challenge scale problems so those are my thoughts for today and thank you for your attention so we give you a certificate really quickly so we rest have a small token of our appreciation so that's good it's a liquid and a chocolate token oh yes okay a solid and liquid some questions and just wait for the microphone also so we'll start with you sit easy this is a bit of a prehistoric question so I apologize for that but in nineteen seventy or something mizzium pepper did this book where they basically said the neural network structure with a bunch of inputs and non-linearity and so forth could not compute anything interesting and that killed the the field for four years whatever that might and then there was a spike majestic neural networks in the late 80s and it went down again I guess that was from the invention of the back propagation and stunned by the way then it came back so unit said the GPUs and larger data sets were contributing to the revival of that right so Miss Kim pampered I think said that that kind of scaling doesn't address the main problem but I think what they left elephant was since it's a nonlinear system adding layers could do interesting things and so is what is it that makes neural networks today be able to compute interesting things that they couldn't do before it can't be just the scaling of the training data size and the processor is it the number of layers old I think it is the number of layers I'm not specifically familiar with with that early work by Minsky at all but it's clear that you can compute very different things of the multi-layer Network then you could with an arbitrarily complex single later an hour the fact that you really can do features of features of features in an intuitive sense allows you to build up a complexity of object that could never be recognized singly and you couldn't even look at that before because the CPU they've got a complexity of having the multiple layers not really was hard to figure out how to meand in the back propagation through multiple layers and so intellipedia figured out you know sort of that thing the chain rule of differentiation really didn't work and a jeweler tool to be able to figure out how to allocate the error across different layers in that propagation so no matter you came on the values from two years ago you you wouldn't be able to train these and then you just were left but a problem people know how to solve it didn't have the compute to attack even today people are using hundreds of GPS and still spending weeks in training these things sometimes a bunch of startups out there that are building supercomputers just for training on hours just like they were doing the late 80s yeah but there's more layers and maybe that's the yeah okay thanks a lot yeah that's my interpretation I haven't been in this field for 20 years so I'm sort of you know I'm plea to store it but you know from a different got it Prakash it from below e this is and I'm just wondering because it's coming against you rather than banging variants you he said no question a theory states that you can trade off so much bandwidth or so much computing so much computer we planned this are there any theories being well yeah that's a good question and and I am not aware of any solid theory on that question just because the big hand is not necessarily huge between layers doesn't necessarily mean that there isn't a lot of bandwidth as you saw sometimes the coefficient dance is very high and so you do have to deal with these high high coefficient bandwidth coming in it's just it's unit direction the only green coefficient C and you're not creating any correlation but you wouldn't sort of expect that you could make trade-offs by for example replacing computation that a look ups or doing some pre-computation of absurd functions so you didn't have to redo all of that computation but I haven't looked closely at the question of one of the best ways to make that trade-off I think your generalization generally is often ways to trade one yet the other is all my ass to discussions the trivia replacing with total yes and computational that he said mobile is computing you punish coming back from this right I think the decision that computing should not be done the cloud but rather to the edge I am appears that of course you lengthen them by itself if provided the battery operated but it's not going to happen look at the storage could actually the whole day rascal but also be some way to say this is something that could be somebody analytical musim city terrace in probably a way they can a man register constantly he's saying however they don't know me yeah I would like to see some theoretical background of it yeah it's possible you I think the biggest variable there is going to be what's the rate of change of the days because you know it is a completely static you can distribute it anywhere so long as you can afford to store it locally and it would be very nice because you have the typically low latency and low energy to get to that data the data set is changing rapidly you probably don't want to be continuously obligating your entire data set to every engine over the universe and so depending on the nature of the data you're going to either want to bring the requests all the way to the data or send the data most of the way back to the requests and I think it depends on the particulars of the question but it is in a sense when you refer back to that distance versus versus energy diagram and how where's the way honor my school there's a question here okay you got microphone inside us so I had a fun alisha bumper cars cushion and it's the only how do the Aleutian neural networks are are putting it off with respect of latency like we can see that that errors go down and there is a you have to drive an agency and ever and calibrations but how does it compare with latency is there any trade off between agency and computation that we can make by being in real time um well latency is partly a function of how parallel your computing resources are there's a lot of inherent parallelism within a given layer but you do have to compute the layers sort of in order you do have to do new computation at layer one before you do it later too and that will set some new latency on getting through a CNN in general they are computationally hard enough that on today's available are there they will have higher latency than some of the classical imaging techniques that have been used before just because they have so much more compute and and so ironically any CN NS are certainly not below energy solution and they are probably not in general the low latency solution but they are the solution that can find these about find these these better answers more reliably so what will happen I think is that the CN NS will drive significant changes in our computational architectures where the parallelism of the architecture reflects the kind of parallelism that you have in the convolutional structures and therefore the latency that you experience in doing a CNN will not be really proportional to the number of naxx it will be proportional to some other characteristic once you extract it out the parallelism in the in the hardware that we've done some internal experiments on some very highly parallel CNN engines which are doing many hundreds of multiply accumulates per cycle and in a single core getting up into the better part of a trillion multiply ads per cycle so over a trillion opts for a second in for doing these sorts of computations so yeah it costs you something in terms of hardware it probably cost you something in terms of energy but we're driving down to that that premium in and therefore reducing its impact on energy on latency on cost in order to be able to do these things in that this projection that you know lots of stuff will become CNN base it's true so you know that everything is layered and then the computation is sort of localized yeah which is either you have some answers from the previous layer and your computing something in the local layer it seems to me that if you've solved the memory bandwidth problem you've got a really good engine and you'll never cajon so what is it that employ yourself or chip companies don't just solve a memory bandwidth problem because it's not really compute problem anymore it never is in this case it seems like buildings localized you have some answers from the previous layer yeah you're going to compute something locally and then you're going to push something in the next player yeah it looks like the communication problem inside of the layer is it terribly hard you know sort of have this is probably right the distance problem that you articulated is no longer there because everything is local well you still potentially have to bring the coefficients in associated with that layer right here so it's it's a trade-off between Bastian banner if you made the little memory large enough to hold all my inputs and all of my coefficients then the only thing that changes is with each to image I've got new input right patients are completely constant but you know if your memory gets big enough it becomes a communication problem because you can't put it on ship so Cujo so so yeah there's the vasty bandwidth trade-off but there usually is so could you comment on like you know microns comment I which is really big memory and a really little computer kind of a engine yeah I hadn't studied ok I looked at it recently i mean i think that the ratio of computer memory capacity is highly probable dependent and you know people can propose solutions along the continuum and there would be a separate problems for which is brilliant it's a breakthrough but usually when you get out to those extremes you also find that's a relatively small problem set that you will need a huge amount of compute and no memory or need a huge amount of memory and now compute ok take a quarter out of time pretty much so why is your more questions yeah but i can talk online let's thank you you 