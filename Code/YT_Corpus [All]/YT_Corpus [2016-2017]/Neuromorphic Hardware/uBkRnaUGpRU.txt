 I think maybe one of the questions that I was hoping we could start off with is just whether you know in the discussions over the past 24 hours you know we've heard from like as we said on that last slide the experimentalists the theorists and neuroscience and the and the engineers and I was just wondering if in sitting through the talks if anyone heard any principles from one of the domains that they don't currently work in that they thought would be relevant to their domain you know whether there's any information transfer from all of us having sat in this room or did we just hear the parts that we already knew and then just sort of filtered out all the other stuff so I know that's a hard question to answer but I think it might be a good way to start off the panel and then pick just take the questions well I'm not sure if this is exactly transfer you have in mind but the the wafer scale systems being built in Europe were very surprising and impressive to me and very interesting to me and i justified my flying out from boston for sure just to hear that one talk well i guess i have a question about how approximate computing as you know what what lessons you've learned about the principles that allow approximate computing to give gains in speed and efficiency and whether that where's the connection with the brain right it's clear that the neural components are noisy and it's very plausible that the brain does a lot of if you know approximate computing but what can we learn from engineering to the brain about approximate computing where do you think the connections are well yeah so you know as I mentioned the work I ended up doing was inspired by some of Carver Meads ideas and obviously you know this is a very interesting and engineered or not engineered system that I that I drew from I've learned that software a lot of software does not need the kind of machines that we traditionally built I am hopeful that if this work goes forward and people are writing code the other hardware engineers will realize oh I don't have to build things that run Windows necessarily or always and that there will be a proliferation of new kinds of computing architectures that based on this approximate work and then of course I hope that if we had say a billion core machine that was compact and not too expensive and it could have one in every University that that might enable the kinds of experimental you know computational experimental research that would accelerate the work in neuroscience let me let me make an observation here so all computing is approximate computing it's interesting how people sort of come up with new names for all things periodically and approximately computing is one of these so at one point I'm I worked on radar front ends there are four bits right because that's the SNR you have coming off the antenna and all signal processing systems are highly tuned to the number of bits that you need to represent the data and it's approximate right you're not ripping all computing is approximate right we don't have an infinite bit representation for stuff now the reason why everybody uses single or double precision floating point numbers is because they're lazy right to if you actually you know think about how many bits you need to represent something and of accurately model the error and see if you're getting the wrong answer it requires a lot of work and what what pushes people to go to more approximate representations from the less approximate representation of 32 or 64 bit floating point numbers the desire for speed and efficiency but people have been doing that since as long as there have been computers in fact they did it more in the early days of computing when things were more expensive and I think that what's interesting about neural networks is numerous studies people have done on precision indicate that you need very few bits in fact some people have done studies showing that you can get by with one bit precision for for these networks right and I agree of course you know in the 50s I mean analog computing and really on a lot computing early digital computing I think that it's as again as a programmer the surprise to me is that I can be almost as lazy using pretty low quality arithmetic and get good quality results and that that has been a surprise to me and I think I think relevant so so I was a little confused by the power efficiencies that you were able to get from that work so so clearly using lower precision is part of that but as we heard from bill it's all moving data that ultimately drives drives power so so so I guess I missed the part of your talk where where you describe where you must have described how how you reduced data movement right so he's run a model that was big enough that he needed to pull a lots of data from off-chip right I basically concluded as you know everybody else has that to first order you can't bring stuff in off Jim you can't do it if you want to use the inherent power of the silicon so we put as much memory as we can with each core right and you know again as a programmer i can say i'm willing to suffer I think there are people that are willing to suffer for their their little as you but you couldn't for example run vgg net on that chip there's just not enough memory to do on a single now that's a prototype it was a one-million-dollar chip it's like a seedling right it's like gosh you've got something working for a million dollars so so yeah you'd want significantly bigger chips and if you do that you get you know amounts of memory that are reasonable especially with the kind of work that song Hannah's been showing in your groups have been showing so but you have to give up the one you have to give up the model of gosh there's you know a terabyte at work 10 gigabytes of memory and I can just get to it because you can so fine face it deal with it how hard is the programming and my experience is not nearly as bad as I kind of would have guessed it would be and obviously that depends on the application totally depends on the interrogation yeah so so different question uh the the encoding that you think you've discovered in neurons 444 location I thought was really cool I thought that was really neat um are there reasons why you would argue we should go back and reconsider how we encode numbers in computers yeah that's an interesting question so it turns out that it turns out that this solution of well we're possibly the one thing that we don't know is whether or not the brain is actually exploiting these properties I talked about the properties i talked about are sort of things that you can infer from the observations of the neural response but whether or not the brain actually exploits these properties that exists is an open question first of all you know there is a number representation that's almost exactly the same thing based on the Chinese remainder theorem is the same residue no no system is the Chinese remainder theorem base number system and in fact the interesting thing is after understanding these properties of the neural system I discovered that some hardware devices were built for signal processing applications where instead of a fixed-base binary number system arithmetic they were built around residue number system arithmetic with Chinese remainder theorem because of this need to not have to carry digits over and so because signal processing has lots of additions of large numbers it was a hardware efficiency gain by doing it this way cool so I think in this case the there was an independent discovery by by engineers of the system but I think if you proceed in the same vein I think the brain might actually have insights about how to do things with very different representations than the using conventional devices I'd like to hear your opinions on appropriate application domains for various levels of accuracy in these networks for instance to land something on the moon you might need six significant figures of accuracy but to fly a little drone around with reasonable skill you might only need to I'd like to hear your thoughts so so just one comment about that the fact that the hardware does say you know one percent error preparation does not mean that the final results need to be at that level or worse I try to give some examples where it was quite efficient to you know iterative iterative algorithms I think you mentioned it over find refinement algorithm is where you just quite efficiently can knock the arrow down especially when you have the CPU or what GPU collaborating with this kind of arithmetic so so that's a big you know people expect oh it's got to be worse than the heart the final results got to be worse than the hardware it's not at all an algorithms tend to fall into two categories almost all the neural network algorithms need really low precision because they're aggregating a lot of data and and kind of summarizing it to draw conclusions and that winds up not needing great precision at any of the intermediate steps what needs great precision are things like certain numerical methods where you wind up taking two very large numbers and subtracting them from one another and the answer is important and there if you don't have high precision your answer is garbage and so it really depends on whether you're doing these differential algorithms or you're subtracting two big numbers and you care about the answer or whether you're doing summarization algorithms where you can wind up summarizing a lot of very noisy data and getting a very accurate result but I'd mention it like on that FFT example I think I showed that doesn't that does do subtractions and there are places where you get you know zero is when you shouldn't have gotten 20 and then you divide by that you get some explosion but there were higher level places in the software where you could fix that very efficiently so all the creativity of programming can be brought to bear in this domain and I found that it's often surprisingly helpful you can you can solve things even though it might have initially seemed like you you couldn't so I agree with bills point completely it's just that there are places higher up in the software stack where things can be fixed efficiently enough to be useful oh I guess this question is mainly for bill it's a different threat though i'd be curious as to what you're seeing out there in this regard that in this community is a lot of interest right now in algorithms that can learn on the fly arm that can deal with unstructured data and for example we've got a robotics project where the robot has to work out its own map of this universe and I'm curious what you're seeing and of course you're rightly so looked at algorithms require a lot of training and then affixed essentially what are you seeing in terms of interests out there in in in these unstructured or less structured problems that yeah well he was a huge amount of interest in unsupervised learning just because it's expensive to get labeled data and so if you can have unsupervised or semi-supervised learning it's a huge a huge advantage because you know for example it's easy to acquire you know hundreds of thousands of hours of video of cars driving on the road it's really hard and expensive to pay lots of people in various parts of the world via Mechanical Turk to take all of those videos and label every lane marker and every sign and every car and pedestrian and bicycle and dog and cat that's in them and and so you know things where it's semi-supervised where you can label some data from previous data where you generate synthetic models and therefore you know the ground truth because it was a geometric model from which you generated the training data from from computer graphics or reinforcement learning i think is very very popular technique where if you can simply learn you know based on eventual outcome and then propagate that back to the sequence of events that led to the outcome those are all powerful techniques r you don't need labeled data and say what's that going to do to your architecture so do you think not very much I think we're trying to be general enough with the architecture to not specialized it so that for at least four things we could anticipate possibly happening we will do well on thank you I have a one questioner after seeing this now very nice introduction slider that the Henderson I wondered whether possibly this emphasis on computation power of the brain mrs. possibly the essence of why the brains really good I don't see our brains really as a super Turing machine I think it's extremely good in learning I think primarily in adapting information and also transfer learned knowledge to new scenarios and I think I could possibly think it's a incredible good learning machine and mediocre general-purpose computing machine then I think some of these questions even about the Olaf spikes would appear differently because for example as you know spike is really an important aspect of simply synaptic plasticity right which you wouldn't have the spiking simply certain causal chain of events and certain causal chains are then imprinted through plasticity mechanism right so therefore one has also to reconsider the creation about the role of the spike what could it provide for learning system and then also I think we so actually how much no intellectual new ideas come out of optimizing now just hardware for one particular learning ireson deep learning algorithms right but we all know this is perhaps no I don't know a few percent of the type of learning algorithms that are running in the brain then right so we have to now consider what is the range of possible learning algorithm what makes them powerful and then rethink no but how does the hardware support or this variety of learning algorithms yeah yeah so clearly in in in our traditional computing models the plasticity is entirely in software and and that's not the case in the brain and so there's there's this I think I think that makes it hard for us certainly makes me hard for me to wrap my head around what the right models are and what the right abstractions are we certainly have you know reentrant software and you know software that rewrites itself but it's not as rich is what the brain does clearly the brain is doing why am I saying this to you guys you all know the brain works differently than computers do and and learns differently then then our then our best algorithms learn I don't know for me what's what's fascinating about this meeting in the in the intersection of communities at this meeting is I think that the there are different perspectives different biases different blind spots that between us I think we can help we can help identify and help and I'll potentially make progress on there's no I have a two operators to say this now so I'll say it a third time I think that there are lessons the computer architects can learn from ways that the the brain works in ways that the brain processes information and we're still trying to figure out exactly what they are just to follow up on Logans common a lot of us believe that spike trains out of fundamental data structure in the brain and it allows reasoning about time which as you know in computer science is not possible and because everything is based on a counter there is no notion of time and so that angle appears to us to be actually exceeding important in particular also when it comes to accessing memory we don't know what memory is but we know that it's accessed via spikes essentially because everything around the hippocampus is in terms of spike trains and and so to disregard this might be actually really a huge fundamental problem can I ask a question about that question because I'm very interested so when we say that computers can't i'm not sure i'm quoting you correctly so please correct me if I'm wrong Computers can't reason about time or compute about time I mean of course you can write a layer of software above a digital machine that is keeping track of time and we do that all lots of kinds of systems electronic CAD tools and so on there is no program verification when you have real-time systems is just that Kluge's it's just right sorry it's a kludge there is no notion of program verification when when you have a real-time system is just not possible to be done and and days this notion of time does not appear explicitly in a program it does not explicitly appear as as was pointed out by surya in information theory there's no notion of time now what we ship real time systems when we ship dynamical continues because the reason about times but they're not verified they're not verified and there's no way to actually reason about time the notion of time doesn't appear explicitly there's a kludge and there's a way to deal with it approximately but you know formally one cannot discuss it or has not been discussed in the past and it looks as a lot of neuromorphic engineers have mentioned here that spikes carry the notion of time intrinsically they don't have to think about it so they are very happy about this so so you know this this that this notion of time as you know physics talk about space-time is probably exceedingly important it appears in dynamical system models control theories tend to work with this but not information theorists and not computer scientists yeah but I would disagree that they can't be validated I mean there are strong theories you know though back to the lou and leland scheduling theory that if you loaded a computer system appropriately will meet its real time constraints with nearest deadline scheduling and you can then validate the control systems algorithms using dynamical control theory and validate that a control system implemented on a digital system will in fact be correct it would not pass any serious you know formal test I don't know as you want you would understand in validation or programs those people who flew here on aircraft under blue and air I'm scribal systems are running on yes I understand but every time you take the aircraft and you know the theory you know you wonder whether you're going to arrive at the destination so all right I I think that I think the key point certainly abstracting from what I've heard here today are not today but over the last few days is it is that the way the timing of spike trains seems to be a critical aspect of how the brain encodes information and we don't really have a counterpart to that in the way we think about conventional computing and so so perhaps there's there's something really powerful there that we just don't fully understand you I think in terms of neural networks it's a data representation I was just having this debate a little bit earlier in that you know in a sort of you know deep learning network we represent an activation by a code I won't even say a number of at a code and we use that code to encode the intensity of a stimulus a spike training codes in intensity of a stimulus over time with the number of spikes with a certain frequency and from a I'll disagree with something you said that that spike trains are inherently efficient they're actually inherently horribly inefficient for two reasons one is it's a unary encoding of intensity so it sort of represent something that's a thousand times stronger I need a thousand spikes rather than ten spikes and the other is that it's a toggling representation whereas in seamless every time I change the state from high to low I dissipate power so in fact it's much less efficient to represent something with a spike train where I'm toggling up and down than it is to represent it with a continuous code word so so again experts in the room so correct me if I'm wrong that the key thing about spikes is that there is there a sparse encoding and and sparsity is a good thing sparse is a good thing but you don't need spikes to be sparse there are other ways to be sparse for sure yeah I think the issue with spikes is it's an evolutionary constraint i mean there's a beautiful book called principles of neural design by laughlin and sterling where they ask how much energy does a bit cost to move around in the brain and you know if it's diffusion of transmitters through the synaptic cleft very it's just a few Katie not much but it's slow right if it's if it's propagation of passive action potentials through a dendrite it's a little bit more it's you know it's like thousands of Katie but it's a little bit faster and then the spike is it costs a lot of Katie right to transmit a spike but it's incredibly fast so you know when evolution is computing simultaneous with molecules with passive potentials and with actively generated potentials and I think that's that's why we have spikes I'm not entirely sure that there's an exceedingly good computational reason for having spikes as can be seen by the avoidance of spikes in neuromorphic hardware more generally and you know spike timing is preserved in the sensory periphery but there's less evidence that it's very very well preserved deep in the interior of the brain far from the sensory periphery hippocampus is a bit of an exception with face possession and so on but so two sides I mean I think this conversation I mean to go back to evolve Kang said I think you can't actually I mean what's important to relate to what you just said is learning is critical in this right you know we talked about neuro codes and we talked about spike times and all that the reason you know you I don't think we can abstract away from the fact that the brain is updating its codes in many many areas now we talked about a little bit the grid cells may not be it may be a special case they were closer to a binary code where you don't want to be shifting the representations but deepens hippocampus deep in a number of regions you want to update the codes and the reason you have a sparse spiking code may very well be coupled to the ability to update that coat and change it dynamically within milliseconds or within days and to simply think of okay well spikes are energy efficient for you know they you know unary code versus binary code they're offering something else and I think that's something else is that they're valuable in a way that's different than what we do with conventional computers there's there's a reason why on my slide I said they were important or maybe not it's there's not consensus in this room for sure I don't know why a spike would be more or less if I want to make a synapse for lack of a better term which is basically a variable weight multiplication of an input stimulus to produce an output stimulus perhaps with a non-linear function involved I could make that as plastic or as non-plastic as I want with a continuous representation or with the spike I don't see why the spike representation makes it more more plastic so it may not make it more plastic but I think you you call out spikes for being unary and I would argue unary is more is easier to do learning on than binary simply because if you want you know it's kind of analog in its representation by but in within the time domain and so if you want to make some spike spike time represent something a little bit more than something that already was you just move it a little bit further on time a little bit back in time as opposed to a binary code where you're actually having to figure out which bits you need to flip for a given update it's a subtle and maybe a little subtle thing but I think that that unary coding does add something fundamentally different than when we think about binary codes so I guess um I'd like to follow up on Wolfgang's question and point in his comment and you know one way in which von Neumann computers you know one thing that distinguishes them is that you know instructions and data are stored in the same way right it'sit's instructions and data or all these numbers that are pushed through the system and in in neural systems you know we don't exactly know where memory resides we we think that it's probably local to the computation as well so there's a circuit the wiring of the circuit dictates what computation the circuit performs but also since the units are sort of individually more memory less it also is the recurrent circuitry and the circuit that does memory and I think trying to understand how it is that you can have the same way to the determine computation and at the same time you have a demand in the weights to have memory in the same system and then you add to that the complication that activity in the circuit or pulling up a memory then drives activation which through plasticity rules is going to alter the synapse and the circuit I think that's a that's a question we've just barely begun to grapple with how is it that you can have a system where you're storing memory on the same ways that instruct the computation and each time you do something with it it changes right and I think that's definitely a place where we have to be thinking about and where they're going to be a lot of contrast with a fundament architects I actually had a question maybe a little bit more on the earlier point about the spikes which is when we keep talking about spikes as being very energy efficient maybe the question I want to throw out is do we really mean that for a biological system because the cost the you know the biological voltages if I remember and maybe somebody can correct me or more on the order of a hundred millivolts so you know in that case yes maybe it's an incredibly energy efficient way to transmit information or biological system but we're not really trying to make a biological system we can't so for our system where the cost of charging is much higher in where we try to ensure that you know a spike that gets through gets through with one hundred percent reliability in you know our brains misfire constantly and still somehow we are correct and and come out with a coherent answer but for our systems the system's we're trying to design maybe the cost the energy cost of the spike is just you know a lot higher than it would be in a biological system and the other question or point I want to kind of make is I've really been on the anti spike camp for a long time but I have to admit yesterday I mean I was hearing from from Yaka Yaka most talk you know a lot of the different ways that you could really encode information in a spike so I think it's not just a matter of sending out spikes which represent just intensity or which represent just a rate code I mean that it seems to me like that we could design much more efficient ways of transmitting the same amount of information rather than representing it as a spike but if we could cleverly really understand how the brain encodes this information with all these different kinds of coding schemes you know maybe that's where the key is and we don't necessarily have to mimic that with this with a train of spikes but we could figure out our efficient way do that in the kind of hardware that we can design since we can't design a you know exactly the way the you know a system like the brain is just not possible I think it's really good point i think you know the spiking system the brain has evolved because of a certain set of cost and feasibility constraints that biological systems have and i guarantee you if your goal is to transmit some information from one place to another electronically you would not use that as an energy efficient way if you get an information theorist to come up with a coding scheme they would come up with something very different because it's a different set of costs and constraints yeah I i have read a study I don't have the reference right immediately but I can look it up after the silver and gift anyone wants it that the metabolic cost for moving a spike is want to order some magnitude higher than charging a CMOS line for the equivalent amount of bits moved so I think we need to be cautious about saying that spikes are an incredibly efficient thing an incredible thing about them is that the organization of the brain makes good use of them not that they are more efficient than the equivalent CMOS yeah just a different comment basically until now most of the information representation in computers has been done by sampling way from cinda in a space domain if you like and what we started doing with spikes essentially we started representing information in the time domain so it sort of space-time swap if you like now if you look at power budget they all the time there's pressure to push them down if you take now let's say supply voltage of 1 volt and you would like to represent information with 20 bits precision you have to be able to measure a micro volt which is essentially not possible in commodity environment okay just other question but as we move into nanotechnology it's much easier to measure time then is measured that to measure space and as a result there is a very natural push now from a technology standpoint as we go deeper and deeper in our technology to measure time because it is easier to do then measure space and it it appears that biological systems they are there because you know you look at a synapse you see you know a gap of 50 nanometers and so this is nanotechnology it's as if the to actually the converging meaning nanotechnology and biology systems as you know we know them and so there is a very strong argument historically speaking in the 30s we had analog representations then the digital representations came in it was true sampling and sampling in the space domain and we moved away from the to the tube placed technology to transistor technology and as we move into more and more into nanotechnology it's now more natural actually to measure time than measuring space and that might be actually a key component into making quote-unquote the decision whether you go with one technology of the other although as you said strictly speaking they appear to be pleased conceptually you know the same level I just I mentioned when I started the work I was doing I I was trying to do it using analog representations looking at spikes couldn't find a way to do it but I view this digital thing I'm doing as a sort of interim interim implementation of a programmable approximate computer and if analog techniques if anybody thinks they know how to do it I I believe you know there's a 10x or hundred x improvement that could be done in the better implementation of the ideas I've presented I I would have to hear anybody who thinks they know how to yeah do that yeah I'll say I agree with the observation that it's actually much easier to represent great precision in time than it is and for example voltage or current values actually time is the one thing we're able to get down to a few parts per million on but if you look at different things you have to do if you have to operate on data in electronic systems you typically want to do that the voltage or current I mean it's just easier to do the operations whether it be digital or analog by having everything together in one clock cycle and do a simple computation that's what our technology is evolved to do if you're moving data most people actually use both dimensions so if you look at that most state-of-the-art communication systems they encode information in magnitude and in phase and so they're taking advantage of being being able to use both dimensions of voltage and time with each symbol so if you look at a typical for example cell phone encoding scheme which might be you know 64 Pam it's got both an eye in a queue component so your encoding information in in phase which really means the time domain as well as in the in the voltage domain so I think this is actually one of the key points is kind of how do you actually use the time information and spikes so I agree kind of your size if you're using a unary rate code spikes don't make sense it's very inefficient but my understanding is that the brain doesn't use that may be at the immediate like I you'll have a rate coding but it quickly gets translated into some sort of a sparse distributed representation where you're no longer using that unary rate code and so then that's kind of where the spikes become efficient so when you've changed to this different coding method I think that might be kind of part of why using spikes can be useful so maybe just one comment to follow bunda I think one of the key things is how is the information really represented all the conventional techniques we've used so far are symbolic manipulation and symbolic manipulation only and you have to have your symbols be accurate to be able to do the computation you want to do potentially in the brain it's not a static symbol but it's a spatiotemporal representation of the information and that's really the core benefit of that system it's not it's not just the energy such as a time but it's a difference of how information is represented processed or recalled using that kind of approach rather than just a static symbol if I can add to that I think I mean some of the things that we're discussing would really cancel each other out would it would be really clear that having spikes is actually efficient if you think of spikes those packets and if you think of asynchronous logic and it's sending our packets already it's the most optimal engine from the engineering point of view you know post frequency modulation and sending a synchronous packets is one of the most optimal energy efficient way of transferring data so it is it is somewhat compatible with the solutions that the brain found to transmit signals long distances I had a question for the panel on you know there seems like there's been a lot of discussion on the locality constraints of the brain and for the approximate computing systems for the you know presumably the error-correcting codes although maybe not in this sort of general purpose hopfield nets at some level they have to be local as opposed to global error correcting and I was just wondering if there's a sort of general consensus on a theory for how to decompose an arbitrary problem that for example that the brain appears to be able to solve into locally constrained computations right I mean when we don't have most people don't think the brain has a central controller in the sense of a cpu and so somehow we have to break everything into something that's locally computable without some sort of global reconciliation or obvious global reconciliation I was just wondering if there's in all the separate fields whether there's a theory of how to do this efficiently so that it's more compatible with the kinds of local memory processor units that we might be moving towards you know at large scale whether they be approximate computers or next generation GPUs are you talking about problems related to how the brain works or the general space of computing I'm really just talking about the general space of you know if your computer is organized the way that yours is Joe or you know the CPU you know your specialized CPUs are you know are there sort of general theories for here's how I can take a sort of arbitrary problem statement and put it into that architecture where it's computable right i mean presumably the brain is doing that all the time as i formulate a problem i don't really have the option to go to like central you need to go to my you know to go to my ram and go to my hard disk and cash things in various places the way that a von Oy minh machine would I mean you know I could certainly obviously be wrong but I tend to think of it as similar to the general space of programming where it's like somebody says my mom says well how do you program a computer it's like you know there's a lot of different things you ways depends on the problem there's courses and books and it's just a big space of knowledge and it's a mess and so little bit in terms of locality which i think is what you're asking about it tends to naturally fall out when you draw know a signal flow diagram you know in a you know vision system for example you have input images and then you've got you know convolutional layers and ultimately a fully connected layers and you have downstream logic that acts on the output of the network to make decisions maybe it feeds a mapping a slam process maybe it feeds a control process that's doing path planning and as you draw the signal flow diagram you can draw boxes around things look at the number of bits per second the flow between them and also talk about the total amount of state of both activations and weights that need to be in each box and that drives the localization I mean it many of these algorithms just naturally our local because there's a small number of bits you know of the activations flowing between boxes and lots of bits in the box that never have to move so so yeah so certain problems it's easy just relatively easy to see other problems you know talk to a quantum chromodynamics people and they say I had this five dimensional structure how am I going to fit it into your two-dimensional mesh it's like well you know I don't know it's can take some thinking so I think it depends a lot of the current problems and vision and speech and learning seemed to fit nicely you know I don't know if it's an accident that the cortex is sort of a roughly two dayish thing but in general Computing's arbitrary and and I don't know how you do it in general it's a good area for study you should you should fund a bunch of software projects the learning part though doesn't always fit modular Lee into because if you do end to end training of a deep neural that then you have to propagate signals from end to end and I think that's one direction where we could make some innovations where you can try to come up with local topologies and semi local learning rules where you don't need to propagate information from end to end yet you can still solve a global problem that you know just in general like parallel optimization theory there are some cases of that like you know things like consensus optimization 80mm and things I've got that for example steve boyd is popularizing and working on they have some ideas there but i think we need an explosion of ideas there in order to really leverage the parallelism offered by neuromorphic computing so we also exciting attention to deep neural networks what is the limitation what they cannot do and what are the applications that waiting for a different neural architectures so you said what can they not do and yeah I mean so we they're real and they're really bad for example at physical simulations maybe of people who want to run a climate model it requires you know propagation of state through a set of differential equations over time and this is one of these examples where you are subtracting two big numbers to get the difference which we then add your state to move it to the next time step and you can't do that with a neural network right it's it's not accurate at doing these sort of you know time integral 3d model types of computations that's not what it's designed for good at but it tends to be really good at any recognition task where you have you know a bunch of input sensors with information on it you want to build feature recognizers to make sense out of that so I mean there is a repeated themes that we need better different neural series and i was wondering ok we have deep deep neural networks and they're doing quite fine on the problems here actually mentioned and what other neural architectures we need to actually explore so I think it's a twelve o'clock I don't know if you want to finish if there's a follow-up to that question and then we can end the panel did anyone want to have a final comment on that kind of quick follow on to that yes Lisa I want to ask how pervasive do you think neural inspired processors are going to become in the future so in other words are almost all risc processor is going to have a new orleans by processor as a coprocessor next to them or are these going to be fairly niche specialized applications I mean systems I think that they're going to be really prevalent in all it you all input of real-world data in that anything sensory whether it's sort of speech or still images or video or you know radars or wide ours or any kind of sensor is going to have immediately behind it you know in early inspire tip of some kind but I think deeper parts of the system are still going to rely on conventional computing to do kind of very non neural ask dealing with you know finite states and physical models of systems and things that often have to be represented in higher higher precision and more exact computation so I don't think either is going away but I think dealing with sensory data there's no better way of doing it yeah and I think dealing the sensory data is going to become increasingly important machines are going to be you know dangling off people's ears and in your buttons and and they have and they'll want them to understand speech and see and you know think a little bit and so I think if you look at the number of machines in the world it's going to be you know they're all essentially all going to have perception and related capabilities yeah I'll agree with that right i mean pattern recognition is one of the you know it's a it's a ubiquitous problem it's a problem that we're good at solving as humans are machines haven't been good at solving those but now that we have our detectors that can solve pattern recognition problems if there are cheap and efficient solutions for it they're going to proliferate you know feels as diverse as astrophysics high energy physics for detecting particle events for looking to see whether you know this restaurant matches the restaurant you're looking for from google maps i mean all of these things involve some form of pattern recognition from and i don't see any reason why they wouldn't proliferate ok well let's thank our panel you 