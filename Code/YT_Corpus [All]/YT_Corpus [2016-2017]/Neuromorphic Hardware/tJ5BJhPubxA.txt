 all right we're gonna change the face how many people here how about see you have kids okay roughly about how many plan to have kids these are mine I love them dearly and I promise you this talk will have something to do with your kids okay let me introduce myself first I am Christopher grin and if you look at my Twitter handle here I've been all of these I've been a hacker since age 13 since coming to the US by the beginning of the PC revolution at the end of the Vietnam War my first computer was a TI 99 for a I did my I grew up in Silicon Valley fortunately and when to you know work on BSD at Berkeley that was my undergrad and it might did my PhD at Stanford and then became a professor and I started the computer engineering program at the Hong Kong University of Science the technology almost 25 years ago if anybody here use gmail calendar Docs spreadsheets Google Apps and so as you do you've used the product that I worked on it we will today I run a company called I called a Remo and Remo is a new name that we just took on some people here how many people here know that him add a towel yeah so nobody could pronounce at a towel so we decided to change to you know a Remo I just talked to one of the speakers yeah I know you you're from detail so that's why we changed so yeah so we are backed by andreessen horowitz and light speed and beta which is our new york city connection and also the introduction to the map here so we are with what I tell the team on an intelligence augmentation journey and right today we are at the very beginning of that journey we call that self-service predictive analytics I'll show you what I mean by that late but what really gets to be out of bed every day certainly that and where we're going in terms of pervasive data science and ultimately human machine augmentation all of which is about intelligence augmentation and this is the kind of thing that we do we make it super easy for a business user and a data scientist to collaborate and when I say collaboration I don't mean just you know certainly having the same workspace but how does a data scientist leverage him or herself and build a model once and operationalize it for a thousand business users to use so that's the kind of thing that can get done switched to this is always dangerous laptop so I did earlier so if you know by the way this software so unique on a repo web interface it looks like this and I used to work on Google Docs so this looks a lot like Google Docs suddenly a lot more beautiful and then there's dashboards which everybody loves in data sets and the idea is that we allow people to work on very large-scale data so here's a quick example of later data set that is out in 23 million rows this is flight data under 20 3.5 million rows over 20 year period published by the Department of Transportation and I'm just gonna ask a few questions about it hopefully it'll run well so I can do this just natural language I can type and it'll suggest just like a Google search arrival over the way and day a week so if I click on this what it'll do is the system knows enough that it knows the schema and it knows that day a week is a categorical variable so it'll do it aggregation so the answer to this will be sort of the average or the total delay by day of week so what I whenever I do this demo I make people guess which day of the week is the worst day to fly in the continental US over 20 years from 1980 a 2008 a lot of Thursdays and Fridays and we're gonna how many people think it's Thursday okay so maybe you know five percent of the audience how many people think it's Friday all right let's see the wisdom of the crowd works okay when I click on this of course it also allows me to to dimensionalize it into you know Airport and other thing but I'm just going to click on this please work oh yes it does that just happened in real time right I did not be aggravate this and this is we use spark for in-memory processing but the idea is that we've made it's so simple to use that I as a business user I have not done any you know any kind of sequel the cool thing is the next actually may do this let me filter it so you can see I can do that we're gonna do it by origin flight a weird Airport okay okay I haven't done this yet see so I do that so first of all the people that accept fide a was correct right oh whoa oh geez we're so sorry you have negative delay okay Wow okay well okay it's possible so again I did not type in any sequel and the cool thing is I'm gonna do forecast arrival to write in next 30 days okay and what I do that a model is a run in the coaster and it just essentially did what a data scientist would be called to do building models using R and so on and then operationalize that for the business user so here we have the forecast the gray is the actual and the Green is the it and then forecast into the future I just I think I've sort of stopped the demos there you can go into you know various other functionality but I think you get the idea you can schedule this to have it run every day you can share it out you can have PDF with the idea you can see here is actually I'll just do one more thing because I was told some of the audience members are actually geeks or that weird Airport okay so that okay that also happened just in an instant 2.3 million flights out of a hundred twenty-three coming out of of LGA so the idea here is that you speak your natural language you know if you speak English then you use natural act natural English if you prefer sequel then you sequel there's also support for our Python and and so on so that's what we mean by collaboration is that you bring your skills and what I learned from Google Apps particular spreadsheets is that when you bring people with different skills and different perspectives into the same workspace whether synchronous asynchronous you get decision making much more far more effective than if you just have you know one type of people working in one silo and that's really the state of a lot of tools today but you FBI tools that are exclusively for business users that you've got our Python SAS and so on that are exclusively for for advanced analysts and we want to break those barriers so you have a pretty good idea what we do the question is how do we do it so it turns out we are very very advanced geeks so when we when when the my team handles this you know blending for this talk and I asked them what do I talk about wow I'm really ran out of time and they say well you know deep learning and AI and I said which deep learning or AI and then they said well what about the audience or the audience is very strongly technical but they're also very sophisticated business users so I said what do I do so I'm gonna talk about both okay and I'm gonna fly through this very fast because I've consumed a lot of time so so you get the benefit of you know see how we build deep learning at a Remo for our customers and I'm gonna go through let me go back here if you if I move through faster here just Google spark deep learning reference architecture I sit on the spark Technology Council advisory board and we have published this into that github directory and so the idea I want to communicate here is you look at deep learning it has a lot of very powerful use cases IOT customer segmentation fraud detection and it can solve some of these problems in new and unique ways and powerful ways that our previous algorithms could not and one of the decisions you have to make when you do this is that what platform that you use you know is a GPU supercomputer or distributed processing and the right answer to every question is it depends right and so it depends because it's not just a fastest configuration that you want some of our customers already have a spark production cluster so to them they just want deep learning as an additional workload so for them it makes a lot of sense just to do pure spark deep learning right it doesn't make sense for them to build a whole bunch of Nvidia boxes people who want you know pure compute speed and very fast training and can afford to do that so they would build the third case that tensorflow supercomputer I think I'm gonna because of the in the interest of time and I would do want to get to the AI part I want to share with you how you look at distributing deep learning how many people here are familiar with the gradient descent right just an optimization technique it turns out most machine learning is is come down to some kind of optimization technique like gradient descent and when you distribute deep learning it's no different you've got a whole bunch you've got a brain you've got a a a deep network it has a bunch of parameters it starts out being let's call it random and then you feed it some training data and well then Jen predict something and then you look at that predictions they know that's wrong and then the error gets propagated back and you you essentially propagate the gradient to change the parameters again so there's always going to be two steps if you remember nothing else about that there's going to be step one you will compute the gradient and in step two you apply those gradients to your existing parameters that's the descents them and that gets repeated you know millions of times tens of thousands of times and when you distribute it you have a choice of who does what right who bears the computing of the gradients and who does the computation of the descent and then also in the vertical direction do you do data parallel that is do you split the data into multiple shards and then have one node handle one patch of the data and or do you also split the model because in deep learning the model can be very larger than if it on one machine so this is a generalized architecture and you can mix and match a number of these so these are the four architectural options that we have built one is SPARC only the other is a Luxio storage that is we use Alexio a tachyon as the parameter server the server of the model the third one is essentially the the HTTP model of parameter server and the final one is we actually have on the parameter server we also compute the gradient so it I'm sorry it will also update the the model so it turns out the fourth one is the most efficient one because it doesn't have to communicate the model back and forth anymore okay so with that I'm gonna show you some quick results from that and what do we learn one of the things we learn is that that fourth configuration gives you the best speed-up okay as expected because it minimizes the communication the model going back and forth between the gradient computer and and the model update we also compare GPU versus CPU and the the main thing you look at here is that pee-yew speeds up by a factor of eight or ten when you have a single machine configuration but it turns out when you distribute GPU that gain decreases the green versus the yellow and the reason for that is that even though the computation speeds up very much you still have to communicate models around so the one thing that you remember is that and you know chart after chart you will see that there is a communication bottleneck as you have to ship the model back and forth right this chart shows in particular that no matter how many cores you have even if you have an infinite number of course there's going to be some amount of time needed to communication to communicate that model what it means is that so once I get here how many people have heard of the Ignite talks so I just recently given a night talk on this once I get through this it's gonna advance every 15 seconds no matter what this is what I worry about right and I'm sort of responsible responsible party for this this is gonna you know people are talking about advanced super intelligence and artificial superintelligence what's gonna happen right so as I say part of what I do at a Remo is advancing the state-of-the-art in this sort of stuff and so when I think about the future how many years from now we can compute that you know artificial superintelligence will arrive I think many of us have seen this from Norway but why right so the audience I want to talk to you about this is the upper quadrant they're the people that think it'll happen it's just a matter of time either a pessimistic or optimistic and I hope that by the time I'm done with the next few minutes you'll become optimistic if you look at the Google brain it's at the level of about a cockroach okay and it's not about million neurons and if you compute the current energy required per neuron and extrapolate that it will take about 50 years to get equivalent human brain there's another metric you look at in terms of traverse edges per second and when you look at that and you say well when would it take for it to become human in terms of cost that's in about 16 years so about twenty or forty years you know essentially we're gonna die right so what does it matter well I think at this point I'm just going to change the topics I'm going to talk about yes something else I want to tell you about Erik Weihenmayer here he's actually blind since a thirteen and yet here you see him having a device and he's playing tic-tac-toe with his girl and he's climbing in fact he's the first blind person to climb Mount Everest and he does that with this device that translates video signals into electrical signal that can fuel on his tongue right and after six or eight weeks of training flying people can learn to protocol see through their tongue and get the brain learned to detect those signals here's another example of a damaged a wrap with a damaged hippocampus which is the area the region responsible for memory and with the chip connecting that actually restoring and even enhancing the memory they can turn it off do you turn it off the rapp forgets here's an example of distributed processing perhaps and monkeys any single one cannot see all the information needed but when they can buy them then they can process information better than any single being so my idea is what if we could augment ourselves with this AI stuff we're building that's that's probably the way out that's probably the way we get away with it and if you think about it may look scary but how is that different from my taking we're putting on a pair of glasses that I take it off you know I thought meant it myself I put them on I'm a little more capable you know we have prosthetics for people who can actually run so we are the ASI know we could be right so if we implement this and we implement brain-machine interfaces within the next 50 years we can become that hyper evolved being but if you think about human evolution for millions of years is always about increasing intelligence so there's an opportunity here for us to essentially use the machines that we build to become hyper intelligent so this is my call to action let's do this right we need advances in Hardware in wetware in software right neuroscience combined with computer science and we need to move from a von Neumann model to a neuromorphic computing paradigm so we stand I see at the fourth of a row to the left we basically die as a human race if we take the right course it does challenge what we think of to be human but we'll survive because I want my children and your children and their children to continue to inherit the earth thank you thank you wow you covered a lot of ground so just one question from me and then I'll open up to people so you know the question of sort of deep learning a machine learning in the enterprise is it always interesting because it's an aspect of what the technology can do and then whether people are able to do anything with it so if you think of the arbitrage between technology and I guess almost a social engineering that's required for people to ask the right questions that machine learning and deep learning can solving the enterprise what's your what's your take on this what's the when you go and sell to a company do you feel that people know how to harness the power of what you're offering I I guess I understand that question as the way we approach business users at a Remo is that we actually respect them a lot and some people build software that say you know let's dump them down in fact this is users are very sophisticated people they just pick a different language so earlier we had a discussion about the challenge of the business user having a lot of business rules in their head and yet they can't translate them to sequel so one of the things we do like you can imagine we do a lot of things under the hood in terms of technology we actually split the ETL process both cognitively as well as technologically into two parts one we call a ETL and the other we call me ETL ETL is enterprise 80 ATL and BTL business ETL essentially we provide tools for people to the EEG is essentially responsible for data structures now converting a timestamp into Monday's you know Tuesdays and so on where the basic user should expect that the data set is already come already well structured but they will then easily from that imposed business rules on it and by separating these variables you can essentially have that collaboration between data engineering and and and business quite quite smoothly so you saw some examples of the ability to do that because when I do that demo if you think a little bit you say well how did the data get to be that way that business user Kent can think about that so there was an ETL process that took it up to that point very interesting questions do you have capability to deal with unstructured data as well as the structured plan or it's only when a data is very clean and very structured yeah so yeah that's a great that comes exactly you know from the last point I made is that the when we think of it's unstructured data we think of a sort of hierarchical or be textual data and so there is a data engineering process to take dirty data and make it quote-unquote clean but not necessarily with business rules built in yet that's one set the other the kind of data that we really like is is time series events so tweets would be time series the bubble vocation and time series and we're able to using deep learning techniques be able to take these sequence ounces of event whether they are lined up in time or not be able to recognize these patterns so you use for example RN and recurrent networks as opposed to CNN and that's another way we look at you know essentially handling unstructured data and converting them into text images whether you have text images or structured numbers they all end up being vectors that get set into the deep networks that we work on so yes I understand all you associate brain and data but how do you associate brain and emotion something associated ok let me plug my own blog if you if you google for algorithms of the mind it's a it's an entry if a article that I wrote the thesis saying that not it's not so much it's certainly that neuroscience teaches us a lot about how to build computers or build machine or deep networks as we build these things at least from my perspective we're learning a lot about how our brains may work so to answer your question if you think about these deep layers there are multiple layers and at the very top we give them a label you know dark cat chair and so on and so forth so if you think about language in our minds there are a lot more concepts that we hold then there are words for so words are essentially the final layer the label that we acquired two things so the layers below that is what I think of as intuition right I actually think it is that concretely meaningful so emotion feelings and so on are all representations in my deep network and they essentially they resonate when I listen to a particularly beautiful passage of music there are certain neurons that resonate and that's why I feel good at it you know generates a whole bunch of the endorphins but I think at the core its electrochemical one last question for me and something fairly different if you were not doing this what other sort of AI tech company would you be doing I guess in other words who do you think is doing super interesting work in AI these days what what startups whether they're friends of yours or people that you respect just curious okay well there's a to vastly different questions I love doing this I can't imagine doing anything else i Remo will outlast my lifetime I've built two companies before and I sold them successfully that's that's not my game anymore in terms of interesting companies I I can name many in terms of technology and and ambitions and so on I will say though I think in terms of looking at it from an investment perspective right every cycle even though you know there's gold it's really big at the end right there's always gonna be a billion dollars lost before you get past that and I think that today we're still at that stage right I think the great companies of the leveraging a eyes and so on have not been born yet and so our strategy at a Remo is not to go and create the new AI today it is to quickly apply right we know the state-of-the-art in LSD m's and so on and then find business problems and apply them so we can do fraud detection quite amazingly well I didn't show some of the examples around how we do visualization TTT visualization and then the fraudulent cluster just light up by themselves and so on and I think that's where the sort of some of the better companies are applying these techniques but I think in terms of fundamentally you know this next wave of AI these companies have not been created yet and that's the strong AI companies or vertically I companies or companies yeah I I first I have moved past the the labels of strong AI versus weak AI I think that was a controversy of as recently as say two three years ago today it is already permeated the consciousness of people just think of AI in general and sort of it's more of it inexorable increase in abilities and so on right so for example you see self-driving car is is already a fact so but I think in terms of companies that that sort of born to do AI and you know do just that and create the next big thing yeah there's always something that but you and I sit here cannot tell what it is yet and I think that that generation has yet to come maybe within the next five to ten years that's great combat open thank you so much appreciate it thank you 