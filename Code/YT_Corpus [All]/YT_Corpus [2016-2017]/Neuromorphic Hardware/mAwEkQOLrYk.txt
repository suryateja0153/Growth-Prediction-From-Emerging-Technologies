 first of all I want to apologize that I'm not there in person but I'm getting a fairly important talk to the darker management and we made the decision would be better for me to be here to help prepare the presentation also I've pulled together a selection of slides they're fairly busy they have a lot of detail to them but they have been approved for public distribution and so I can send you copies if you want to send me an email and also i think the conference will be putting them up on a webpage so it's hard for us to get slides approved and so I'm choosing and how to slide from previously approved presentations what I'd like to do first though before talking about the cortical processor is to discuss the upside program now I'm on slide to the reason for this is that the upside program although not specifically a neuromorphic program ends up developing a lot of neuromorphic technology and I thought it would be of interest to this audience to discuss some of that briefly one of the things that we had hoped to do all of us who helped put this workshop together and run it over the last few years is to infuse into a much more industrial application orientation so that we're looking at practical utilization of the neural inspired computing and so outside really does that and so I thought what I would do many of you may not be familiar with the program is go over it briefly and then the latter half of my 20 minutes I'm going to discuss the cortical processor and that status and where that program is going so I'm on slide three one of the big problems in the DoD and this is true in commercial settings as well is significant limitations and the power available for doing computations if you're on an embedded platform you're very constrained and the kinds of computations are able to do and number of DoD platforms that are very imaging-based and are highly power constrained the purpose of upside then was to take existing applications and redevelop them using more advanced computational models many of the models that we end up using a neural in nature and demonstrate significant improvements in power efficiency so in discharge the vertical axis is really talking in terms of using a notion of capturing features on an image as an ocean of computation and actually shows that we have a fairly significant efficiency ceiling this is a best we can hope to do with the digital paradigms that we have even if we're optimizing architectures to various degrees the goal the upside program then was several orders of magnitude improvement and in the mixed signal portion of the program what we are hoping to achieve is a thousand x improvement in power efficiency 100x improvement in performance and then we also had an emerging devices component which applied in other two orders of magnitude improvement in speed and power efficiency p the way we're doing this I'm on slide for now is to exploit the physics of the devices in the example here the example shows coupled oscillators which perform which create an Associated memory which is then can be used to do feature extraction and it does best friends at sochi memories can be shown to do a beijing like a most likely feature extraction so this particular example doesn't show a neuromorphic model but some of our some of the performance on a team are using norfolk models to accomplish the same functionality the second insight we had was that we would be able to find a small number of computational abstractions and those could be used throughout the pipeline so we didn't get stuck in the situation where you're building an analog chip a different analog chip for each stage of the pipeline what we wanted was some universality of and still using primarily analog components put the pipe together what's up mentions on the slide is the fact that we also had a accuracy requirement so the performers on the program would reenact sisting application and in spite of using extensive analog computing they had to demonstrate they lost no accuracy in their redeployed application and in fact in many of the cases they actually will be gaining accuracy I'm going to next slide slide 5 I don't have time to go through all the various performers we have four main teams but I thought I would at least show quickly one of the performers some of you may be familiar with the Argus system it is a 1.8 giga pixel sensor flies in a pod it does what's called white area motion imaging or whammy and it basically the data are processed through a number of stages so there's non-uniform compensation compensation there's debating there's feature extraction target tracking there's a variety of probabilistic techniques of used to do that and ultimately classification of the targets the goal would be to track a 10,000 or more targets what we find today as August is very platform limited can only track roughly about a hundred targets or moving element the current pipeline is support has a roughly three and a half kilowatt power requirement and with the upside technology the projection is that we would be they will need to roughly hundred times faster and thousands X power efficiency improvement allowing them to process 40 gig of pixels per second and this is through the entire pipeline and then also designing it at five watts so this is an example of what's possible with neuromorphic light computation using analog computing another part of this program is also mr. work as a group to California at Santa Barbara and Sunni in Stoneybrook that are developing my mr. technology some of you in baby fillet with Dmitri stripped off and they're getting some very interesting results using remember mr. to do neuromorphic like computation the next slide I'm run slide 6 another one of our performers at the University of Michigan and they're working with gar Canyon who I think may be in the audience their guards developing algorithm very similar to the one that Bruno developed a locally competitive algorithm and using that to do various kinds of imaging next slide slide 7 the group is at Michigan agenda Jang and michael Flynn developed today digital spiking neural net chip for implementing LCA and are moving to a hybrid chip which does spiking but does fundamentally analog computation and they're looking also at achieving them that sounds and acts and 100x requirements of the upside program next slide slide 8 also at Michigan way Lou has been developing my mr. technologies for a number of years and have some very nice results in using them to do no morphic computation next slide slide 9 another one of our main performers is hrl and they have a complex pipeline very similar to the bae systems and they're building a number of custom chips again these are not necessarily neural but have do have a very compatible with neural like ideas and again leveraging primarily analog computing what I put up here is just two of the devices so we do have in addition to the memristors we have some devices that are being developed by the hrl program also for doing various kinds of bath mats associate and pattern recognition and these devices actually operate as oscillators and coupled oscillators creating these neuromorphic kinds of computation next slide and slide 10 the last performer on our team at the University of Tennessee at Knoxville and they drink Jeremy Holman is a primary chip design they're working with the p.i itamar RL and also working with Sylvio supper at Stanford on building an imaging pipeline and one of the segments of the pipeline uses this deep learning chip architecture the weights are it's entirely analog weights are implemented stored in floating gate technology and they're getting extremely good power efficiency as are the other performers in the outside program so I'd like to shift ears now come on slide 11 the reason I went to the upside as I mentioned was to give you a sense that there's analog technology some of it directly comes out of neuromorphic computing some of it is using neuromorphic like elements and achieving some significant objectives and upside is programs sell at four and a half year program it's about two and a half years in so there's roughly a year to go here and a half to go and we'll start seeing complete test beds using chips are being built now now we're starting to put them into test beds and we'll be able to demonstrate functionality for all the football for the pipeline's in early 2017 the next question is where do I go next with this if you look at a sense of data processing pipeline this is sort of a cartoon real pipelines are tend to be far more complex than this some of you see I've shown this graph many times before but the front end we kind of we know the algorithms we're able to be able to highly optimized silicon to do some of these algorithms as we as we move back into the system we start trying to make much more complex decisions we're extracting ever more complex data from the sensitive is flowing through the pipeline and then we start running and other problems we don't have we don't have good algorithms for the kind of things we're doing it to back again but also we still have significant speed problems the Argus system if is tracking a thousand objects or ten thousand objects then what the odd who there is a human is can't really intervene in that data stream and try to figure out which objects are the most important to look at more closely and so their scenes needs to be another couple of stages on these sensor data processing pipelines that bring this knowledge base component to the pipeline and that then is the goal of the cortical processor program in addition to Center data processing if you go to the next slide slide 12 there's in my mind there's a lot of problems in the uod systems that could also leverage the ability to train systems on their workloads rather than actually writing error-prone and difficult to debug it difficult to append and change microcode so in addition to looking at machine learning to help analyze data also looking at machine learning to eventually build much more complex systems to this end I'm on slide 13 DARPA has put together fairly complex and comprehensive cortical processes study I mentioned it briefly at the nice conference last year the process the studies are now in process they're roughly about halfway through and some of them were a little more than halfway and then the writers have started later so where we have some that are only maybe three months into their efforts the purpose of this study was to investigate the algorithm space and to understand if there's opportunity for DARPA to invest in new kinds of algorithms that maybe allow us to move beyond having huge training data sets and going the smaller training data sets being able to train a real time and have a system adapt in real time rather than using offline training being able to use unsupervised make much more extensive use of unsupervised training or what some people call weekly supervised what you maybe have occasional labels but not fully labeled training data being able to handle temple data more efficiently and then finally learning to perform inference over to capture an input form inference over complex structure in data so we have a number of studies that are strictly algorithm there's no hardware analysis in this part of in this study and trying to understand some of these the goal is to determine whether there are biologically inspired features it could be added to the existing machine learning to create hybrid algorithms that might be it will address some of these some of these issues the program is focused is although I mentioned earlier the ability to control and adaptive system the program is primarily focused on imaging partly because that's a very important space for the DoD and partly because these are really hard problems so I can't give you any the results today but me and I wanted to let you know that the program the court of posture studies are ongoing and running whether there will be a workshop or not at the end to present results I don't know yet one of the things that I also want to mention in the presentation is that my time at DARPA ends and I'm actually leave DARPA next week and moved back to Portland State University we have hired an individual we have recruited an individual who is going to a security check who will then hopefully come in and pick up the cortical processor and see it through to a complete program and the next couple slides 14-15 I'm not going to go into these at all the purpose of these slides were to present to darpa examples of computational techniques that come from biology and to give a sense of how they could be used in real systems the cortical processor studies that we have many of them are using some of these very few we're using all of these techniques but they together I take or promising and provide some insight into where we might go with machine learning algorithms so that's slide 13 or excuse me 14 and 15 slide 16 and adjusts going back to the notion of structure and data I believe that probably the most important thing that these algorithms and do for us is to provide a wave or it can capture complex structure exists in the data and then leverage do inference over that structure so for example in the photo on the upper right side is up is an architecture it's drawn some tracks and the notion there is that besides understanding the tracks you you start to see scenarios you see tracks that coordinate with each other tracks that have are involved in largest scenarios and the ability to recognize those as beyond our capability today but it's a kind of things that we'd like to see the cortical process and be able to do next slide slide 17 and to that end one of the critical process of studies were doing is working with Fei Fei Lee to capture complex structure and then her case to actually do storytelling over the structure that she's captured in images next slide slide 18 although the studies don't actually involve any hardware we do feel that the cortical processor program will have a significant hardware component just as GPUs have gone a long ways to enabling a deep learning space and the algorithms in that space so we believe that specialized processors that are mazda these algorithms can also have a significant impact on the liability and the actual adoption of cortical like algorithms detail rhythms are very different from the algorithms we do today and my other things are sparse there's partially connected there sparsely activated that you sparse distributed data representations that changes the connection significantly they do low precision typically one bit precision maybe a few bits precision their compute low low precision sometimes there's biking models sometimes not I think the jury is out on whether spiking models themselves provide more capabilities to the algorithm but I do believe that spiking models do provide some significant optimization at the hardware level in terms of being only need needing willing to activate a connection when I spike arise and only having to commute compute updates to a neuron state when spikes arrive and could be could provide significant power saving we've seen some of that in the true north chip that IBM has developed and is now getting applications ported to that is my last slide as I went again i apologize for a very dense as the fast set of slides and also that i can't be there with you i would very much like to participate it more directly in the conference so with that I think I'll take any questions I be hard to do questions if away but I'm more than willing to give it a shot 