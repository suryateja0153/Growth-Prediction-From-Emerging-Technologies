 hello everyone my name is Peter and my third-year project aims to create two very different systems one that recognizes visual symbols and another to control the movements of two humanoid robots while these don't seem particularly connected at first glance that driving desire behind both of them is to use biologically inspired technologies to implement them as a result both of them have been created using a neural simulator which mimics the behavior of mammalian brains by simulating their activity of groups of interconnected spiking neurons while also running on hardware which matches the brains organization the system that I have designed and implemented using spiking neurons controls the arm movements for a pair of robots part of an art exhibition at the Manchester art gallery the idea of the art piece originated from a Swedish artist who collaborated with the team of Swedish mechatronics students in order to build the robots at kth Stockholm they sit in their chairs discussing the nature of consciousness and identity by following a predefined script but may be interrupted by onlookers and will react by telling them to be quiet or go away scope of the project was to imbue the robots with human-like gestures by raising arms in the air when silencing the audience and just during naturally while talking in order to achieve the desired behavior I have engineered the network consisting of interconnected groups of neurons on screen is the full network controlling the robots at the gallery presented at an interactive user interface at the bottom of the screen lay the simulation controls with speed of simulation compared to real-time the current time of the simulation a history controller which affects how much previous information is presented on the output graph and the start and stop button on the left are the inputs to the simulation there only uses to be able to interact with it they do not have a biological counterpart outputs are on the right while in the middle we have several groups of neurons into acting with each other this area can be treated as a black box which executes one of three predefined actions with either one or both of the arms when the simulation starts the default action is action 1 which means that the robot is gesturing for this action the basal ganglia and thalamus models cooperate to inhibit the neural populations representing the arm positions needed to silence the audience this means that the trajectory computing system will take the sound signal in this case an artificial sinusoid and a base positioned and compute the path to be taken by the robotic arms so as to accomplish a gesture movement while they speak furthermore the system selects a random arm to use for gesturing whenever the robots move on to a different sentence this is accomplished by using a direction selection subsystem relying on computing the dot product between the input vector and two predefined orthonormal vectors representing the left and right directions the result of applying the two operations is that we now know the angle between the given and both of the predefined vectors thus we can make a decision upon which arm to use finally in an analogous fashion to gesturing the system can also allow the robots to silence the audience or simply idle while not talking silencing is done by raising one or both of the arms in a predefined position with the hand close to the mouth and the idling is done by keeping the arms still at the base position as a starting point for the vision system I have created a simulation which uses the dynamic vision system equipped on a push bot in order to follow flashing light this is not a standard camera which operates at a fixed refresh rate it operates similarly to a biological read level changes in local brightness represented as spike trains this behavior is achieved by implementing a winner-takes-all network based on mutual inhibition in other words each region of the receptive field excites or pulls into one of nine corresponding groups of neurons and each of these inhibit all the others except themselves each group of neurons also called a population or ensemble of neurons have a recurring connection so as to offer them a form of memory this network will have the effect of selecting the population which encodes the largest value while all the others are inhibited thus driving the motors in the direction of the region with the highest brightness this approach ignores any shape information relying exclusively on the brightness of the flashing LED it would be a lot more interesting if the pushpot would react the symbols or shapes rather than purely on light intensity to this end I have trained the population to react preferentially to a bar of a particular orientation represented by a Gabor filter the outlet of the network is a value representing the confidence that the input filter is at the preferred orientation placing four of these groups of neurons trained to react preferentially to a different orientation bar is the same Network and selecting the one with the highest output we have created a system that can provide edge detection for a relatively small area of the receptive field however if we consider that we want to create a simulation that is biologically possible we need to be able to scale it up and also use possible structures from within the visual cortex thus the basal ganglia used previously for selecting the most likely orientation is removed and replaced with a winner-takes-all Network the model used as a hierarchical feed-forward network represented here only in its first layers but sufficiently deep to be able to extract some basic behavior the behavior we have now imbued into this network allows a push bot to respond to simple symbols to advance or retreat based on the dominant orientation in the scene this was an immensely interesting and demanding project with deliverables which accomplished robotic motion control and some basic symbol recognition using the neural engineering frameworks precepts the nangou simulator the biologically inspired silicon sensor retina and the spinnaker neuromorphic computation platform thank you for watching this very high-level Whistlestop tour of my third-year project 