 now chrome tsukamoto apologetic burg on Urantia Chicago Sumida st. Jean Genet serious Imboden yo Tata give it to do until Monday or Congress in jang hee-jae results in a new or negate Honda Hiroshima on Turkish media the John Piper also Gary estimate Papillon in a product to be internal computing lab Putin MK so come go gay except garages all contain to save me rope tow hitch receiver class Echo Canyon el pecho special Sumida tena koe de demain han go-eun booted on human gun hisses an en caso tsukushi is Damita satu boner when Semiramis Kissimmee de Mayo recognizes that a girl Hey Roman Empire creatinine Kadamba tree gas emitters on median is has a name - name pyramid on paper uses predictive Ericsson Yendo ball instead of technology so Tonga Tonga Maha toggle University of Massachusetts at Amherst reso Chandigarh Sox Agra Crego Purdue University Chandigarh Parcells education de Haan kabhi khushi build American ideology IBM a PJ was Nyong'o Center Chalmers memo Eatonton Umberto Intel's a fellow is in terminal computing lab so general Matheson give a box an IBM Power PC our Intel köszönöm computer on set on MIT architecture pionen processes on internal Union bribery charges Nidal each s millionaire Intel achievement a Nevada smell each an e-learning multi military architecture Gaborik jung-geun ro cook church under language engine working I Triple E hello rose and jungles Ojeda Yunel TV box and cathode are passionate regional the quest for the ultimate learning motion media check on hardware Elena England's next incident deep learning creo conductor Oh Sara Moulton scenario sis rocks Colonel computing whatever their yeah some journalism she would talk on you know hogwash Shabana she doing the option to be good morning you I the human race we have very ambitious people and this talk is about one such ambition and the ambition is to build or create something that's better than its creator you can imagine it's like someone some would argue whether it's even possible but since no one has proven that to be impossible our quest goes on and more importantly I hope you'll realize through this talk that even if we fail in this ambitious mission we would still have actually solved some of the very hard problems of our society and if the stock is primarily about taking computing to actually solving social problems a whole class of problems that today it does not address so with that let me begin and we in our lab in our research have been doing parallel computing for a long time this is what I did at IBM also and I continue that work at Intel pal computing which is only needed when you have a lot of compute to deal with or lot of compute to deliver as traditionally is how we have been solving hard problems but before we go there let's look at how human beings interface to a computer right if you have a problem in our mind and we want computers to solve it like previous speaker was saying we typically program things we explain that problem somehow to the computer computer figures it out returns the answers back right at the end user level we can only figure out if the computer has really gotten that problem right or wrong by looking at it's one of two kinds of outputs one is called mining which means can the computer go back in the archives and find similar things so I use an example if I show a flower to a computer right I don't exactly know how computer is going to model that flower or like that Apple in that example in the trashcan example or tomato the computer but doesn't matter if the computer can go and find similar objects then it must have the right model of that thing that I had the problem or object or event I had in my head similarly if the computer really understands that event or the object it should be able to create similar things it should be able to synthesize or simulate in the future so its ability to go back in time and find similar things like when we do when we typically Google things or its ability to go in the future and predict something or simulate or synthesize such a flower if it doesn't exist that's how we know that the computer has a good understanding of it right so end-user can only judge the goodness of a computer's understanding of a problem by one of those two ways mining or synthesis so if computer lien stands what is a certain object or an event object like a tomato event like a trading opportunity its goodness of that understanding can be judged if it can find similar instances or creates similar instances and answer what if questions futuristic questions if it can predict whether it's going to rain or not it must understand what it is to model a rain or more or have weather prediction right so these three core components of an application namely how do you really recognize something and how do you then find similar instances or or create or simulate future instances what is it made up of in schools when we teach them there are all kinds of deeper fields that are underneath it behind it the computer science algorithms like the last speaker is saying now the the the tragedy however is that these things are not taught in a loop or not taught together in even in computer science classes the database guy doesn't talk to graphics guy they don't talk to the speech guys they are all different disciplines even in schools right so what is really needed here is now to be able to close this loop so that we can actually be solving these problems in a and that's how we can solve newer and newer problems let me now go back to where the crux of the problem is it's in this ability to model things the quote-unquote the recognition part of it computers ability today is very very limited so most of the cycles are spent in mining and synthesis variable cycles are spent in actually understanding or modeling complex objects okay now why is that the case because so far we have been limited to what you see on your right the inside-out way of doing things meaning we are limited to only be able to process in computer things for which we have a nice formulation it could be Schrodinger equation Newton's equation neighbor Stokes things which have nice formulations meaning some smart guys some scientists gave us the right formulation we throw a lot of compute flops to solve them to solve those equations but there's an all 99.9% of the problems in the world are not lucky enough to have a formulation like the one you see there they have not lucky enough to have a guy like Newton or Einstein so those are the problems on the left side the outside-in problems where you have to come from outside and just by looking at the output you somehow try to guess what could be going on what could be a good way to represent or model that right that's where we are now mostly focused because this is how we can handle 99% of the problems that so far are out of reach for compute now why have we not done this so far very simple the theory has been known for a long time but we did not have enough data or enough compute to have a reasonable confidence in the reverse-engineered representation or model of the problem okay which is what is changing now how because now we have lot of data lot of data and lot of computer and you can clearly see through this example for example video has grown skyrocketed clearly the eyeballs in the world have not grown world population has not grown like this so the only way this amount of data can ever be analyzed is by computers is by machines not by humans now therefore what is happening is edge devices as the digitization takes place edge devices are sensing and that's where humans come to interface the computing and at the back end large amount of data is being collected that's the quote unquote big data servers and the oh and the model building or understanding can only be done at the back end because that's the only place you have enough data okay so the only and it's very very powerful it's powerful in the sense that you can imagine that it's a it's the effect of internet in the past it was very hard to collect collect enough data you can imagine if you're a computer right - understands my accent IBM had such products via voice a guy like me my accent will take a lifetime learning for it right but now million people can speak to it with my accent and they all go to the server so it's like tossing a coin million times versus tossing million coins at once so for processes where that stationarity holds you actually have the power of Internet to train it in a second which would have taken otherwise lifetime so what if you look at what has happened today if you add up the flops of top supercomputers and the data centers in the world you already have beyond exaflop today where we are right there's no single computer with exaflop but computers already have excel ops now at the same time you also look at the number of internet internet bytes that we are generating it's already crossed zettabyte so if you look at the ratio of the two which means for every byte that is put on the internet we roughly have 10 raised to 4 flops to process to decide whether to throw the byte or put it on the internet which is not much why is it not much because look at the rate at which they're growing compute is barely doubling every year and a half but data is doubling more than every year that that difference in rate means that pretty soon you'll not even have one flop to decide whether to throw the byte of keep the byte which means it's not sustainable all it means is that the need for compute can only grow like I said human beings are not going to process the data at the rate at which it's being generated they'll take a lifetime to process to see a 13 seconds of data that gets generated as a video for example so it's all can only be consumed by machines which means the computer requirements can only skyrocket from here if this approach the outside in approach works so that's the hard problem so what does it mean what did what has it means meant so far that in in in key cases we see glimpse of how the kind of problems we can now solve and these things get a lot of press as you can imagine right with applying this amount of compute on this amount of data we are now able to see at least examples where computers are able to beat machines are able to beat best humans such as the Jeopardy game the alphago least hurdle level line champion and the Google car and the car self-driving cars in general you can see each example here that what is fundamentally happening is with this amount of compute and this amount of data we are able to finally solve some non trivial problems what kind of problems are these they're not purely computational problems let's understand what kind of problems are these so it's fundamentally happening is the following there has been a nice division of labor between machines and humans what is the division of labor that's what keeps us all employed the division of labor is computers are the best at number crunching so no one tries to challenge a machine today unless you're Shakuntala Devi you don't challenge a machine for multiplying or factorizing numbers at the same time humans are the ones that are making decisions at every level whether it's a matter of deciding the grade of his kid in school where where to be whether to build where to build a hospital whether a guy gets has a heart attack or or just gas problems medicines schools went to where to farm or who to put in jail who's a bad guy who is a good guy you may be getting stock exchange data from every Stock Exchange on the world in or on your phone it still you're likely to ask your friend should I buy Intel or sell Intel computers do not help you with that simple decision-making they can stream the whole world's data to you but the decision-making is entirely human today this is the balance this is the division of labor that is about to be disrupted machines are now claiming no I can actually do decision-making as well like you saw I don't get headaches I don't have biases I don't suddenly decide to text let me decide things now this is fundamentally what is going to this is enormous implications for society and I will discuss some of these but I am primarily here to discuss the compute implications as to why this is now possible and what is it and what and what are we doing to understand this and enable this so what does this mean what it means is almost no less radical than the first revolution that took place sometime back Industrial Revolution right which created an efficiency of in mechanical efficiency of machines which was unparalleled and that's how factories happened right Mills now we are talking about a new revolution of information age where we are actually creating another second generation Mills 2.0 which is an acronym it's I coined this acronym it stands for machine intelligence LED services important thing to note here is it's machine intelligence is in the leadership so unlike human in the leadership and machines helping machine intelligence for a class of problems are now claiming to be in the leadership so humans literally take the back seat that's precisely what is happening in self-driving car literally all right very few people would have ever predicted it will happen at this pace so these class of services are the ones where machines are actually doing a job better than human and therefore they have earned the leadership position in that decision-making and the only place it was practiced so far was actually programmed trading on Wall Street where machines do the trading humans later find out whether the trading was done right whether they got the bid or not now same kind of latency is what is required in self-driving but it's not just trading you're now going to kill someone in that millisecond of wrong decision so it's a much harder real-time decision-making task that machines are signing up for our gearing up to deliver now but let's think let's go back to the best decision maker of today and the human human beings are very good decision maker but let's realize it's a very complex process and learning and decision-making is very very complex and I encourage all of you to read this book thinking fast thinking slow by professor Daniel Kahn I can a man at Princeton and Noble or it right and if you read this book you'll realize that how complex human decision-making is we don't always do a lot of compute to decide something in fact his theory is that we decide something our system one decides it's a gut feel decision and then we rationalize it somehow okay so human beings solve this problem and not exactly how we would try to solve through brute force compute very differently at the same time it's important to understand because computationally these problems are NP hard right meaning the complexity is not n raised to something it's something raised to n which is very very different okay so what is there for other thing to notice about how we solve these problems that anything that is human services led where humans are in the lead the variance in quality is enormous this is a huge difference between anything humans do when it comes to the quality there's a huge difference between a terrible singer and the best singer terrible player the best player terrible politician the best politician terrible doctor the best doctor terrible teacher the best teacher anything humans do the output quality is hugely different from worst to best so the variance is huge all it means is the machines don't have to beat the very best it's probably an hour miss it's probably impossible task even though it gets a lot of press but even if they can do better job than the worst which is much much easier still very hard and slowly begin to rise at the top they can replace and do a far better and far more predictable job and what it means is efficiency of many social problems will go up and all those problems like efficiency of how we teach efficiency of how we farm efficiency of how we treat people all of those problems will see a rise in efficiency if we can now begin to solve those problems so it also means finally that for us who design machines it's also time to go and look back at our fundamental model of computing why these problems are so hard for us for the kind of machines we have design and let's go back and realise that seventy years back almost john von neumann right he first came up with the current model of computing by actually analyzing brains right i actually trying to figure out how we think this is the ultimate computer that's how he came up with this split of computing and code and program and memory that's the cost quote unquote von Norman model so this model is a model that has served as 70 years it's probably time to go back and revisit it because clearly there is no such thing as code and memory here so we are also trying to understand that so this is about trying to understand humans and trying to understand how they think it's not we don't have to literally reproduce it remember to build a plane we didn't really recreate a bird it doesn't have a wing it doesn't flap but we still be good to understand even though we're not literally trying to reproduce we're trying to do something better so in our research in in pill we are looking at this problem at all three levels at the level of science meaning at the level of how people think how humans think and we have a collaboration and and I'll get to some details in each case and the secondly at the level of computer science which is would be most of my talk the algorithms the compute problems right machine learning and finally at the level of systems science computer science computer systems how can we actually build something inspired by this much different from what we do today and actually thereby solve a new complexity class of problems okay so I'll discuss all three of these and I in each case I'll have one in deep in-depth example at depth example and it's just so that I'm sure that some of you probably would love to see things a little bit deeper but otherwise I'll mostly be at higher level so let's start with the middle one the computer science part of the algorithms right like the copilot I love that code that do things that don't scale so primarily we're focused here on those algorithms right the ones that today today do not scale and how do we make them scale so that we can deal with the data and the rate at which compute is being offered by technology so deep learning gets lot of press these days why I'm not going to go into two totally separate explanation for this but let me note few things the most important thing here is that it learns on its own a deeper representation of an object okay deeper part is very important so that's what's called deep learning and it learns on its own is the most important part because what is really excitement about its success is that it is finally for the first time automating or the feature extraction part we are not we have not solved the unsupervised learning problem yet we still need supervision but the supervision needed the labeling needed is not one of experts but you and I 150 countries unemployed people can look at the images and call them cat dog just label them it's very easy that part it's much harder to count on experts who can tell you what makes a cat a cat okay that's what we are counting on so far we were counting on expert so far that's why these systems are called expert systems experts are not even born once essentially right and how do they think how did the how did Newton even think of gravitation while an Apple fall who knows it's very super complex so we are now more reliant on expert is the biggest reason for excitement that's what deep learning does it automatically finds the important features and by only counting on non experts to label the data or sometimes just by watching them that's the primary reason for excitement but we you and I should still be curious as to why this works what are the theoretical reasons for it to work the theoretical underpinnings are not quite there yet just to be very clear mostly what we have learned is statistical recognitions to take statistical pattern recognition which is good enough for mining but if you actually want to synthesize things or simulate or predict you need a generative model a model that can predict right because they're called generative models because this gives you godlike ability to synthesize something to create something we're not quite there yet we are mostly talking statistical pattern recognition but still it's exciting because it can actually automate feature extraction so let's understand the reason so it works it's known and I have some results here that you can look up that it's a very old theorem which is the most fundamental theorem for deep learning is that a complex functions first of all all we are talking about in this case are complex functions what makes the function of complex it's rapidly changing number one number two non-convex rapidly changing function that's what we're trying to learn something here or stock market doesn't matter right what makes that hard a complex function if you are forced to represent it in K layers you need exponential number of compute units the nodes but if you add one more layer the amount of compute goes down and becomes polynomial that's the most fundamental result which tells you that a deeper representation is more compute efficient representation intuitively you can imagine trying to do all forms of logic eight in one level of logic by encoding the truth table but then truth tables and 2 raise to 4 2 raise to 8 they grow very rapidly but if you can do that since we do not have exponential compute we need deeper representation what was the problem with that you could not train and learn the deeper representation we never had enough data to learn the deeper representation that's what we now have to the extent it works it works but that training itself theoretically is np-hard problem we do not have any exponential compute so how are we doing it there are some new results that are coming up which actually show that under certain conditions of sparsity and on both input and the weights that the problem is not np-hard but more polynomially tractable those are new results all it means that in real world probably the weights and the inputs do have non-random distributions there is some pattern that's what makes this problem tractable now theoretical results aside it's working the most exciting thing so let's look at now a hard problem in there and now this is one of my deep dive I'll quickly flip through some files so one of the hard problem is looking at the output layer we have in most real-world problem the output has many many items in there just imagine the number of objects in the world it's not just thousand objects like imagenet they probably Millions even though they're very many kinds of vegetables they're very many kinds of people they're very many kinds of words in a vocabulary roughly million what we're solving today and only toy problems right when only 1,000 classes or few classes can be detected and we do that by creating something called fully connected layer which is where the complexity comes so we are learning and we have new research now which we just published which comes up with which is about coming up with a new sampling function so that you can actually sample the output layer if you cannot deal with that output layer if the problem is not scalable the current approaches to be planning are not scalable so what we have done is we have focused on like you see that that function write that in a given state you come up with a new sampling functions so that you can actually sub sample the output layer now the details are in here the challenge again becomes how do you multiply skewed matrices which high performance computing is ignored for a long time meaning very large dimension on one side and small dimension on the other side such matrices very small kernels for convolution one by one two by two these are the things which are not done by machines very efficiently today if we do those things and a new novel way to sample write the net result was that we were able to actually achieve very significant significant performance for a problem which is non-trivial namely the world the NLP problem meaning given a word you are trying to predict your next word and sometimes remember you have to look at the entire sentence so the entire paragraph entire page to get the context of that word the meaning of that word that's why we picked this problem which is a non convolutional neural network something where a time dependency exists you need lot of context which is true for most of our problems in fact the model is called long and short term memory where you brain things like that it has it has some time component in it by solving this problem by understanding this problem through a new sampling right like I said we were able to scale it we were able to scale it and that and again I am skipping the details but the scaling you can see here that something which was very hard we are now able to solve this problem in matter of days this problem traditionally would have taken us more than many months of training ok but even with 64,000 word vocabulary the real vocab arrays are many million words we are able to bring down that many months of training by scaling it and in fact with unoptimized code it was probably close to a year so that ear is going down two months is going down two days as you can see here is because of the innovations at all levels right and some cases we are doing much better than the best results on some GPU CPUs GPUs CP is not the problem it's the algorithm like I always said the thousand x peter per 100x speed-up happens at the algorithms level after that you only lose you pray that you lose less and less at compilers and machine level but algorithms is where hundred x speed ups happen so I like that last speaker suggestion that algorithm should be taught very very early on in schools so moving on we didn't stop there we are now looking at even harder problems which are not neural network problems which are not constrained by the constraints of neural networks which from this stage to this stage to this stage but more general-purpose networks the network set which actually give you the power of being able to explain right being able to generate right the Bayesian in a bayesian problems now in this case we took up we signed up for a challenge for from NSF for a plant right it's a plant and the data given is microarray meaning all the genes of the plant and how the behavior of the of those genes right what happened and you're trying to reverse-engineer the gene regulatory network which is a very complex network and imagine this is very similar analogous to if you're given a data on who's calling home and you're trying to reverse engineer a social network right so it's the same kind of a problem now anyone can be friends with anyone any two or three or any number of genes can together form a network to control the feature that we see on right so this is a very hard problem for the first time we were able to solve this problem at the scale of number of genes tens of thousands and we were able to reconstruct the gene regulatory Network of this plant by going on a machine which is number on supercomputer the machine in China right Tianna supercomputer so by that amount of compute but the good part is we were able to solve a Bayesian inference Inc problem at the level of tens of thousands of nodes and the net-net was and this would have taken 27 years we were able to do it in eight and a half minutes that's the power of compute we are talking about if you do the algorithm part right and you're able to apply these today's machines in today's compute technology we're able to do them in real time now the net is that in this case how do you know that you've done the right thing what we did then we concluded some new genes which are responsible for a plant function now in this case you can actually order seeds of a plant which certain genes knocked out almost like the Mendelian experiment we did that experiment to convince ourselves we actually discovered the right new genes part of a new gene regulator Network for a plant feature now this is how I'm suggesting or I'm hypothesizing hypothesizing that science will be done not in the wet lab but driven largely by machines being in the lead and actually guiding you where then the lab work the scientists work is minimized to only focus on certain genes or certain synthetic material synthetic plant drug design if we are able to do these things at this scale so moving on now let's talk about the the other part the neuroscience part right so what the how do we really still understand the most complex regulatory network right here now we're not talking 15,000 genes we're talking hundred billion neurons we have signed up to do that too we are trying to understand how they work how they interact how they encode things the encoding here is the crux of the whole problem it's a very sparse representation right so the key here we have this is not our observation this is a work point that we have signed up for five year collaboration Princeton the same person Daniel Kahneman Scholl right we're where we are focusing on attention attention is the key that's so-called that's what mind is you may all be looking at me listening to me still made me thinking your grandma so what you're thinking may be very different than what you're looking because you may be an attentional state which is completely blocking what I'm saying this guy talks too fast I don't understand what he's saying let me start thinking what am I going to do tonight right so if I know what your the attentional state you are in I can really figure out much sooner what are much easier what the which neurons are firing without that it's a huge state space that's what we did in the plant case by Bayesian approach by conditioning it we were able to cut down the state space dramatically that same conditioning here the condition is coming from the state you are in I'll give a very simple example if you are trying to reverse engineer what a ship may be doing the billions of transistors but if you know that the chip is chip can ultimately only execute some hundreds of instructions so if you know the instruction is executing the lower door a store you can literally trace the path only few transistors are active rights to the billions I have no role same thing here if I know the state you are in I have a much easier time monitoring and predicting what you're thinking and otherwise it's a hopeless task so that's what we are doing we believe that there only finite number of potential states that the human being human mind human brain has they call mind can we discover those can we read reverse engineer those so this is the collaboration we have signed up with Princeton and this for the first time in there psychologists and neurologists are in one building this is the point I was trying to make even these academic disciplines have not worked together psychologists live in their own world and they are always live in their own world for the first time they have joined forces and now they found a compute problem that whole loop takes 60 hours we need to happen seconds so while the guys in fMRI wealth you can actually decode and do causality on the correlations we are seeing on the large amount of data so what we have done here we our goal is that in the end over next five years we have a quickly go through it our goal is to be able to decode more than hundred attentional states of the brain if we can do that what can we accomplish it be something like this you'll have a jeopardy like game but machine is simply showing video of what you're thinking not what you're talking not what you're doing you can imagine the power of this how by actually knowing the state you're in which makes this problem much more computationally tractable okay it's similar to the mind-reading exercises that Tom Mitchell and I'll did few years back but that was only limited to ten images right so moving on what I'll just show you again one or two results we have now able to collect large amount of data which they are collecting from animal experiments or human experiments they have the right approvals to do that we are aligning those images even that's so that across multiple subjects this is a very difficult thing to align so here even across people these things differ so you have to might have to properly align them that's how we get large amount of data because one individual does not give you enough data so with large amount of data across multi-subject we now are doing topographical analysis right early did deep learning then I did basically inferencing now the third class of algorithms topology right so topological problems we are discovering we believe that there only is few centers few topological entities here few though we are discovering them to top topographical analysis to see can we really find those centers of attention right so we are looking at in this case we have sped up two different algorithms by more than as you can see 104 thousand fold I am NOT here to brag speed up all I am here to tell you is that these techniques for these neurology and psychology are not finally happening in real-time we are taking less than three minute or less than 17 minutes per iteration all it means they can real-time experiment this is what takes correlation and brings causality so that you can truly discover what is causing what and how exactly which state I am in and how do people think how's information encoded what are the basic primitives what is the instruction set of human machine human brain just like the instruction set of a processor right moving on we believe that we also have at least some understanding of these machines and that's how we're building new machines new trying new silicon which is more which is a non von Norman machine meaning it has no distinction of program and memory it is actually blurring all those distinctions and this is I'll skip this but this is what's called neuromorphic computing right and in neuromorphic computing our focus is to actually build a chip write a chip that is very different from a varmint computing machines which should be focused on very small kernels but these machine these kernels will actually give us hundredfold efficiency energy efficiency we are doing primarily CMOS we are also incorporating some non CMOS emerging nano oscillator and some new technology non CMOS technologies in this to actually build a chip with her index more efficiency so I'll quickly go through this and I'll skip the details just essentially important point is that we are actually building a silicon where we believe we can get hundred x efficiency at the level of these very core kernels not a full computer but the core kernel so that this could be an accelerator for the traditional machines and for simple things like can we actually do some pattern matching problem constraint satisfaction problem and deliver 100x efficiency energy efficiency that's what this one does now just just let's not one piece of data and this roughly you take roughly four and half a four or five Giga ups per what is what the efficiency of this is that's how you're able to do for 20 watt however many billion operations here today's machines are also roughly at the same level four and a half gigaflops per watt so we are roughly at the same energy efficiency level what's the biggest reason why we don't have this amount of to this kind of compute problem because our interconnect architecture is very different we spend enormous amount communicating has a positive computing brain each neuron is connected 10,000 other neurons we don't build the chips our one node of a machine is barely connected to three three or four other nodes high radix networks are very very difficult here each one is connected 10,000 that's the kind of new interconnect new machines we are talking about very different architecture very different programming you send 10,000 messages only some of them will make it but you can reach 10,000 neighbors in one hop okay so let me now try to sum up what is happening is that we are at an unprecedented convergence of massive compute with massive data and this will fundamentally change what how we have done computing and what computing has done for us it takes computing a new realm of applications applications which is so far completely untouched by Moore's Law ok these are basically social efficiency in efficient ways of solving problems like in farming like in teaching like in health like in transportation and that's what we are now will finally get there we'll finally have enough flops I often say that when scientific problems end with excel flops the mom-and-pop problems begin this is a typical myth that we lean out of compute for what for weather prediction who needs that in fact the mom and pop problems the men mundane simple problems is where that exaflop s-- our minimum requirement and so and then begin after that so let me give the same example that was earlier chosen for example you can then tackle a real problem real problem of farm farming right how do you improve cropping it's not a million parameter problem our typical imagenet deep learning challenges are roughly ten so million parameters if you look at a simple problem of how to improve a crop hill in a single season single crop single country brilliant press parameters so if you if you can solve this problem through deep learning which is what we hope to be able to do in a matter of two three years trillion parameter less than hour of training time for complex neural network topologies what it will let us do is a little lettuce challenge and solve such problems when all you're trying to do is improve the crop yield by one percent which is almost like doubling at one point two to two point four percent yield growth okay for CL crops so that you can simply feed enough people by 2050 nothing more than that just to be able to feed the population growth you need to be able to improve the crop fields by that small percent but it's a very very complex problem we are hoping to be able to apply computing to such problems synthetic materials synthetic drugs new ways of improving yield thank you that's pretty much my summary this is the first time we can actually take supercomputing to the masses directly right it's almost like scientist or computer guys went to God and said can we think of some problem where masses ask for lots of compute exaflop of compute and not just some scientist for weather prediction our material science and god thought and said okay let there be compute and that's what this steep learning is it let the recompute like problem because it's a real-time loop because there's no perfect teacher there's no perfect training so training keeps happening and once you train the model it's a very compact model ship it to the phone and there's no perfect training you keep learning more and more it's a lifelong learning like the last speaker said machines and humans will both be in school lifelong learning and probably when unsupervised we don't even go to school you just show up on the street and you learn real-time and lifelong and that's what this enables it's the first time we can see supercomputing to edge devices back to supercomputing because they're billions of edge devices data comes back training happens new model goals improve training happens new model goes it's a lifelong process thank you dr. berry thank you for the very intriguing lecture yeah you know Carmen and the American goes in the top job I'm Kim jong-in mills which as an option is checked onion such as an optional Damien 2,000 meals most intelligence related services goes wrong panel to our question yeah to an England Ecuador emotion you know morphic computing me in burrito Mopsy since Roy Huxtable Tony could own a Nancy Drew future grow Higurashi Tannen German so in Ghana's economy aqua to Nega regime foreign currency issue Jazeera computer cargo number cruncher in Ibiza piscina NJ recommender yo charisma a prone on cinema gajanan decision maker Chino echoes random young female in I mean Florida Jesus come on Joe comida salga young or August okay currently many multinational corporations like Google IBM Facebook and in terror aggressively jumping into a ie related technology development in your perspective what level of development does a I stand today and water Intel's strong points in this field yes our primary distinction is technology as our goal is to make like I said earlier make these things scale and make them delivered at the cost and efficiency that most people can afford so our primary goal here is to it deliver the technology so that you can actually scale these algorithms and deliver them how do you deliver supercomputing to masses and that's where Intel comes in that's how you're different from all of them we want to help them all okay thank you now we anticipate that the future of human lives change with the development a AI that is able to replace most of our considered people shot is there any West representative example of a change in the human society due to the advancement of AI yes two things one it like I said it takes computing into a ram of new applications applications which are social applications problems which are social problems for the first time computing and begin to tackle those and society is operating at the level far below desired in all those areas health transportation farming name it so there's a dire need to improve efficiency in those areas and that's what this will let computing do computing will finally actually play a role where it matters where society desperately needs what it means for people like us individually is that it actually lets you focus on things that you are very good at things that you really like as opposed to just Monday in court at learning engineering medical whatever that's like I said there's a huge difference between the best and the worst in any human endeavor this lets you focus on things that you are really much better than average why because you have some real skill and you don't have to work and look at things where you're really hardly good so each one of us will then have get to focus on things that I enjoy things that we are really good at and machines are not going to be able to take over you in those domains anytime soon so that's I think as the implications for us thank you then how will the advancement of a technology innovate the development of science and technology for instance the eternal research foundation Korea is R&D funding agency which supports diverse area of basics science then for the future development of AI in Korea especially which areas basic research should be strengthened first you think yes I love it also repeat the last speaker said there's a big need for bringing quote/unquote computational literacy now I'm talking about a higher level of computational literacy namely in how do you deal with huge amount of data huge amount of compute right without and these people I'm talking about that kind of literacy for people in different domains musicians health professions scientists right how do you bring make them computing aware they generally do MATLAB level programming how do you actually make them computing aware so that they can benefit from an actually process and deliver and and learn insights so the teaching of computing needs to become essential in all disciplines starting from undergraduates and starting from schools and all the way to even graduate schools and especially in the science part of computer science needs to be taught taught differently but needs to be taught and it'll let us solve problems like I said which so far have been out of reach for computing in how do you do better chemistry better material science better better design of strategy okay she guy named Jay one man I'm China and India audience were to draw on chinma na Jeong Hangul but you Europe Chi energy ready Europe's energy ratio cannon young kooky one cupcake Perpetua engineer free until Ganon hangers rival Sunita a predominantly I come Eng kim jungnam cases take me to repeat one is one morning delicious okay yo leo of Siena Julia cannon Yong Guk even a petrol engine and children and Angus Reilly samhita a production area I don't think so I mean I don't I mean I won't be wouldn't violate any principles of physics in this whole matter we do baby backs on him whatever parent don't go me opinion Tom didn't assume that yay God with our generation we created and taking win generalizes come a couch a given to God ought to teach it gosh girl yes that's a very good question who should be held accountable and and this is again a legal ethical matter right which is being hotly debated there was a recent change in some states law changed very few car is self parking itself and something bad happens it hits other car the car owner is responsible not the car manufacturer such changes will probably take place many places the bottom line is if the self-driving car is more is safer than human humanly driven cars those cars will be on the road right they'll never be perfect they'll still make mistakes and who is responsible in those cases it's again beyond my expertise but it's being debated hotly and let's see at least some states have are now passing or making the owner responsible as opposed to the the manufacturer a bodacious scale yeah Sachi Yi Qi Lang says I'm a total sucker golden era 2007 hittin our equation in and Amador Iran hi computing power dome optional erosion of the thousand kilometer current high computing power in Mandarin and they're using an Intel a contribution to Castle Coco who is Brennan the Samsung ginger button and OJ usually about John Doe Uganda after Aiden England entire unit such as an opinion she paddle get some char give dinner a virtuous man on the Chopin jet car Matsuda thank you nepeta to ship product base which I knew is homing go inside on him yet - come back soon Takumi de niro my sanity little saltier than you come 70 do a Sukkah my initial Smita he goes through such a quantity pagano's when saw but she the rocker guessing me that how much each six chicken cut cooter got sponsored speaking Inuit in day oh oh it's a it's a spongy a tiger Haru - wanted to peel onions Shankaracharya Sumida struggle man she go - a man never kiss Mita 