 about creating augmented reality apps we will use later on you can use SDK we used it because I work for Pikachu stands for years for nine years started when we haven't had any augmented reality SDK we just have the wiki to a browser which was this geo based our application and some points of interested loading I want you next train station or restaurant this hope that with them kind of extracted into an SDK and SDK for geo AR that's when I really started developing 40 SDKs on writing code 40 sdk since day one okay what are we going to do today first we maybe talk a little bit about augmented reality afterwards show me something about the wicked sdk how that works and then i would like to put in some some demos some live demos showing you some code and if you want to you can try to right away with me if not I would like to do an kind of a coding session afterwards so I will run through some demos show you some code snippets and how it works and afterwards we just can try to do something else I'm here for questions okay first question who is familiar with a augmented reality who has done something with augmented reality which kind of SDKs frameworks did you use anyone else someone else but you all kind of know what augmented reality is or should we cover that as well okay so a wicked we have been the first application on the App Store's place that was back in 2008 I guess we now have the largest independent firm meaning that we are the only company in the markets which is not owned by another computing so everything is really wicked you and we do all that stuff so for Laura roughly a little bit more than 10,000 EPS are using our SDK to get out of debt we have 100 million app installs on the stores including our own browser that we still have some somehow around developers currently we have so you're in quite a good community the forums are very frequently visited and we try to have solving problems and people getting their application just another few information that did you get an understanding of how big we can do this currently 20 out of the 100 best global brands are using our SDK and what can I raise to K 2 so we have gol that's what we started that's our initial GA our is what does those points of interest adore they call it most of the time you have them flying around from the camera and they kind of indicate you it works with GPS coordinates so you have anything defined that those who show up at certain GPS locations or longitude values but to really it's a nice case sometimes it makes sense sometimes not for example today a morning um I was at the Amsterdam main train station and I've never been here before and I only knew that I had to take from 26 to get here and I was standing in front of the main station and I had no idea where should I need to go and which just pointed me into the right direction right kind of basic idea of where I need to go that would be something that you can do with your what else can we do we have 2d image recognition and tracking that we edit 2012 we started with having all your working around beforeö that didn't work out that well and 2d image recognition affecting tension and since that we are currently improving that and to the image recognition for those of you who are not familiar with that you used it for example to augment the poster catalog something like that most of the use cases that people are telling us this is those two free catalog so that's our own source book based company they have a book on satellite images of various cities landscapes Shanghai I guess and they had an augmentation for certain buildings in the town people get more information about the building links to the videos or websites the next or the last big part of our SDK is freaky tracking in SM technologies it's takes this 2d image recognition to the next step for 2d we are kind of analyzing a 2d image finding picture points scans a whole room and creates a so you can then afterwards really maybe of we know gives you so many opportunities to do for example a really working in the navigation and navigation which is really accurate on a centimeter level and not just give you basic directions so for example this main station I mentioned before from the training the main station and the areas before it you could really draw a line on the ground but enough talking I wanted to show you that you get an understanding there was just to give you what we are okay so what is you I hope you have now kind of an idea of what you can do with our SDK so GA are 2d and 3d recognition and we have split that into different which comes with a JavaScript API or and Objective C and Java API since we are here at the fungal conference where everything is about web technologies which runs for is available for iOS Android and how does this work we we have implemented a SDK in JavaScript processing algorithms that would be way too slow so our SDK is implemented in C++ and fasciculus best part is thanks camera OpenGL rendering those 2d and 3d computer vision algorithms we have also the same or image processing and the cloud but still the cheapest best part depends on some headphones specifics for example the camera access what meant ability does not work without a camera and the camera access is only available through the iOS Android SDK so we have a layup to access those platform specifics mainly yeah sensor values and to get an OpenGL context to be able to draw something on top of that we put JavaScript layer which kind of extract all those C++ and Objective C and Java code we've been you can define how your augmented reality experience then should work afterwards what did you do test is to represent your locations on top of that SDK you can build your application or you can choose to use another extension of our SDK to be able to connect our SDK at the end you would also be able to kind of write a plugin again for our SDK to kind of implement your own computer vision algorithms or so the question maybe for one or the other guy in here is how do we execute this JavaScript code you know I heard that the SDK itself is implemented in C++ and that works through something that we called an architecture and architecture is simply HTML Javascript and CSS files the combination of those three parts let's how you how we define or how you can define an AR experience that's possible with this vegetable script tag that only is available or useful in combination with our SDK because that tells once a webview tries to lock this tag our SDK knows what this architecture really is and how to inject that into my view that view architect you those are that's what how ours DK is built I will explain it in the middle so the main point here is the JavaScript does not do any of the real work that's done in C++ Java Script only triggers and controls our engine in the background so how does our SDK view works we call it architect view because it's a combination of two things it's first an OpenGL view which does rendering of the camera stream and rendering out augmentations and on top of that we have a back view and in that view as on transparent background so everything if you do not define any Content that you you would just see the plain of Jerry but if you start to add some HTML you can kind of create this hybrid of having the camera feet in the background and then your own content on top of it so for example yeah you use it the best you can define that in HTML and do it in the rendering in the background which is then also platform-independent so you write your architect word once and load that on iOS and Android a couple of small slits about our fungal plugin and then we are going to really do some work today and show you how it really works our cordova plugin is a standard plugin you can install it standard blackman commands it's the heads are the different on I was in Android on iOS week present or rep that all up in our own new controller and on Android we just add and native Android view on top of the existing code replication and our plugin is also integrated in the latest phone give developer app so they released an update I guess four days ago and you don't have to or you can try out a or by just using different gift I've happen again to a little bit kind of a visualization the blue layer is the standard Cordova that you I guess you only know a quarter of applications simply a bit fewer so native I was an Android application to chose one back view and our SDK kind of puts on top our architect you saying that is quite important because it should make your vero the decoder by application and our SDK you they don't know each other we are just in that native application another layer on top of the webview so you don't really have a kind of it looks like it what are you four user you see our architecture and nothing of the original forget application but you might need to communicate between those two words because your native application your pocket applications to get some information out of our out of the AR experience so we kind of have a solution for it and it's quite easy within our bunker dragon you have to function the call that's called for javascript and passing some JavaScript code that you want to execute in the context of our review to get them back from our architect view into your applications you can change the look location of your document and change it to a location that we kind of detect and note and then prevent the bad people actually changing the location but just passing on this URL to the original fungal application yeah to be able to do that you need to just call this set on your elbow callback function and passing in a custom function that we then call every time you try to change it a patient okay that's the so far the overview of our SDK and how it works just to give you a kind of idea of what you can do in your experience or what kind of augmentations we have you can create images you can some okay but I would like to show you now is how a Jew augmented reality experience would work or what you need to do to create this and afterwards for image recognition and for both of those I will show you some demo what you need to do for joy is actually quite simple you define a geolocation object and you repeat that with your latitude and longitude values let's find that specifies specifies a point somewhere to define what place you use and draw a subclass and that's Wednesday I showed you on the previous slide and to combine that you need to use something that we call a Geo object to object combines a location which then looks like something that those circuits on vacations the boxes of Geo objects and these things should represent a marker you mean the attitude here okay how does this works let's see so the setup that I'm using now is the following I use the phone desktop app and traded standard project yesterday so it's really if I wanted so the application is not running on your exit and that's the standard PhoneGap template that you get when you launch the debut so nothing special so far what we are going to do now is at it's a project on on my system and what you expect from standard whatever application the main index.html which gives you this only robot the JavaScript fact that the main fixture is this wherever you come over just switch because otherwise things look at the main index.js that you all know from empty for replication this night that's just how you load or define the applications using another plugin I guess you're all familiar with that nothing special so what I did now is this load architecure function that's the name like this where the extra books and where the person is just a check to death devices really they are in a but there are some especially Android devices out there which for example don't compass inside or maybe the CPU is just too slow and 3d recognition wouldn't work so it's kind of our recommendation that you use this API to check the thief at runtime the actual device is capable of doing what you wanna do we define this and then the most important thing architect that's where we are passing in the path to the architect good and also specify those required features again so repair features are they are giving us a hint if what your intention is if you want to do to you specified you as a widget we want to do some to do passing in another identify and based on that we kind of change what the SDK is actually loading and with a startup configuration you can define it the front or back end should be used on most use cases to you I guess you won't have to deliver a scene camera of the device you could and then this is our Quebec function that would be called if we change the occupation but for now so only two lines are they missing right after we created our SDK the first one defines the path to this main HTML file and the first part we use the photo of that plugin to get the path on the device for the applications installed and then just specify the last remaining then we call this just just one thing and that's the architecture itself we create one you can name this folder however you wanted for me that fantasy and what edit now is an architecture as I already said architect nothing board and HTML Javascript have jQuery and some CSS to make it a little bit prettier but is it and for the assets that's the image we are going to use icky righteousness so as I mentioned on the iPad I simply installed the fungal developed application loading screen and it's now and the cameras running and what this quiet location demo or architecure does it places employee at the random location around you I don't have GPS that's bet that's true they are Jesus never work in rooms tested it out yesterday in my hotel and was working but that's what can happen with you burnt we simply move forward 22d recognition because to the recognition what do you need but it kind of the same requirements as before just one more what we need is Target collection Target collection defines which 2d images you want to recognize and you create on our website what we then need is kind of the adjudication we had before for you AR is tracker file for computer vision to create a tracker we need this WTC file and the path to that and then this can handle the content of it the content itself is our representation of what was interesting for us in that reference image the next thing we need is again draw it so what we are actually one to show and then it's like before digi object we have something called trackable to the object which connects a tracker and a drawable we have that because a tracker or Target collection can contain up to 1000 different target images and you might want to have different orientation for different images you create different variables and specify here with image you born on or means there is only one limitation that we have is and that is that you can only load once record at a time you can then define multiple trackables referring to this record but we can only create one record hopefully getting overtly done for death so what we do now is we replace the architect the only thing we need to change here is to have to remain context action and if we have a look at the architect working here mostly the same as before in this HTML main javascript file that contains the code of what should be done and the assets that changed lately we have two pages for the organization's and then there's WTC file and that contains the reference images that we wanna recognize later on in that case two pages out of sports magazine the JavaScript code is I forgot to show you that before that's all in Dubai to create an architect word to define your audience what we are doing is that's a big creator attract object you're going to disturb UTC palette and we are passing a custom function that is called once the WTC file is loaded if WTC contains two images it's almost instantly loaded but if there are a couple of hundred images included might take some time to actually love that it's also possible to put this WTC on a different server and Java so depending on the internet connectivity speak might also take a little bit to download and the augmentation is declining here the first line creates a image resource so an object referring to an image and then with the next porn that much global we actually can kind of change how that image should be rendered at which position you can define offsets relative to the reference image and you can see at the end the trackable to the object kind of combines the tractor with its profits and right over here where the store is now you can put in the name of the reference image you wanna write a specific augmentation for or just put in this door and kind of use it as a wildcard can say okay Jacob of 2d is valid for all reference images that are found or that are included in the tracker and currently found in chemistry I was getting complicated because it needs to get to schools when iucd I've been living or running the camera and if I have a look at the to the image we kind of detect it and throughout this surfboard to recognize it you kind of need to be at most in front of it but then in doing the tracking I guess it's 170 something degrees yeah I mean you can be a little bit further behind but initially it needs to take a lot of certain percentage of the camera I sent ya issues if you know keel the patient inside agreed why not combine 2d into 3d 2d to recognize which room you are and then overlay the 3d to pinpoint just at 50 we are kind of referring to an GPS the reference point will be zero zero point will be where you stand with 2d napping I'm not quite sure they understand but yeah from that moment we start with 3d calculations that's kind of how we teach working within kind of somehow works but you don't need to have the initial 2d image to to know where you are but really you kind of what we have before for 2d is this WTC file containing those reference images for 3d tracking the process of creating that would be that you need to go to the place having a special application running from us and simply that application is a camera recording what you see and processing what kind of picture points there are and out of that training wonderful out of interesting points and you would then instead of the WTC know this new map collection and then it's for our SDK the same again we have the live camera feed analyze every frame of the camera and try to match that points that we found previously so with that if what you were trained to learn your first step once we found one of those now it's the point again in the chemistry we already know where we are and we don't need a new initialization it might change if you have a really large and behind meant for pretty tricking for example airport terminal that's quite huge the new products would be and in terms of price as it would be hundreds of megabytes and would need quite some time to initially locate your position meaning 60 seconds for something else and to reuse that we are planning to kind of make it possible to give our SDK hint where you are roughly around and that could be then a GPS location yeah but the GPS is not that reliable because inside you might not have a signature this match you know suppose which it's hard to say it always depends on the use case in the head depends on the lightning condition it depends on lots of different valleys I can't give you any specific answer to that that depends on you need to try it out mainly because if you have a look at such an image it's not that we have however might find some more interesting points over here and maybe over there and then it recommends where those distortions are or where the dirges if it's everywhere we have kind of found a lot of interesting points and that area is covered it's hard for us to to refine it but I don't think that we will find a lot of information over here and if you covered it somehow just matter maybe we can I could not common between see that although most of you just covered we still sorry more than 50% of covered and still to it's always fun to play around yeah see how he gets confused I guess I only have a couple of stats left which is okay just entering the trade out yourself part of the structure so what I thought is maybe you want to try out to create your own target collection based on some paintings and define organizations for that or put some markers at home get the conference locations that's the steps the main steps some of us you need to create an architecture meaning HTML and JavaScript class you need to add them to the project we need to load them with our SDK refer to the wiki reply ingest it's only one function you to call that architect and then in a javascript file you can do whatever you want everything that's possible with our API just as a quick demo maybe we add the functionality to basic silent on enter field of vision trigger so what this does is as soon as we recognize the image we should see an alert saying found and then the name of the target every time sorry you see page 1 that's the name of the I'm just missing explanation how to create at a beauty supply you can do this on our website I prepared a tenth of the Sun so let's talk advantage of the Pikachu just come yeah you need to create a comfort two images from the samurai and what we do now is we run our algorithms to kind of instruct interesting ones out of those two images and book that's all writing that kind of indicates how well the recognition might work so one star saying that chances are that we find it but it's kind of hard for us to stars should be quite good and three stars of courses should is the ideal image I mean the quality of a target image depends on if it fits just the red square we are unable to detect that this there is no information in there it's kind of this heads or this robot and kind of did a lot of interesting parts for us the painting is quite as varied out and you don't have that much contrast or edges inside which any Red Square would get me if it finds the Red Square anywhere in the world yeah our algorithm does not work on it's not based on color so for us everything's black and white and then it doesn't matter anymore and then a square is just an Arab okay wants you so far edit some images you can download the WTC and you get an email once it's ready or once it's generated you can download it I did that yesterday with those two images and will then look like that WCC color containing those two images and mentioning the name of the target that you then can use for the checkable any questions or suggestions or questions that we should or I should answer yeah there's that like a free viewer yeah for the SDK we are also able to load 3d models and we have our own 50 bottle format and we have an desktop application to convert FBX into our Informer and once you have installed that application you get this quick little plugin installed but we don't have such a tool for windows on windows you would need to refer to the to the web page and kind of get the information from there but it's very handy for development if you have this WTC file on your system yeah I'm not sure if there is a website from Adobe where all of those lights are collected but otherwise I can put it on github and does that work with an experiment again say you want an image we're just gonna hide in the background no and the target image is there is no transparency about you yeah but you could upload different images with different backgrounds as I said up to 1000 so and if not you can switch to cloud recognition and then it's I don't know 100 thousand sorcerer you get a free trial lesson so once you've done our SDK you get a good trial lessons and you can do everything everything is unlocked you just have a watermark in the screen and after that we have different lessons models depending on the I guess the number of installed applications no no you go completely free of what you want Ottomans that's just one of the demo demos that we have so you're not good for recognition you're restricted to the number of scans but for duty offline recognitions are what we did now there is no limitation in the number of scans or the size of the collection anything else our Christians coding related illuminated or setup related because what I heard so powerful it from the phones and from all these that people are kind of confused with this parka tech world and what it really is how how it's loaded I guess decision ends X or already ended we started 1215 so I guess it's officially lunch break or 