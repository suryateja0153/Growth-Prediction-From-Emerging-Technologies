 thank you very much Derek and I think the posters should stay because this is a two-day workshop tomorrow there will be another poster session so we can put the posters on this side tomorrow and so hopefully these can stay and we can carry on tomorrow so while people settle down you're all set up it's a pleasure to introduce unto one body from facebook AI research Antoine will tell us about memory networks success stories and challenges thank you very much for giving this talk take it away Thank You Arthur for inviting me here very happy we're happy to present this work in front of the crowd and welcoming all your suggestions and comments throughout the talk it's not my only my talks to talk of is the work of a lot of people at Facebook so I'm listing them here so I'm going to talk about general direction of work we're following not everything is my work so yeah so I'm going I'm going to structure the talking in two parts and we're going to to see how it goes so I would stick to to start with reasoning in artificial world which is something we've been working on so that's the first of all I want to start with nutrition regardless regards to this twist workshops and regarding to what has been said this much this morning about the fact that we are trying to to merge maybe we could use in symbolic system some neural networks to do some kind of some part of the task Oh like as joshin would say maybe you could try to do to learn like to translate English to church for instance why not I would like to say like from the other side of the outer part like people from the no network community are using symbols right now as well and they are trying to maybe not at the neural network as one part of a big system but monitoring to see if the knowledge work can you is to learn to use symbols and so I'm just going I'm doing to talk to talk about one kind of model we've been working on which we call the memory network actually we are not the only one and a lot of work are currently being developed whereas the neural Turing machine the stack augmented RNN and holiday letter should base model they are always all in the field of trying to have no networks that learn to use into control structure memories made of usually symbols so that's that's that's what's important to to have in nine and small advertisement we have a workshop about that tomorrow so you're interesting in that a lot of these models are going to be discussed at length tomorrow so today I want you to talk about the memory network and and I'm going to explain you what what we did what we're trying to do and what we cannot do yet and what we would like to be able to do all this kind of stuff so I one going to too much detail about what the memory know it network is because i'd like to try to go into more like what what they do but i would still explain so the first memory network we proposed earlier this year is a is basically two parts so we made it for question answering on stories but if she can be applied to other settings so let's say you have a story like a very simple story don't pick up the approach and went to the office john went to the kitchen Jon drop the apple and then you get a question where was the apple before the kitchen so this is all symbols words the memory networks works by putting the story starring the story in a memory so here in this case each memory slot is actually containing one sentence we are going to talk about what we could do as well in this part but this one sentence for each memory slot and then I remember I network module is basically going to do what is now usually called attention over the memory by trying to combine using the question to combine it with the memory slot usually trying to select the memory slot that are actually relevant to answer the question end of story and when basically the mother is happy by having picked like the memories that are you necessary to answer the question basically combine them with the question and return the answer so this was the the first memory network we propose and basically can be summarized like this with two two parts one part which is the memory and link on the left and one part which is the controller on the right that does the the input and the output and what we see here is that basically the attention mechanism is here so you have the memory slot and what you do is that you combine the question with which memory and using this combination you try to select what are the good memories in this case with Max but i'm going to say how we can relax that and for the first memory network the problem was the first problem is that we needed to supervise it twice basically we need to supervise at the memory level which is like we needed to tell the model which memory in needed to attend to in training which is super costly and not realistic and we also needed to supervise supervised it at the answer level by saying which is the answer so that was the first mother and and of course the we are supervising the attention which was too much so uh what what people from the lab came up with and what was presented as an oral at nips like two or three days ago it's the end to end Ramona's work and basically the idea is the is the same the same idea except that in this case we don't have our attention anymore so we don't have a max over the memories but it's now softmax so once again I don't want to go too much into detail because it's been presented already but the ID the key ID for us now is that you don't need to have supervision at the memory level you just need to have the supervision at the answer level so it's changed a lot of things because the mother us to figure out three training just given the answer what are the memory slot that are relevant to answer and on several example that i'm going to show now it's actually able to do it with success something that that does that is important for the memory network in that way we've been working on where is this question of hops on the memory so actually you go to the question and then from the question you go to the memory and then you go to the answer okay but actually you can actually go in the memory several time before actually trying to read on the answer so basically you can have a record on process over the memory before actually returning the answer so this is what we what we call the up process so this is what is the depicted here so basically you have your question you do attention about the memory you have a reputation that is a combination of the attention and the question but before answering you can go back to the memory due attention twice and three times and this is actually interesting as you can see that maybe maybe maybe the first step of attention is going to fetch like some memories that are important at first but given those ones maybe you want to go back to the memory to fetch some others because you need them to to reason a bit a bit deeper so to to develop the memo network it to test and that's why I started this section by reasoning in artificial world's we developed what we call the two maybe task so this were like a set of 20 task that where our stories with question pal with an answer and the goal was to try to build a model that would be able to solve them and that's how we came up with the memory network but actually since then like it's been used like in more than 20 papers so it's not like tied to memory and talk in any way so I'm going to describe some of them because it's something that we like and we would like VIP if you are using them so first of all the task are available on the website and the code to generate the task is also available so we can actually generate new tasks so the idea of this task that we are going to create synthetic task a set of 20 ml actually we're going to extend it and we would like to have a system that is able to solve all of them with the fewest example possible because you could they are pretty simple so you can serve there with millions but the idea is to try to solve them with less than that okay so so how did we do that oh yeah that's what is it so we generated the test by having a simulation which is basically a set with people location objects and a set of action that the person could do in this world and then we basically can a simple action given a set of constraints that basically impose like a world coherence we can generate small stories like this by having like Jason go kitchen Jason get milk Jason got office so of course you cannot drop an object if you don't have it I mean this is the kind of constraint amusing and then you can ask question about location of some object for instance and we have this which is the machine language and then we using a grammar we can actually transfer that into English or not so the code actually allows you to generate the stories in English or in indy or adjusting symbols so if you don't want to mess up with the language you can actually add the task like this if you just want to know if the the system is able to match the symbols so so using this this work this the system which is very simple which energy it so 20 task from super simple to a bit more complicated so the first one which is a sanity check which is like the simpler story ever Johnny's in the playground Bobby's in the office where is John so this is like like most ponder can do it but and very simple so this is very easy and and then you can go to yeah so is we we can raise the difficulty by saying that you need one supporting fact to answer so just one answer this is what basically we expect the model to fetch in the memory to be able to solve and then you can go to to supporting facts by asking people cannot pickup object and they can we can ask where are the objects so you have to know who has the object and you get to know where is this person okay and then you can go to three supporting facts so I'm not going to last although we didn't go up to 20 supporting fact could have but it's a bitter wasn't the case but then we could test other things so we could test like just other ways of raising things saying that for instance you just ask truth value about facts so now you dope you can't really reach on a world of the story you have to make a black you what is truth his phones and other that were inspired for instance by the vinograd scheme a challenge or you can try to have some object that can fit in other object and try to ask if the system is able to solve this so the football fits in the suitcase the suit care fits in the cupboard the box of chocolate is smaller than the football will the box of chocolate fit in the suitcase okay so now you also have to fetch the vias memory slot the good ones and you also have to try to combine them the nice way to basically perform a basic reasoning so all the task are not like very hard it wasn't it wasn't the case and now I actually tried to make other ones but they were testing different capabilities and he was actually interesting and so we use them to device memory Network well actually I want to mention that people have been using like symbolic method to solve them so and it can work as well like reaching graph like some knowledge graph the rips in the task and then try to do influence on the graph it can work then the trade-off is how much prior knowledge do you use to construct your graph what are the basically the the conclusion that you can draw after that so and when you have the task then you can have like the this kind of dashboard which was pretty useful for us so we tell you 20 task and then you on the we consider that you solve the task if you are above 95 per set of accuracy on the test set because basically they are easy so you have to be able to solve and you can see ok what does the why are we so this is is in the full supervision so what I showed at the beginning and this is what using like the week supervision suggests the supervision at the answer level so the end-to-end process so it's worse as expected because less information and this is one thousand stories which we think is actually a good size too to test the model but if you increase the training set to ten thousand stories this is interesting but for us it gets to be too much to be realistic but at least you know that this model that has less supervision actually needs more example to be able to achieve the task but I can do it eventually and the other one of course also do it and there are a couple of task that are still difficult but they have been sold by other models actually I mean one of them not the other one so what is nice with this task is that we can actually look at what the mother is doing and especially in the case of the end-to-end memory network it was actually pretty nice because I talked about this multiple up so we can see where the attention is going but by trying to solve the task and so the this model is not given the supervision of the fact to fetch in training you just given the answer so in this case this is the first task where it's john young sized bathroom so the system in training is just given stories like this question and the answer and i ND catch here where which which is the supporting fact they just like to look at it and of course you see that this is a hyper perimeter so the mother is going to do three up anytime we have to specify that but for this case when you just need one supporting fact basically is going to pick this one with the highest probability and actually the second one that has a bit of way it is actually the other one was John what is mentioned and what what is he doing here very simple is going to fetch like the fact we're journey invention and it's going to put more weight on the latest one so we also have an idea of the location of the memory slot it's actually very important for this kind of thing so and of course if you fetch several times going to run force is no nature okay so this one was easy but if you go to more complex one like the one about the size so does the suitcase fit in the chocolate then you can have funny things so then you can go like to try to father the chest is bigger than chocolate this is going to be the first one but as you can see actually the knowledge is pretty spread but then the second fetch is going to fetch like oh the suitcase is bigger than the chest and the third one is going to say oh and the chest is bigger than the chocolate so Danielle so is new so actually you can see that the attention is moving around during the hub because you're basically carrying over there is also this one which is pretty nice if you say brian is a frog lilies gray brian is yellow julius is green greg is a frog which is the color of greg then you start by saying what kind of animal is greg this is the first thing that the bottle does and you say so it is a pro well who is also frog this is bryan and what's the color of Brian so this is we are very happy of seeing that educate the model trains itself to find what is important and and actually very recently it's been on archive like 3oh three weeks ago was submitted via clear the same author so after slam sinus better they also extended this to do on the maze so the idea is to try to navigate the creative like ten problems that are maze related so basically this is the agent and the goal I guess this one is to go into this these are goals go on go to go three go for goal five and so you're given the maze and you're also given instruction like go to go to and goal five without telling them where they are just didn't them their name and then the model has to find out his way to actually solo task this is one the other one for instead they can do is that when with a switch and the door so basically you have to go to go on but to do so you have to go here to open the door basically so this is the kind of task they did and in this case they can actually use the memory networks exactly the same way except that it's train with ruffle front learning in this case and the way they do is that they create this this story which is but it's actually not read your story animals very structured it gives you the position of each of the block and that type and then you get a question which in this case is now the instruction what you have to go to and then the model has to find its way to basically deal with that and doing like a lot of ops try to find the answer and you can see the attention so we don't see well but the attention is going to show you to which block the model is attending it's actually pretty interesting because for the wii this is the switch and basically the mother is going to look at the door the goal and where is the key and it's going and you can see how it evolves they are very nice videos on youtube about what it does and i can give you the link you're interested yep yeah it's one mother it's when a creature so I cheer for them for the maze I guess this is the same architecture but maybe the train the weights are not the shared for all the task I don't know should ask them for the baby task we have to we have to two sets you train you have the same architecture we train one for each of the 20 or one when you turn everybody together and actually it works as well so this is the the end of the load first path which was the success part about what by the other part is not complete failure but that is everything this is the artificial reasoning we're basically we try to reuse this artificial world to try to find if we can find architecture that can actually conduct like simple reasoning and that proves very useful but of course the other part of the of the work is try to see and this is the question we get like all the time it's a very good question is like what you've shown the wavy task is very nice it does nothing to do in real language so i can i know that what you can do is actually applicable in real language it's kind of stuff and um and so and of course we have the same questions and we would like to be able to answer so we are working on that and so i'm going to present you two sets of experiment that we did in more like realistic settings and the kind of conclusion that we have so the first one is a large-scale question answering so basically in this case you're giving a knowledge base like psych or in this case it's free base and you're giving questions about some facts and you want to get the answer from it so let's say for instance you can ask which forest is files clicks in the answer and ended up being like non Tala National Forest and and how is fact expressed in the knowledge base because you have one fact that says that fire Creek is content by this year this so if you're familiar with knowledge base you know you know this and so in this case this experiment for us is we did we worked with what we call simple question which mean that the answer is only one fact so this part of the experiments like how accurately can retrieve fact from the knowledge base the combination of fact is actually something that we working on it and not we working on it another task in this case was like a very large number of fact what can we do and so can-can the model trying learn to retrieve some fact okay and to do that actually we try to we don't do out code too much so basically we we need a training set and we didn't find a big one so we created a big big tragic set like 100,000 questions without our of this format so you have written by English in this case talk English speaker so an English like 100,000 question that are paired with like fact from the knowledge base and you can download those if you want to to use it for so there is a training set test set and varied set so how do we use the memory network in this case well small small as the same it's actually the same code so you have the question and then you have free base that is in this case the memory is basically all free base so and what you would like to do if if the video monitor was very smart is that when you ask the question is going to do the hop into the whole knowledge base and it's great to answer and find a good fact and answer because it's a bit too big 22 million in this case so what we have to do it that works you to use some some heuristics that are actually pretty common and I would like to go around without damages right now we can't so you have some string matching and an inverted index and we are going to say that we're only going to fetch to keep in the knowledge base the fact that are connected to some entities mentioning the question which is something pretty stand out and we have to do it otherwise the model account and when we have that so we have this radius memory and then we can use the scoring function that we learn to find a good fact and return the answer this case return Indians are super easy because it's always the objective in fact so actually we did this before the end-to-end system so we compare afterwards and twin system but we did it before and so we needed the supervision at the fact level which is in this case was not given all the time and so what we did is that we created a very simple heuristic for supervising so let's say you're just given train training set the question and the answer and so when the fact is not given if you need it and it can actually this kind of realistic a positive now because in some cases it can be a nice alternative so actually the end-to-end training and it can actually be more efficient so you have the question and then you have the answer and what you do that you're just great going to create like some hypotheses so what could be the supervision and this is like super simple you just take all the fact of the knowledge base that contain the answer in training you of it and that contain also one of the entities that you found in by three matching so like gollum is a character created by Tolkien the orbit has been written by Tolkien so you can have a bunch of those sometimes you have one it's actually good Co if you are multiple then you can use other your istic to basically only select one if you want to actually train the max and in this case we try several alternatives but a nice why need to find you have embeddings for all the questions on the fact so you can find which is the closest in your meeting space you can also use like tf-idf measures of you ladies and actually on another set of experiment not on this exact data set been another one we compared with end-to-end memory network and actually this this is more efficient for this case when the when you don't learn for you don't look for multiple facts it's actually a good idea you can be generalized to other settings so using that we did a bunch of experiment so basically wanted to compare we like master stone our method of semantic parsing like the work of Jeanette and Berlin for instance and we show that we are competitive using something like I just should and there's also something i would like to show you in this side which i think is very interesting for the old general question answering problem so we have like here three columns because they are three datasets so super question is the one we created and to ever get against other well so compared with web question which is another data set create that my special young slab and much smaller and so what we leak is that the little found the training source influence so basically what does it gives if I train on web question and test and simple question if I train on simple question and testable question extra all the combination and basically it's it's it's pretty interesting it's like the training set of web questions like 3,000 question I think so it's very small so you can send here we have like 80,000 question you could say well we could just get rid of them it may don't change much and actually the performance you lose like 10 points it's like it's like dramatic okay and of course the other way if you just ran on simple question you intestines on on web question which is small and you test on the bigger one you lose also 15 points because and so basically all this shows like the training set are too small but it can operate from to train and basically the coverage even though if you have 100,000 question you see level to need million fact and actually the coverage of this data set and this data set are just different and so you are if you're learning a system and you want to apply it here you're basically learning a system on another domain and you basically have to do like domain adaptation to go from one to the other of you ladies so it's very difficult to actually actually know what the capability of the system are doing because the the coverage of this kind of data set is actually is always first 15 minutes oh I have six minutes here yeah okay yeah so basically this set of experiment what did we learn and not what are the what are the lessons and what do we want to work on is like basically if you want to do like this logical question answering even if you have like some kind of memory network each side you still need engineering especially dealing with the knowledge base trying to find the entities right now we come to without that this is issue about the training data we would like to be able to learn to do it but actually you need a lot because the coverage is actually very important it doesn't transfer very well and then there is this part which is also compared to a link to this one is actually a ticket resting is that in this case basically the engineering is going to sub sub sub select a set of so it's not learned so we basically I was a first first stage which is like completely automatic and it would actually be interesting to have also learning in this case don't know how so the second experiment and and the last one because I don't have enough time it's about another data set we released about children book understanding so it was really supposed to be like closer from what I presented about the baby task and we did that with an intern this summer so we went into Gutenberg book with our open domain and then we created a tie set by looking at twenty sentences actually showed ten here between the data set and taking the next one and removing one word from it and the goal of the of the system is basically to try to fill in the blank so it's not exactly question answering you can generate a lot like this and it's similar to that I said that smear is almost at the same time just before by a big man about like a news article and slightly different sitting and so the idea was here was like okay well we have the memory network he was in the story I give you stories and then you are going to show me how well you can work on real language because because so you take the story you can put it the memory and then you can do soft attention weaved into a memory network or the other tension i mean we compare and then of course you have the system to return you an answer which is the candidates supposed to fill in the blank here this is from Alice in Wonderland it's actually very hard the children books are not much easier than actually anything else so it was not may be a good idea to focus on that but anyway and so what were the question working on this project to actually so what was very nice that it's a problem which looks like the other one well actually is actually very very different as well and so one of the key question we ended up is how do you store and it is actually general question actually i think is good much beyond this work basically that what's nice with the memory network if the memory is clean enough ice well structure we have a process that that learns to basically operate there okay if you give it training set but so the question here for the baby test was easy you take one sentence at a time it's fine because once that is actually one idea so it's it's it's made for it but in this case when you are very long sentences like this and they can have like a lot of information inside how do you structure the memory becomes actually an interesting question so you can do like sentential version which is basically one sentence which is the same set you guys do maybe task so we tried it and you can have lexical which is like one world so basically you end up like if you huge memory and memory just one word and actually people show that it works quite well for language modeling natural language money and you can also do something in between which is like consider as a coalition style but just simply windows so you're going to specify window size and then you're going to to pass window across the whole text and then you're going to store just windows of like five or seven words in the in the memory and this is what you're going to to use to basically try to answer and what is interesting for the windows is that actually they can hello strong supervision the self supervision because you can say that you can see the window as one fact of a Down edge base and and basically each window if you keep only the window on the candidates because you're given the candidates but not even each window is actually associated with one candidate so you can see that as a fact and you can do the same idea you can say I'm training I'm going to have this story see that's going to say the positive supervision is actually the window that has sated with young sir because the answer or one of them and you have to select one of them using another heuristic and I'm going to use that for training so could actually do the same thing I want to mention that there is another work that actually use RNN to encode the memories and it's actually interesting you show some interesting results on the baby task and it's so i would go quickly on this table but it's actually a quadratic to look at because we compared a lot of methods on this and we also company depending on the type of world which is actually very interesting if you try to present to predict prepositions try to predict verbs such break nouns we've tried to predict like named a disease like the names who's winning what's interesting and we compare like humans so we asked you meant to actually do the task and they are roughly into the 80 h is something accuracy except for proposition where they're worse I think this is all English like for the earliest 20th century and actually the more used to like Facebook style English so they were confused I guess then you have like the leg standard language modeling approaches like with language models with cash extra some embedding model that don't use the New World Order just so it could be seen as memo network without the memory so and then lstm different kinds and using like attention use a convolutional attention on the story and then different kind of memory network the one using world do one using Windows the one using the full sentences and the one using the windows + the self supervision ok and I'm going to be quick but what is nice that for the standard language modeling case like the one where you predict the valve in the proposition that are very tight to the syntax the air HTM sorry the best which correspond to what has been shown most in language modeling can we sit actually there a similar actually better than human for preposition I don't know if we can say about that but it's actually pretty remarkable and and on the other end of the spectrum of course the performance drops when you go to an entity from 80 to like 40 because it's much other and it's really like text dependent and in this case the memory network is getting like better better results especially when the mem the window memory is used because you can actually have like more focus on what the different character doing and if use the self supervision actually the boost is huge so we can discuss actuator much to say about them and what I want to show in 61 of the challenge is actually the best memory network except for the if use the world memories you need a lot of hops because each memory is one world so if you want to have like an idea of the meaning of the phrase you have to go multiple hope to combine them together so it's seven hubs but for the all the other one only one up so does it mean that we can't use the up process with the memory at work or does it mean that the data set is not actually made the way it should yeah and so this is basically what i said as just to cook food ya self supervision versus and train training we have to understand better what's happening why don't we use multiple up why does it well why does it work better I think we have to work more on that and just something about just a comment I had the discussion just before the talked about multiple memory sauce because there of course understanding just by giving the story understanding the story is maybe too short and you need an idea of that the common sense and common knowledge so I just want to mention that we are also released another data set about dialogue and in this case so it's basically the same idea but you try to predict the next act in a dialogue and in this case it comes from reddit and in this case we come by two sorts of memories so the input is a sentence we try to predict the next one like a dialogue style and what do we have in the memory we have the previous act of the dialogue which would act as the story but we also have a knowledge base so which is supposed to give you an idea about the knowledge of the movie domain and so which is so we combine those into the memory and try to have a model that is able to use these two kinds of memories to basically unser and we understand it's this way combining different source of knowledge and different format might be actually a good idea thank you 