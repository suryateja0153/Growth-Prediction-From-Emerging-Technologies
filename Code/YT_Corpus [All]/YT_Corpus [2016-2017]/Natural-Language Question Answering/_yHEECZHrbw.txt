 hi in this project we explore question answering using dynamic memory networks from knowledge in natural language as an introduction question I think is a very difficult and lp task which requires reasoning currently in search engines a myriad of handmade features are used for question answering or structured the knowledge sources which both of them require a lot of human intervention and are not scalable to huge and raw and unseen data sources so the idea of using machine learning to train and system end to end to be able to question answers on on raw data seems very interesting and promising memory networks have shown interesting results in question asking tasks on a small dataset such as B API so in this project we will explore this option as you can see in the picture dynamic memory news four's are typically made of four modules input module question module episodic memory module and answer module input module basically in cause the input facts which we which we're gonna infer from the question answer into an encoded vectors and question module we'll do the same thing over the question which usually are gated recurrent unit episodic memory uses these encoding and to do inference over the data and feeds the result into the answer module which creates the answer in this project we only use the single word answer to make it easier for the model our dataset consists of this data set is from the Microsoft beings explanatory question answer data set which is a very high quality data set and filter to retouches we recreate the three data sets from the same data with different configuration the first set has 2,600 question answers first for training 371 for relation and 250 for test and also the data only includes the top 1,000 URLs with most question answers yeah and in dataset tube we have 14,000 of 100 for training 1904 validation and 2100 for testing lead into a training validation and test sets however the splitting is done differently than ds1 in the way that no question answers in validation and test or validation and training have any comment URLs this is to make sure that model is not overfitting and the dataset tree which is the same as it is to accept that the and only question answers are chosen which the answer has more than ten questions associated with it in terms of the baseline the our baseline is based on a very naive edge line which basically finds the most similar the window of input facts the question and similarity is defined as the sum of the inverse document furniture of the common force in the question and the window in the turret in fact and the highest occurs highest-scoring window is chosen if the answer exists every say that the method has found the answer we saw that it could achieve five percent accuracy however our PD window size of 500 memory net for an end-to-end was able to achieve 99% accuracy on training Terri personal validation and 90% on test as the figures show there is also on ds1 VAR h if 270 about 70% on percent on a validation however there could be cases that the question answers are common between a very similar between training and evaluate validation so if the model even is answering one answer is basically producing one answer for all the crucian associated to one input flag this could actually be falsely shown as high accuracy as you can see in the s2 on this the s3 we see that it's more about 20% and as we have claimed in terms of the finding I think one important issue is that the data is partially the issue is it's a major issue and we don't have much question answers per page and causes a model to overfit and not learn much input facts and much from the input facts we could partially improve that by having by choosing the answers which have more than ten questions however this still needs some improvement also the attention mechanism is not very efficient because it has to attend to hundreds of eggs per page and if you don't have any question answers that we were difficult in terms of the field reaction I think we need to add more creation answers we can extend that to multiple answers which are more human readable and and it's more eco in line with our vision and also improving the attention mechanics as you mentioned by somehow pre-filter in a mud pack or having a more efficient system thank you 