 This is our question generation system. To output N questions, we filter the sentence list to obtain sentences we want to analyze. We use RAKE to extract N keywords and only select sentences that contain a keyword. The three question types are Yes/No, Wh-question and choice question. First, do POS tagging for each sentence. We also tried the Stanford Dependency Parser but it was too slow. We first try generating Yes/No questions, because they lays foundation for other question types. We identify verbs in the main clause and remove the negate RB, such as n't and not. Then, use WordNet Lemmatizer to convert the verb into its base form. Next, map each VB/MD tagged verb phrase to a question header. Move the question header to the front and remove MD or VBZ VBP. Now, we have a Yes/No question. With WordNet antonyms, we can generate choice questions. We get the antonym of the verb/adjective after VBZ. Since we already have the restructured sentence from yes/no question, we can simply insert “or” and the antonym to create a choice question. For Wh-questions, we use Stanford NER tagger to classify noun phrases. However, we had to apply some fix on its output by doing BIO tagging and merging associated prepositions. We then map the 7 classes of NER tags to Wh-headers, remove the original tagged NP and insert the Wh-header in the front to output wh-questions. So far, all questions come from the main clause. However, subordinate/relative clauses may also lead to interesting questions. Tags such as IN, CC and Wh- are strong signals of such and they can be mapped with certain Wh-headers. Different conjunction phrases can have different targets. For subordinate and coordinate conjunctions, we usually want to ask about a sentence’s main clause. For a Wh-pronoun/adverb, we want to ask about the modified person, place or other entities. We then remove our subordinate clauses and generate a wh-question on our target clause or NP using the similar process as before. To reduce noises in our output questions, we (1) remove very short or empty questions, (2) use the language check tool to exclude ungrammatical questions, and (3) fix capitalization and punctuation issues. We maintain a good mix of question types and guarantee one sentiment question. Now let’s introduce the answering system. We have two inputs. The article and the questions. We parse the article into a sentence list, using nltk.tokenizer After that, we run Collections.counter on the sentence list to produce a word count. We map every sentence to a term frequency vector with length |V|, the vocabulary size. We also normalize the vectors where L2 norm = 1, to scale down the effect of words repeated too frequently. Next, reweight each vector by applying Tf-idf weighting, Giving a IDF frequency weighted matrix. As in assignment 8, less frequent words serve better as feature words. Now, we have our final output, a vector space model. Specifically, it’s a n by |V| matrix where |V| is the vocabulary size, and n is the number of sentences. Each row is the processed term vector of a sentence. We generate a query vector from a question in a similar fashion, applying both normalization and idf. The query vector also has length |V|. Finally, we output the best match sentence by taking cosine similarity between the query and all sentence vectors. We returning the sentence most similar to the query. After the preprocessing step, we determine the question type. A question is “either or” if it contains the word “or”. We extract the “A or B” phrase and return the option contained by our best-match sentence. A question is Yes/No type if it begins with words such as “Is” or “Will”. To answer a Yes/No question, tag each word in the question using NLTK tagger. We consider common nouns and cardinal numbers as critical words. We answer yes if and only if all critical words in the question appear in the best match sentence. A question is a sentimental question if it contains the words “positive”, “negative”, or “neutral”. Johnathan will introduce how we answer sentimental questions later. For all other questions, we just output the best match sentence. We previously tried information extraction to produce more succinct answers, but decided not to include this feature, due to less-than-ideal results. Our sentiment analysis component surrounds around the core analysis model, which is implemented using two naive bayes models. This graph shows the flows for sentiment analysis related functions. The left chart shows how our program answers a sentence’s positivity. The analysis model uses unigram and bigram naive bayes models trained on 20 thousand labeled sentences. The training data comes from the conversation based UMichigan Kaggle dataset, and Cornell movie review dataset. Our model uses the unigram model first. If it gives a fairly confident positive or negative score, then it outputs the answer directly. Else, our model uses bigram model to calculate a weighted average, since unigram NB performs more accurately on our dataset than does bigram model, given our dataset’s different nature from wikipedia and limited amount of training data. As for the question generation process, our model takes a list of sentences that it could ask questions on. It uses the analysis model to pick the most positive sentence among all the sentences. Then, it asks if the reference to the sentence subject is positive, neutral, or negative. 