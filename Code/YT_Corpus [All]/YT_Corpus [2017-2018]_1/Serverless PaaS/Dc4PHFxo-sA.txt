 [Music] so oh one thing right at the start I'm not here to sell you no ops or even Google is your sre this is based on my another's experience running services at Google and with conversations with longtime App Engine customers great DevOps engineers or s Ari's if you prefer provide a lot of leverage for moving your engineering organization forward with continuous delivery and accelerating your ability to reliably deliver changes if you don't have anyone actively doing the DevOps role your most experienced developers end up getting pulled into the firefighting efforts involved in running your system so one way or another you're going to end up doing ops and DevOps well you may not do end up doing DevOps you may end up just doing operations we'll talk a little bit about the difference in in a moment so every software system that has customers is going to need care and feeding and maintenance and experience devoxx practitioners understand how to do that more efficiently so DevOps are great I'm not saying throw them out the window I'm not saying don't have them I'm saying let's make them even better common ratio for DevOps to feature software engineers is something on the order of five maybe ten to one for some reasons that I'll talk about in this talk App Engine covers a lot of your DevOps basics for free letting you spend more time in your application that means your DevOps engineers can get more stuff done they can move your organization faster or if you want your DevOps talent can also spend some time improving your application and writing feature code rather than chasing around and putting out fires and fixing your infrastructure because your infrastructure will be beautiful and sparkling I've spent nine of the last twelve years on call at Google half of that was in sre the other half was as a developer supporting systems that I'd written that were not yet accepted by sre because they were new and not critical enough if you went to Ben trainers fireside chat earlier he and Ben much talked about how at Google sre teams come in and focus on the services that are already important and make them better and it's not a given that you have an SRE team at all that's a little different in some organizations an app engine incorporates a lot of the stuff that I've learned about running reliable services it's one of the things that attracted me to working on App Engine for scaling both to millions of queries per second and running hundreds of services with a single small team switching app engine isn't gonna double your productivity I don't think I could promise you anything that would do that but it can give you a measurable improvement in how much time you spend on feature development rather than on the operations stuff that has to get done since we're talking about spending time on development I'm going to talk first about why you might be spending a lot of time on operations so Google s3 has a concept called toil and toil is work that you do to need to do today if it'll come back tomorrow and you'll have to do it again it's not optional you need it in order to keep your service running additionally toil doesn't necessarily require a lot of judgment or creative problem-solving but it has to be done right or your system will be down or broken or slow you might recognize some some of these sorts of things as maintenance operation procedures or other best practices documents for handling various issues that come up with a service now mops can be a good idea for operations that happen infrequently like a manual database failover if you do that once or twice a year but if you're using the same mop once or twice a week that's probably Thailand you should be looking at automating it rather than just streamlining the work that people do and the other the antidote to toil like this is automation so if you can recognize and understand what needs to happen you can find ways to write software that works around or solves the problem so that humans can get back to being creative thinking coding designing listening to customers instead of fighting your infrastructure now there's a whole lot of data center automation software out there already in order to talk about what makes App Engine so useful and special I'm gonna start by talking about some of the other options for data center automation software so at the base of the stack we have what I call fleet management solutions these solutions are targeted managing a fleet hundreds tens thousands of individual heterogeneous servers they're all different or there's enough differences between them that you need to pay attention to those differences they may be running on physical Hardware they may be running on virtual Hardware it might be a mix oftentimes you're assigning workloads to individual servers statically a single server is for a single workload and you don't necessarily automatically scale or if you automatically scale it's a little bit of a manual process using something like a managed instance group or I forget what the Amazon equivalent is load balance group there's a lot of code in these fleet management tools for rolling out and managing operating system and library level patches because every machine is set is different it has its own kernel it has its own installed libraries maybe some of them are red hat maybe some of them are Debian maybe you've got some sauce there or windows and in addition to all of that it also rolls out your application code and manages it but that's just one of several jobs that it's got to do now the great thing about these things is they're really adaptable they're really flexible and they let you automate your software deployments to your VMs and your physical machines and with integration work you can implement health checking and monitoring repairs and things like that so that's actually pretty good in terms of replacing a lot of your toilet the next level of abstraction that you've probably heard about a lot at this conference is container orchestration so at this layer of the stack you're using something like kubernetes or meso s-- or docker and you don't worry so much about managing the base layer of operating systems that's all unified and pretty abstract and instead you're managing resource provisioning by describing your application in terms of resource requirements it has how much RAM how much CPU what network ports what storage you need and then the orchestration layer assigns those resources to physical or virtual nodes automatically and can perform failover and pooling and resource grouping so resource assignment container orchestration can be fairly dynamic and can react to those failures or the load changes by rebalancing the cluster maybe even moving a workload from one machine to another because you've discovered that one machine is overloaded which is great and introduces a whole host of new things new needs that you never realized you had a like service discovery you need to keep track of where everything's running and clearer abstraction boundaries between systems because you need to actually deploy and scale them differently so moving up in this stack moving up in the stack this way frees you from a lot of the OS patch management and deployment toil because you can describe things in a high-level and can give you resource efficiency games by bin packing multiple containers onto the same physical node without managing individual VMs and operating systems additionally because resources are aggregated in this way this can also actually simplify your hardware build-out because you can have a more uniform fleet and have the software be responsible for bin packing things onto smaller machines as needed and so containers are a great tool and container orchestration definitely simplifies packaging and resource sharing but I'm here to talk about another layer above that that removes even more toil and gives you even more efficiency benefits and take your application management to the next level so App Engine is one example of a platform as a service platform service is designed to understand and cater to a particular type of application so you can see here we've App Engine and Heroku and Cloud Foundry that are all in a similar business of handling stateless Web Services but things like lambda lambda and cloud functions are another platform as a service that's targeting a slightly different market where web services aren't really their core thing it's run a little piece of code and spin up very and down very quickly in this serverless environment and then you can see data flow as well is another platform for handling your data processing you wouldn't use it to serve a web application but it's a great way to organize your data management and you're like ETL pipelines stateless Web Services which is what I'm going to focus on for the rest of the talk covers a lot of ground from static or dynamic websites to api's and shim api's which can aggregate content from a number of different backends and provide a seamless content feed to mobile clients or to other api consumers stateless Web Services while they cover a lot of ground they also have a lot of things in common so that gives us the ability to provide out-of-the-box experiences that you'd have to do a lot of integration on some of the lower layers to get so I'll show you an example in just a moment of what the full set up would look like but there's a there's a recipe out there that's pretty easy to find to scale an application using those fleet management solutions it involves Nagios monitoring triggering chef to do a deployment based on high CPU usage so you find your Nagios instance you write you put some stuff in there it has to coordinate with some stuff that's in chef and you own all those integration pieces App Engine provides these as scaling knobs on your application without needing to for you to explicitly say hey here's my monitoring rule and that means do this in terms of deploying another application note App Engine knows how to do that because you're a stateless web service and because your app is driven by HTTP requests we can also give you automatic latency and request load monitoring we can also trace your log statements back to the particular URL handler and highlight problem areas in your application these are some of the tools that internally we build an SRE at Google and by providing this platform as a service we can build them for you in a scalable way as opposed to a bunch of one-off things for individual services so this is an example of how you could build your own comparable platform to some of the stuff that I've talked about that App Engine does there's actually a few more things that I didn't feel like putting in here because it's already a lot of stuff around your application so we'd start with your application you start developing let's say you're using Java and you're using jetty it's great on your desktop but when you want to go to production you'll need to set up auto scaling and multiple copies of your application and then you'll need to put a load balancer in front like nginx and then to debug your application you'll set up the e lk stack so now you've got elasticsearch and log stash and cabana that you're running in addition to your application and nginx you can send a Geo's for monitoring so now you've got some visibility into what's going on and maybe Nagios is monitoring your LK stack as well as nginx in your application and you're getting all these metrics and then you discover that you're suddenly running 30 machines and so why don't we go and take saltstack and use that to actually organize and manage your rollouts congratulations now you've got 31 machines and furthermore I've given you one set of examples here why don't you replace nginx with Envoy or there's a bunch of different log solutions so when you put all the combinations together there's probably a hundred different ways to build this same system is everyone in your organization making the same choices for these are you building you know LK over here and you're using Splunk over there standardizing on one set of solutions makes it a lot easier for people to move from one part of the organization to another and understand what tools are available platform-as-a-service is one way of standardizing those that set of tools so that everyone has a common baseline that they're working from and they don't need to learn a new stack of systems when they go to switch projects so assuming that you're buying what I've got to say so far and that you're interested in how App Engine fits into this I'm gonna explain a little bit about how App Engine is organized before I dive into some of the recipes and features you don't need to know this stuff to start using App Engine and I've met people who spent years building monolithic applications perfectly happy with App Engine and don't necessarily understand or worry about all of this stuff but if you're looking to grow and organize a team and scale out your team more efficiently it can be useful to understand how applications and services and versions all fit together so App Engine subdivides each project which is an application which in App Engine we call it the top level and application in two independent services which can be pushed and scaled separately so you can think of services like micro services they each do a separate thing your micro services can be very small and have a single URL your micro services can be not so micro and can be your entire application if you want each service in turn is made up of a set of deployed versions so you do a deployment and that creates a new version and at the service level you can choose to shift traffic forward and backwards between versions so one of the things that you're deciding when you decide how to split your service up into micro services or split your application up into services is which things should be pushed together which things should be pushed separately if you have three API endpoints that I'll touch the same database and need to use the same schema maybe that's all one service if you have another endpoint that only talks to videos and and cloud storage maybe that should be a separate service maybe you'd rather have it all be bundled together because that's easier for you to manage in your CI system that's up to you within each version App Engine automatically creates creates and destroys census of each version in response to load so every time you write code on App Engine you're writing code for horizontal scaling automatically and we can expand one version to be running on thousands of nodes or scale it down to zero on standard or one on flow or one on the flexible environment if it's not receiving much traffic so moving on from the high-level theory I'm going to show you some examples of how App Engine lets you run your services faster and smoother and apply less DevOps effort to keeping everything running some of you may have seen this before if you're into sre type topics this is Dickerson's hierarchy of reliability reliability is one of the main goals of DevOps reliability of delivery reliability of the service in terms of uptime reliability of process in making sure that things are repeatable and this hierarchy was proposed by Mikey Dickerson who was the architect of the healthcare.gov rescue and a former Googler sorry I actually worked with Mikey and adds essary and this formulation based on his experience makes a lot of sense to me and it's based on Maslow's hierarchy of needs if you're a psychology person if you haven't seen his talk in health care of its there's several versions on YouTube and it's really interesting to hear how he was able to go into a very broken system and find ways to bring that up the hierarchy and introduce stability and reliability into the whole process and it's a great essary story App Engine can help reduce the amount of effort for getting all of these things but I'm going to particularly focus on the ones I've highlighted in blue I realized that's a lot of them but I thought that more in depth would be a little better than covering post-mortems in development so for each of these layers I'm going to be showing you both the built-in functionality and some additional things that you can do to take them to the next level and be more efficient than just the tools that we ship you so starting with monitoring I mentioned earlier App Engine is a platform as-a-service so we already have top level metrics on your application request latency requests per second queue times number of instances and all of that sort of thing can be tracked both across the application as a whole and we can actually also track things like latency and request times on specific URLs if you're interested and the best part is you don't need to add explicit instrumentation into your code since we are offering you the load balancer we can integrate all that monitoring into the load balancing layer and we integrate that with stack driver monitoring and inspection so you can also export specific application metrics directly from your code and align them with the same metrics that are coming from your load balancer if you don't want a direct monitoring dependency in your code you're worried about either the latency or the stability you can configure stack driver to export custom metrics from your logs and those can be aligned with your with a automatically collected instance data this allows you to create a simple single timeline with business metrics like conversion rate and code changes such as latency improvements you can then use this fact driver api's to aggregate and export the data and run regression analysis to figure out things like the effect of latency on conversion rates so putting all your data in one place and giving you powerful API is to extract it let you pull out more value than you might if you were putting everything separately and you had some things in Nagios and some things later on in your ETL pipeline speaking of things like ETL pipelines though the stature of our logging api is really handy for digging into issues the log viewer is good for getting a quick view into what's going on but exporting logs into bigquery is amazing how many of you have used bigquery before you might know what I'm talking about so I can use sequel queries like this to dig in and associate error messages with specific URLs for a specific customer even if I'm getting hundreds or thousands of requests per second you know and I have half a million customers you know I have I have this one customer issue and they say you know this stuff all seems to be wonky but replies are particularly bad so I can dig in and find out that's true but they're not doing very many replies I can also potentially dig in and get the stack traces I can get any other info or warning error messages and you can really pull out a lot of data by putting things in sequel and then throwing bigquery or putting things in bigquery and then using sequel to extract out the information you really need as an additional monitoring tool stackdriver trace is pretty cool for investigating the calls made by a particular request or your back-end connections and it's automatically enabled on standard so you can just try it out and see what it tells you and on flex or on-premise there's a library that you can link in that will give you the same information and I believe it also integrates with some open source tracing tools don't remember the name so incident response everyone's favorite favorite thing to do but when things go wrong you really want to get things back working ASAP you you've got that visceral my customers are angry at me feeling as necessary I had to write a lot of post mortems that basically concluded rollback would have fixed this hours earlier than pushing new code so by now my first instinct whenever I get a page and I am on call not right now but for the app engine management API I am on that on-call rotation my first impact is my first instinct is to find what changed and roll it back even if I don't see any correlation and see if things get better while I go and dig in further so App Engine because we have those multiple deployed versions gives you a quick way to roll back to earlier versions it's a single action it takes less than ten seconds to do and there's also an audit log so you can see what's actually been changed recently which gives you helps you figure out which thing you should roll back so as I mentioned my team actually builds the management api's and we talked when we were building the traffic splitting controls about building an app that just lets you roll back and you can see that my UX skills are probably not the ones you'd want building this so fortunately we have a cloud console mobile app that already lets you do this so don't worry you won't have to suffer through my UI now obviously rollback is not going to fix everything and there's some failures that App Engine will never fix for you but if you look at post-mortems in your organization you'll probably find something similar to Google which is that code and configuration pushes are one of the largest sources of outages it makes sense you're deliberately changing something you think you know what you're doing well you know what you were doing 90% of the time when the other 10% doesn't go so well but you tested the common happy case so rollback solves those problems it doesn't solve back-end services down it doesn't solve user data corruption rollback sometimes will help you there by changing your query back pattern back to what it was before or not corrupting any more user data but you'll still have to fix those yourself and it's still up to you to make the decisions as to when to rollback and how to use the tools to fix things and if you're using App Engine consistently the other benefit is that all your tools look the same and so your most experienced people from one project can help out with another project when everything's on fire so outages not so much fun pushing code seeing your code go live a lot more fun so test and release much better incident management got into the incident response or you don't have any customers but App Engine also has a lot of tools for testing and releases and the multiple versions is the start of that but not the end of the story so you can see here in the production environment we're doing a traffic shift from our current release to the next release and we're canarian it with 10% of traffic so we've got some confidence that doesn't break everything but we're not putting all our users at risk yet so if something goes bad for 1% of our users on the new version it's actually a point one percent outage at that moment so I've still got three nines it's not too bad additionally you can see that we have a test environment that has a whole bunch of other versions running so everybody can have their own version for development which my experience has been drastically accelerates things versus hot seating on one pre production environment so what what does App Engine not cover for testing and release well you'll still need to run your own tests you'll need to write them yourself and you still need a your own CI system and starting them you need to choose your source release and start the deployment yourself App Engine has integrations with tools like Jenkins Travis and circle CI there's a lot of other systems out there a lot of them do have app engine integrations and I've actually built my own little toy CI system at one point just to play around at the management API and most of my time was spent figuring out how to package up my code upload it to cloud storage and then the actual API call at the end is pretty simple so if you're thinking about building your own it's not too scary there's some work you need to do but it's not too scary it's also easy to deploy the same code you can see here to the test environment as to production with a slightly different configuration I know I've seen a couple different patterns for doing this one common one is to have a set of settings in datastore or cloud storage or or potentially even cloud sequel where you connect to an address that's based on your instance name or your app name or something like that which tells you where to find everything else the other pattern which I slightly prefer is to actually define your configuration settings in code and use a configuration class which you select which one of these to use and the benefit of that is that all your configuration changes end up in source control automatically you don't wonder hey who changed this data store setting or you know how did this config file get uploaded to GCS so I'm going to show you an example of how that works so you can see here I define a base configuration class that could be development environment settings that could just be you know just in this case it's none if you're using a base config you are probably doing it wrong you should be getting one of the other ones and it makes it more obvious when something goes wrong then you can see I defined a production configuration and then this magic config basically chooses one of the other configuration classes and extracts values from it if there isn't an overriding environment variable setting and then you can see my top-level config object is an eval that picks one of these classes and defaults to development so that if someone forgets to actually define the environment variable they don't start running against production referencing this and source control you can get code reviews on when you're doing it you can you can actually test your values and make sure that you don't end up with conflicting values as your configuration gets more complex and also when you make can make configuration changes you can roll them out with traffic splitting the same way that you roll out your code changes adding a new back-end maybe you type at it wrong it's good to have that be a sort of a slow process rather than a big bang switchover so so far we talked about the basics of app development but now that we've talked about keeping your head above water capacity planning is one of those things that you always sort of want to get to in the future but App Engine actually lets you get to it never and that's okay because App Engine has automatic scaling it will scale down to zero and it will scale out to thousands of instances automatically and you don't need to do capacity planning and reservations and decide do I gonna need this capacity for year or three years or even do I need to request a quota increase app engine can automatically scale for spikey events with low latency past past 20000 QPS and four small spikes where small is somewhat relative you don't even need to contact Google if you're thinking about launching the next Pokemon go we might like a heads up but if you're not watching Pokemon go and you're launching something that's just going a little bit viral then shouldn't need to talk to Google at all so as I mentioned the standard runtime support scale to zero which makes standard runtimes great for keeping around development tools and dashboards and stuff that's not used all the time but every so often you want to be able to pull it out and you want it to be running immediately standard is also much faster for deployment than flex so if you're trying to iterate on your development cycle you may find that deploying the standard is faster the Flex environment requires a VM to be running in order to collect all of the metrics needed for scaling and for scaling and to determine how much load and so forth is going on but you can stop the versions that you're not using and stop getting built for them so we have a lot of customers who are using App Engine with things like data store or BigTable whoo scale out very wide on their App Engine side and if you're using something that Cloud sequel you'll have to provision your cloud sequel instances so if you can use a horizontally scalable system for your storage you might not have to do any capacity planning at all stuff to do budget planning I've run some demos where I wish I'd done budget planning beforehand so the cloud console which you can see here and the stack driver UI which I'll show you later cover a lot of the basic user experience for managing your app and another thing to note is the cloud console is building on Google's public API so you can also extract all this data to your own consoles if you're building your own view of the world so before I talked in more detail I wanted to highlight a couple of the cool things we've got built in on the left hand side you can see the cloud trace suggestions it's automatically identified several several issues with datastore you probably can't see it here but it says use of offset in datastore queries which is an anti-pattern because when you use an offset datastore still has to read through the first things it just throws them away to hand you the next stuff used query cursors instead many successive datastore put and get calls so by doing these separately my app is paying a latency penalty each time I do a put or I get whereas if I did multiple at the same time or parallelized them I would have much lower latency overall because we understand your architecture and how your back-end our pcs relate to your your front-end URL requests we can actually determine that things like the consecutive put and get calls are coming from the same handler as opposed to different handlers that happen to have similar timing and so we can suggest these sorts of improvements on the right hand side you can see the error reporting has picked up a bunch of 500s that we've served to customers over the last day and highlighted particular issues that we should probably be looking at and fixing again this is stuff that we've automatically built in so you could obviously run this stuff yourself and go through the logs yourself but we've already given you a basic version that will often meet your needs but sometimes you know your workflow better than what we do and so I'm going to talk a little bit about how you can make an even better UX for your team that maybe isn't the same as the you excellent the next team would build so with all these api's there's lots of great ways that you can integrate them together some of them mostly using cloud platform technologies some of them using just a few but using some common patterns to make it easier to debug production so and the other thing is of course if you find these useful do them if you don't if that sounds like a terrible idea to you there's nothing that says everyone must do things the same way in production your team should be consistent but you don't have to be consistent with the next with the team in another company so for us we have a mailing list where we send push announcements and status updates about production in our push instructions we used to have a line that said send me on the mailing list about the push there's two problems with this the first one is that the instruction didn't actually tell you what to say so every person had to interpret that individually and the second one was that a person had to do this and it wasn't just a robot automatically sending something like a summary or hey just pushes started so a lot of you probably use slack or IRC rather than email for keeping a track of what's going on in production you can automatically update slack when their production changes such as traffic splits version deployments or permission changes by setting up a logs filter on the audit log and using that to publish just those audit logs to pub/sub so you can say just users added to a Akal or just traffic splits or new versions and traffic splits but don't tell me about deleting versions and cloud functions can pull things off of that queue and automatically update a slack topic telling you everything that's going on in production did you care about it and then all the rest of the stuff you don't have to go and look at the audit log it's already in the tool that you're already looking at every day you probably also track which commits fix a particular issue but if you've run production systems for a while there's a difference between the fix is committed to source control and the fix is actually running in production it's a very important distinction and sometimes when you have to rollback you want to know what's gonna break when I roll so if you have a consistent way of tracking which CLS are going into a particular release and naming your releases in a way that's coherent you can update your JIRA or whatever using for issue tracking to indicate which things will be broken when you roll back to a particular version and so your rollback is great but sometimes there's a bad thing that will happen at the same time so you need to do some mitigating steps or something like that and this gives you some operational awareness of exactly what's going on and this last integration is a really cool one that I've seen some teams build at Google but everyone seems to have to build their own solution and a lot of this is because what makes it release really bad is not Universal for some people error rate increase for some people it's latency there you may have other metrics but you can have a staff driver alert if you assume you can describe what makes a bad release you can have a stack driver alert that detects that there's a problem like an error rate increase and sends a webhook or an SMS notification to your control stack to automatically rollback that release before a human even has to get involved you may still want to tell a human that this is happening and you may even want to page them and if you're building such a system you probably also want a Deadman switch so that you're not fighting the automation if the release really needs to go out or you really need to make some other changes so I've talked a whole bunch of talk about ways that you can make things more more efficient now I'm gonna do a demo because my understanding is that demos are part of the reason that people come and watch this stuff see whether demo bursts into flames or not so I'm gonna show how you can take off the shelf software not designed for App Engine and run it on App Engine and get some of those App Engine benefits in particular I'm going to use JIRA which is a closed source issue and project tracker typically you run it on a singleton VM and since its close source I really nope don't okay you can cut over now but I still have a little bit more to talk about before definitely don't cut ever now I actually have a couple more slides too so I can't really make it any software changes to make it to to the JIRA software itself to make it run better on App Engine I do use the Flex environment here but I thought that it would be interesting to show you a real application being put on App Engine that wasn't designed that way and JIRA seemed like hey this is something that probably a lot of you have seen or use so my architecture looks pretty simple my developers talk to JIRA running it on App Engine I'm using cloud sequel as the backend database because I don't I'm doing this to not manage servers so I'm not gonna set up my own server my own Oracle or Postgres or something like that I'm gonna use what Google has and I'm gonna have I'm using flex because I'm gonna need to run a little additional code inside the docker container that's not the main web application process and one of the great things about the Flex environment if you're trying to do something like this is that in addition to Java and node in PHP and all those other lovely runtimes flex can run raw docker containers so I start with a container that's built on the Google Java Runtime and then I do a bunch of work to install JIRA and the my sequel connector and so forth I set it to be on the correct port and then you can see that I copy in a wrapper script that does all the fixes I need and a core database config although that wrapper actually takes an environment variable and puts it into the DB config at runtime so that I can change which cloud sequel instance I'm talking to so one interesting feature of JIRA is that it's got this database for all your issues but it actually also built a local on disk Lucena index in order to be able to search all of them so that's kind of big stateful local cash and we're talking about stateless Web Services fortunately JIRA has a rien dexing API so I can use that to at startup and periodically after that reindex my database in order to get JIRA to actually have search working and do things like which issues are assigned to me actually shows up I also use basic scaling or manual scaling to keep exactly one instance running per version so that there's less chance of updates going to one server and then needing to call reindex on another if you were scaling out into your really large that might be a problem most of the time this if you're getting 5 QP s to your JIRA instance something's probably wrong so now if you could switch over so I'm running this off of my Chromebook since that's my primary development machine but this means that I need some place for I can run the raw VM commands so my typical pattern is to spin up a worker vm in the project that I'm using and it makes it really easy to install tools for demos and things like that it also means that I won't have any fancy pretty editors and instead I'll be mostly using tools like VI and less ok here we go we have SSH let's see can everyone read that great ok so before this talk I actually built a new JIRA image based on the fresh out of the oven latest 732 release from March first it was great that they shipped a release because that meant that I wasn't just installing the same software again because I started working on this demo long before they shipped it so you can see in the container registry I have a base image and a JIRA 732 image now I'm going to go back and here is my command - I'll tell you a little bit about what this is doing I don't need help either so I'm deploying a version named 732 based on this JIRA image and an app dot llamo that I have in the same directory the app DML little oops so I say it's a custom run it's a custom run time running on flex I give it a instance tag just in case I needed special firewall rules it turns out I don't I assign two CPUs and two gigs of ram I turn off health checks because JIRA doesn't know to answer 200 at the app engine health check URL so I just say don't worry about that and I have stack driver monitoring do that instead and you can see at the bottom I've set manuals manual scaling to one instance so that I don't end up with multiple copies each with their own big fat local cache the last two flags say whatever versions running now don't stop that the default for flex to save money and confusion is that your old versions are shut down and your new when you deploy a new version so that you don't end up paying for you know half dozen of VMs as you're developing the know promote flag says I'm deploying this new version but please don't make it live yet I'll show you later on in the demo how we make it live obviously if you don't say no promote we could just have it happen as soon as it's ready so as I mentioned the Flex environment can take a few minutes to start the VMS configure the load balancing and so forth and your also takes a few minutes to start from a fresh install so while that's happening I'm going to show you I'm gonna say yes because otherwise we would not get any further and now I am going to show you what we have running so you can see we've got Jerry it looks pretty normal if I log in you can see oh look I've got a bunch of issues assigned to me which is how I'm gonna keep track of what stuff I'm supposed to show you so we're going to start by showing off the log explore some of you've probably already seen this and so by default so this is from the last time I ran it by default this will show you the standard out and stare standard error plus the nginx server that's in front of your application but you can also pull in things like vm events or the syslog from the vm and let's get the whole pile of stuff yeah you can see that we just have a new audit log event here that says that we are starting the operation will tell us that the operation ID and then in the reefs in the proto payload it will tell you this is an audit log and that this is called by service data just down off the pot on the screen this was a create version request and here is all the information about the version so you can see it's flex it's running on the VM manual scaling and some other stuff that we set here's our environment variable so let's go back and ok so we've talked a little bit about the logs Explorer so the next thing I'm going to talk about is how to add log metrics for data so we've got this reindex or reenact singh piece that i'm doing let's take a look and how we can actually keep track of how often we're calling re-indexed and is that working the way we expect so this is the log based metric section of stackdriver you can see it's right next to the log Explorer or the log viewer and in both cases you can use this create metric link to create a new metric that matches a pattern on your application and this is the currently running version and you can see there's a lot of well here's a kernel message from the kernel that's booting up but there's also a lot of health checks and some docker information and stuff like that so I'm actually going to show you the I've already created this metric and you can see if we go in and edit it that it's actually a query against the logs API you can use this same type of query to export stuff to bigquery or to pub/sub and you can see that here we're looking at standard out and standard error for a message that says re-indexing is 100% complete and we have a bunch of these messages from 9 p.m. and from last night when I was working on it so so that's how you set up these metrics let's take a little take a look at what you can do with them after you've set them out so I've added them to the stack driver dashboard and there's no data over the last 1 hour but if we look over the last day we can see that I started some PM's and they did reindex this at that time you can also see the request rate and that I was messing around a bunch with the JR instance earlier today and that at one point the latency was really high but all the rest of the time it was low and here's our CPU usage and it looks like we're really not using two CPUs and probably one would have been sufficient so we figured out a little bit about how to use log metrics and now I'm going to talk about the two different interfaces that you've got for looking at how your app works so one of them is the standard cloud console so one of them is the standard cloud console where you can see the basic graphs and there's a pulldown where you can see information for a particular version and then things like CPU and utilization and latency and so forth the other option is to build a stack driver custom dashboard the advantage of the custom dashboards is that you can put together lots of your data into one place and you can put together different dashboards for specific issues so maybe sometimes you have CPU usage go crazy maybe you're exporting the number of back-end database connections and that goes wild you can have different dashboards for different purposes so let's see how that deployment is going so the deployment is finished I'm going to go back over to JIRA and you can see that right now we are running 731 so if we close if we go over to App Engine and look at the versions that are available we have 731 or we have 732 we have one called base and let's not use this one called bad I'm going to this is an old habit the migrated traffic button is actually a better choice but I'm going to send a hundred percent of the traffic to the new new instance if this was a public site I'd probably start at five or ten percent this is an internal tool so I'm just going to swing all the traffic over to the new site and now let's see what happens if we I don't know how much this gives cash so we're going to reload the page and it's gonna ask me to log in again because it's a new jura instance and it has a different cookie cache and the last pass is not going to help me and so you can see we're back on the same page we were before but now we are running seven three two so that's the end of my demo and when you're ready you can switch back to the other that's the end of my talk [Music] [Music] 