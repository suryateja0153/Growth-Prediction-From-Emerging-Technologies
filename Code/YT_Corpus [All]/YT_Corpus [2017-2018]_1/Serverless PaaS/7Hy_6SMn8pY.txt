 [MUSIC PLAYING] PAUL NEWSON: I'll introduce myself a little bit. As the slide says, my name's Paul Newson. I go by the title SRE Advocate. I feel the need to explain that because to my knowledge, I'm the first person to ever call themselves an SRE Advocate. We have this thing at Google called the developer advocate which is somebody, an engineer who works in developer relations who goes out and talks to developers and interfaces with the developer community. I like to do that same thing but I like to do it with SRE and DevOps folks. And I recently did a mission control rotation. That's a thing at Google where you get to embed yourself with an SRE team and walk in their shoes and carry the pager and live an SRE's life for between 6 and 12 months. It was a fantastic experience. I was really fortunate to have the chance to do it. And now I'm here to hopefully share with you some of the things I learned along the way about designing reliable systems with Cloud infrastructure. So if we're going to talk about designing reliable systems, I do want to get really precise about what we mean by reliable. So when you look up the word reliable, what you get is this. There's some good words in here. Consistently good in quality or performance, sounds kind of like what we're going for. But I actually think the more insightful phrase here is able to be trusted. Because what it comes down to, if your users trust your system, then your system is reliable. If you have earned that trust, then your system is reliable. If your users don't trust your service, then your service is not reliable. And that's going to be different depending on your service. So let's take this system for example. The reliability required for this particular system for those users looking at those windows at the top of the space shuttle is very, very high for them to trust the system. Compare that with, for example, an entertainment service. Users still want to trust this system and reliability is still very important, but I think it's pretty clear that the level of reliability required by a life safety critical system and the level of reliability required by an entertainment system to earn the users' trust is different. And that's because the consequence of a brief temporary outage of the system is very different. In one case, you don't get to play your game for a few minutes. In another case, it may endanger somebody's life. So, we've talked a bit about trust. Your goal is to get your users to trust your service. Trust is context dependent. It's different for life safety versus entertainment versus financial versus retail versus whatever it is your systems do. And the last point I want to make about trust is that it's really hard to measure. In fact, I don't know how to measure trust. Maybe there's a psychologist in the audience today who knows how to measure trust and can approach me after the session and tell me all about it and that would be a great conversation I'd love to have. But the engineer currently standing on the stage doesn't know how to measure trust. Moreover, I have no idea how you would measure trust in a real time way to see if your service is currently delivering on that goal of user trust. So we need to find a proxy. We need to find something we know how to measure which is a proxy for that user's trust. And this brings us to the concept of a service level indicator. The SRE book says, a carefully defined quantitative measure of some aspect of the level of service that is provided. So basically, it's something you can measure about your service. In this particular example, I've got a graph here. It's showing an error percentage. So this particular service that I pulled this graph from is a fairly straightforward HTTP request response kind of service. And this graph is showing us the percentage of requests which resulted in a 500 class error. It is very unlikely you can read the scale on the left, but it's not that important. The important thing is you look at these spikes on the left of the graph and you're like wow, those look pretty scary. Are they a problem? Maybe, maybe not. It really depends on your service. So what transforms a service level indicator into a service level objective is adding a goal. Now this is where we have to look at the things we know how to measure and use the insight that we have about the services we offer to decide what is a threshold that makes sense for this service level indicator. So again, let's go back to this example where we have this HTTP request response pattern. And in this particular case, let's say this service is something where every HTTP request can easily be retried if it fails. In that case three nines might be perfectly adequate to-- a perfectly adequate threshold to set as your service level objective. Because if someone gets a 500 class response, they can retry the request and they'll get a success. And the client will probably do it for them and the user will never know that a 500 was served. In other words, below 99.9% success rate, user trust is not eroded. Now this is not a universal thing. It depends on the service. And this is where we get to apply our user empathy and our engineering judgment to choosing these objectives. So in this case-- I know it's very difficult to read this slide, but those spikes don't even come close to 0.1%. In fact, they're one order of magnitude below that. So these spikes in this case, as scary as they may look, are no problem at all. The last point I'd like to make before moving on from this is to talk about service level agreements. A lot of people use the term service level objective and service level agreement interchangeably. And I think it's very important to tease these two things apart. A service level agreement is a contract with your users. It is a legally binding contract between a service provider and a customer. Quite often an SLA will contain an SLO embedded inside it. But usually, that SLO-- which some might refer to as an external SLO, the SLO you're offering to your customers-- is different from the internal solo SLO you actually use to manage your system. And generally, the internal SLO will be stricter than the one that you put in your service level agreement or SLA. So just to recap, an SLI is something you measure. An SLO is an SLI plus a goal. And an SLA or service level agreement is an SLO or service level objective with penalties. So I could do a whole talk on just how to do this really well and give lots and lots of examples but that'll have to wait for another day. I'd like to move on to the concept of an error budget. The error budget is related to your service level objective or SLO. If you take, say, a three nines SLO and then ask the question, well how often can I be down, that gives you a 0.1% error budget. It's really the same thing. It's just that sometimes your error budget is easier to reason about. For example, if you're running a three nines service, that means you get 43 minutes a month where you get to be down before you have violated your SLO. If you're running a four nines service, that drops down to 4.3 minutes a month. So you can see how this sort of makes it real when you think about how long of a global total outage can I have before I have violated my SLO? And if you look in the five nines column here, you can see it's pretty tough. You get 25.9 seconds a month before you have violated your SLO. That's a pretty high bar. Quite often as you move to more nines in your SLO, you also increase the time window over which you measure that. So you might measure it over 90 days or a whole year, for example. So if we look at five nines over a year, you're looking at about five minutes of a total global outage before you have violated your SLO. So, that's not a whole lot of time. That sort of sets the bar pretty high. How do you go about that? Let's say, for example, that you have an on call rotation and your agreement with your on callers is that they should be hands on keyboard troubleshooting the problem five minutes after the pager goes off. You've just used up your entire SLO budget, your entire error budget for the year waiting for your on caller to put his hands on-- his or her hands on the keyboard. In this case, type faster is probably not the right answer for how to stay within your error budget. Instead what you need to do is apply lots of best practices to your service to keep the humans ahead of the service. To make total global outages, which will consume all of your error budget incredibly quickly, very, very, very, very, very rare. The book is a great start, I'll plug the book a few times. I can't say enough good things about it. You have no excuse not to read it. If you go to this URL, you can read it for free. It's out there under a Creative Commons license. There will be a book signing up in the fifth nine lounge area. I don't know when. But if you go up to the fifth nine, it should be posted. And you get a chance to meet some of the authors as well. And I think they're giving away maybe 100 copies to the first 100 people up there. So go check out the fifth nine. So, how does an error budget help you? Well, it helps guide decision making process about a lot of different things. Any time you change your service, there's a certain amount of risk involved in that and a certain amount of effort in making that change. So for example, should we deploy this new feature? Well there's risk in deploying the new feature. There's effort in deploying the new feature. There is clearly effort in making the new feature. Should we deploy it now? Well, look at your SLO, look at your error budget have you consumed all of your budget for this month? If the answer is yes, you should probably wait. Give your users a break, deploy that next month. How should we trade off resource usage versus reliability? For example, and we'll see some examples later in the talk, you can over provision your service to make it more reliable but that adds to your cost. Is that a worthwhile investment or can you run a little hotter? The answer to that will be in your error budgets. If you are consuming all of your error budget, you probably don't want to run any hotter. If you're not consuming all of your error budget, maybe you should consider running your service a little hotter. We've got developers who can write awesome things. Should they be writing awesome new features or should they be writing code which makes the service more reliable? How do you choose between these two things? They're both great. Well again, look at your error budgets. Are you consuming all of your error budget every month, month after month? Well you should probably be investing a little bit more in reliability to keep ahead of the growth of your service. If you're well within your error budget, bring on the features because you're meeting your SLO easily. Here's a good one, should we do more testing? Testing's good. Testing is always good, right? Again, look at your error budgets month after month after month. If you are consistently putting out releases and not causing outages and not consuming your error budgets, whatever testing you're doing is good enough. And testing more is only going to add cost that provides no value to your users. On the other hand, if your releases are not super reliable, if they cause outages, if they're not going out reliably and are quite often rolled back, maybe investing in testing infrastructure is exactly what you should be doing. But I'm not here to talk about any of those things today. I'm here to talk about architecture. How do you architect your system to minimize total global outages? And how do you trade off the cost of doing so? How do you know whether you should invest in the cost of doing so? And of course it comes back to your service level objective. How can I design my service to meet its service level objectives with minimum cost? So the particular type of-- the particular aspect of architecture I'm going to spend a lot of time on today is how do you use our instances, zones, regions, and multi-regions to assemble a resilient system? And this is going to be the focus of the talk today. So first thing up is instances. This is our most granular building block on the Google Cloud platform. This is a virtual machine running in one of our data centers. Not surprisingly, it runs on a physical machine. Also not surprisingly, there are bad things that can happen to physical machines and you're probably all familiar with these things. DRAM can go bad. NICs can go bad. Power can fail. You can have operating system bugs which crash the machine. If the physical instance dies unexpectedly, then your virtual instance goes with it. Probably no big surprise there. There are also bad things that can happen to the rack as a whole. The power to the rack can fail. The networking can fail at the rack level. Again, any of these things happen, your instance is going to go with it. Again, probably no big surprise there. Now, if you're managing 10 machines or 100 machines these kind of failures are actually quite rare. But when your scale goes up to thousands or tens of thousands of machines, the rare becomes common. And Google and other companies that have been running large fleets of machines have learned that handling reliability and software is more flexible and more cost effective than handling it in hardware. So the reason I'm not up here talking about how we have the world's most reliable hardware is because these things are generally handled at the software layer. That has advantages in that software is much easier to change and improve on over time, but it also allows you to choose different service level objectives for different services running on the same hardware platforms. But let's get back to your instance. Something bad happened. The physical machine failed unexpectedly. What happens to your instance? Well, we start it on some other physical machine automatically for you, unless you've told us not to. But wait, you might be asking. What about the stuff that was on my hard drives? Well if you're storing your stuff in persistent disks, though that data is already stored in a distributed fashion throughout the cluster, which means we can still mount your drives even though we're running on a different physical machine. The exception to this is local SSD which as the name implies is in fact local to the physical machine you are running on. So in a scenario like this, those disks are ephemeral. Now, you need to be aware of that so you choose-- you use them appropriately. Some great uses I've seen for these things are as caches because if the cache goes away, well great, you'll just refill it somewhere else. I've seen it use really effectively for other sort of cache like things like shuffle space for large high performance MapReduce jobs. And I've also heard of people running open source databases which can span across multiple machines and doing that effectively with local SSD. So let's talk about the difference between planned and unplanned failures. Occasionally we need to do stuff to the machines in our data centers or to the racks in our data centers. This would be planned maintenance. This is different than what we've been talking about before. The big difference here is we know it's coming and we can use our live migration feature to get your instance out of the way of the planned maintenance. So basically, this stuff is invisible to you. And that's true even for local SSD. We're transferring the contents of the RAM of your virtual machine from one instance to-- from one physical machine to another already, we can do the same for the bytes on the local SSD. Now I've spent a lot of time talking about how instances fail and I don't want this to scare you. I mean, it's a fact of life. We all have run hardware, we all have had computers fail on us. The computers in Google's data center are no different. But we understand that if your experience on our Cloud platform was that my instances are having unplanned failures all the time, we would lose your trust. So not surprisingly, we have an internal SLO around how much of that is acceptable before we need to do things to mitigate it. And what that means is in aggregate over lots of instances, most instances will have the opportunity to live a long, healthy life if that's what they want to do. That doesn't mean any individual instance can't suffer an unplanned outage. But over a large number of instances, you'll probably find it does not happen often enough to erode your trust of the service. And that's us choosing an SLO with user empathy for you. So now we've talked about instances. Let's step one level up to zones. Instances live in zones. Zones roughly correspond to a cluster. Now the word cluster is a very vague term and has different meanings depending on who you're talking to. In this context, a cluster is basically an extremely well connected set of computers. If you've read the paper we wrote about our Jupiter cluster level networking, that's what we're talking about. You're all in one big happy Jupiter family and you can talk very low latency, very high bandwidth to each other. So if your SLO requires more then you can expect from a single instance, the solution is to run on multiple instances within the zone. Now if any one instance fails, the rest can pick up the load. We have an HTTP or SSL load balancer that you can put in front of your instances to spread the traffic across those instances that will detect the failure and direct traffic to the healthy instances. You can use open source frameworks like Kubernetes, open source databases, for example HBase, to spread your data and your stateless processing across an entire zone. So we also have hosted products which do the same thing for you but with less overhead for you. Container Engine is a hosted Kubernetes solution. Cloud Bigtable is the same Bigtable we wrote the paper about ages ago. It's available to you as a hosted service. Cloud Dataflow lets you run your Apache beam workloads across an entire-- in a distributed fashion across an entire zone. And Cloud Dataproc does the same thing for Hadoop and Spark workloads. So great, we're done, right? We're running across a bunch of machines in a zone. All of those bad things I talked about that can happen to your instances, you're fine. Well, not quite. There are still things that are shared by all the machines running in a cluster or zone. For example, they all share that Jupiter network I was talking about. They at some level share different pieces of the power hierarchy which feeds power to all this. And they're all in the same physical building. So if there's a fire in that building, it's going to affect more than just one instance at a time. On the software side, the cluster level operating system which we've published some information about called Borg or the cluster level distributed file system, which we've also published some information about called Colossus, they can have-- if they get unhealthy at a cluster level, that's going to affect more than one instance at a time. So we call all of these things correlated failures, failures that affect or problems that affect more than one instance in a zone at the same time because the failure mode is related to the zone or cluster, not to the-- something bad happening to the individual instance. So how do we address this? Well, enter regions. So a region is what we tend to call roughly speaking a campus in Google. You can think of it as a set of buildings hosting Google clusters that are in close physical proximity to each other. This photo here is of our data center campus in The Dalles, Oregon, right there on the Columbia River. This is home to our US West One region. So you can see a couple of buildings in here. Regions share less things than zones. They are on different networking fabrics. They're running independent Borg cluster operating system instances. They're running separate distributed storage systems. So a lot of the failure modes I talked about where you could get a correlated failure within a zone will not happen to a region at a time. Additionally, we take pains to manage our software in such a way that if we're going to perform an upgrade, for example to the Borg operating system in one zone, we're not simultaneously doing that in another zone. So that if there is a problem, we have time to detect it and it's only going to affect one zone at a time. So we try as hard as we can to restrict these correlated failures to a single zone. However, physically close is good because we can provide lots and lots of low latency, high bandwidth between the zones in a region. But physically close is also kind of bad because for some extremely rare forms of failure, they may be correlated to an entire region. So for example, earthquakes would kind of affect a region all at once or hurricanes might affect a region all at once. And in one particular made up case, we had a region attacked by zombies. No, it's not real. We do this thing called DIRT. You can see it on the slide there. You can go read about this in the ACMQ. But basically, we do these disaster readiness tests all the time at Google. So while we haven't actually suffered a regional failure, we practice things like that to make sure we know we can handle it if the situation comes along. So how do you protect against these extremely rare but still possible regional scoped problems? Well, we have one the one region. So kind of the same solution. When the zone wasn't giving you enough reliability to meet your SLO and you stepped up to running in a region, multiple zones in a region, if a single region isn't going to give you enough resilience to meet your SLO, then you can step up and run in multiple regions. And again, the HTTP load balancer is perfectly happy to balance your traffic across instances in multiple regions just like it could do it in multiple loads. And that works great for stateless servers, request response servers. But what about your persistence layers? What about your storage? What about your messaging? How do you run those across regions? Well, , it's a hard problem but Google offers some services in that persistent layer that already run in what we call multi-regions. So a multi-region is generally-- we have three right now, United States, Europe, and Asia. These services run in multiple campuses across these broad geographical areas so they are not vulnerable to single region failures. We currently have Cloud Storage, which provides multi-region support, Cloud Datastore, which also provides multi-region support, and Cloud Pub/Sub, which is a persistent messaging service. So these are automatically spread across regions. So if you let Google worry about your persistence layers using one of these services, then all you have left to worry about are your stateless servers, whatever they may be. And as we've talked about before, you put those behind one of our HTTP load balancers and you can have that traffic spread across multiple regions. I'm going to spend some time digging deeper into that particular thing. So I'm going to start with a fairly-- a very oversimplified version of this. Here we have a representation of a service. It's running across 10 instances in two different zones in two different regions. So it's now a multi-regional service. So it's insulated from individual instance failures, it's insulated from zone failures, and it's insulated from regional failures. In this case, what the numbers mean here is each server can only handle 100 requests per second before becoming overloaded. So I've colored them yellow to represent hey, these are maxed out. They're good but they're maxed out. And then I carry that yellow up to show hey, we got 500 requests coming into US East One, 500 coming into Europe West One, and 1000 coming into the service globally. So now what happens if, say, US East One B goes down on us? We have one of these correlated failures. Now what? Well the load balancer will detect that the instances are no longer healthy. It will stop sending traffic there and all that traffic will be directed at Europe West One. Well, now we're overloaded. Now we're asking each server to provide 200 requests per second or 200% of what it's capable of doing without being overloaded. I'd like to put an aside in here and say it's a really good idea to test how your service behaves when it's overloaded. Because no matter how much you try, sometimes things are going to happen which is going to put your service into a little bit or a lot of overload and it's really nice if your service degrades somewhat gracefully in those situations versus a total global failure. But how do we get out of this bad situation? Well, we stand up five new instances in the zone that's still good and we're back to being able to serve without being overloaded. Now depending on your SLO, you could use humans to do this. When your service gets overloaded, you hopefully have a service level indicator for that. It goes over a threshold, a human gets paged. Five minutes later, they're at their keyboard saying, wow we're really overloaded. I'm going to spin up new instances to take up this slack. You could do it that way but you can see that putting a human in the loop there adds a lot of time. You're overloaded for longer. Now maybe that's fine for your SLO, depending on how many nines you're shooting for. If it's not, you can use robots to do this for you. Specifically, we have something called an autoscaler. And you can use that with Container Engine, you can use that with something called Managed Instance Groups where you essentially tell it, hey, each instance can take 100 requests per second. So when you see 1,000 coming in here, please spin up to 10 instances. So this solution has-- even this over simplified version of the solution still has two problems. The first problem is you are putting yourself in a situation where when there is a failure, you will have temporary overload. So that's problem one. Problem two is what if you can't spin up five more instances in that replacement zone? What? If you submit those requests and it says sorry, we couldn't bring up instances for you. And now you're back in overload and you're kind of stuck there. So why would this happen? Well, this is where I have to talk about stockouts. I know it may appear that we have infinite capacity but the truth of the matter is we don't. And to explain that, I want to talk about banks for a minute. So banks maintain this wonderful illusion that I can walk up at any time and say give me my money. And as long as everybody who is a customer of that bank or a significant percentage of them doesn't walk up at exactly the same time and ask for most or all of their money, it's fine. Because they keep a certain amount of cash in reserve so that when you walk up to the bank, they're able to give you your cash. So they keep this reserve around. So Cloud capacity is kind of like that. We keep an inventory around such that most of the time if you come up to us and say, hey, I need five instances, we're like great, here you go. Here's five instances. And as long as the requests-- we add customers at sort of a normal rate and those customers grow their services at sort of a normal rate, we can keep ahead of that. We're really good at keeping ahead of that and providing this illusion of infinite capacity so that every time you ask for five more instances, we're like, fantastic, here you go. However, if we get a very, very large inorganic, unpredicted spike, we could consume our inventory. Now we're going to add inventory as quickly as we can. It's not like a bank run where a bank actually fails and then everybody loses their money. We're just going to start adding inventory as quickly as we can. But this inorganic demand spike may cause this temporary stockout situation. What could cause such an inorganic spike in demand in a particular cell? What if we had a cell failure and everybody is fleeing from the cell that's no longer there into other cells? That kind of sounds like an unexpected inorganic spike in traffic. Everybody suddenly wants more capacity, or some fraction of our customers wants more capacity. So how do you protect yourself against this? Well the simple solution is ask for the instances in advance. So in this particular case, what I'm showing here is we have five instances in each zone which are serving the traffic and we have five spare instances in each zone. Now, this is grossly oversimplified because load balancers are really good, not surprisingly, at balancing loads so really, you get this where all the instances are at 50% capacity instead of some maxed out and some doing nothing. Now when the zone fails, the traffic just spills over and now we're at 100% capacity in the zone that's already there. You are no longer exposed to a stockout risk because you already have the instances you need. And you don't get a temporary overload because you already had the capacity spun up. However, it costs you double. You're running 20 instances now instead of the 10 that's required to serve the traffic. So can we do better than that? What if we go across five zones? So here, each zone is responsible for dealing with 200 requests per second and we have three spare instances in three of the zones. Again, because load balancing, it would really look like this. The question is, why do we have three spares instead of two? We only need to be able to soak up 200 extra requests per second if a zone fails. And the answer is because the zone that fails could be one that has a spare in it. So you have to be prepared for any of your zones to fail. So this is great. Again, no overload, no stockout risk, but its only 30% overhead. This is a huge improvement. We've gone from 100% overhead to 30% overhead. Can we do even better? If five zones was good, why not 10? So here, we only have two spares. Again, the spares aren't really spares. It would spread the traffic out like this. And now a failure of one zone again just spills over. This is great now. We're at 20% overhead and any one zone can fail. But-- oh, this is what we call n plus one. Any one zone can fail and we're fine. So we have n plus one reliability here. But we've now opened ourselves to a new failure mode. Do you see it? Let me give you a hint. Hello again, Matthew. So this is Hurricane Matthew. Hurricane Matthew paid a visit to South Carolina last October. That also happens to be where US East One is. In the end we ran some generators for a while but there was no interruption to service to any of our services. But it's an interesting thought experiment to ask what if we had a complete regional failure? Are you insulated against that? And in this case, that takes out two zones. So by adding a second zone in each region, we actually increased our exposure to the regional outage. Because before, losing a region or losing a zone was the same thing. Now they're different. So in this case, we're back in the same position we were in before where we're temporarily overloaded. But you'll notice we're like 10% to 12% overloaded. It's not quite as catastrophic a situation as it was before. And we have eight places where if we can bring up just one instance in any of these eight places, we're good again. So the risk is a lot lower than it was before when we were only running in two zones. But if that risk is not acceptable given your SLO, then we get back to a situation where you're going to run three spares. So now you're at 30% overhead again, and it kind of looks a lot like the five zone solution. But now we're almost at what we would call n plus 2 where I can almost lose any two zones and I'm still good. It's only if I lose two of the zones with the spares in it where I'm at a problem. So running in 10 zones is better than running in five because you almost get to n plus 2 with 30% overhead whereas before. You were only at n plus one with 30% overhead. So there has been an improvement. So one of the simplifying things-- assumptions I've made is this notion that we have exactly 1,000 requests per second flowing in all day, every day. Every day is the same and we can statically choose how many servers we have. Of course that's not how most services are. Let's pretend we have a service that has [INAUDIBLE] cycle where 1,000 is the trough and 2,000 is the peak. So let's look at what it looks like at the peak. And let's expand our footprint into Asia Northeast. Now we're running in 12 zones. And let's make all the zones the same. If we run three instances per region, we don't quite have enough capacity and if we run four instances per region, it looks like we're wasting capacity because all these things are running in the mid 80s. And the overhead here is about 20%. We're running 24 instances when 20 is all that should be required to serve 2000 requests per second. That 20% overhead kind of sounds familiar. That's where we ended up before. Now if we lose a single zone, life's not too bad. A bunch of it can failover within the region, which is good. And then the rest can sort of spill over to the other regions which are not running super hot. And if we lose two zones, whether it's in a region or across two regions, it doesn't matter because all the zones are now the same size. That's kind of cool. And we can lose two zones and we are now running at 100%. No overload, no stockout risk. That was for the peak of 2000. So basically with 20% overhead running across 12 zones, we've now got to n plus two, a true n plus two. We can lose any two zones. So that was the peak. But for the trough, if we stick with our static two instances per zone, now we're vastly over provisioned. Like, we're running down in the 40%. This is a huge waste of-- is not a reliability problem. Clearly we can withstand all kinds of failures right now. But it is a cost problem. If only there was something that would scale the number of servers automatically to the amount of load that was coming in. But of course there is, I mentioned it earlier. We can use managed instance groups or our Container engine to do that for us. So what if it's scaled down to one instance? Now we're in this situation where we have one instance per zone and we're still at n plus two. So life looks pretty good. So the autoscaler can save you money by reducing your number of instances during your troughs. And if you configure it with some slacks, so you sort of say, hey I want to run around 80% utilization, then if you suffer a failure, that 20% becomes where that traffic goes in the case of a failure. So now we're back to our 2000. Life is good again. The other simplifying assumption I've made here, you'll notice that our traffic is beautifully spread absolutely evenly throughout all of our regions. If only the world could do this for us. It's never going to be like that. Your traffic comes from everywhere. You don't get to pick where your traffic comes from. So for most-- for many services, the offered traffic kind of follows the sun because it's based on humans being awake and doing things with computers. And you want to serve as close as you can to the people who are submitting requests to you. You don't want to send the request halfway around the world if you can avoid it. So a fairly typical service might have a peak around 1700 UTC. That's the evening in Europe and it's the morning on the West Coast and it's the middle of the day on the east coast of North America. So that's when some services sort of have their peak. So let's pretend that we're doing that and we have kind of this pattern. With our static two servers, two instances per zone, what we see is we're offering a lot more traffic in Europe than it can handle so it kind of spills over to US East which is now spilling over to US Central which is now spilling over to US West One. It works. We're not overloaded. All the requests get served. But we're not serving them as close to our users as we might like which means they're probably seeing additional latency. And if we lose Europe West One, it's going to get kind of ugly at that point. We're going to be doing a lot spilling. So instead, what if we use our autoscaler and we say, hey, run as many services as you need to keep under 80%. You're going to end up with something that looks like this. And there's some overhead here. We're running 27 instances total when you should only need 20. But it's all in-- it's all region, which means you're giving your users a better experience. You're serving them closer to where they live. And you've got a lot of spare capacity kicking around here. So if you lose one of your big zones, a lot of that slack can be taken up in region and then there's a little bit of spill over to another region. But of course you have an autoscaler on these things. So those instances that are now running at higher than 80% are going to feed back to the autoscaler. Hey by the way, I'm above 80% and the autoscaler is going to say, well gee, I better bring up some more capacity. In the end, you should converge to something like this where the traffic is being offered in region and the four instances we lost from the failed zone get brought up automatically in the same-- in another zone in the same region. Wouldn't it be nice if we didn't have to worry about zones though? We thought so too. So if you like the autoscaler and you like that idea, we've got this thing called a Regional Managed Instance Group. And basically what it says is, hey, run me some instances in these zones in these regions and I don't care that much about what zones they're in. Just make sure I've got enough in each region. So it takes us from here where we're caring about what zone things are in to this mental model where we're only caring about what regions are in. So, I talked a lot about the autoscaler. If you want to know more about that, if that sounds like something you could use, there's going to be a talk tomorrow. I encourage you to go see it if that sounds like a tool that you would like to have in your toolbox. So that was a lot of slides on traffic spilling here and traffic spilling there and scaling automatically and where do you want to run it and things like that. There's a different path if you want to take it and that is App Engine. App Engine is sort of our-- it has all of this built in, so to speak. You just give us your code and we run it and you get auto scaling and you get the failover. We're handling all these things under the cover. You give us your code and your application runs. So if that sounds interesting to you, I encourage you to check out this talk which is Writing Infinitely Scalable and High Performance Apps with App Engine. And if you might be sitting in the audience saying, you know what, I checked that out two to three years ago. It didn't fit for what I wanted to do. I encourage you to check out this talk which is, You Can Run That on App Engine? We've done a lot to the service recently. And if you have looked at it in the past but have not looked at it recently, you might find that it fits what you need to do a lot better than it used to. So I'm not going to go any more in-depth on that. Go see these experts. So a quick recap, what have we talked about? We talked about reliability and trust and how they're related to each other. We talked about service level indicators, service level objectives, service level agreements, and error budgets. We've gone through instances, zones, regions, and multi-regions. We've talked about the bad things that can happen to them. And then we've talked in great detail about how to spread across these zones and regions and how to avoid overload and stockouts. So in a sense, what I've done is I have exposed you to what I think are some really awesome tools for building super reliable systems. Like, if you have dig a hole, that's going to dig you a really good hole. I'd like to use that. That looks like fun. But really sometimes, you don't need a hole that big. Sometimes a shovel will do the job just fine. So again, how do you decide if you want to use the shovel or the backhoe? You look at your service level objective. So here are my rough rules of thumb about when you need to go-- the backhoe is on the far right, the shovel is on the far left. If you're looking for less than two nines of reliability, you're probably going to be just fine on a single instance. You've got enough error budget there that if your instance fails, you just-- you take your backups which you have, right? And you restore them on another instance and off you go. Or if everything is already in persistent disk and you've been taking snapshots, then it just comes back up for you. So that might be totally fine. A zone fails. Well, you might have to do some manual stuff where you bring it up in another zone, but you've got enough time in your error budget for humans to take these actions. So you don't need the backhoe for that job. If you want a couple of nines, running in a single zone should be perfectly adequate. When you get up to three nines, you should be looking at running in a single region. If you want four nines, now you need to be spread across multiple regions if you really want to achieve that. Now, before I get to the five nines case, there's a very important point I want to make. This is sort of the price of entry. This is-- if you build on the single zone, single region, multiple regions, you have the ability to get two nines or three nines or four nines. But it's not a guarantee depending on what you put on top of it. You need to manage your stuff with reliability as well. The number of nines your customer sees depends not just on our infrastructure, but also on your part of the stack. So this is guidance but it doesn't mean you will get this if you follow it. Everything needs to work together to provide those nines to your customers. Which brings me to the five nines case. If you're looking for five nines, you should probably be talking to CRE. What's CRE? CRE is Customer Reliability Engineering. So, it's what you get when you take the principles and lessons of SRE and apply them towards customers. It means Google SREs working with your SRE or ops team to achieve your desired reliability goals. If that sounds like I'm quoting, I am. we want to drive more nines to our mutual customers. You're running your stuff on our stuff. We want our mutual customers to be happy. Customer Reliability Engineering is an effort to make that happen. If this sounds interesting to you, you should talk to your sales rep and they can start that conversation. Now if you'd like to learn more about Customer Reliability Engineering, there's a talk for that. This is a talk by Luke Stone. Luke, is that you out there? No, all right. I thought Luke was in the audience. This is a talk by Luke Stone, in fact, right after this. Please add 30 minutes to all of these times because I didn't know the keynote was going to run over. So if you are interested in learning more about what a CRE engagement looks like, this is the talk for you. And then Luke also has some war stories to share from his time as an SRE and as a director of this CRE initiative. So I encourage you to go check those out if this is interesting to you. The last thing I'll leave you with is come drop by the fifth nine lounge. If you are interested in reliability, if you're the sort of person who carries quote a pager, come hang out. Your tribe is there at the fifth nine on the third floor. There will be a book signing as I mentioned earlier. And there's lots of Google SREs and CREs there for you to chat with. I plan to spend a lot of time there so if you think of something you want to talk to me about or you just want to tell me where my talk was wrong, come on up. I'd love to hear it. Thank you so much for giving me your time today. I really, really appreciate it. I sincerely hope you found this worthwhile. [MUSIC PLAYING] 