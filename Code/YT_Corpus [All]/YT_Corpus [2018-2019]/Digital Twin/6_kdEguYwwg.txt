 [Music] Hey good afternoon thanks for coming I'm Aaron Subramanian I lead the data science and analytics team at Baker Hughes G company will talk today about how we deal with analytics at scale for the industrial world specifically for oil and gas but in more in general what are the scalable aspects and what are the commonalities of analytics in the industrial world right and just to give you a little bit of an introduction of the kinds of problems that we're interested in and by we I mean the larger GE so this is the largest G company so for those of you don't know every two seconds then airplane takes off somewhere in the world with a G engine on it about 30% of the world's power is generated by a GE turbine of one kind or the other right and we formed make a huge G company last July by merging G oil and gas with Baker Hughes two companies which by their own merits we're a large chance in their own merits but the merger brought together the first world's first full stream oil and gas company by that we mean the first company in the world that can do exploration for oil extract oil ship the oil refine it and then do something with the refined products right so make energy you make something else with the products the only company the first company in the world that can do that at scale now to do any of these individual things by itself is a big feat doing all of these things simultaneously in one company requires us to build a lot of models repeatedly and I've not even touched upon what we do in the healthcare space or in the transportation space for example these are just simple examples and to do this at scale daily we have to build thousands of models and for us models are any representations that connect a set of inputs to a set of outputs now that can be as simple as a simple equation or a full-blown partial differential equation that takes about two months to solve on the world's fastest super computer right all of those things for us is the model now to do this effectively we of course need the raw material for it which is data and we deal with data in two ways so we have what we call two big dichotomies of silos the first silo is data the second silo is analytics or second silo is subject matter expertise I'll first tackle the data silos and if you look at across the entire industrial spectrum the first one of course is oil and gas here listed from there to aviation they take an aircraft that takes off from New York lands in London the actress was designed about 20 years ago the engines are not collecting that much data that's roughly about a terabyte of data generated right not much of it is stored not much of it is analyzed but today's newest aircraft generate about a terabyte per hour of flight right take that in the oil and gas world where an offshore oil rig for the past 30 years has been generating roughly fifteen to twenty thousand tags for the data every two seconds and almost all of it has been stored compared to the power or transportation or the aviation industry the oil and gas industry has at least a couple of orders of magnitude more in terms of data primarily because they had to deal with uncertainty from the get-go there's very little you know about the reservoir there's very little you can actually model or control about what happens subsurface but as an engine it's complex it's extraordinarily sophisticated but you control almost everything around that particular field so then certainties exist but they're far smaller right so the datasets that you see in the oil and gas world significantly outstrip anything else in the industrial world but if you compare that to any data sets in the ERP systems tradition of the ERP world for the last 20 years the last 30 years where you think about any transactional data sets any kind of maintenance records any kind of information that you've been sharing and storing and retrieving in the traditional enterprise world dwarf gets dwarfed by the industrial data set by at least two to five orders of magnitude both in terms of volumes and but and there are no systems that have been developed that would tackle something at that scale even the modern industrial the Internet to dot all kind of technologies where we track everything that everybody is doing in this room everybody that's doing anywhere in the world where they are where they're going what they're probably going to do it still gets dwarfed significantly and I'll give you some examples of how we go about solving it but this is just one side of the story right the data silo is one side of the story that everybody talks about the other side of the story is the analytic silo or the subject matter expert silos where people have been building analytics for decades people have been building analytics for 50 years what if we have the luxury of going and building a model from scratch from a data set that you still have to prepare painstakingly that's a great thing you have a lot of frameworks out there today to go build but if you had models that have been in production for decades that people understood have been using it in production today folks were using it may not necessarily understand all the nuances of the model you don't necessarily want to go touch it you don't necessarily want to go tweak it or change it you want to be careful about the same thing in the financial world where you really don't want to go change the financial metrics without understanding what went on for 20 years internally right very very similar things here the only difference is the consequences of doing something wrong can be catastrophic imagine we have to decide based on a models prediction whether I have to shut down an offshore oil rig which is producing about two million barrels a day I have to predict whether a particular engine or a particular aircraft needs to be pulled for maintenance this take before this take off or is it okay to leave the airport today quite consequences of a wrong decision are catastrophic and false negatives are really not okay so there are no models that we generally tend to build where false negatives are okay and false positive rate so that's why generally high roughly around twenty to thirty percent would be okay for a model in the industrial world but a false negative rate meaning the risk of me saying something is okay when it's really not okay has to be zero or as close to zero as possible right that's where the consequences come in and analytic silos exist because of that because it takes a lot of expertise for somebody to build and maintain these models now for us to come in and scale this if I have to democratize analytics for 5000 engineers in a company like Baker Hughes for example I can't necessarily go in and say it drop everything you're doing here is the brand new framework go build all your model to the brand new framework some would but most of them would still go back to Excel spreadsheets and in some cases their Fortran code but that's what works and they know it works right so we need to somehow bridge these silos together both the data silos and analytic silos and fundamentally getting a federated orchestration engine where you have a lot of these models talking to each other not necessarily independent islands but actually models that talk to each other is fundamental if you want to scale analytics in the industrial world and I'll show you some examples of that but example that you see here where there is a Python model that gives an output that gets fed into a Fortran 77 model which we can only probably run as a black box that then talks to a model that's written and go that then talks with model that's written and see it's a daily reality for us it's not a nice italic example in fact there are very very few examples where we had to build a model that gets you to an outcome that's just written in one framework okay now I've listed some of these things some of these things could be provocative some of these things you could say myths or reality depending on your point of view and what your experiences have been but we've heard all of these things and we talked to customers on a daily basis and irrespective of where they are in their journey for digitalization where they are in their journey from moving to the cloud or not whether they believe in AI or whether they think it's a myth a lot of these things are what customers tell us daily now there is some truth to a lot of these statements but they hide the bigger truth right so for example one of the one of my favorites is when people say neural networks are all new the general statement is I've done this in the 90s when I was doing my Master's they're really bad they got to about 60% accuracy they really never got anything meaningful and engineering partland so we moved on what's so new about all these new neural networks are coming up and the other thing I didn't list here was people telling oh I've done deep learning 20 years ago and for those of you do deep learning here you know that's really a silly statement and but the reason why people walk around with these myths is because they haven't gotten the outcomes that everybody seems to be promising with analytics and by the way the analytics promise has been the same for the last 25 years there used to be the digital well of the future and that actually came out and late 80s and early 90s where they wanted to go censored up the entire oil field and say this is going to create a huge revolution that's like me coming and telling you email is going to make you more productive so it's it's just has to be balanced with what and how you're actually applying things to and just to bring home the point I want to walk you through a few examples and they're one of the favorite things and just a full disclosure right so my background is through-and-through aerospace engineering I grew up in the physics world we did data science as a necessity we got into doing large-scale data science because physics alone was not sufficient but the general pushback from most subject matter experts is the world has been run from by physics we understand it that's the only thing that can solve all kinds of complicated problems right so if you were to predict purely from data where a particular stone would land if you throw it and you don't know anything about gravity the only thing you need to go by is observations of each of the stones that are being thrown if you have to only purely build that model this is how it looks like so the projectile is going we don't know anything more than saying it's a selenium projection if there's the first time I'm observing it takes me more than 10,000 observations to track that the stone is even going to come down I don't know anything about gravity right and even the simplest model the most sophisticated model you can build requires a lot of observations on the flip side if you're a high school student and you're given the same problem see he stopped working you know he will tell me that all the data you need is just one data point all you need is the velocity the speed and the angle at which you threw the stone at anywhere in the universe where there is gravity you can predict precisely where the stone Atlanta right even middle school physics will tell you that the problem really is reality if I didn't know exactly what angle I threw this doughnut I just add a 1% uncertainty in the initial angle my output changes were about 20% if I add a 5 percent uncertainty in wind speed all of a sudden you have 100% uncertainty in where the stone will land and this is the simplest physics problem you can tackle it's just one variable and just very very little uncertainties imagine if you have to go predict whether a particular engine is going to have a fault or I have to shut down an offshore oil rig massive problem thousands of variables lots of uncertainties lots of things you don't know you still have to make a decision so physics alone can get us astray significantly in fact in this problem if we add a few more uncertainties it'll tell you to the stone is flying backwards so here to be very careful with that and the other thing that people generally don't like to talk about is the fact that yes data is cool it's everywhere everybody likes to use it but the reality is not much of it is useful so this is a real use case where the data set is from an actual assets think of this is a very large scale asset it could be a jet engine it could be a gas turbine it could be any of those kinds of things we are the equipment manufacturer we meaning GE and B a GE we have all of the data sets about how it was operated so the top left gives you the operating condition one of the variables that are being measured for the entirety of this particular assets life the actual data there is in blue dots everything else there is what we estimated or generated through other sources the reason we had to do that was because here in our databases systems of record it says there's a started operating in 2001 like that's their operating condition but that's it actually started operating in 1994 and there's lots of things that happened to it before our systems of record actually caught up and if I had to go predict anything about the system and I say I only go by the blue dots it doesn't matter how sophisticated my techniques are how sophisticated my data scientists are or even the physics based modelers are the models would still be wrong because data is only about 60 percent correct right so we had to go collect for it and while we started digging into this over the last decade or so we realize it's not just missing data is all not made equal we actually had to go invent different ways of categorizing missing data so you have that's why you have partially missing one to fully missing one two three and things like that and each one has to be estimated using different techniques so imagine the data scale we started with that includes the missing data if we add on the augmentation activity that goes on to add the missing data back in that problem becomes two or three times worse in terms of volumes because the minute you go add data in and if I change the color coding to all be just blue it'll be very hard for you to tell which was estimated in which was real right then you were to figure out which models would use which valve version of the data sets that you're using and how do you keep them all in sync that becomes a massive problem as well and just to show you the quality of how well we can actually estimate the top right figure blue histogram tells you the actual data set the mean is around 559 with the standard deviation of about 33 the estimated values are very very close it's why 60 versus 31 so it's very hard for someone to tell after a while but the dataset was real or not if you didn't properly tag them and this happens regularly right this happens on a engineers desk but they would have changed something in the dataset passed on the data said to somebody else they built a model that had a significant consequence and they pass it on and there's no systems of record for any of us and that becomes the actual data of record as well moving forward so really quickly industrial analytics for us is a combination of three things people have been doing analytics for a long time combining domain knowledge with standard software techniques right the new kids on the block are new way of doing data science mainly deep learning based data science and new way of building software on the cloud micro services self scalable right the traditional solutions are a combination of domain knowledge with software I would say finding very very hard problems or finding solutions to hard problems like finding a needle in a haystack is a combination of modern data science with modern software predicting what I'm going to buy before I actually go and buy it hard problem predicting how long it'll take me to go home before I actually tell you when I'm going to go home again a hard problem in the large cases it's been solved a lot of cases it's already happening today the case in point for industrial analytics it's really a combination of all three things because it's not even finding a needle in a haystack it's finding a needle in an ocean of data because most of the times the one thing that you're trying to go predict you don't have any data for so if you're trying to predict for example when would a bearing fail you would have hundreds of sensors on a gas turbine but you don't have a sensor on a bearing that's about to fail many times the sensors that you have are all confounded and not necessarily calibrated in the last twenty years so how exactly do you make sense of it to predict something that you're going to go predict without understanding the sensors around that particular piece that's really the problem that the industrial world faces and that is not just one e teutonic problem not just one problem here almost every problem of value is a combination of all three things and we got here not as an epiphany moment where we just sat there and said this is the hardest problem we need to solve these are the problems we have to solve every day these are the problems our customers have resolved every day if we could solve it with traditional techniques we would if we could solve it just purely with data science we would because that's the easiest thing to do there's a lot of techniques out there there are of systems out there that let you go do that the reason we had to go combine all three things painstakingly because that's the only thing that actually gets us to a solution that's meaningful right now we talk about digital twins a lot and the world talks about digital twins a lot today to the extent that it's almost an abuse term for us digital twins are very special we get there because we can't solve these problems with any of these individual techniques by themselves and the way we define it is it's a digital representation of a physical system or an asset or a process that has a few characteristics it has to be life it has to be continuously updated and it has to be adaptable and scalable all at the same time now imagine I have to do something specific or I have to make something specific and scalable at the same time meaning I go build a model off a particular asset or a particular system I make it so specific that it's specific to that particular system it doesn't scale by definition so how do you go build something that's both specific and scalable even worse I made it specific okay I can somehow make it scalable I now have to go make it adaptable meaning if I go change something in the physical system I change a pump out I change a compressor out I should be able to change only that portion of my entire digital twin and make the whole thing work again I don't have to go back and rebuild the whole thing alright those are very very hard things to do and the reason we have to go do it is because we have to run digital twins thousands of times a day for every single implementation we have out there so it's hundreds of thousands of digital twins running thousands of times a day so we can't really run these at scale if we have to go do these things manually typically how these things are done you get a team of six to ten PhDs with some designers who are experts in the system let them loose for about six months and then they come up with one model that's specific but non scalable and order non adaptable and the reason why we had to go mix all these three things together so we always start with physics that's physics space so that we can actually build on top of it where physics allows us to do no nodes so if you know something is going to be a problem physics will get you to a precise solution very quickly probabilistic s-- because that's where we started with without uncertainties I really cannot predict the real world right deep learning allows us to really sleep reasonably well at night because it allows us to go and deploy these systems to watch for things that I know are going to go wrong but I don't know what they are so I know something is going to go wrong that's all I know I don't know where I don't know when so those systems allow us to go watch and adapt the models for that right so I'll give you a few examples of that right I'll make it a little concrete for you for what a digital twin is take if somebody comes and tells you they have a digital - enough about and in this particular case this is a well with an artificial lift system think of that as a pump it's an electric submersible pump that's about two miles deep under the under the earth when we say it's a digital - enough well we connect it as a warm model that's the IPR curve - a pipe model - a pump model - a tubing model that finally gets you a pressure differential at the end and what you're trying to predict is how do I change the operating conditions of the pump and the well to make sure that I produce as much as possible without damaging anything but the well as well as the pump and to do this roughly one digital twin has about eight models running in sequence along with an iterative loop internally so it's not just a linear sequence right and this we would consider a fairly moderate level complexity digital - now the way we use it is an operator would go in and say okay how do i improve my production from my set of wells to answer that question on one well I typically have to run a digital twin at least a few thousand times and to get a plot like this where I have a plot of power versus total liquid flow from that particular well by changing all the parameters all the gray dots each one of those grey dots is a prediction or an influence through the digital - at the best family of solutions are on the blue dots the problem is today I might be operating on the black diamond without knowing what my space looks like I might just randomly increase power and hit any one of those red dots on the top where I increased power meaning I'm spending more and I'm actually producing less that happens a lot where intuition intuitively you can go tweak some things you really don't know where you are in your design space in this case it's a fairly simple problem we're only changing about six parameters but if you're an oil field there are a few hundred wells you're automatically going into a few thousand parameter optimization problem very quickly and that happens that needs to happen a couple of times a day but today the best you can do with the tools out there not just in oil and gas but in aviation in power and any one of these industries is to run an optimization like this not even a global optimizer but a local optimizer it can take anywhere from 20 minutes to about two hours for an advanced engineer to go run it right so we took this as an experiment and about summer of last year we went and built the digital twin for a single bomb he took about three minutes for a while we scaled it with the techniques we talked about up to now combined physics data science and software scalable on the cloud and about six months we got a thousand X improvement going from a single optimization that took about three minutes to run to a cluster of wells where about 20 wells were there in a cluster you can optimize it in about 30 seconds right the reason why you get a thousand X improvement is because it's not a one-shot optimization it's a game of whack-a-mole so you go do something else in one well something else goes wrong in another do you fix it go somewhere else change something something else goes wrong so you do that iteratively multiple times in fact to get to that solution you need to run the model about 20,000 times to get to that answer that's really where you're getting the thousand X improvement but we wanted to push it one level higher saying okay we keep the models the same we scale the software we push it to the cloud they actually orchestrate all these models to run on the cloud and see what actually can be done at a field level optimization the solution doesn't exist today and to be honest none of our customers even asked us for it in fact when we go and present it the 20 well optimization the response was this is great today we do that once every six months this one might let us do it once every two weeks and the field level optimization the response was we really did not this is even possible and the response was actually opposite they came back and said can you do 5,000 Wells can you do 9000 well simultaneously because the pace at which this is moving the six-month deadline was just because we had our internal conferences we actually had it scale in about three months what I'll show you how we walk through that and in just a year for an engineer to go from maybe doing a single bill optimization every six months to doing a field level optimization a couple of times a day it's a phenomenal improvement okay and the the notion of using deep learning in traditional industries like oil and gas or aviation of power but they're very risk-averse right they don't really want to trust the model by itself so here's some of the things that people generally talk about anomaly detection but the things that resonate with industrial customers is people don't really want to go set rules most of anomaly detection systems out there want to tell you to come and say here is my threshold for a set of variables set my thresholds and after that I'll go tell you when something is going to go wrong and this has to be done a priori before it goes and catches things so the biggest promise of automated anomaly detection with deep learning primarily is you can actually catch anomalies without necessarily knowing what these anomalies are beforehand I don't have to train for anomalies or another train rules in fact I don't even need to tell it what are the sets of inputs I need to be looking at as long as you have a data stream coming in and as long as you knew what the normal state was I can go say what was abnormal what that was the first foray for us in 2d planning but from there we actually went into deploying a live application for a customer where the customer was recording about ten thousand sensors off an offshore oil rig actually this was done by the customer actually already pushed the data to Google Cloud all we had to do was train with a six months data set they had which was normal with no abnormalities and within about a 6-week period from starting from scratch going from not knowing what the data is not even knowing what the asset was to having an anomaly detection working in an application roughly took us about six weeks and this particular application that you see here this failure mode that was caught that's shown in the top right where it's just an anomaly score has a combination of about 16 different sensors we called an anomaly then the next immediate question is okay this is something going wrong but is this just a blip or can you tell me what is causing this problem that's the plot on the bottom right and I'm not sure if you're showing it properly here but the colors actually indicate as a function of time which variables are actually affecting the anomaly score so you can actually start going and figuring out what is causing the anomaly that's the next step the step after that is actually figuring out what is the actual cause what is the root cause actually right and to do that you actually train with maintenance logs along with this time series data sets as well so this is actually not to encoder running for instead of about 24 different variables over a period of two years builds running live on GCP in about six weeks right the other kind of systems that are actually fairly effective for using in deep learning is image recognition kind of use cases right so this is a use case where if you've driven from here to LA you'd have seen a lot of these pumps these are called rod lift pumps and the biggest problem with rod lift firms are they fail quite often and the failure modes generally are categorized using these images which is a plot of the force versus displacement of the rod that keeps going in and out right and depending on how that shape looks that what an experienced engineer can say what is the actual failure mode the problem is to failure modes look more or less the same right so it depends on who you ask and when you ask in terms of finding out what exactly is a failure mode until you actually have to stop operations and then pull it back up right and whereas if you use traditional techniques to do distance based metrics to say this is the overall graph tell me what this new graph looks like and what is it closest to you get to an accuracy of about 60% right so we took the latest just an image object recognition model augmented its using x for learning with just the failure mode images that we had and let it loose so if I run this demo for you you can see most of the detections where on the left hand side is the data set that's coming in for a particular pump for a particular model of pump on the right hand side is what the model is predicting real-time so this is a deep learning model built and deployed for inference on in this particular case the times are actually running on an edge device with a small Jetson with the timing that we are actually running right so you can see instantly it is catching with under one second delay what the actual failure mode is and this model was really not that optimized yet we just took the standard model that was published augmented it with the data sets we had an accuracy went from about 65 percent to 93 percent right the data sets were not even that high so they're only a few hundred failure modes and if you optimize the model so you can get up to about 93 percent to about 97 percent the last three percent is something that you have to go collect more data for so these are some of the examples that we are building and deploying in the field both with just traditional deep learning augmenting with deep learning right the other kind of deep learning models that we're building quite effectively are the knowledge systems and the requests for this one came from a couple of customers where the request was we have a lot of documents that we have generated over the last 30 to 50 years we have a lot of training material that we generally use for our new recruits to be trained on our operations and to make sure that we can actually get them up to speed very quickly with our subject matter experts what most of the industries are facing today are a dichotomy from the first problem is their tribe they're losing a significant portion of their experience to workforce in the next five to ten years but the other side the new workforce that's coming in are used to learning and interacting in a very very different way they don't necessarily want to sit through three weeks of training on just going through and doing one task they want to be able to go and learn they want to be able to go search and find information as they need it the problem is to enable that and to bridge the gap between people who are retiring and the people who are coming in you need a knowledge system that can quickly learn and adapt right so in this case we built up the knowledge system so we had the standard English language NLP models we took those and we trained those models to understand technical English so how do you go interpret tables how do you go into prayer tables and figure captions how do you go get a natural language query to return an answer that's not a pure retrieval but actually something that generates text so we have to go build that and after we built that we realized that the corpus of data that we had to go build it had to be expanded with all the subject matter expertise we had internally in Baker Hughes G right so we made an English language speaking model understand the technical language based on all the technical language documents that are out there in the world then we made that model into an oil and gas person actually we made that into a B a GE person because it understood our language and we took that model and tried to play to a customer and realized it was like be a GE person talking to a person in BP yourself we can understand each other but then we don't know our lingo exactly we have our own acronyms so we had to go get the documents from the customers to augment this model as well so just to get to a system that understands a natural language query respond to it in the customers context to get to for example a cust user input going in having a spelling correction that's oil and gas specific and customer specific and go through and give recommendations based on what answers could be given because if we go and ask a general question you can of course go to Google or any of the search engines and then ask a general question you're gonna get a very very generic answer what they really want is an answer that's specific to the person who's asking the question in the context of the company that's being asked so to just get this one simple example working where somebody went and type tried typing in gammer all right so if you go try that in Google today it'll get connected as grammar or grammar early or any of the many english-language connections right what they actually wanted was a gamma board or a HT gamma elite that is specific to that particular customer they also wanted grammerly and other things but they would be ranked down much below what a general-purpose search engine would give you imagine extending this out where you just go start entering natural language queries and a report an actual full-blown report gets generated for you to go train yourself for the query that you entered that's really where the oil and gas language model is getting used in fact one of the the models that we built with which requires significant compute is the oil and gas language model and we continue to evolve that as we speak and we started of course with English but because we're an international company and also the corpus of understanding about any of the industrial world it's not just in English we actually are starting to build translators from English into all kinds of other languages as well okay now the other thing we want to talk about was democratizing analytics right now we talked about data silos we talked about analytics or subject matter expert silos but there's also the other problem that we constantly come across hey this is working on my laptop or on my system I have shipped it to an DevOps or a cloud rob's or another analytics person now it's your problem to run it it just works fine on my system right containers solve that significantly but we had to up skill or up level or analytics folks because they come not just from software or programming backgrounds but they come from deep engineering and physics and domain expertise backgrounds so we just did a quick check of how long it took us to on board somebody so usually based on a person's background it would take anywhere from a week to a couple of months to fully on board somebody to be productive in building new analytics and pushing it into our production systems right and that was before containers before we had CI CD pipelines both on the cloud on somebody's laptop there's more or less mirror at the same way so it's not just about upscaling it's also down scaling how do you down scale your overall system so that somebody can do their work in their small ecosystem and then push it back into that overall systems as well right so we went from somebody taking anywhere from a week to two months to a couple of hours and these were on tested on a variety of people where you had a lot of expertise with containers and almost no expertise in containers in fact we had people who came in with absolutely no expertise in containers or programming who still got up and running in the first day to push production code right the other one was we went through painful quarterly releases it used to be nerve-wracking sitting right before midnight because we had already promised our leadership that we're gonna release today and something that was working just fine the day before or even in the morning would break just ten minutes before production we've all been there and it was painful painful quarterly releases - today we can actually do continuous releases from a person who's built analytics but does not necessarily understand the overall working mechanics of a large-scale enterprise software system pushing something making it into production and actually being ok pushing it into production right and we can do that in some specific cases of course I'm not claiming we do that on all cases but for the capability for the analytics person to write code and the same code to be shipped to production is fundamentally if we have to scale any of this ok now I'll quickly run you through a demo the demo I'm going to run through is trying to build a few thousand models rather quickly on DCP and enable this we almost all of our services using cloud native Google services and we have an internal tool that we also used as an analytics engine for all of our products for the customers call apply day I we are running apply day eye on DCP courses so it's our core running but using the GCP core services so we are using the kubernetes engine we're using say all of the storage services that Google provides natively including all the perimeter security TNS cloud dns and firewall rules and everything ok and hopefully the demo gods are smiling on us let's see if this works ok can we switch ok perfect so what are you gonna see here you can see we have a kubernetes engine we have the cluster running that's fairly large now we also have both the browser-based interface that we won't be demoing today for a citizen data scientists or subject matter experts to go in and build a lot of models but we have a CLI that a more advanced person can go build a lot of models very very quickly as well so if I do my CLI on the left-hand side you can see I can do a lot of things I can run login run a few models I can I check the status of the models but we can also do guided build ok and guided build is where I don't necessarily know much about my problem I was given a data set by somebody I need to go build a lot of models with the data set I don't necessarily know what models I need to build I don't know what techniques are going to be useful I don't know if there is a lot of missing data in it I'm not given much information about the data set and that's a very standard use case as well where somebody gets dumped with the data set and say go do something with it right oh there may be an outcome but they may not say how to get to an outcome so about 60 to 80 percent of a subject matter experts time goes into figuring out what do I do with this data set right so this is trying to figure out how do you reduce that 60 to 80 percent of your time to maybe 20 to 30 percent rather than spending 80 percent of your time just valuing in the dark ok now we already logged into the CLI if I go into our actual kubernetes engine as usual the token has expired let me make sure I get my token you okay so we've extended kubernetes to have a graph tab that'll tell you what services you're running how many replicates you're running what pods are running what tasks are running and we had to do this because without this we really couldn't go in and figure out how to debug not only how to debug really what is the scale we are running it the numbers that come out and you have to go do looking looking at logs takes phenomenally longer time then for us to go look at something like this and get to an answer quickly now these are all the services that are running already these are the core services that you saw in the architecture diagram where this is just the applied AI running and waiting for somebody to issue any commands and what you see here in this big flower is all our tasks services so we have 50 services running waiting for any of the tasks to come in if I go in here and say AI guided build and I'm saying use a local file and saying out of all the different combinations of inputs you can use use the top 10 inputs that the model is automatically selecting has to be the most important as a first pass so this file the data file has thousand variables they're all correlated with each other that's real data set and you as the data scientist of a subject matter expert can go in and figure out which variables are important before going and building a model what I'm asking is go build all possible models for all possible combinations of variables selecting the first ten important variables per model that's what this command would do hit enter what it's doing is it's going off in this particular case it has 200 variables it's running thousand fifty six total models and you can see it's pushing off all of those tasks runs to the kubernetes engine and as soon as the kubernetes engine gets all of these tasks launched it'll then send back and you can see the graph already updating it's actually faster on the screen here but each of the color nodes that are coming up each one of those nodes are actual models that are getting spun up and as we're speaking we've already spun up 693 models and its life updating as we speak in terms of going off and pushing it to GK live off of the conference Wi-Fi here so as we speaking we just took a dataset push it out without knowing what is actually in there asking the system going to figure out how many models can you build what are the combination of inputs that will actually work for each of those models go build it and come back to me and show me a result and all this running live on a system that's live on Google Cloud in this particular case it's now already finished building about 730 models right and the responses that you're getting back on the screen there are running live with all of these containers getting spawned off inside the kubernetes engine with all of the connections between the different services running ok now can we switch back to the so what just happened here was this data set we took this data set with in this case 200 different variables with a combination of correlations between them the plot you see here each color is a correlation between one variable another variable right the system went in looked at the correlations looked at a lot of the other heuristics in the data set figure out what models to build in a variety of different techniques ran the models and came back with an answer for you right now to run this the scale of the nodes that ran live in a few minutes the 50 nodes running 4800 CPUs with about 30 terabytes of RAM yes you just spun it up for about 5 minutes came back down but you saved an engineer maybe two weeks worth of work and they can actually go do work on what is more meaningful for them to go ask questions and get answers from rather than figuring out what is this data said actually telling them right and we have a lot of workflows built in internally for us to go get the the acceleration of analytics possible internally as well now we than this on not on Wi-Fi but actually today morning with the proper internet connection we could run 30,000 models live loading up the kubernetes cluster that I just showed you wait about sixty percent utilization okay and the graph actually looks like that it's it went live in about two and a half minutes spinning out all the thirty thousand models like the nice thing about this is not all thirty thousand actually succeed a lot of times when you try to push 30,000 models through cluster that is of this size a lot of those things fail that have to be restarted automatically and having gke having the cloud native services it automatically restart helps us tremendously as well really not really worry about the infrastructure at all in this particular case right so I won't leave you with two thoughts one is we're not experimenting here we're not really saying this is an abstract thought maybe that'll be available in the future in five years time or something like that we're saying industry is using this today a lot of these applications may not seem like additional deep learning applications traditional cloud applications but a lot of these applications are moving to the cloud as we speak today and a lot of industries are actually embracing it not necessarily resisting it and challenges still do exist but the biggest challenge of course is accessibility so everybody says they have a lot of data but when you go in and say can you give me the datasets that becomes a six months to a one-year exercise because internally they have to go figure out how to go get the data who has access to these datasets and the biggest hurdle today really is not even just to herd the datasets or the talent it's really the changing the mindsets where everybody wants to do something with analytics everybody wants to do something with cloud everybody understands that is going to be valuable to them it's about how they contextualize that value to their actual context that's the biggest hurdle today so I'll leave you with that thanks a lot thank you thank you [Music] 