 all right welcome how is everyone doing today yeah all right so welcome to session the sizing the mixed reality revolution the past present and future mixed reality Department perspective long title I know but I hope this session won't be too long for you and you get some valuable time out if the next 45 minutes what we're going to talk about is the past in the present I will show you some of the applications we have developed and tell you the lessons we learnt from those and hopefully you can apply some of that to your own applications and we will quickly move into the future which is of course mixed reality and artificial intelligence and I'm going to show you a demo and you will also get the source code of the demo which is using Windows machine learning running on the hololens with the April 2018 update for near real-time object recognition and then at the end we spend a good chunk of the time like 20 minutes on my top 10 Holland's development recommendations 2018 and there will be lots of good press practices there at the end so there won't be any intro to hololens development or unity development or anything like that if you're very new to the topic there were some good sessions here build you can follow the recording I also did some talks at other conferences like unite which are up on YouTube and at the end of the deck I will have a slide with all the links I mentioned throughout the talk so you can give it get a snapshot of that my name is Renee I am director global innovation at valorum I also run the immersive experiences team there so I don't work for Microsoft we are a mixed reality partner and have been developing for the hololens since 2015 and i'm also windows development MVP and regional director and I've been doing computer graphics and also deep learning since maybe around 15 years or so alright so let's get started quick video about the company I work for valorum I'm headquartered in Kansas City but we have offices all across the globe I actually I'm from Germany and we don't just do a Mercer we also do cloud computing and digital transformation so we think about the whole digit transformation holistically from the back end to the front end front end could be an app could be 3d it could be a Hollande's app whatever right this video is less than a minute shows you some of my team members and shows you most importantly also some of the hololens applications we have to and let's take a look [Music] [Applause] [Music] [Applause] [Music] [Music] [Applause] [Music] okay enough of the marketing huh let's get started past in the present so we started in 2015 we all saw the announcement or probably most of you saw the announcement at Build 2015 with the Holland's we were all excited a couple of us got the opportunity to actually try it out and the holographic Academy and so that was cool but we didn't have the device available so we use some other devices like to give y'all or the Google co-op board and especially the co-op board you can you know just insult your phone and you have to cut out for the camera so we could actually run some a our frameworks on top like warrior so we could already experiment with the user experience and could try out some you know some things but also make a cell familiar with a unity development environment because this is what is mainly being used for mixed well tier later on we got invited to the hololens agency program which is not a mixed reality polymer program and we got access to hololens emulator and the sdk so we got the origami running in the emulator which probably a lot of you have done as well and later on we also got some of our apps running and we got T devices and we got the origami running in the real world so we were all excited about it and that is the team as it's currently we are at 8 + 2 people so not everyone is on the photo here but yeah back then it was like two people also so we've grown a team quite a bit very specialized skill set so for example we have a 3d artist who does 3d optimization or actually also can create 3d models from scratch we have a 3d animator and she can take those 3d models and bring them to life with really amazing animations and we have UX lead for example who also takes care of division and makes sure the user experience and the UI is well designed and works good works well we also have unity developers of course which are really specialized in unity and computer graphics development and then we have more of a generic C++ his shop developer that typically take care of the backend integration and lower level things which week then can there show up in unity as libraries if you don't have the luxury of having such a team I will give you some recommendations at the end how you can leverage tribal knowledge out there and get going even as an indie developer right so what are the lessons learned and I just picked free applications here for that because most of the stuff we do is the NDA and also I wanted to have something that fits for the rest of the session here so one app we did last year of full build was the tire Explorer application and we showed that actually at the expo there so you have probably seen some apps there this week they have these aquariums so we had our Explorer running there as well and well pretty exciting and a couple of lessons around here so what we're first of all using is so-called memorial model targets and actually let me roll the video here with quick so what you can see there first of all this is hollow beam I'm going to talk about Halloween in a couple of slides but now look at the guy his name is Justin he's looking at this real world tile right and then real physical tires being recognized so what we're using here is again with warrior model targets and what you can do with that is you can take an existing CAD model of a real world object and use that for trekking so there's no marker there's no traditional marker like QR code or anything on that real world object you can reuse the real world object you need to take a CAD model of that and optimize it optimization is important because it needs to fit into the rendering budget of the hololens right it needs to be around 100,000 polygons and all the lesson learned with using model targets you need to experiment a bit what is part of your marker based model because for example we have only had the rim in there sorry we didn't have the rim in there and then we later on edit the rim to the 3d model and of course the tracking was much better because there were much more feature points to track another thing you can see there in the top there is this light mounted and this light gives us a bit of a bit of a better contrast on the outside of the tire so there's a bit more contrast on the rubber part and that tracked much better so there's a bit a bit of things to experiment but model thoughts is really awesome it's a great experience for the user on what you could also see all the rendering of those tire components so we all use custom shadows there if you're not familiar what a sharer is it's basically a little program that runs on the graphics code that defines how an object will look like so how does it you know reflect light and so on and all of those components you'll use custom shaders so how did we develop those we used shadow Forge which is a graph based shaded editing tool so you don't have to write all the shader code from scratch you can actually use some tooling to put some notes together and that worked well but at the end we actually still optimize the code it automatically produced right there was tools are pretty cool to get started but if you want to get the latest latest piece of performance out of it you still need to hunt you in a bit so that's my recommendation to you as well by the way shadow Forge was just open source and also released for free because the developer thinks that it's probably not worth it to invest any more cars in unity 2018 ships the own graph based shader editing tool so you probably want to check that out if you're looking into shaders yeah then we have hollow beam here this my colleague rod Shelton from Seattle we call this version of him the hollow rod and he's explaining about those pirate components right and I'm going to tell you but about Halloween in a second this is another piece we did with an automotive client and this application I picked it because it's using machine learning and you can see this person they're wearing a whole lens so he looks at that component takes a snapshot and once that snapshot is taken it is sent to a back-end where we have a deep learning framework that was trained to recognize certain object so it will recognize that object and then pull in some extra in flew about it and awesome part about it is the user can just use their hands for the real world tools right their wine computer on the head with the hololens with this place we have automatic recognition of those objects so they can use their hands for real world tools it was a really amazing use case back then we had to use a back-end for the object recognition now we can do this on the device which I'm going to show you in a couple of slides using Windows machine learning and that is always a challenge if you go to those factories right connectivity can be tricky because there's a lot of communication already in the area so now that we're able to run it on device is amazing right another example hollow beam our immersive telepresence solution which we're working on so what I'll be doing here we're taking a 3d depth camera like the connects all the intel realsense we take the depth data plus the color data and code that and then stream that over internet just using normal internet connection three to five M bit adaptive streaming and then we can reconstruct it in real time as a point cloud on the hololens so how are we doing that by the way let me play the video in the background so you can see some stuff while I talk about it what we're using is WebRTC for transmission which stands for web real-time communication it's an open standard and there's also an open source implementation mainly driven by Google actually what we did is we forked that because we need to figure out how we can actually transmit the depth data right so what we're doing here we changed how the video stream is being produced and we take the depth data and encode it in a special format so that we don't have any compression artifacts when we decode it because if you apply to a small more impact or video compression to you know adapt data stream you will have lots of compression artifacts and the finger might be behind you or something like that because that data is a bit different and just normal color later right so that's the secret here basically if you want to do something like that look into WebRTC that's really - way to go so how are we achieving real-time rendering on the hall lens of all of those points we're using also a custom shader all handwritten it's a geometry shader which gets fed into so basically we decode the video on the hall lens using Windows Media Foundation so it all happens on a GPU already the video decoding and the hololens supports h.264 as a video codec for hardware decoding on the device so that's how you can actually pretty fast decode the video stream then we take that as a video texture and then feed that into the geometry shader which is then admitting all those points so if you want to render that amount of data in real time there's no other way than using a custom shader cool so you know here's another video which we just did the other week so we also have now collaboration features implemented as you can see we can play a hollow beam chess and you cannot see the other person but now we also edit the feature where we can collaborate on 3d models I can do things like that and you see there's almost no latency so that's in Germany work and use a okay let's talk about the future so what is the future again makes reality an AI right so let the man speak himself we all know him we all love him Alex kipman technical fellow and the visionary behind Holland's interconnect and as you can see in this LinkedIn post he pretty much made it clear also such an Adela pretty much made it clear right this is the future mixed reality perception and all official intelligence it's really what we're looking at and you know the awesome pieces we can do some of that already on the hololens these days but first of all what kind of AI are we actually talking about these days this and I don't want to spend too long on that but I thought it made sense because it's often so confused the whole terminology right so when we talk about AI artificial intelligence it's basically algorithms and techniques to simulate human intelligence right and a subset of AI is machine learning right and that can have other algorithms like support vector machines random forest trees and all of those fun things and the novel subset of machine learning itself is deep learning and deep learning leverages deep neural networks or artificial neural networks and this is where all the progress is happening these days right every Auto day you see a new deep learning model that is doing amazing swings we have not imagined a couple of years ago and again this is where all the progress is happening so when it's being talked about AI it's mainly about deep learning these days so what is deep neural network what is deep learning again just in a nutshell much better explanations I will have some good links at the end where you can learn that stuff and indeed so other fish in neural networks tries to simulate an animal brain and the neurons in that case are connected with synapses right and for an artificial neural network those neurons or basic computing functions which can also be tuned with a couple of parameters but all the data all the training data is stored in the connections in the synapses if you will rate these are connections and it's basically floating point values as you can see if you have lots of those floating point values you end up with a vector and if you have lots of vectors you get a matrix and then you have basically a huge multi dimensional matrix and that's where all the training data is being stored so how do you train the neural network a bunch of different ways to do that most commonly these days is still supervised learning although reinforcement learning is gaining quite a bit of traction but for our use case for the model I'm going to show you it's it's supervised learning so you a trake you take a training data set we've also contains the desired output you run it through a neural network and then on the output side you check to actually output with the desired one and then you probably have an arrow and that arrow is being that back propagated right and the back propagation basically adjusts the weights so it adjusts those values that are connected between the neurons and this way it can store all that information and if a train if it's trained well at the end it will generalize information and endless the so-called inference phase evaluation phase so you can give it data it hasn't seen before and it will predict the correct output result and you know what the awesome piece about it is we can run inference on the device now with Windows 10 April 2018 update there's a good session about winning model just in a nutshell I call win ml the Direct X of AI because what it does is it obstructs away the hardware so you don't have to worry too much about it all those specifics of if you're onyx model runs on a GPU CPU or one of those new or like you know tensile processing unit whatever is right whatever a seller a ttle chip it runs on you don't have to worry about it it basically you just have your abstraction layer which you really care about so you could take a model trained it in the cloud you could use things like custom vision AI which is really easy to use you just upload a couple of images label them then you can hit the button to train it and then you can export that as a so-called oh in an X or onyx model format and then you can take that onyx model for month with the pre trained data and deployed it as part of your application and run it in real time or well almost real-time but you can run it on the device with win ml and that's really awesome because things are getting you know the puzzle pieces are fitting together because we can use best of both worlds they're good beefy machines in the cloud to train the data and very short latency on the device for on device inference yep and you probably have seen the announcement about the HB UV next the next version of the Holland's will further and curate a I with the new HP you but we can already do some stuff on the Holland's we have available these days so let's finally get to the demo typically I would now take my whole lens and put it on and show you what I'm doing but the thing is for my object recognition demo I'm using the camera so I cannot live streamed at the same time to my computer when I'm already in my app using all using the camera what is that going to show you is a video how that works and then we're diving into the code I'm showing you the most important code snippets and you will get all the source code they are with the link on my github page alright so let's take a look at the video here so I recorded that at home I took some of the place was likely a studio couch 2 point 5 meter in front of you but don't tell my kids that I took those plush toys when they were in school this is likely a Teddy 73 centimeter in front of you so it recognizes the object we use spatial mapping to measure the distance to the object right and then we have a speech output this might be a Seeley ham Terrier 57 centimeter in front of you I'm using a squeezing that model year which is trained this is likely at 877 centimeter in front of you and image that database is a you know a very default database it can recognize this mafia lasas 69 centimeter in front of you this might be a west highland white terrier 70 centimeter in front of you and examples examples could be of course retail right just think about you put on device well this is likely a basketball 78 centimeter in front of you well think about a storage warehouse worker right they need to find stuff this is likely a soccer ball one point one meter in front of you well think about phone and show assembly all right you you buy some phone and you look at it and it will actually show you the assembly step this is likely an upright piano two point one meter in front of you so pretty cool stuff well let's switch machines here and I'm going to show you the unity scene and also a bit of Visual Studio alright cool so this is the unity scene pretty easy we just basically have a main camera here with the this text below it which you have seen in the video and then most importantly we have our DNN model here so there is a custom script I've written which is called DNN model behavioral right there and I'm going to show you what it does in a couple of seconds and then we have another game object here called spatial mapping so I'm just using the spatial mapping Collider in that particular piece as you probably know there's also spatial mapping renderer but I'm not interested in seeing this spatial measure of the whole lens but I'm really interested is only the collision because what I'm doing for measuring the distance I'm shooting out array from the user's head which is the main camera and do the raycast into the scene and then I get where it hits probably the spatial mesh I can just use the distance right that's what I'm using for the output when it tells you oh this is blah blah blah 50 centimeter in front of you so we're using the spatial mapping Collider there well then we have the gaze course or like this little green cursor you have seen pretty straightforward but also pay attention to the project setting here so we have our resources folder which is that special unity folder where you know all the stuff that isn't in resource folder will be shipped as part of your package workload and there's a label as the JSON file in here as you can see it has indices and then you have all those labels attached to it again the image net database 1000 objects so in my output layoffs manual Network I have 1000 Iran so this is just using the index mapping from the output layer or neuron which is actually is right which so when neuron what is it for example of neuron with index 6 fires I know it's actually stingray so I can just map that in the UI that's all and then we have our squeeze net onyx model as part of the workload here ok so let's switch gears to your visual studio and I'm going to show you the code behind all of that so this is the DNN model behavior here and as you know this thought method is called by unity once this game object or once this component is executed the first time and most importantly is this part here so there's a caste custom class for squeeze net model and there's a method I implement it called load model async let's jump to that you can see there's this load model acing and there's a bit of code here to pause the JSON file a pretty straightforward and by the way I took most of the sample from the official Windows machine learning Paige but they have a 2d uwp app what I made is I transform that into a 3d app using unity added spatial mapping and all of that on top of it right so you can also find some good examples they are from the Windows machine learning team all right so we passed the labels and now we actually know it need to load our Onix model and as you can see those api's are still in in preview mode so they even have it in the name and of course I would have loved to actually use this API here which is called load model from stream async but this frozen not implemented exception at the moment so we actually have to load it from a file and we have to do a bit of a workaround to actually get it from the resource folder into file that you WP on those stands so shout out to my friend Mike Talty he had a very good blog post out there that shows how to do exactly that how you can get it from the resource folder into a uwp file for loading it alright so we loaded it after that we're setting a bunch of options here for example a preview pre filled device kinds I could say run on the GPU run on the CPU of course most most importantly what I really I want to do is run on the GPU but biotechs 12 driver is not available in Holland so we have to run it on the CPU right after that we get some more descriptions like input image description and here's another awesome piece I want to highlight is that when m/l already provides you the officials to actually handle images so you don't have to worry about transforming your pixels in the floating point values which you need to input into your neural network it will already take care of this because it understands images basically and then we also get our output tensile description so tensile is basically a fancy name for vector so output vector of the values which is 1000 elements okay so we got our model loaded here next piece what we need to do is we need to initialize our camera and all a custom classes are written media capture er which uses uwp media capture internally and I don't want to dive too deep into that you can check it out later in the source code again it's all available one thing I want to highlight though is this piece here so I'm calling the method I've written stop start capturing and I pass in the input description so I pass in the pixel format and the width and the height I got from the win ml model loading so that's also pretty nice so they actually can tell me what is the expected input size they would actually take care of automatically down scaling it or scaling it to the right size so you don't have to worry about it you can just pass in whatever image you have but in order to save some cycles and get best performance I actually can also ask Uwe media CAPTCHA to give me the frames and the right size already right so that's what we're doing here so I'm just passing in those values all right so if your model loaded we have to capturing initialized and then we do our tasks run so I run the whole processing in a background thread I get the latest frame from the camera I pass that into this method here every eight frame this will run the evaluation on it I check if there is a result returned and if there is a result I will actually have to dispatch to them UI thread because I'm running on a background thread right and I want to update UI elements then there's a bit of string formatting here and at the end this is the code that does the speech synthesizers output and I'm using a PC from the mixed reality toolkit which is called text-to-speech and that wraps the Windows 10 speech synthesizes api's in such a way that you can actually use it with a unity audio source so that's pretty awesome and it's really simple you just call the start speaking method here and as you can see it just takes a string as an input so you can use any string and it will just say it so it's super straightforward alright that's pretty much all I wanted to show you for the demo and again you can get all the source code on my github page again at the end that will also be a slide with all the links on it so you can just grab a snapshot of the last one there cool so let's move on let's spend the next 20 minutes on my top 10 hololens development recommendations 2018 number 10 trip the hololens is a mobile device that has not changed because the hololens is a mobile device it's awesome it's untitled you can just freely walk around but of course you cannot expect full computing power of desktop graphics card that's just it right what we noticed it's mainly fillrate bound which means if you're rendering objects very close to the users eye they take up a lot of pixels and if you have a heavy pixel shader running on that it will drop the performance quite quite a bit also if you overlay objects for example if you're using the unity UI system and they have sometimes panels and then you stack another thing on top of it and another thing it will remember all of those again and again so all those pixels will be rendered again and again which is called over drawing and you definitely want to avoid that because this is really expensive on the device what you can use is you can use some optimized shaders from the mixed reality toolkit or sometimes you can even get away with an unlit shader so you pre compute the shading you pre compute that and bake that into the texture right that's smaller good option so keep your pixel shader very simple and don't overdraw right so that's that's really these main things you need to keep in mind because you want to have real-time rendering performance you can also do other things of course because sometimes you really have 3d models from clients that are way too big way too many polygons millions of polygons right and you cannot render those on the hololens what you need to do is you need to do polygon reduction I actually in fact gave a talk at the vision VR air summit about some of those strategies we also published a blog post on the valorum block about different strategies so you can dive deeper into those but in a nutshell you can use automatic reduction there are some good tools available on Microsoft for example they acquire simplygon so they have a pretty good tooling and that's actually what we're using so how freely oddest many uses simply gone for doing a bit of base reduction and then we do the fine tuning off the walls right if they're like for example any hours that happen anything is didn't optimize so well so you can hand tune them off the words another option is you probably have maybe you have seen the talk by unity yesterday Bolivar about the Pixies collaboration so unity guys have a collaboration with Pixies which is another really nice product you might want to check out for polygon reduction and this video is showing you another piece called Umbra so just look at the bridge there so once I'm looking there it fills in those details so this is called dynamic level of detail that's another product you can look into the original model here is the 40 million polygon model and well you can remember that with some of course you see there are some things that might not look perfect but you can at least render down the hall lens and it is that it does dynamic level detail right so it fills in those pieces if you actually look at them if you actually need to render them if you don't want to do that if you don't if you're not allow it to live with those little odd effects or if you cannot actually reduce it there's another way you can offload it so you can use a piece called holographic removing well holographic removing does is it takes input from the hololens like your positional and rotational data the gesture input in all of those sends that to PC and actually a PC is rendering the scene right and then it streams the render frames back to the hololens over Wi-Fi that works pretty well especially for static models let me show that video real quick here so you can see there's a laptop in the background that's my laptop running unity and in the front well that's what I see with the hololens and you can see if I shoot this stuff there it will actually also update there in the PC because the PC is actually rendering the scene right as you can see the latency is really not dead well it's not a long latency it's pretty short and that's really boils down to having good Wi-Fi setup so it's definitely a good solution if you have very large models and there is no option for reducing those up front you can use holographic removing unity 20 18.2 will actually allow you to not just run it in the unity editor but you can actually you know generate an XC file from it so you can run the holographic recording just as an XC file as an as a program right so you don't have to run all that stuff in there editor of unity design in 3d for the holographic frame yep so we're still seeing quite a bit of 2d UI paradigms in the holographic applications or mixed reality applications but the good thing is this is changing quite a bit and in fact there was a lot of 3d UI is being used these days so because we're live in a free world we have a device that is capable of understanding the world around us and we want to remember free DUI in that because this is what we're used to so think about that another part of course think about the holographic frame some people call the holographic frame the field of view it's basically the display size of the hololens right it's not full-faced so one strategy you can use is for example let me show the video here real quick so for this demo the user was allowed to place this hologram initially and if it's if the user wants to place it too close we're not allowing it you see it turns red the user can only place the hologram at a certain distance and this will make sure that this hologram fully fits into the holographic frame right and the tire is not being cut off at the top of the button and the good thing is the user can just always walk closer because we have an untitled device you don't need to zoom in just walk closer right so keep that in mind designed for the holographic frame and give the user a great user experience let your apps live in the real world the hololens supports spatial mapping so it has an understanding of your environment so make sure that you actually place holographic objects in the real world and avoid this so-called era of floating sticker effect where just put in a virtual object in front of the user right place them on the real world table place them under the table for example but make them part of the real world that's really important you can also use things like a or markers like image targets so you use an image and then span in virtual objects or the other piece I've shown you in this video earlier where we're using model targets who can use a real-world physical object and then augment it with extra information this is truly mixed reality right we're augmenting real-world objects with things you would otherwise not be able to see let it focus and stay in court yep so there is the stabilization plane in the hall lens so what is the stabilization plane the stabilization plane is a 3d plane so it's a point and a normal and the Hollande's runtime will make sure every virtual object every hologram that intersects with that plane is most stable so it doesn't a lot so what you once you have is of course that holographic object users looking at is using this API good news is with unity 2017 dot four and the hololens RS for or april 2018 update you can use a new feature which is called depth buffer sharing so you can just go into unity player settings enable depth buffer sharing and that will basically use the depth buffer right and pick the closest object and we'll set the stabilization plane at that object so it makes sure the closest object is always the most stable one let me show you a bit to make it a bit make the explanation a bit better here so where you see in the big frame there is the device pool and in the lower right this is what i record it with the hololens so let's take a look at the video so in the device pool there was this 3d view where you can actually show the stabilization plane so you can debug it in your application so if you take a look here at this red plane if I look at the dog it will actually snap through the dog so even if I move slightly the stabilization plane will still be at the holographic object and make sure it's in the scene same thing here with the cat and yep so again you can use automatic that for sharing or use it manually and there's an API called focus point so you can set the focus point manually but please make sure to make yourself familiar with it because I still see a lot of apps that did or a lot and this is typically the case when you not care about sterilization plane another piece I want to highlight is called the world anchor and world anchor is basically a piece by the hololens runtime assigns the coordinate system to a virtual object so you can assign your hologram a colonnades system and then the hololens will make sure that this particular piece stays at the real world position even if you for example walk outside of the room and walk back in and we'll still be relative at this real world position so how does it look like you can also enable that here show you spatial anchors so you see this little there's little coordinate systems those little gizmos right next to my head there and these all the world anchors so it has one for the cat and one for the dog of course and this is a way to also debug down nice future with the world anchors this you can also well you can reload them so you can save them you can persist them once you reload the application you can bring them to life again and there will be at the same position where you left them so special sorry stabilization plane and world anchors please use those share your Holograms with the world so how can you share your Holograms with the world well first of all as a Microsoft store there's a section for mixed reality so you can submit your applications to the store we used it quite a bit for marketing purposes but also to shared with all clients because there's a new feature they just released where you can have a kind of a private mode where only a user law so basically you would take the Microsoft account from the users you want to share the application with then add those to this application and only those people can actually download the app so this is a good way for beta testing or if you don't want to shout in public and just with some specific clients that's one way another way to share your Holograms well you can make a video right you have seen the mixed reality capture that's what I have been using here so the hololens has this little camera on the top and you can use it for recording and in Denville ads of the virtual objects on top of that the mixed reality capture here this is an example has some downsides first of all it does the encoding of the video on the device so that takes a bit of time so it will actually cap your application to 40 frames per second another pieces camera on the hololens the RGB camera is not that great so you don't have really a good camera there and the third one is the blending is not as it is when you actually look through the whole lens and there are some other things you can do for example spectator view so Mike if you have seen some of the keynotes from Microsoft they use this special camera setup but I have a whole lens mounted on top of a really good camera like a DSLR camera what it does is it actually also does the rendering on a PC so you just used it hololens for the trekking and diesel Kumar feeders both sent to the PC PC Randall's the scene and all the composition good news is Microsoft open-source dead so you can take a look at the spectator viewer it's all open source and also some companies actually make their own version for example we made an one which is called mirror which is looking like this so you have a much better quality of course rade much better camera so we can shoot 4k and you can see the rendering is much nice one and this is a way how you can share your Holograms with the world leverage to tribal knowledge yep so there are awesome small people out there and actually recognize some of them or hearing audience which are very active in the community so we have a very good and active community for hololens and mixed reality in general and you can ask those people super nice folks right they can help you and especially if you're an indie that just wants to get started there's so much good stuff you can use from from this tribal knowledge right so here's a quick video it was a screen cap I made this short link bitly mr expert and it will bring you to this page which has all the nice links so you can go to the forums and you know just ask a question in the forum or you could use the holiday belle person or slack channel really really good slack channel with an awesome amount of small people and we have the mixed reality unity toolkit which is an open-source project that provides you script samples shaders and all of that you get going much faster it's mainly driven by Microsoft but they also take community contributions and in fact one of my team members Steven Hodgson as one of the main contributors here so shout-out to him also unity has their own forums unity also has yep Academy Academy of course you can use the holographic Academy and look into some of those tutorials and yeah unity forums if you're really getting started with unity year it's also a good resource think outside of the box so I've shown you Halle beam where we're taking a depth camera like to connect and can take the data and visualize it in the hall lens the hall lens is already packed with an amazing amount of sensors but there's even more around us right here's a lot example while we're using the Kinect so take a look at the top middle you might spot to connect there so what we did here we synchronize the coordinate space of the Kinect with the one from the hololens so we can use the skeletal tracking of the Kinect and stream that in real time into the hololens and do things like that the delay you notice here is actually not to transfer it's the delay that comes directly from the Kinect skeletal tracking right and that's just one example there's lots of devices out there right small home devices we heard a lot about about the intelligent edge and IOT devices and in fact like this little guy here write this as R or IOT button is a Wi-Fi stack so you can just use the hololens and connect with it you know because it's supporting Wi-Fi and you can do some fun things probably and so just press the button and then well I don't know hologram a little tip here or you can probably figure out some creative things with it long story short the whole lens is Windows 10 device supporting you WP you have the whole network stack streaming sockets all of the goodies so you can pump in a lot of data make use of that build for the whole mixed reality platform yep the hololens is a Windows 10 device running the windows mixed reality platform but it's not the only device that runs it you probably have seen some of those immersive devices some people call them occluded mixed reality devices I'll just say it's about reality devices right but they run also on machines that support Windows 10 and the mixed reality platform so you can build for the whole platform and not just for one device category the nice thing is most api's work of course because it's the same platform and also the MRT K has a bunch of goodies in there here's for example a piece from a dong-young park he added 200 gestures to the MRT K recently so you can see he's using two energized shows with the hololens and then he can use the same to enter gestures but with motion controllers on immersive headset right so here it is with the motion controllers what you probably want to do is if you run the application on the immersive device you want to Randall virtual environment of course and a ground plane or something like that because the user is not seeing the real world them and you want them provide some grounding there yep and most importantly learn about deep learning right you cannot avoid that that's that's really something you need to make yourself familiar with so you don't have to be a resource or you don't have to create those models from scratch right this is stuff that goes into scientific papers but at least you need to be familiar with the concepts you need to understand what is a convolutional neural network what is an lsdm network how do I apply that right you need to be familiar with those those kind of concepts because you can take existing models already that are pre-trained you can use custom vision to make your own object recognizing model that's easy but still you need to be familiar you know what is an output layer what is it in tensor and how does all those things work and what how can I interpret the results so learn a bit about it and of course always think about it the implications of your work right you have a super powerful tool at your hands right so think about that you're doing good we do good things with that that's another important piece you probably have here in the keynote is about responsibility so especially in this age now with AI you as a developer have a huge responsibility about doing good things with it right because you can do pretty bad stuff with it so think about the ethical implications there all right so here are the resources if you want to get started with AI you can see the Microsoft AI school at the bottom there it just opened opened it up to the public so you can follow some of those and what I would highly recommend you check out Brendan Rowe or deep learning videos on YouTube I just link to here and he does an amazing job explaining very complex topics like convolutional network in a video that is like a bit more than twenty minutes and it does it very well visually so check out those videos another one is of course mixed reality community as I mentioned and then you have a link to my blog so I will post the slides and source code links and all of that stuff on my blog and at the bottom right you see my twitter handle so you might want to follow me there cuz I will it was share once I push this stuff to my blog demo code and github poeple a couple of other talks I did before and that's pretty oh yeah and related build session so some of those already happens so you might want to catch the recordings some of those are still today and tomorrow good stuff about Windows machine learning and so on so check those out with that I thank you a lot for your attention and please every eight this session and I wish you a pleasant rest of the built conference but actually we still have a one and a half minutes so if you have a question please feel free to come up to the mic and ask it now otherwise I will stick around for the rest of the conference so you can find me and ask me if you want alright thanks a lot and dry the rest of the conference [Applause] 