 welcome everyone thank you for joining us for this session as I mentioned you probably have seen us all running around here my name is Henry and that's Ted you've probably seen Ted he's one of our most famous presenters here so we're having him work extra hard you've probably seen us scurrying around trying to get all these connections going we're doing around much to actually show you how to deploy to three different types of devices today and we're gonna see a demo for a fourth one so we're going to be doing a lot of on stage switching between monitors so please forgive us in advance if you see the wrong monitor at some point in time we're hoping to get through the demo quickly so we can get questions but if you have a burning question that cannot wait it's okay you can stop us we'll take one or two as we're going through the demos we understand that we're switching between very different spaces quickly in terms of the type of devices that we're going to be targeting and if you have something that you want to ask just show the question or stand up I'll be glad to repeat it and take it great so without further ado so magenta I think I'll be giving you a brief intro about how we think about AI and Microsoft on the edge and we're going to be doing a demo on the heavy edge we're going to be looking at an moving down the spectrum to one a smaller device that Cimino board will be doing a demo with a drone and then we'll be doing the official introduction of the vision developer kit that you probably saw a sati announcement and as I said questions are more than welcome so just to get an idea if you can raise your hand if you are primarily an IOT developer okay that's a fair number of people excellent so we're going to spin on sorry one more if you are an AI developer if you're doing some type of machine learning or AI okay fewer so we're going to spend a little bit of time I explaining the core principles behind AI development and in fact what you you saw at the beginning when I was taking video of Dave it's really our mark of what it takes to collect data so that was her data collection and we'll see how it hooks up into the rest of the presentation in a second but when you come to Microsoft you're looking at our products both on machine learning and AI and you're probably one of two broad categories you're either looking to build your own or customize something that we had given you or you're just looking to consume if you're looking to consume you're going to be going to cognitive services you're going to be given a REST API you communicate with it and your application is AI enabled there being a couple of great sessions on that we're not going to be talking about this use case we're going to be down going down the rabbit hole of custom builds now when you're building it on your own you have two options you can either go code first or you can go and use visual tooling we had announced a couple years back a visual tooling solution that we called our ml studio we're now going to be talking about that we're going to be sticking to the left we're going to be talking about code first now when we're talking about code first we're talking about the different components on that platform and you could be targeting either on-prem with an ulcer bur and that will support your sequel server and Hadoop you could be going directly to the cloud you've seen a lot of talks today about the cloud you could be going to the edge which is what we're going to be focusing on and in the future you will be going to mobile as well you'll see a little bit of development and deployment to mobile but it's just a teaser of what's to come as I mentioned the cloud has been broadly covered will touch some of those components as they are laid they were leveraging them to train models but our deployments are going to be primarily going to the edge in fact all of the demos are going to be going to the edge so when we took when we think about the development lifecycle and Microsoft we look at the AI development lifecycle and it starts with data so you've heard in many many of our Talk's today yesterday and today data is the most important component to get high-quality AI now gathering data on labeling it it's very complex so what you saw when I took the video from Dave is me gathering data I'm gonna be doing transfer learning which means that I'm gonna be using a model that has been pre trained and teaching it to recognize Dave along with other things so gathering that data can be challenging but it doesn't have to be a showstopper there's very interesting ways to crowdsource that data we started right now with we taking these videos you'll see that there's more and more people that are issuing applications for free where you give your data there's no mystery on why a lot of companies have free photo sharing apps where you can actually providing this data for for companies to leverage as part of these efforts with data courser doesn't work you're going to be doing some level of training of with that data and that means that you need to do data curation not all the data is good some of the data will need to be changed and you will meet me you will be needing to modify that data based on that data you will create a new model and that model will be packaged and deployed with an application and then when the application is running data will be collected you'll notice that there are four personas that we are identifying this data engineers any data engineers in the room great because I have nothing to show data scientists any data scientists okay even fewer we're going to be assuming that a data scientist has already worked on and in fact many of them have created very interesting models and networks that we can reuse so we will be reusing that the vast majority of the people in the room are app developers we're going to be focusing squarely on app development and the DevOps or model manager is a new role that we're seeing surface as more and more models are being deployed into the cloud on the edge when you have thousands or hundreds of thousands of models that are deployed and you're rotating on a daily or weekly basis you have the need for a persona that's completely dedicated monitoring this this ecosystem so our machine learning satisfies all four were going to be focusing on the app developer primarily today so uh as I mentioned I wanted to recap a little bit what does a machine learning imply so machine learning are we're going to be talking about neural networks to to make it even more to make it more focused start with an original network that you could either have created on yourself or you could be reusing if it was created by someone in our example today we're going to be using a mobile net very popular network and that network is really a series of layers that are obstructing information about the particular subject matter in this case was going to be seen three types of scenarios we're going to be seeing images being inferred we're going to be seeing thermal sensor data being inferred we're going to be seeing satellite imaging being imported so when you have that network you go and you leverage a framework now there are several frameworks out there tensorflow cafe are two of the most popular right now cafe to actually buy towards Kara's these frameworks are really just compilations of libraries that help you either create these networks or modify these networks in order to modify these networks and to replace those last layers of the network with the information that's specific to what you're creating you need a lot of information in this case these are additional images those images that we just took are going to be part of that information once you have this data you can run the algorithms to retrain your network at then is when you get a model so when we talk about a model we're talking about this trained or retrained model that has gone through this process of taking the network leveraging the images using the framework replacing the last few layers of that couple in this case the last two layers of the network and it gives you a new model now when you have that model you need to use it the best way to think about it is think of it almost as an asset in your in your library and you include that in a project and create what we call an a scoring file scoring file is a term that comes from machines from data scientists in machine learning it really could be an application a web service definition is some way to take input and give it to the model the model is executed inside of a run time which is corresponds to the framework you've used and that is your application when you have your application you want to deploy it now you have two options to deploy you can either deploy it with Dockers into the into the cloud or you can deploy it with those same Dockers now to the LTH you could also go native you will notice that we have a reference between message-passing up here and co-located most of the models that are deployed on the cloud are message passing meaning that you put a service and the model is doing only that it's basically getting information processing it through the model initially in a response and it is a service that is offered to the rest of the network you can in fact and you will see in one of our demos deploy the same way on the edge so you could have a central node that is providing services and you have systems that are providing it in information and it has course the data and then it issues results either to those systems or upstream to the cloud this works well with systems that are doing what we call batch processing meaning that you capture a bunch of data or a little bit and you pass it it's processed and a decision is made is not real-time as you see more and more of these devices like if this guy was doing that it would be fairly challenging for him to avoid a human because by the time that that the message has gone back and forth he's probably hit me at that point so that's what we're calling a co-located application is running right next to where the sensors are and it's operating within it io th has a great job that's a great job of allowing you to have that message passing infrastructure and have the inference in component or this is calling bounine are their own device on the hub or on the clap the challenge is that if you're doing that with things like images that need to be processed at 30 frames per second even the penalty that you pay on going up and down the stack every time that you write a message within the same machine is too much so in that case you will be creating an application that incorporates both the logic as well at the business logic as well as the model and you run it inside of that docker container that's what you will see us doing for the camera so the way am i telling you all of this it's a little bit of nuance but it makes the whole world of difference traditionally if you go and you look at any tutorial online it will be telling you how to do these web services but in IOT age especially if you're trying to run on device you have to make decisions in terms of even milliseconds of delay that that could be too long for you you can't do this you get to decide which of the two approaches you're going to follow and you're going to see both here our platform supports both but that's why we focus so much on oh it's running on device as well versus go into the cloud and so on and of course you can also run a native you see an example of that as well we will be running attached to the or as close as we can to the sensors or native and that's a Windows application you'll be seeing that them as well for a fairly heavy amount of data transfer to you get here so what is really the key body proposition that we have for you we are looking to provide you with an ease of discovery what that means is that there's some idea there's a large number of models that are available for you to choose both to retrain I mean and your organization's as they as they ramp up there will be tons of models that are created within the organization and you need to be able to find them we currently maintain a gallery of models for different applications inside of our machine learning that you guys can leverage and reuse but we're also empowering companies to create their own repositories of models and we help you choose between these those models and find them in your organization now once you have found you want to integrate them and to integrate them you're going to use the tools that you're familiar with you're going to see that most of machine learning has traditionally been done in Python it's traditionally been done just in Jupiter notebooks it hasn't been done in the tools that were you're most familiar with that's why we have such an emphasis on vs code and Visual Studio as well as an SDK that you can leverage both within your tools or our other open source tools like PyCharm and so on once you have created your applications you want an ease of deployment solution what that means is that we have this concept that I'll explore in a bit by which you are able to think about your models as assets that can be deployed anywhere in the cloud and we give you the ability to do make this deployment either using your command line SDK or our web interface and the reach of deployment you see the reach of where we're deploying we're going from I don't know if you can see it here on this side we have that server we're going from that is really heavy edging from that heavy edge to this guy to the drone and to our new camera the reason that we can do that is that we understand enough about the model so that you don't have to so what we'll do is we'll train a model that will be deployed to all of these devices and finally because we have helped you through your journey and we have an understanding of where you've made these deployments we provide a single pane of glass to monitor the web services that you have deployed the heavy aged applications you have run in all the drones circling around in your company that are running machine learning models they all might be using the same model or a slight variations of it and we allow you to look at them through a single pane of glass and be able to manage them now you've heard me talk about heavy edge and light edge what does that really mean these tables hoping to illustrate that so Asher and the cloud we have primarily CPU and GPU systems and now we have area 10 FPGA based systems so that's heavy iron we have thousands of servers running for you and they are running these models natively to Windows or when when it's a native Windows model like in FPGA or in containers using docker when it's running in the Linux systems then comes a heavy age and you notice that their computational power is reducing as we go from left to right these are systems that are big enough like that system that can even host FPGAs inside of them their systems that can have GPUs orders that are a little bit nimbler like the minnowboard so you're going to be seen here which actually run x64 systems or ARM processor based ones which are raspberry PI's but that's not where it ends there is even lighter edge these are even a smaller ARM processors these are rtos based systems in many cases what's running on your cars were turning on your security cameras what's running on your phones all those sets and phones are really more heavy edge technologies ideally you'd like to deploy a container but that's not always the case in many cases you have to go Android native Android us and support containers so you have to go native in some cases you're going iOS device in some cases you really is going real-time OS these are the West is running on your cars for instance and I'm not talking about the autonomous riding component but the one that tells you what music to listen to the head up heads up computer that comes which is independent from your autonomous driving computer so we're talking about the light edge we're talking about that type of systems a system like the developer kit a visionary developer key that we introduced is in the light edge because it's co-located with those sensors and it's performing all the inference in there and it runs on batteries right now this guy it's running on batteries but so what we're going to do through our demos is we're going to be moving through the through the edge so we're going to start with a very heavy edge and we're going to start with FPGA and for that let me introduce Ted again and it'll be walking us through the demo right great thanks Henry all right good afternoon and so that's how many we talked about us we talked about the spectrum I'm not sure if you can see that but there are two servers in there and the whole idea here is a as an edge device we can put CPUs GPUs and then we're also announcing the ability to put an FPGA in that server rack so you think about the requirements from an IOT edge perspective why you might need to have that compute power on the edge so imagine you have fifty or a hundred surveillance cameras and you want to do image recognition image processing on all of them GPUs are great but also FPGAs now processing more than 500 images per second you think about just having one chip to be able to do something like that so the first thing I'm going to show is just is to go into the FPGA machine hold on let me take away this okay so what I have here is I'm basically remoting into a server and if you look into the devices manager you can see here that there's an actual FPGA device in this server and so the way that I ot edge works if for those of you who are not familiar but you've probably seen a lot of us sessions on this already is that IOT Edge enables you to be able to container rice the functions and the things that you want it to do and then IOT hub will facilitate deploying those deploying the configuration to the edge device and then that edge device will pull down the containers with the right things so you can compose the same types of pipelines in the cloud that then you can also do on the edge so for example the kinds of pipelines you might have would be a data ingest you might have a container that knows how to talk to a camera and ingest images you might have a stream analytics container that does a real-time stream processing of data you might have an image processing container you might have a anomaly detection or predictive maintenance model these are all things you can containerize and then pull down to the edge and so what we have here is one of these live machines and you can see this it's in the booths so if you go to the IOT booth you'll see a real live machine with a real FPGA and they're running running this real model right now I'm just remoting into it but basically in this case we have a we have a demo that we will show and this is basically J Bowl tables a manufacturing customer and manufacturing company what they do is they want to analyze images of circuit boards for manufacturing defects and so using a ai model to look at images to identify whether they're manufacturing defects or not so what I have here is a is basically this model running and I'm going to kick it up and you can see that just by sending one thread I'm already getting close to 60 images per second and you think about on a CPU you get up only about 6 images per second so right now with just one thread hammering the FPGA on the on the on the device itself I'm getting 10 times better performance than CPU so I'm just going to ink the number of threads and you can see that I can get a little bit more performance and throughput out of here see if we kick it up even further but the whole idea here is I just have 12 threads that's just sending a lot of images to this FPGA model and the whole point of this is that now I can take it and I can process up to about 500 images per second all on this edge device and so this is just an illustration of the ability to be able to give you the flexibility of putting different types of hard work on these edge devices starting with super super SuperDuper heavy servant grade hardware so that's basically what we're what we're able to show from an FPGA perspective so let's go back to the deck here which that which is 96 it's okay okay so on you remember the picture right the video I took from Dave by the way we're just parsing this around because we're actually training while we're doing this so I while there was talking to you guys I did a couple of things that video that I took went into onedrive so I just pulled it from onedrive so what you're seeing here is me just leveraging ffmpeg so you can run ffmpeg give it a video and it will generate frames for you so that's how you go to about 300 frames for Dave rather than having a whole photo session for him once you have the frames collected you can use a command that we have a Z copy that would actually take those images and connect you directly to the to the blob storage so I put it on the cloud from that from that point forward any time that I want to use those images I can just do it directly on the cloud I don't have to upload anymore and that's what we did it uploaded about 233 images now with those 233 images I can now go and start the training session for that but I'll give you more details about it in a second can you go great so while we're doing this remember this is how we collected the data right now we're gonna do a demo of a minnow board which is an intellect 64 system it's connected to temperature sensors and we're going to be training it with BS code plus our ml so the next step after you've collected the data like with the way we did it would be to hey go ahead and train it so that is going to be showing you how to train it using BS code and our services when you think about our ml the important thing is to remember that it's a set of services in the cloud so you can access them from any platform where you're a capable of issue interest a PI commands or an SDK call and then Ted is going to be showing you also how to do that deployment using the CLI which we also support on to the edge okay all right so next up as Henry mentioned now we're going to take this end-to-end data science journey so the scenario that we have is imagine you have a factory and there are a bunch of temperature sensors and all these temperature sensors are sending in data and you'll want to collect the data and you want to predict whether there's an anomaly in the temperature that's coming through and then imagine those temperature sensors are connected to this mineral board right here so i OT device we went from super heavy edge to a lighter edge here but something that can still run a docker container and so this is the the edge device that I'll be referring to so the first step the first step is to train our model and so I'm going to jump over to vs code and here in vs code basically the idea here is I'm able to as a data scientist just code and my Python using the frameworks that I like to use and I can just do all of that in vs code so in vs code I'm then able to connect to our experimentation service and the experimentation service gives me the ability to bring in the compute context that makes the most sense so for example I may be wanting to experiment very quickly I may want to just submit jobs and train on my local machine on my local workstation I just want to try different things quickly iterate quickly with a small data set just to get a feel for the data and then start creating a model I can just do that on my local machine then when I want to train on the larger data set with a lot more compute power I can spin up a spark cluster for example I can spin up another cluster and submit my job there now with a big data and big compute I can then train my model on something like that if I want to train a deep neural net and I want to spin up a GPU cluster with Azure batch AI then I'm able to spin up a GPU cluster on Azure submit my job on to that GPU cluster run that job and then get the model output so everything from here basically what you can do is click on submit job and these are the various a compute targets that you have from an azure machine learning perspective to be able to train your model and then so the next step after training that model is to is to containerize it so the output of your training will essentially give you these model artifacts and the model artifacts could be a Python pickle file basically the model serialize as a pickle file it could be a tensorflow check point you know just various outputs of these of these various frameworks and what you want to do now is to containerize them in a docker container so we have a container building service what it will do as you can see here is that it's taking that model file that model pickle file it's taking what we call a driver file which will enable you to define what happens every time that model is called and then and then it'll containerize it so this will be a container a docker container with a REST API and now this this doctor' container can be deployed to the cloud you can deploy this the cloud or anything else that can run a docker container we have this REST API that you can send data to and you can get a response back also included in this docker container is the IOT Edge SDK meaning that now if you deploy this onto an edge device you'll be able to compose that pipeline on this edge device such as this minnowboard and then you'll be able to run that model directly on to an IOT edge device that's incorporated with the IOT edge runtime so the the container building does take a few minutes and basically what I'm going to do is just kick this off and then I've already built the built a container and so we'll show the the output there so let's go over to my IOT hub and in my IOT hub I'm going to go into IOT edge and in IOT edge I have the capability to register various edge devices so for example I have my mineral board here and the nice thing about IOT hub is now I can keep track and manage of all my edge devices and all the various modules and containers that are deployed so for example if I'm a retailer and I feel Evan thousand stores all across North America I might tag various edge devices and say these are all of my devices in California these are all my devices in Washington these are all my devices in stores that are in cities with over a hundred thousand people I can tag them however I want and now let's say there's a new model and the new machine learning model I can say deploy this model to all-to-all edge devices in California for example that's what I can do with this so as you can see here on the device you can see that I have the IOT Edge runtime I have my temperature majority deployed and my anomaly module if I wanted to create new modules I can just click set modules here at i/o T edge module and basically give the name and the container registry that contains it and then I'll be able to let it know which ones to which ones to pull down onto the edge device IOT edge also gives you the ability to compose routes and so the idea here is with all these different containers you can configure where the data is going to flow and IOT edge facilitates the flow of that data so going back to that example maybe you have an ingest module that takes in data from a temperature you have a stream analytics module and you have a machine learning module that does anomaly detection so you say the the temperature sensors will send data to my ingest module the ingest module will send data to my stream analytics module the output of the stream analytics module will go to my machine learning module and the output of that may go to the cloud for example so that's what you're seeing here my temperature sensor module is the output of this one of the temperature sensing module is going into my anomaly anomaly detection module and the output of my anomaly detection module is going into the cloud but I can also send that to another container for example so all these are things that you can configure in the cloud and then when you click Submit this JSON document will be sent down to this minnowboard and then the minnowboard the iot edge runtime and agent on here will pull down the write containers from the reg from the correct registries and instantiate them on something like this so after doing that let's jump over to the actual minnowboard and this minnowboard is running ubuntu here and let me see if I could increase the font size a little bit here all right so on this actual device I have all of the containers that have been deployed and so you can see here I have my nomally detection module I have my temperature sensor module these are all containers that have been deployed onto these devices and they're running and they're talking to the IOT edge runtime and then I can monitor what's happening on this device so what I'm doing now is monitoring events from this minnowboard and you can see here that it's able to detect that an anomaly was detected or things are normal for example so now I have the ability to be able to to kind of summarize everything that I just did build a machine learning model containerize it as a docker container deploy that onto an edge device and then now I can monitor these events to see whether there's an anomaly or not on this edge device so that's the end to end of training deploying a model and deploying it onto an edge device all right thanks awesome so that was that was the second demo for the edge I don't know about you guys but first time I tried to do these I got confused the second time I got confused again so what helps me is to actually reiterate the components and what we have seen so let's see if that helps us good so it's all starts with an idea model creation and you've seen how it is when you have that model you can deploy it that you can think of a cognitive service or the images that you create on your browser using custom a vision that I as a way to create them the path that we have just gone through it's a little bit more complex but in essence which we're following the same pattern we're creating a model we're putting it in an image and we're deploying it somewhat how do you go about executing it that image needs to be run in one of the devices you've seen it you've seen those images running in two types of devices so far the heavy age and the minnowboard you can also go and trade it and that's the process that I'm going to walk you through as well Ted also went through that process he did a little bit more streamlined I'm gonna go in extreme detail about how do we go about training it so the part we're going to be following right now remember we captured that data we're going to be preparing and registry in that model and of course you can go directly from there to create an image and deploy it but we're actually gonna go and create an application for that model when we create that application we can also go to an image and get it deployed which is exactly what Ted just showed you but we are actually gonna go and deploy it through IOT edge so we're going to go directly in through IO th and IO th is the one that's actually going to put the application on the device and then we're going to monitor that model and the application and add additional data that data is going to help us improve the model later on so that's what we're going to try to do again so if you remember where we left Dave images is that we had upload those images into the cloud and we had them ready to be used so how are we going to consume them let's go to visual studio code we're gonna use retrain Oh how does that work no wonder you're such a so this BS code this is the this is how you train a tensor flow model so extreme detail you can see how large the the code base is here but let me highlight important parts on the code base here so we're gonna go to do you remember the images and how we put them in a blob storage we're first gonna download those and I'm gonna move quickly on these because he has my key and I don't want to share it with you but you download these images those images get downloaded into assist into the file system that you're using in this case we're going to be connecting connecting to one of the VMS on the cloud that already has tensorflow we provision tensorflow for you in these machines in 2d SVM and it's going to download those images once you have those images downloaded it's actually going to start training it so it's going to start passing those images and starting to generate bottlenecks and being able to generate the tensors and all the data science component I'm not going to bore you with the details what's going to happen at the end is that it's going to generate a model and that model is going to be three scroll a little too much that model is going to be stored into this output of tray of models and then we have the training summaries and their labels so two things are important to us the model itself and the labels that are going to be generated and notice that were using that architecture mobile net here so when we have those directories in output those files we're going to go into retrained those files are returned graph and retrain labels after we have them those are too large to run on on the systems that we're going to target next the drone and the camera so we're going to quantize them so quantization is a process by which we basically reduce the operations to two 8-bit operations and we're going to be creating that new quantized model and storing it locally the advantage of using Azure ml is that all of these files you get with the mod so when you take one of from a gallery one of our examples or solutions you get the the code that was used to train it if you go into the packages and that's why the announcement about packages is so big if you want to train any classifier any visionary classified you just download package the package for vision AI that we have and that includes everything it includes even a sample data set for image of training but it includes your application it includes your training code and it includes the evaluation code as well now I could run it directly from here or I could use another one of our surfaces which is the workbench now the workbench this is a trainer that I that I have done earlier today but I can always go here and start a new training session so I can this is yet another interface you can do this from the command line and in fact I'm gonna do it from the command line you can deduce where to do it you want to do it locally do you want to do it in a docker container do you wanna build it in the BM and this is the BMI created and what do you want to do that you want to retrain do you want to quantize do you want to test it you can also do the same thing just from the command line so what I'm going to do is I'm gonna i should have made this bigger sorry about that so here the command that I'm gonna that I'm gonna issue is as your ml experiment submit and then I'm gonna say the I'm training I'm targeting the VM and I'm gonna ask it to run retrain when I do that what it's doing is its copying my file into the cloud it's putting it on that VM and it's starting a security in that environment as the VM boots the first thing that we do is we provision all of the dependencies that we had in for that file tensorflow numpy all of the dependencies it gives me a run ID that's my that's how I keep track of it and then it starts to execute and you can see starting to download by the way the classifier we're training is a cat and dog classifier I'm using the cargo krandall cat and dog image data set but we also add a date so hopefully our classifier is going to be able to distinguish between cats dogs and dave so it's doing its thing it's downloading the images there and what you will notice is that this is the training it's running so it's going to take a fair amount of time so what I did on the background is I kick-started another training set so I have finished the previous training set and I ended up getting the two images now what I'm gonna do is I'm going to target this drone with that so how am I going to do that this drone has a controller which is these this Android phone that's how I control the drone we have the controller now our friends from DJI you saw their announcement have been kind enough that they have an SDK for this drone that you can leverage both from Windows and from Android I'm going to do it from the phone because Satya already they went from Windows and it's pretty cool it's actually Sam but it's pretty cool so I'm gonna do it with the phone it's not gonna be as cool but we'll try so the phone is not running Windows this phone is running Android so I'm gonna create an Android app when these finishes when one of these run finishes those files the graph on the graph file and the labels Fowler created and then deposited in my assets directory if anybody has has programmed in in Android you know that there's an assets directory where you can put components so it's placed a graph here and shibi labels around here and labels there so they were finished a few minutes ago and this is my code this is the sample code if you go into custom vision that AI for instance and you choose export to TF he also gives you code very similar to these that you can use so if you wanted to try tonight just go custom vision that AI put your images I told you really how you can do the trick to get images really fast put them in that get them trained and get your transfer flow you can do that you can get the model and the example so what's interesting about the DJI sdk they have great talks online so i'm not gonna bore you with the details but what basically the application is doing is that it opens our Wi-Fi connection to the drone that's how it controls it but it also streams the images now in order to stream it is stream sisters ry UV and those images need to be translated to JPEGs because if you remember I transform the images from Dave into JPEGs my model only understands JPEGs so my code is going to go and take those images and then it's going to pre-process those images and that's the what you're seeing here because the model was trained with images that were of a particular characteristic particular size what you didn't see in the in in the training code is that that's 224 by 224 images yeah they're very small but then I don't have that much room here to actually put all the images so this is going to be doing a pre-processing of those images and then I'm gonna be able to run them through the classifier literally I'm just calling the classifier which incorporates those assets so let's try it I'm just gonna run this here very safe passcode and when I ran it so there's my pixel and here's where you gotta remember to be kind because we did this life we have another model to test in case this fails but I have not seen it so this is going to be our first time so it's building it right now I'm gonna get my drone ready there we go and now we need our trusty props for the images themselves which they should be around here did you see the images of cats and folder go so let me see how are we doing this is the app itself let's see if it actually run super secure okay that's the app so I'm gonna open the connection to the drone and there's the drone we're all live that's awesome so Dave I'm going to ask you to come here please so I'm gonna hit classify which means that I am gonna begin to classify these images so that's just the world yes and now that it's a doll very good and that is a cat and the moment of truth that's Dave hey wait again it has to work Dave there you go thank you whoo we dodged that one so uh that's how you build it so you can go there to come to night and you can build one like this just for drafting my instructions they'll be much better online if you go to custom vision so going back to what did we just do so if we're all the back and forth with this right but so we trained as I said you can train in the blimp round so just go custom vision that AI it's a cognitive service you can just go train it there you can also go look for pre-trained models or examples that we have so you go to that website gallery that I show that AI we have examples solutions that have for both vision as well as anomaly detection and so on you can train it both with Jupiter notebooks vs code Visual Studio or the azure ml workbench that I showed you and then you can deploy it with CLI and SDK and then of course you can do that with a web browser vs code and Visual Studio after that you should be able to monitor it as well and we'll show you that which is basically you're going to our into our system and you're able to see all the different models that we have deployed and you get information about those models as you go on and collect this information you will be able to get even better infirmary results with your with your models I'm sorry but this one in particular seems to looks like I'm in the wrong bank one second please okay great I found my place again okay okay so we successfully did their own demo so that's great but um what are the challenges of running these models from the edge more specifically right because so far seems like everything is easy it should all work well as we go even smaller to a system like this or even it smaller than that you have you're going to face reduced compute now reduce compute is imminent because you have much less power so you need to add hardware acceleration now the hardware acceleration task to go beyond your arm and CPU so you could go GPU now the challenge is that when you go to a GPU you don't have a common hardware abstraction for neural networks GPUs for these devices from non-existent to adrenal moly NVIDIA GPUs as you go down that list it gets even more and more expensive and then you have to face driver version fragmentation if you might have an Adreno GPU but the driver that shipped because the odm decided to put an early driver of that GPU that's an exposure to hardware acceleration for your neural network so you're out of luck you have to worry about all of those components and of course your cost increases the alternative is to use a neural network accelerator now there are many options for neural network acceleration and you need to be familiar with every platform if you were to do that so how can we help you with that with that diversity that's when we got together with our friends from Qualcomm and decided to tackle this by certifying support for a target platform and you hear from my friends are dueling us in a few minutes about why we chose Qualcomm but we chose Qualcomm as our first partner for this and we look to incorporate incorporated as a first class deployment target meaning that you can leverage all of the tools that we've shown you today and target directly that device as simple as any of the other systems you will be using the same pipeline so same training same components same BS code interface and hopefully we will enable an ecosystem of devices that use this board so that's how we ended up with a vision AI Developer Kit it's a fairly beefy Linux based system that it's gonna run your models on device on the edge this system you can get access to it you just got to vision ai developer kit efficiently eye dev kit comm or you can see us in the boot because the system uses the same software pipeline you can start leveraging our pipeline to create the models now and there will be a limited release of training examples and training network trained networks that you can use soon so that's why you should register you get access to those even before you get your device you will be able to leverage those solutions and those causative services to create your systems your applications those get translated into docker that are it's a version of docker that is hardware accelerated it will include sorry not doctor itself but the image inside of the application inside of docker as well as the mobile hard work so that meaning that they're targeting this particular platform and they are leveraging Azure IOT edge to reach the camera so it's basically the same process with an elaborated for you today the magic is happening on the device and to tell us more about that magic let's welcome shadow from Qualcomm every thank Cindy for walking us through this hi I'm Cheryl I'm from Qualcomm I'm a product manager over there as part of our collaboration over here with Henry and the extended Microsoft team when we thought about bringing the vision AI Developer Kit we said the kid needs to be more power-efficient at the same time do inferencing at the edge in a manner that it gives optimal performance so we came up with this kit in coordination with some new platform that we were announcing so few weeks back we had announced a platform called vision intelligence platform that's what we used for this kit and in essence the whole concept of vision vision intelligence platform is it's based on a heterogeneous architecture so it has an integrated ISP it has an integrated GPU it has an integrated display processing unit besides it also comes with an integrated Wi-Fi Bluetooth connection and an integrated arm CPU core and an additional component which we call as a vector processor or so as to say HVX an extension of a DSP when we look at doing inferencing at the edge power is optimal but at the same time you want to get more information so there are three options on this platform to do inferencing at the edge one you can run your fixed 8-bit precision networks on the arm CPU or you can run floating-point 32-bit networks precision networks on the arm CPU but apart from CPU we have GPU as well it comes with the audrina 615 a GPU again on a GPU can run floating-point 16 or 32 networks but then we also had another core which we call as the DSP or the hexagon vector processing unit that is basically the most power efficient core in the whole processor and we intend that developers can use that core mainly for the neural processing engines that core right now supports the 8-bit appreciation so definitely once the model is quantized it can be run on this core in order to support this heterogeneous architecture what we have done is we have a developer kit that we call as Snapdragon neural processing engine npe this is an SDK which is available on Qualcomm developer Network anyone can go right now and download it as part of this kit what you get is a conversion tool so once you take a model from tensorflow or cafe you can convert it to a run time for the Snapdragon processor and then the kit provides you access to whether you want to run it on CPU or you want to run it on GPU or you want to run it on HVX so we provide you that flexibility and once you have that conversion done on probably a laptop or a machine that you have you can load that model generally on a device now in collaboration with Henry steam as part of the whole azure IOT edge effort what can be done now is you can train the model on the cloud just how Henry did you can train for day of space on the cloud put it on the edge device through the docker container so now you can do the training on the fly you can use the high compute power on the cloud but then do the inferencing at the edge at the same time you can take care of privacy reliability as well as latency aspects if mission-critical activities are to be done at the edge so that's where are the Qualcomm QCs 603 platform comes in and then it comes with all the SDK just for the edge device so you can have the SDK for RT edge you are can also have the snappy SDK for the device thanks thank you thank you great so let's see in action that can you deploy a container so as sure do mentioned they support several frameworks if you notice one of the frameworks that they support it's a tensor flow and inside of it the network the network that they support is mobile net so we have a mobile net remember that's why they want to train so we're going to deploy it onto the device so we're gonna try to sweep it back alright so basically we're going to be doing the same thing we're back in our IOT hub and we will look into our iot edge and so you can imagine that camera is a is registered with our IOT hub and this is our cue CN n device right here and this is something that I had just because it's something that can just run various containers I just deployed the temperature sensor model on there anyway just to make sure that it that it works but but in essence what I can do let's have to deploy an IOT edge module on here and I'll give it the name just give it the name this is an image recognition model for example and then give it the location of that repository save this and to next year whoops and then I'm just submit this and what's essentially happening now is I'm putting together that JSON document and configuring that I could configure the routes and say okay I'm going to take an image from the camera and then the image when the camera is going to be going to be processed in that AI module for image recognition for example and so what's happening now is that I'm getting status back from the camera and right now it's saying this image recognition is doing as a pending deployment and so depending on how long it's going to take this this is going down this JSON object is going down onto the camera the camera recognizes and the IOT Edge runtime says okay I need to pull down this container so it's going to go to that registry and it's going to pull down that container and depending on how long that it takes for the container to go down onto the camera it's going to do that instantiate that docker image into into that docker container and then be able to run that module right so so in spirit of transparency we've already trained a network that's much better than mine to run out the camera and it's already deployed on the camera so if we can just look at the camera so that's the camera it's been looking at me the whole time so you've seen it making cameos as we go and there's your tap and there's your dog and the camera is running on batteries again so that's the visionary eye developer kit and as I said you guys can start working with it in terms of software the SDK from Qualcomm is available you are more than welcome to go and be see the boot register online to get access to early samples of the code for you guys to play with it you can target the cloud initially and when they when the camera becomes a bubble you will be able to deploy into into the camera once again that's vision AI dev kit comm and that's it thank you if you have questions [Applause] 