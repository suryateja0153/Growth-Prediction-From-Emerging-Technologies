 hey everyone my name is Lukas I'm the senior program manager lead for the Windows AI platform team so far a build you may have noticed the theme AI we've done a lot of talking about AI both on the intelligent cloud and now on the intelligent edge today I'm here to talk to you about the Windows AI platform on the intelligent edge so with a quick show of hands how many of you today do some sort of machine learning or AI technique in your current applications that's awesome that's almost like half of you that's great for the other half you may be wondering you know as a developer why should I care about AI half the people in this room already do and the simple reason is as developers were often asked to solve incredibly hard problems for instance some of these problems are very difficult to solve when the only tool we have available to us is traditional algorithms and procedural code consider the sort of code you need to write it in your line of business application you are asked to classify whether a specific customer may be a retention risk for renewing their services with you or not imagine the code you would have to write if in your application you had to take a specific action but only if a particular condition or object was detected in an image or in a video stream maybe in your application sometimes you're asked to find a defective part as that part moves quickly across a manufacturing process these three problems have one thing in common they're somewhat easy for humans to solve with human intuition but each one of them is incredibly difficult to program a computer to do this is especially true again if the only tool we have available to us are traditional algorithms and procedural code this is where era AI can help and we know this because I'm Microsoft we're using AI today to solve some hard problems here's one example the windows Inc platform uses AI techniques today to figure out conceptual meaning from physical ink strokes that you as a user may draw on the screen using AI techniques the ink platform today can figure out whether a set of strokes represents text or whether it represents shapes with this knowledge we can now start figuring out is this text part of a word knowing that something is a word we can start figuring out is this word part of a longer sentence knowing that we can do more conceptual understanding and figure out are these sentences part of a paragraph and when you think about what else is happening in this diagram some of these lines have little annotated bullet notes next to them and using AI we can figure out hey these lines seem to be part of a list and we should conceptually think of them as a list all of these inferences are incredibly difficult to do using traditional algorithms but with AI we can do it and because of that we can go the next step and figure out additional things for instance there's some text up here that looks like a phone number using AI we can understand that this may be a phone number and it may be associated with I believe Tom's name is somewhere in the doodle and now that we know that we also see that looks like there may be an invite to an event one week away again figuring out all these conceptual properties from just physical ink strokes is incredibly hard if you don't have AI to help you as you've heard in the keynotes this morning Microsoft has made a lot of investments in AI and we've worked with developers across the industry to understand what AI solutions developers need many of Microsoft's investments to date have been in the Azure cloud we have a set of products that can help you do many different things as your cognitive services gives you cloud base pre build AI api's that you can use in your application to help you solve hard problems with vision speech language search and others custom vision allows you to create MA that are tailored to your unique scenario to help you identify objects and pictures Azure machine learning services gives you an end-to-end solution for preparing your data using that data to create your AI and being able to deploy that AI - fully managed web services and likewise if you're more of a do-it-yourselfer as your batch AI gives you infrastructure where you can scale your AI training in the cloud across a bunch of machines that have GPUs but as developers we sometime need these AI capabilities on our edge devices where our application runs close to the data there are three reasons for this first low latency sometimes we need real-time results for instance if you are evaluating whether an object appears in a video stream you can't necessarily wait for the round-trip cost of time to the cloud in order to use AI to tell you that yes in fact the object that you care about is in this image it's also true if you're trying to use AI in really high performance scenarios for instance imagine using AI to power your game engine so that the non player characters do intelligent things the second reason is scalability if today you have a service that's already deployed at scale and you are looking to add intelligence into that service being able to do AI processing on the edge where your service runs allows you to optimize your operational costs this is because you only have to worry about spending money to train that AI and Azure a cloud and you don't necessarily have to worry about every AI operation getting sent out to the cloud for evaluation the third reason why AI processing on the edge is important is flexibility sometimes our data is just going to be too large to send off to the cloud other times we may just not be connected to the cloud how many of you flew in Seattle today only one person that's amazing that person may not have had Wi-Fi on the flight that they came in on but we still want our applications to do intelligent things for them even when there are 40,000 feet in the sky likewise whether due to customer policy or preference sometimes we just can't send the data off the device to the cloud for AI processing so when we think about bringing AI processing to the edge on Windows there's three things that we think about from a Windows AI platform perspective one we want to give you developers a platform that you can easily use to bring powerful intelligent experiences into your applications two we do it in a way where we can use those exact same tools in that exact same platform to help improve Microsoft products make Windows smarter and all of our products smarter and three as you heard in this morning's keynote we want to make sure that a Windows AI platform integrates fully with all the Microsoft AI offerings that we have both on the intelligent cloud and on the intelligent edge I'm happy to say that in the Windows 10 April 2018 update we're releasing Windows ml a new API that allows developers to use pre-trained machine learning models to evaluate hard problems on the edge while using the full hardware capabilities that are available on that end note this is a unique offering for Microsoft because we are in a position where we can train models in the cloud bring them down to the edge where you can evaluate them with Windows machine learning optimized down to the silicon level let me show you how here is an application that we have that does something called style transfer you may have seen applications like this before what they do is take an image and apply a visual style to it to produce the image on the right this one particularly is using Windows ml to power this experience and because it's using Windows ml there's a couple cool things to know first every single one of these visual styles like you've seen just change over I actually didn't have to write any new code to do each one of these output images has been generated by the machine learning model that's been trained over diverse datasets from famous artworks when you think about the advantages of the intelligent edge in this case I would not be able to do this scenario taking input from the camera stack and doing this very computationally expensive processing on it with us fast framerate as I have right here at the only tool I had available to me was to send it off to the cloud for evaluation to each one of these frames is fairly large and I'm sending a lot of them for processing again from a scalability perspective I may not be able to push all of this data off the intelligent edge and three for those of you who have maybe walked by the Windows AI platform boof in the expo hall I may have asked you to dance in front of this demo just for fun and you have to ask yourself a question sometimes do we want images beamed off the device especially when we're doing something as silly as this that's where having the intelligent edge really helps with your our applications and because this is using Windows ml there's also a few more nice features to note first we've optimized the path from the camera down to the GPU where this model is being evaluated that means we've gone rid of as many redundant copies and streamlined the path as much as possible - this is right now running on the NVIDIA GPU that's inside my surface book however I can take this exact same app and run it on any DirectX 12 GPU to get hardware acceleration as a developer my code doesn't change as the app moves across the diversity of the windows hardware ecosystem let me show you how I did this as I noted before every single one of those visual outputs was the result of a machine learning model evaluation machine learning is a very powerful tool it works by finding patterns in existing data so that when new data is given to a machine learning model it can use that previous experience to identify new patterns we have a saying when the solution is difficult to describe describe the data instead and what we mean by that is if in your line of business application if you can ask yourself what is the really hard problem that my application is trying to solve and phrase it with clarity and then if you can go off and identify a data set an existing data set that is pre labeled with the desired result you can use machine learning to help you find the new pattern in new data when you get it for example now I can ask myself the question will my customer renew their services and if they do which services will they renew if I have existing data that's shown which customers tended to be ones who left my service versus ones who stayed I could use that data to train a model that will help me answer this question in my application running on the edge let's review the conceptual pattern at work here we call this from ideas to inferences we start with our application again in our application we ask what is that hard question once we've identified that hard question we can find the data that's pre labeled with our desired result and we can take that to the many offerings that Microsoft has in deserve to help us train a model so for instance we could use Azure machine learning services we can use custom vision and the output of that training is something called a model in this case you see that the model has the word onyx underneath it I'll tell you more about onyx later for now all you need to understand is that it's an industry standard model interchange format once you have an onyx model Visual Studio makes it easy for you to start adding intelligence to your application if you are writing a uwp project when you add an onyx file to that project visual studio will automatically generate Windows machine learning inference code for you this is all code that you could have written yourself however Visual Studio makes it convenient for you now that I've added that onyx file to my application I can start wiring up my application data that new data that I want to find a pattern over into the Windows Windows ml API calls and because I'm using Windows ml I know that model will be evaluated on the full diversity of the windows hardware ecosystem using any dx12 GPU without me having to write any different code to handle a specific different GPU device pardon me there so when I was writing that style transfer app Windows ml helped solve three problems for me first as a developer I didn't have to worry about creating each one of those visual styles myself I could focus on the data that I have and the scenario that I wanted to enable in this case a visual style transfer Windows ml has a built-in evaluation engine in Windows which allowed me not to focus on infrastructure and deploying machine learning engines but just focused on scenarios to Windows ml enables me to use models trained in a variety of different frameworks so long as I can get that model into an onyx file I can use a Windows ml 3 Windows ml offers hardware acceleration to give me fast evaluation results and we do this across any dx12 GPU because we use Direct X a lot of you in this room may know that it has a proven history of high performance across a wide range of devices and we do this by offering both a 132 and a uwp API for you to use so half of you in this room raise your hand previously saying that you're using some form of machine learning solution in your application I want you to think about how Windows ml can now make your lives a whole lot easier when your data science organization brings you a set of models that they want to see used in your application you no longer have to worry about well how do I get the complimentary evaluation library deployed of my application to evaluate that model Windows ml does it for you likewise if you need hardware acceleration in the past you would have had to code against specific third-party SDKs with Windows ml you don't need to do that anymore I'm about to jump into some code but let's review the windows I'm Mel calling pattern before I do there's four steps first you use the windows ml API to load a model from an Onix resource into memory you then wire up your application inputs into the inputs that the model expects for those of you who aren't familiar with models at this stage just think of models as a function they take inputs they take outputs and inside they encapsulate a bunch of functionality once I've hired up my inputs I can run a valuation on that model to generate outputs and then I can interpret those outputs to drive my application experience let's put it all together with a quick code sample on the screen you see a very simple uwp application there isn't too much special going on in the UI it's a simple and canvas control and a couple of buttons however we wrote this app such that when a user draws something in the in canvas control we can use the amnesty which is a model trained on a wide variety of handwritten images to detect what digit the user is drawing I mentioned earlier Windows has a very good ink platform and this isn't how you should go think about doing ink in your application but it's a very good example of how Windows ml works so if I for instance draw a 3 in the same canvas and I hit the recognized button in this case I thought it was a tube but that's more of a statement of my poor penmanship so let's draw a tube and it thought it was a tube now I'll try a 4 and windows ml evaluating this model determined that I had drawn a 4 let me show you the word flow to actually get this going this is the visual studio solution for this project I'm not going to show you code in Visual Studio I'll do that in the PowerPoint deck because it's easier to read but I want to show you how easy it is to get started with onyx models in Visual Studio for those of you who may have a hard time seeing the mouse I'm bright clicking right now on the assets folder to add a new existing item I'm going to navigate to a folder where I know my onyx file lives and I'm going to add it before I do I want you to notice that in the list of files there's going to be a new file that appears as soon as I do that awesome and when I did that mm is that CS appear let me show you what the code that was generated looks like Visual Studio generated three classes for me within MSDS these are the first two one of them is an input class and the other one is an output class I'm gonna really geek out here over a couple of properties but bear with me for a sec the first thing you'll notice is that there is a getter setter property for something called input 3 that takes a video frame as a data type now input 3 was actually the named input that the model described Windows ml using Visual Studio cogeneration tells me what that name is I could have also used other tools like Metron to inspect the model find out its metadata and properties the important thing to note on this line is the data type video frame machine learning models in general do not understand images and my application actually created a software bitmap of the ink canvas before I fed it into my machine learning model however since machine learning models don't tend to understand images rather very long tensors of individual pixel data massaged into this long list it creates a problem that's not how developers actually code against images Windows ml makes working with image models easy video frame is a common data type to get an image source out of a camera or a bitmap on disk underneath the covers when Windows ml detects that you're loading an image model it specifies video frame as an input type when you give it a video frame for the input Windows ml will automatically convert that image into the tensor layout that the model expects it does another thing for you that's really handy models rarely work on the full sized image a lot of models just expect a very small cropped version of the image something like 224 by 224 pixels as a developer we don't want you to have to spend time inspecting the model to understand exactly the size of image that it expects so Windows ml does it for you when you hand in a video frame as an input we will automatically scale and drop the image to what the model expects and lay it out in that tensor format moving on to the output class you'll notice right now that it's purely a list the outputs for models can be one of several different data types in this case I know this model returns me a list of ten items the list will be the probabilities that the model thinks are what I had drawn the digit that the user is drawn on the screen the index of the highest probability corresponds to a digit however other models will return different data types so for instance classification models that can tell you the types of objects that are in images will actually return dictionaries that are already pre labeled for you so you know which label corresponds to which probability moving on there's one more class that Visual Studio generated for people and this one does the heavy lifting so much so that the curly brackets are a little strange on the slide but I'll need the next few slides to explain us the first bit of code just takes a standard uwp storage file which is the model asset that I've included with my application and loads it into memory it loads it asynchronously so my application remains fluid throughout this whole process and in fact all of Windows ml's operations are asynchronous once I have this model loaded I can call evaluate to actually start getting answers to my question you'll notice one new concept in the slide something called bind in order to feed your application inputs into a machine learning model you need to wire them up we do this by binding so in the two lines that bind the input and output you see that the generated code has actually taken the named properties that the model was expecting input 3 and something called plus 214 with a lot more characters afterwards and binded my specific application data to it once that's done I can just call evaluate async and now will automatically populate output class with the models results this is all code that you don't have to write as soon as you add an onyx file to your project Visual Studio just did it for you let's look at the code that you actually have to write and it's not a lot so you know I'm not gonna speak too much to the initializations we're using Windows I got machine learning dot preview Windows ml is in Developer Preview in the April 2018 release I then have some code that takes a storage file and actually loads the Onix file which I've done hooked up and generate a code and this is where the real action happens this is the click event handler for my application so when I clicked recognize in the app here's all the code I needed to do in order to get an answer from my machine learning model the first line is just a quick helper function that I wrote that took the ink canvas output and saved it as a bitmap in memory so I can use it as a video frame into my evaluation I called evaluate async and the next two lines are just me interpreting the results from that model that's it in less than 11 lines of code I have actually evaluated a machine learning model with Windows ml and the really amazing part is if I add a one more line to say that I wanted this evaluation to run on the GPU it would run on any DirectX 12 GPU it's super easy to use now you've heard me use the word onyx several times I mentioned before that it's an industry standard for model interchange it's actually also a community project started by Microsoft and Facebook which defines a computational graph and a set of defined operators that can be used within that graph this format supports a wide array of machine learning models from classical ml to convolutional neural networks and you can learn more about it at onyx dot AI but you want to get started with your application so you're probably wondering well how do I get these onyx models the easiest way if you just want to start playing around and exercise common scenarios is to use the curated experience on asher AI gallery which has a set of onyx models that you can use in your application with Windows ml today as your custom vision services allows you to easily take a set of images and train your own image classification model so you can use the output of that process to figure out things such as is there a surface book and an image that I'm looking at and I'll show you that in a little bit Azure machine learning services as I mentioned before is an end-to-end solution to help you prepare your data and train your own machine learning model that you can then export in onyx and use with Windows ml on the edge and if today you have existing model assets and one of several different formats via core ml so I could learn Lib SVM we have a converter tool which is a Python package called win ml tools that you can use to convert those models into onyx to use in your application let me show you a couple of these ways of gaining models and a few conceptual examples the first example I want to show you is how easy it is to write your own application that does image classification on the edge where your application runs in this app we've trained a machine learning model using custom vision AI to recognize whether a device that's in the picture is a surface pro or a surface studio here you can see that when I fed it an image it was able to use that model to decide the probability of about 99 percent that in fact there is a surface pro in this image the really awesome thing is with all those optimized camera paths and fast GPU acceleration and also optimized instruction sets on CPUs up to avx-512 I can also do this on video and I'm hoping the demo gods are friendly to me because this is a risky demo to do under these lighting conditions but remember this model is trained to detect whether there's a surface pro or a surface studio and image right now the model is not detecting anything let's see what happens when I bring up a surface pro they can now detect that there is a surface pro in this image and the really exciting part is all I needed to do to create this experience was use a data set of about 30 images been into custom vision to create that model this model took me minutes to create and if you actually want to figure out the full end-to-end details of how to use this we have a great session on Wednesday about visual intelligence and adding it to your uwp application where you will use this as a sample lab remember this model took me minutes to make and it took me the same 11 lines of code to actually use in my application it's just as simple as that M this example so if a very low investment you can now using NY and Microsoft technologies evaluate image classification models using the hardware that's available on the end where your application runs but let's consider a different example I showed you that the model could detect the surface pro but what if I happen to have held up an Xbox today the model would have said there is nothing detected because the model was not trained on any images of X boxes one of the patterns that you guys can use to help make the model better over time is to then ask a user when something is not detected to help you label that data in the previous application I could have easily added some UI that would have saved the image for me when it was not something was not detected and exposed the set of options that a user could have selected to label the image once I've accumulated enough of these samples over time I could feed them back to custom vision services in order to train my model such that next time when a user held up the previously undetectable object they would now be able to detect it using this model from an application code perspective zero code changes beyond adding that UI you would have to do absolutely nothing different to evaluate that model but you may be wondering you know images are awesome my application doesn't need to classify images but I do need to solve other hard problems that may depend on a lot of data and we have example for that too here's a very simple application the UI is so minimalist that's almost distracting however you notice that as I saw it starts sliding my finger across the UI the bottom of the page changes the label to identify what kind of flower subspecies the petal measurement that I just had modified sorry the supple width measurement that I just had modified may classify too now you may be wondering why am I talking about flowers the important concept to remember here is not that I burned an app that you can use to identify what particular type of flower maps to a certain measurement the important concept here is that you can do this with your data just as easily this application underneath the covers again just took a model and is using Windows ml to evaluate it I am doing nothing specific for flower detection let's look at the data that was needed to generate something like this this is the iris data set it's almost a hello world of the data science scientist world it's such a famous data set that was collected back in the 1920s on various flower measurements that it's actually used as the getting started tutorial for Azure machine learning services if you follow that tutorial and bring your own business data to answer your own hard business question that's pre labeled in the same way this is you and your own applications can use a machine learning model again with the same 11 lines of code that it took to operationalize every other model I shown you to power your experiences that is really really powerful stuff now that I've shown you a couple examples of how easy it is for you to add AI and ML solutions into your application I want to show you a couple of examples of how we're using it at Microsoft today the first one is from our Windows Photos app team who in the April 2018 update updated the Windows photo app to use Windows ml to classify images here's what they had to say [Music] we all have so many photos and we want to help you find people places and things you care about in the photos help you tell your stories and also add a little magic with 3d special effects the first time I integrated win them out into a product I quickly saw 20% increase in performance and when I turn on the GPU we saw 80% performance increase when I mouse saved us a ton of time we were able to focus on our data our models and our user scenarios rather than store compliance and how to run across Hardware win ml has allowed us to experiment with doing skeleton tracking with your basic webcam our group decided to dance in front of the camera and we're able to track joints and movements winner Mao is groundbreaking it is the first api that allows everyone to evaluate machine learning models both for uwp apps and desktop apps it's also accessible to all developers it allows developers to seamlessly use CPU and GPU to evaluate their models we're so excited about the win ml update because we'll get to focus on our scenarios our data and our models and when ml takes care of the rest we can't wait to see what others can feel with winter now [Music] and you can use that today there's a second example that I want to show you that's more of a proof of concept you may have heard about a new feature in the April 2018 update called timeline timeline is a powerful organizational tool that allows you to see what activities you were doing on your pcs in the last day and the last week in the last month it does this by creating a set of tiles for each one of those activities creating this very visual experience now one of the really hard things is a lot of these images are sourced from the website that you happen to have visited to represent that website and not all of those images are going to be perfectly composed so when timeline gets an image that's slightly too big to fit inside the tile that it wants to show it crops the image and today it does this by cropping on the top left-hand corner of the image to the expense that it needs however sometimes this can create slightly miss framed images so for example in the bottom left-hand corner here you can see that you know at some point I went to Wikipedia and I looked up sheet and in this case instead of getting a picture of sheep it actually ended up cropping to the top of the image which happened to be more mountains looking at some of the other images you can see that the car and the motorcycle in the middle images are cropped somewhat strangely their wheels are cut off again timeline today has no knowledge of what images or what objects are in the images that it shows for its UI so it's hard to figure out where to crop the image let me show you a proof of concept that the shell team recently implemented which uses Windows machine learning on each one of these images to identify the location of certain objects in those images to create a way better framing and for this I'm going to switch to my demo machine quickly so here I am just a new VM and when I go to Timeline you know some of those websites that I have previously visited now have much better visual experiences for instance the image of the motorcycle in the car is now way better framed it's because time line used Windows ml had evaluated a model on the edge using the GPU to determine that there is a meaningful conceptual object in this image and should crop around it as opposed to potentially bisecting it and just to show you this is for real you know last night with all the excitement around build I couldn't fall asleep so I started going on Wikipedia again and reading up about sheep hopefully it would help me count sheep and help me fall asleep so here is the article that I looked at and now remember in the previous example the picture that time line used focused more on the mountains and cut off a lot of the sheep but now if I close Wikipedia and go back to timeline you can see with Windows ml it has created an image that's cropped on the subject matter in the image this is really powerful stuff to be able to do on the edge because there's no way that we could deliver this sort of experience for all of our users without having Windows em out everything you've seen so far is available today in the April 2018 Windows 10 update but I mentioned at the beginning we are the Windows AI platform team and as a platform team it's our never ending goal to make sure that Windows ml can be used by you guys just make your product successful our platform will keep evolving and I want to give you a sneak peek of some of the things we have in our roadmap I mentioned earlier that Windows ml today is in a Developer Preview namespace we are working hard in our next release to take it out of preview this is more than just a name change ever since Windows ml was announced publicly back in March at the windows developer day we've heard some really great and insightful feedback from developers in the insiders program and we've used this feedback to improve the platform when Windows ml comes out of preview we will have added functionality in order to allow you to load a model from a stream so you don't just have to depend on storage files to do so another commonly requested a-calling powder of Windows ml is developers have found that it's rare that you just have one model that runs your entire experience usually what you're doing actually is evaluating multiple models in series the calling pattern of Windows ml today is such that you load a model find evaluate get the outputs and you could do this a synchronously for multiple models at the same time but we wanted to make sure that the platform was as optimized for this calling pattern as possible so we are looking at what it means to allow you to chain multiple model evaluations together such that the hardware can constantly get utilized without any sort of breaks and work without calling pattern that's more natural to developers today in future releases we are looking at adding new model capabilities today Windows ml supports the valuation of models with floating point 32 data types onyx the underlying format supports data types of FP 16 we're looking to add the support to Windows ml so the model files that you use can become smaller and also so when they're loader ends memory they take up less space in memory when your application is running likewise as awesome as onyx is for defining a standard operator set this is a very fast-moving space the research community is constantly coming up with new models and within those models new mathematical operators to drive the experience we want to make sure that Windows ml can evaluate models that are on the bleeding edge and we're looking at what it means to add custom operator support into Windows I'm out what that means for you is you can use onyx models that define operators beyond the standard onyx operator set and still have Windows ml evaluate those because you can provide the implementation for that operator in future releases you will see even faster hardware acceleration today Windows machine learning can evaluate a model on any DirectX 12 GPU it uses DirectX and specifically direct compute to implement every single one of the Onyx operators so it can run on a wide diverse set of GPU hardware however we know GPU vendors know their hardware best and for some of these operators they actually have optimized paths that they use on their device in order to get even better performance than our broad implementation in future releases we are looking at needs of having the GPU be able to provide those optimized paths such that from a developer perspective your code never changes evaluation just gets faster and you never have to worry about specifically what device you're running on you'll just always get the best performance from Windows I'm out lastly back in march i'm windows developer day we showed you some exciting early work that we were partnering with intel on in order to be able to do Windows ml machine learning evaluations on the Intel Nvidia CPU this were continues but it's very important work from a developer perspective no matter where your model is being evaluated we never want to see you guys have to write code that's device specific and because of this I'm happy to announce today that Microsoft is also working closely with Qualcomm to bring Windows ml support to the Snapdragon chipsets so we're getting near the end of the talk and the one thing that I want you to walk away with is you should go out and try Windows ml if you want to learn more you can go to this URL which is our current documentation to learn much more about Windows ml you can explore the full set of Microsoft end-to-end AI solutions and this conference is a great opportunity to do that from Azure machine learning services custom vision add your batch AI all the way down to the intelligent edge with Windows ml for those of you who today work with a data science organization and again you may have a problem where those organizations provide you models in various different formats that then make it difficult for you to operationalize them in your application talk to your data scientist about onyx onyx will make it easy for you to use that model with Windows ml to get hardware acceleration on the edge if you have questions thoughts feel free to reach out to my team on ask Windows ml I'm Microsoft comm and lastly we have boof in the expo hall please come visit say hi ask us any questions you have and as always please complete your session evaluation we really value your feedback to make these experiences better for you thank you [Applause] question sure it's not the same thing so when or m/l dawn net has been recently announced to help you use c-sharp in order to be able to train your models part of training is also being able to evaluate your models so the whole goal of that effort is to allow developers who are familiar with c-sharp who wants to start doing model training to be able to do that from the environment that they're familiar with Windows ml is an API that you can take the outputs of that training and use in your application to get hardware acceleration for model evaluation Windows ml should be thought mainly as an evaluation platform ml dog nudge should be thought of as a training platform question number two you then you and you and then we'll come back to you I'm sorry I can't quite hear that question is that microphone actually working how about we line up at the microphone we'll start with you okay so what control do we have as developers on the underlying ml technology like like if you want to say we use marginal space learning or boosting trees or deep learning do you give any sort of control sure so the question that you're asking is probably more cater towards training those are all techniques used to generate a model once you have that model saved as an Onix file when you give it to Windows ml it handles like how to load it into the engine and how to build out the computational graph and evaluate every single one of the operators in that graph so you actually don't have that control because that's more of a training affordance than an evaluation for affordance once you have that model windows ml is good to go thank you so what's the plan to support the Windows 10 IOT core so this actually works on IOT today yes does it support arm v7 devices like raspberry pi 3 I I don't know about raspberry pi but it does work on arm today ok thank you yep hi um there's a great diversity of GPUs out there what's your min spec for GPU acceleration if you have a DirectX 12 GPU Windows ml can run on it if the question is more around performance that's gonna be highly contextual to the model and the network that you're trying to run however if it's a DX 12 GPU you can try it out and gauge whether the performance is adequate for your scenario that was sort of my follow-up question do you have any sort of data that sort of shows how performance would scale and I give an application the different reviews integrative GPUs two screens I don't have data on hand conceptually though obviously the more teraflops you have a lot of this's matrix math it would scale with teraflops I'm short no where is it open-source Windows ml is not open source now it is part of the Windows operating system alright because I was the next question if it's tied to Windows because the name sort of indicates that yes so Windows ml is a platform feature within Windows the valuation engine is actually built into Windows and the API ships as part of our SDK onyx however is an open source project so the model interchange our using is thank you can we get benefit from both is it possible to do that I'm sorry can you stop a little closer to the microphone and Azure ml right shut so how can we get benefit from each other you should think of them as complementary affordances a sure machine learning services are a great way for you to start training your model and generate that onyx asset if as a developer you feel the need to done take that model and deploy it into one of a Jers managed web services because that is the flexibility you need in your app that's great do it if you have a need to be able to take our model to an edge device because perhaps that device can't send data off of it or just isn't connected Windows ml allows you to do that on any Windows device so you should think of them as complementary evaluation platforms and Azure ml also is an amazing training environment to create your model I'm sorry I can't hear that question you generate the own excel for the image processing point of view right so if you want to there's some security I mean in security processing system like a throne like suppose if I create the some hypothetical s like my virtual guard right how's God right so which is I mean opening the door through IOT based on the image rate which is the house owner rate or maybe is the house members so they can do that race so can we build I mean something like this you know what can we jump into that question after talk I would love to understand a little bit better but I'm just having a hard time hearing the microphone right now but if you can stay around for another 5-10 minutes I'd love to help you address that question thank you a couple of questions do you plan to support heterogeneous computing like CPU GPU running from the same model or breaking the one model into multiple layer or models itself that's something that we're always thinking about like there is a trade-off to be made when you do that sometimes the cost of moving resources between sort of the Centers of processing actually makes doing the evaluation across multiple places less advantageous than one would think but it's something that we're thinking about we don't have plans right now okay and for the custom primitives like cutting edge primitives which is not supported maybe in UNIX but not in minimal can we add new language like currently I think hlsl is a default and CUDA or any other primitives can be added through the IT vendors but from the ISP vendors if someone wants to add ogl directly accessing your meta commands or directly to the HLSL site can be a new language is that supported oh it will we support it so yes two questions there the first one is in regards to sort of the parity between onyx and Windows ml it's our principle that what is defined sort of in the version of onyx that we claim to support we have full support for that operator set there will always be a couple of there places where we may not have one particular operator supported across both CPU and GPU but that's generally a guiding principle in regards to well as a developer how do I provide that custom operator implementation that's something that we're investigating right now and if you have needs there I would love to understand them a little bit more ok thanks thank you in the earlier demo you were using the surface pro pictures and you said like there were 30 images were used for training the model were you using transfer learning technique I just so you would have to go talk with the custom vision folks to get an answer of exactly how they used ML techniques to train that model that is sort of their realm of expertise and I don't think I can give you an answer to that one today sorry because I was thinking the 30 images are not enough to actually recognize an object especially like a surface probe in this case it actually was but if you go to the expo hall and look for a big sign that says theater - very close underneath that sign there's actually a booth with the custom vision team and you can ask that question there and they will be able to give you a way more intelligent answer than I can thank you I think it actually has answered that question because yes it's possible to train the model using just 30 images and actually Google has a great demo of that train the machine comm or you can do it in a browser without a team and GPU so it's a pretty simple conditionally neural network my question is are there any api's for let's say retraining on the edge if you need to adjust your model on the edge for each specific device and if there are no are they planned so Windows ml today is purely an evaluation API sort of conceptually things like reinforcement learning are things that we think about I don't have any plans to share with you right now as far as where that is in the roadmap but these are topics that we're definitely interested in thinking about and does it do any batching when you do the inference like five requests will get badged and they know very little iess than the GPU so the underlying implementation tries to squeeze out the most performance from our drug compute layer as possible so if you have a specific question there I would love you or I love to point you out our devs to sort of understand the underlying implementation a little bit more but in general we program the GPUs to get as much performance done out of them as possible and the interesting thing is the windows AI platform team is actually the same team that owns the Windows graphics staff so it's the same team that owns direct acts and sort of all the api's that you use for rendering in your application so we have a very intimate knowledge of how to program GPUs with DirectX to get the best possible performance out of them and how does all of this relate to battery savings and just settings in the operating system for to save me or to save battery so if I if I for example to select battery saving mode in my Windows 10 machine would that somehow influence Windows ml or is it just completely separate thing right now no you know the things sort of consider here is that by being able to talk to the GPU using the same sort of DirectX infrastructure that we use for rendering a lot of the power management affordances that you would expect for rendering we also get when talking to the direct compute stack the interesting pivot though is you know the vision that we have is that we want to see evaluations use more broadly not only by your products but like within our products as well and just like today you know rendering of a GPU is pretty much ubiquitous like everything you see on the screen is hitting a GPU we're expecting you know ml workloads to be as frequently utilized so that's definitely an area that we are very keen on making sure that we land properly to not only be as performant as possible but also make sure that it doesn't impact you know a user's battery life and sort of all the other considerations that we have to make when using that resource thank you thank you hi we played it with ml we loved it is great and looks awesome and I have a couple questions in there why is most of the demos and the samples we play with is all your WP application right sure so how friendly of the win ml to you know the native environment right for sure I'm more like you know most of detection work on the desktop is like you know your MTF driver or surveys right so is that can be directly individual so that type of applications so just to make sure I understand correctly how friendly is windows ml2 win32 developers yes yes right so in the April 2018 update there are two flavors of api's like you saw in all the samples uwp and we also have a calm layer that the uwp layer actually projects from in the developer preview one of our goals was to get both these api's out there so we can start getting feedback from you guys to understand what changes we need to drive into the API to make this you know a long-term robust platform for you so as I was talking about moving Windows ml out of Developer Preview one of those goals is to look at that API surface and make sure both for win32 and you WP developers it's as powerful and capable as possible to enable you guys to do your thing again either on win32 or uwp all right great the second question is you know you if I want to bring this win ml to the production release with my software in the race so any any you know constraints or you know extra library needs to be you know attached to it or would would that be automatically you know generator when we do in the Installer I so know this is one of the great problems that Windows ml solves if you have a model in onyx you don't have to worry about deploying an evaluation engine that knows onyx windows is that engine so if your endpoint that you're evaluating that model on is on April 20 18 update of Windows 10 or newer you don't have to deploy any additional libraries you just worry about writing your application focus on creating that model that you want to use to drive your experiences as a platform we have the infrastructure already in the OS we have the ability to talk to the GPUs on your behalf and the OS you do not have to worry about deploying additional libraries to do that well that's great so the last question it's like you know we talk about a CPU and a GPU right so is any you know Maxon's Menominee's in that to automatically you know engage with a VPS by a vision processing unit inner sure sorry i've missed last part of the question I heard vpu I didn't hear the preceding question oh it's just a is any you know doesn't win ma automatically you know recognize vbeaute and then execute you know you know what would be the performance compared to the other scenes I see what you're saying so what we showed a windows developer day was a very early preview of an effort that we're doing to create a driver for such devices the end goal of that effort is such that when such a device exists on your machine or a machine that your application is evaluating on it registers itself with the OS in a way where Windows ml just knows how to talk to that device the whole point is regardless of what kind of AI accelerator you have so long as it implements this new driver model as a developer you can just select oh I want to evaluate on that device you can renew mer 8 the devices and select which one you want but your application code would not change because you're talking to you know either the CPU the GPU or the VP you all right thank you great thank you everyone enjoy the rest of building 