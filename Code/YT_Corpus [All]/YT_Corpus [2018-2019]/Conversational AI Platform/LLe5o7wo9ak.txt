 okay well go ahead and get started thank you for coming and welcome to the session that we're gonna do we're gonna talk about AI at Microsoft how we do it and how you can - let me first introduce myself my name is Eric Boyd and I lead the AI platform team at Microsoft the AI platform team is chartered with building the infrastructure the tools all the main systems that are used for helping developers all across the company really scale out all of the AI systems and and just really make it easy for people to do machine learning and then to bring those same things to our tools for developers and make them all the lessons and learnings that we've had internally and make those available and so what I wanted to do today is walk through a little bit of the history of how we've done machine learning at Microsoft some of the lessons that we've learned and how we've brought those together and and how we're bringing those into our tools for developers we'll start talking through how the AI platform was built from Bing and from Bing ads and what we learned there how we started to expand that across the rest of Microsoft how we started to cooperate with the broader and open ecosystem that's forming around AI and how we're bringing those services together in Azure and making them available so Microsoft has built several billion dollar businesses that are primarily run on AI there's a tremendous amount of lessons that you learn when you do something like that the stumbles that you take along the way the wrong paths the understanding what it is that really makes something tick and how to make it really work in a special seamless fantastic way and I wanted to start by telling you my story and when I joined Microsoft I started on the Bing ads team and wanted to talk a little bit about the journey that we went through and the ads team and and what we pulled out of that now there used to be charts like this that got shown in prestigious publications like Silicon Alley insider I don't think anyone's heard of it anymore I think if they're probably out of business they liked the snarky hey look at how terrible Microsoft is and all the money that they're investing in bing and what a disaster they don't know what they're doing and you know this charts several years old but even at the time that they were publishing this there was a clear shape that was going on search is a tremendously expensive business to get into to serve just a single user you still have to crawl the entire web and so you have to index every single page so that whatever query they might have you have an answer to even if there's only a single user that you've got in it but as you start to grow and learn and get more information about the system then you can start to improve it and so you can see sort of the trends in important fact that's when I joined Microsoft these are just facts draw your own conclusions just you know just giving you the information but they stopped publishing this chart and the reason they stopped publishing this chart in January of 2013 is because it became less interesting to have a snarky chart about how Microsoft is gonna actually really grow and do fantastic in this business and you know what is step three is every South Park fan knows its profit and so being is now a tremendously profitable business it's not one that we talk about the numbers publicly but it's a very big profitable business and so what I want to talk about a little bit is how did we get here what are the things that we went through one of the changes that we had to go through and how do we really accelerate and build a momentum to build such a strong monetization engine in Bing and so when I started at Microsoft and I started on the ads team we shipped our software about once every six months and that seemed crazy to me then it seems totally crazy to me now but remember where Microsoft was coming from Microsoft was a company that shipped office once every three years and that was the model that they thought about so six months was pretty fast and they did about three experiments per month and an experiment in this parlance is anything that I might want to change if I add a feature to my machine learned model if I change a color if I do something like that they did about three of those a month that they tried to sort of look and learn something from and they made a lot of big bets they pushed on the click prediction engine which is one of the most you know important pieces of technology in the advertising system they came up with this new model that they were going to try and just completely replace the old one with and they spent about four months developing it and when they started to expose it to users and started to launch it it tremendously underperformed the old model and so we just you had done the wrong thing and so we needed to learn from that and figure out how do we you know take those learnings to improve the systems going forward and so what did we do we started to focus our teams instead of focusing on the single metric and ads you focus on the revenue per thousand searches rpm and the more you can improve our p.m. that's just free money that just falls into your lap but having every team try and improve our p.m. really was to diffuse and it was really hard to tell what people were actually moving so we focused teams on an individual metric this is the selection team given a query you're supposed to go and pull all the ads out of the entire corpus that we've got and your measure is recall every ad that's relevant I want to see it in the corpus that we selected this team is focused on P click the probability of click given a user I want you to know how likely they are to click with this query this ad and and really nail that and really push the team's to just focus on their individual metric and move it and we really wanted to get them to experiment much much faster the rate of learning is the key thing in any online business and particularly in a machine learning business you have to find a way where you can try a lot of experiments really really quickly and iterate and learn and over a few years we went from doing about three experiments a month to now we do more than a thousand experiments every month and so every time if you go to Bing and you look at the search result that you get it is almost certainly unique in some dimension that you probably can't even perceive from anyone else who goes and does it because there's so many experiments that are running simultaneously across all of the different things and we're learning each and every time with it and so some of the key lessons that we took away from you know working through this and learning with our team is first that when you experiment most the ideas fail and 90% sounds like a lot but that's been pretty consistent we found 90% of the idea that we tried do fail and so you have to have a culture that's gonna embrace that and be content with that but at the same time that really highlights just how fast you need to move to actually learn something and find something that turns into that 10% that actually moves your business forward and the 90% of ideas fail it's very interesting we'd run all these experiments where people work very hard and about a third of them actually make things worse and so that's really frustrating about another third just don't make any difference whatsoever and then one third actually will improve things but why do I say ninety percent fail well I could make the ads blink everyone remembers their blink tag from Netscape and the click-through rate would go through the roof we'd make a lot of money and you'd piss off every user so we're not going to put the blink tag in there so you need to make sure that all of the metrics are moving in the right direction so really need to focus on the speed and the iteration but to do that effectively you have to build infrastructure that just makes it simple for people to experiment to try their idea to get it into production to get the feedback from the users and to iterate and learn this didn't work let me tweak it let me try this let me try this new feature let me try another idea I have 10 ideas let's try all of them at the same time and really building that muscle with really great infrastructure and as we focused on that we started to see some pretty strong results so the graph on the right shows the number of experiments ether experiments will talk more about ether as we go but it's an experimentation management system we use and you can see we added a whole lot more experiments and started just doing it a much faster pace and the graph on the left hand side shows the relevance increased over time and so very strong correlation you increase the experimentation rate you increase the relevance we also saw it in our revenue per thousand searches this graph shows the revenue per thousand searches over three subsequent years the red year being the oldest year and you can see the spike at Christmas that we see every year if you don't think ads run on Christmas well everything runs on Christmas Christmas starts in September in ads but you see the spike and the red spike that's lower than just about every of the blue points so the next year our system was so much better than even the spike you see Christmas wasn't as good as we were doing the next year because we didn't proved the engine and a similar story the green year we keep improving the system each and every year taking every idea we can find we've done a lot more with deep learning and built a lot of new power by you know being able to leverage all those different models and make the system a whole lot richer and we built a whole lot of infrastructure with this and we learned a lot and you know this is a time period that have been sort of describing it's probably about the last eight or nine years of sort of lessons and growth that we've had in the being system and what I want to do now is talk a little bit about how Bing is using this today in its relevant system and so if you think about a search scenario you know it's sort of evolved we used to be very keyword based and so you'd frame your query as canned soda expiration date and of course nobody talks like that that would be weird you would say how long does a canned soda last and increasingly as there are more devices and ways that you're going to communicate with things the natural language evolution is really pushing you to want to be able to use a natural language expression how long does it can soda last when I'm talking to Cortana on a speak or on a PC when I'm using hololens or all of the different devices that we talked about this morning the proliferation of devices is going to be huge and so how do you make this much more natural in all those scenarios and so how do you ensure the best results when you have a natural language query like that one of the things that we did is we wanted to get a more semantic understanding of what is this query actually mean and so we built a deep learn model that translates each word and represents it as a 300 dimensional vector and you know you take we had tremendous amount of data in the Bing query logs that we could train on to sort of learn these are the different words this is the corpus we've got these are the ones that are associated with each other and build a model that's going to build a 300 dimensional vector and part of the beauty of deep learning is you don't have to have any real context or understanding of what the dimensions and the vector means you need to train a model and the system itself will extract the information and the features from it but you can see we took those 300 dimensions and then projected them into two dimensions to make a graph but things cluster together right lobster and crab and meat and steak like those all sound like things that are roughly in the same area you can see McDonald's with an ass and McDonald's with an apostrophe you're almost right on top of each other and so you get some confidence that all right this has learned the right thing and the semantic knowledge about it and now you can move to a vector-based retrieval approach where what you do is you take all of the query that you've got how long does it can sew to last and then you convert it into this vector base you run it through the inference of the model that you created and trained and then you've also run all of your web results against that same thing and you've scored them against your same model so now you can do an approximate nearest neighbor search looking for which of the vectors that are most closely aligned and then you feed that into your ranking index and then you do as well the traditional reverse index and posting lists and gather the results that you've got there but now you've really supplemented your results with a much richer set of data from this deep learning model that you put in there so how does it work how does it perform if we look at the search results for Bing on how long does it can soda last there are a couple interesting things that really jump off the page with this first off is that we just answered the question right up top you can see we found an article that has an answer to the question and we highlighted the right answers nine months and three to four months for diet soda which was news to me the diet soda doesn't last as long but sort of pulled that answer right up top the other thing that jumps out is all of the different ways to express this soft drinks doesn't appear in your quote in your query at all unopened room-temperature pop to those few of you from the Midwest who use the word pop we still will find your words as well can a soda carbonated drink and unopened can of soda and so just by using that sort of the vector and the query similarity you can pull all the information out of that and really get pretty fantastic results coming back so the platform that we use and this is what we're going to talk a lot about in this is this is the the way that we think about this Scott walked through in his keynote this morning when you think about deep learning when you think about machine learning any type of AI there's a data prep phase there's a building train phase and there's a deploy phase and so in being the infrastructure that we built is of course there's the web corpus we've gone and crawled the entire web and we store that in our big data store that we call cosmos and then we also take the Bing history all of the queries and things that people have done and we store that in cosmos as well and then we build put that into ether which is ether is a system that we've built internally for managing all of the experiments and all of the workflow that a machine-learning developer needs to do and we'll talk more about ether in just a minute but they build and train the models on ether and then when they actually want to do the training work GPUs are unfortunately and we have clusters of thousands of them but you need a very sophisticated management system to make sure you're allocating the jobs to the right places that you've got the quotas managed appropriately and so that's a system internally that we call fili which manages the clusters of GPUs to go and efficiently do our gr training and then you need to deploy them and so for being for a while we've been deploying it and inferencing on FPGAs and we talked this morning about project brainwave and somehow that fpga work in bing is now coming to see the light of day and so you'd inference it on bing and really get a lot of acceleration from that and so that's the platform that we've put together for how Bing works and what I want to do now is have a young G Kim walk through the ether system that we talked about and how they use that and a lot of their systems alright thank you very good hi my name is Yong su Kim and I work in being relevant in the eye but number wires we've never had a problem with wires before they go alright sorry about the technical difficulties as always so I work in bringing relevance in AI as Eric mentioned we do a lot of deep learning and machine learning in in being and this is a platform called ether that we use for our development I must say ether really helps us to keep our sanity and you will see what I mean by there pretty soon so this is ether client which provides me graphical UI to manage my experiment if your preferred programming ether also provides visual studio plugins so that you can programmatically manage your experiment as well so this is very typical ether experiment a graph and purple node represents our data sources and the green node is either modules ether modules represents any arbitrary executables Python script cosmos script command lines and so on so on the right side of the ether client there is a search pane for data sources and modules so these are data sources and modules published by others that I can be used for my experiment so I never really have to start from scratch another thing I wanted to point out is some of the modules are encapsulating other modules so for example this fuzzy match module when I click details it shows the content of the module so when I go back to my original either experiment this looks pretty manageable but this mod this experiment is actually composed of 560 modules and the other day was looking at another experiment that was composed of 12,000 modules so now you know what I mean by ether actually helping our sanity next I want to briefly touch four steps that we go through as we are developing our models so those are first steps data preparation training validation and deployment so the ether experiment that we have been looking at is actually representing our data preparation step so what it does is it retrieves our data from cosmos which Eric mentioned earlier which is our big data storage and it runs a bunch of processes also running on cosmos and it produces our training data as well as validation and the testing data so once this ether experiment is completed now we have those data sets so the next step is training so I'll flip to our training ether experiment and in this experiment what ether does is it grabs our Python source code as well as its dependencies from git repository and it grabs our training data from cosmos and it packs them and submit to Philly which is GPU based compute cluster that Eric mentioned earlier so once this ether experiment is completed now we have a newly trained model so the third step is validation so this either experiments represent our validation process it pretty much evaluates our newly trained model against our current model and the last step is deployment during deployment experiment we retrieve our newly trained model with its artifact and submit it to our hosting environment called the lis or deep learning inference system so these are first steps that we take during our development and each of the step is represented either experiment so before I conclude the demo I want to actually show two of my favorite features in ether so I'll flip back to the the first experiment and bring that this menu either supports clone so from any experiment I can clone the experiment and I can make modification and submit my newly modified experiment and you can imagine this feature is very very heavily used in my team because it made iterating and reproducing experience much easier not only that it really empowers us to build on top of each other's idea so for example we had an ether experiment for superable prediction with a few modification we could actually predict Oskar using the same experiment so the second feature which is my favorite in order to show you I'm gonna just make a quick modification to this experiment so I'm going to change the parameter from fifty to hundred and save and make the corresponding description update and submit this experiment once it is submitted I get a link to the ether experiment and this is how the ether experiment looks like the color code gray means this module has not been executed yet once this is executed it will turn to green I know data probe actually usually takes a while because we're dealing with tons of data so let's see how it is progressing I'm going to refresh from word time so this module has been already executed but we're actually dealing with like terabytes of data so usually it doesn't take few seconds to process it but it could actually be completed because we're reusing the output from the previous experiment so this recycle ID means ether back-end in the ether back-end we use cached data so that it doesn't have to rerun the module which is pretty awesome because I can save my time and the resources thank you thank you the experience that youngji described of a developer cloning a model changing something in it and then running it that's the experience that most of our machine learning experts go and do every day and that's where really the iteration speed comes from you cache the history you start with something that's already been done you can build abstractions if I need to extract a particular data set and transform it in a particular way you can really leverage that and so a lot of power really comes from that and the being relevant side now moving on beyond being you know we took the platform that we built and started working with more and of the groups across Microsoft as we see AI is really transforming every business in the world and Microsoft is no exception and so each and every team that we look at is doing AI in almost some way shape or form and many products that you wouldn't even think of as being Naturals for needing machine learning in them actually have a lot of machine learning in them and we're really able to accelerate those teams by using all of the infrastructure and the learnings about the iteration speed and what we're trying to do in the failure rates and all of those things we can bring that to those teams to really accelerate them and you know as we culture change is always hard and so as we work with some of those teams it can take some time to sort of warm up with those ideas you know I mentioned that Microsoft had a history of it took a long time to sort of ship things and people sort of knew what they wanted to ship so one of the teams I was working with I was about to do their first machine learning experiment and they built a model and it was adding some new value and their feature and they were very excited about it and we were going to do the experiment and get learning's and the data back and I got this email from a person I no longer named hey we've decided to ship the xxx feature without running the experiment we're quite confident in the feature our team has a rich history of shipping features without any experimentation something one may be alien to folks from Bing and unfortunately I'm a little bit snarky and so I replied I have a rich history of driving my car with my eyes closed I'm quite confident I know the way to work and I've only crashed a few times the key message there is that's what it's like if you're not gonna run an experiment you're not gonna gather this data back why are you driving with your eyes closed open your eyes there's amazing things that you can learn from this and I'm happy to say that I no longer receive emails like this and the company has really embraced the culture of let's gather and learn data from our users and let's iterate quickly and experiment on small sets and expand from there but culture change takes some time but one of the next airs I want to talk about is PowerPoint designer and you know PowerPoint is not a product that I think of as having a lot of machine learning in it and so product PowerPoint designer is you know this is a traditional slide you might see it's kind of boring text and some bullets and the designer is sort of the right-hand pane that's trying to suggest hey you can make this look better and nicer and as someone who creates a lot of PowerPoint myself it's hard it is hard to come up with the right designs to make it engaging and interesting and really find the ways to capture what am I trying to communicate in a nice good succinct way and you know so the PowerPoint team have built this feature and they built it first using a rule-based model and so rule-based model if I see these bullets then make this suggestion humans sort of curated these are the list of things we want to do and so the blue line represents how the rule based model worked and they then built a machine learning model and the PowerPoint team cares about two key metrics the first one is how many of the suggestions were kept overall out of all the suggestions that we show and how many people said no that's a good one let me keep that and the second one is the keep rate so you can think of the keep rate as the percentage if I showed it to you 3% were kept and the top line you know there's some you know you also involve in there the fact if I showed it to every single person every single slide or something like that versus not every slide and so you can see at the start that you know the machine learned version the redline is not being kept as often as the blue line they trained a model purely using offline data and it wasn't performing particularly well but then they started to use the inert user interactions if they start to feedback hey this user clicked on this one so that's a positive label that this was a good interaction and feedback that through and you start to see the machine learning model starting to improve it slide kept rate and then the next thing that they do is they move to a reinforcement learning approach and you know reinforcement learning really takes two key concepts if I have sort of a great answer then I'm going to exploit that and this is something that in machine learning you see all the time we're going to exploit what we know but with reinforcement learning you need some amount of exploration and so when do I go and explore and try something new or different to learn hey this is actually even a better idea than what we had before and so what you can see is they spiked the slide keep rate and then they turned up the volume on how often it triggered which kept the total number that we're being taken to go up and then the next step that they went is they first changed the scale of the slide so that it would show up because they really you know made something work tremendously well they started to feed more and more data on all of the you know publicly available slides that they had and just looking for ways that they can feed all of the different combinations and ways that you can go and train this model and so now we have what's a really rich and robust feature as a part of PowerPoint that was really accelerated the rule-based model is nowhere near sort of the volume or the sort of value to users but by using the machine learn model you can make it much much richer and so this is an example of something I would not have guessed outside that of course that should be a machine learned feature but is that a tremendous value to PowerPoint from it and again the infrastructure that was used was the infrastructure that we built and learned from Bing and so the same stage is the data the building trained in the deploy they had all of the PowerPoint user behavior what are the slides people are clicking on and how are they using it the corpuses of different slide decks that are available and open on the open Internet that they went and learned from they did data prep using Azure data bricks and stored the data in cosmos as we talked about they used ether which we just talked about to manage all the models and the experimentation and sort of iterating quickly through that they used TLC TLC is an internal library a c-sharp library that's been optimized to run ml algorithms at high speed is used tremendously all across bing and we'll talk more about TLC and how that's making its way out as well and then they went and deployed it into their application and the PowerPoint application that you're using today is benefiting from all of this so what I wanted to do is ask a non to come up and have him walk through some of the great things that we're doing with the PowerPoint designer see if we can get this right thank you Eric hello everyone my name is Anand Balachandran and I'm a program manager representing a team a big team of machine learning scientists data scientists and engineers that built PowerPoint designer how many of you are familiar with PowerPoint designer today okay not a whole chunk of the room so I have the privilege of telling you what PowerPoint designer is so when the team brainstorm and created PowerPoint designer write the one mantra that we took is what does it take to make a deck that's filled with boring slides boring bunch of text and address the gap between a deck that's just filled with boring slides and text to a deck that can help you close your deal so the one operating principle in PowerPoint designer is to help each and every one of you be a better storyteller so like Eric mentioned we had the privilege of looking at vast number of decks that are present out in the wild and we could actually label this data by looking at patterns of words that were present in PowerPoint one of the one of the privileges we have in Microsoft is to share a bunch of natural language technology that can address by using traditional natural language libraries that can understand parts of speech and apply word breaking and parts of speech detection to words so the rule-based model that Eric showed you first was able to understand what are those types of words that describe a timeline what are the types of words that describe a process and so we could infuse that knowledge into the product and say for the initial rule-based model we could apply a certain set of word mappings to a certain set of designs and of course it is that system that has picked up and revolutionized into a completely machine learning based system so let us look at some examples of how that works here is the same slide that Eric showed a little while ago that talks about the legislative history in the country and you can clearly see that when I launched the design ideas pane PowerPoint is able to detect the presence of a bunch of dates in the slide which obviously represents a time line and it says look what I have a timeline based view and representation for you so I'm gonna go ahead and choose that design and doesn't that make the slide more impactful so similarly I have another situation where I'm describing just a simple process that takes you know a description of how to make espresso you buy some beans you grind them you categorize the beans tamp them well and then once you get your brew going you have great espresso and guess what PowerPoint is able to detect that this is a process and it actually shows this as a flow diagram so what we've done through the process here is of course design ideas in PowerPoint are invoked both reactively as well as proactively in many cases when you are in the Taiping loop and you finish typing a bunch of text and your foreground let's say goes into idle powerpoint designer says okay here is my chance to suggest an intelligent design for this slide and it pops right in in you know as least intrusive as possible and offers you a bunch of suggestions and if you're done with that suggestion you can dismiss the design ideas pane and go back to working on your slide deck until the next opportunity for us to prompt you a more intelligent and applicable design the reactive way in which you can invoke designer if the automatic firing is not happening for a particular slide for some reason is you can always go to the ribbon and launch the design ideas pane and there you have it there will be those designs lately like Eric pointed out after we started seeing the uptick in the designers performance we started thinking hey how do we introduce exploration in order to do better designing now like Eric said exploitation is when in a world of recommendations you always want instant gratification so you give the highest intensity instantaneous reward for a particular suggestion but oftentimes we notice that a machine learning system that is built on reinforcement learning can pick up when you start to explore with lower rank suggestions by fitting them in into your existing design suggestions so this is precisely what we did in the world of icons let's take a look at that so PowerPoint has a bunch of icons that we ship within the product and so we could take the power of word embeddings and combine a bag-of-words the map to a certain icon and we said how wonderful it would be if we took a slide that has text and give an icon-based representation for that slide and let me invoke design ideas one more time and there you have it I have an idea slide with an icon representation to take one of our in product icons and make the rendition better and this is a technique where we're using exploration where icon is not something you may expect immediately when you had that text in that slide but we were able to explore what our model gave as a lower priority such a lower rank suggestion and push it up in the stack so just a moment for me to reflect back to what youngji said around ether and TLC this is the system we have been using in PowerPoint designer to train our models over the last year and a half and the thing the the great advantage of using tools like ether and TLC is that it helps us build workflows and graphs of chaining models now powerpoint design are the effects that you saw today is not the effect of one single model there's a composition happening on the slide and it's a combinatorial problem because a slide deck has pictures it has text and it has small dirt and SmartArt is where you emboss you take and take an image or a particular piece of art that tells the story and you emboss the words on top of that smart art so you have machine learning models that are predicting the appropriate SmartArt the best way to crop and position the picture and then you have the overlay of text on top of the SmartArt and that's a combinatorial problem so we have a layer of models that are running in a workflow and then we have a rancor that runs on top of all of that to rank all the suggestions of the results of these models and this is where a system like ether TLC helps us tremendously so if I have one call to action for you all it is to use PowerPoint and rejuvenate your presentations and become that better storyteller and stop creating boring slides Factory hey come on I always worry about telling the PowerPoint designer story in the middle of my own PowerPoint presentation cuz if you don't like the slides I clearly should have used designer a little bit more but yeah a lot that we've learned in taking again the infrastructure that we built and bringing it to other places across the company and so that's really the story that that I want to land with you is we've looking at AI across Microsoft you know really every business is investing in it and this is one of the great things about being at Microsoft is we are such a big company that you know we have thousands of data scientists kneei developers building models every day and they use basically every technique and idea and tool and platform out there from classical machine learning to new and advanced deep learning from you know working on very compliant data with customer data where you need to be very careful at how you manage it and who's allowed to access it to public datasets to publicly crawling the web we use lots of frameworks we've developed we use virtually every open-source framework that's out there and we need to deploy it to basically everywhere and so it really has set us up well to learn all of the different things that people might want to do to be productive as an AI scientist as a machine learning expert and this is how Scott sort of framed earlier today the work of a machine learning developer a data scientist really comes in these three phases you prepare the data you then build it and train the model and there's a lot of iteration there as you think about the changes that you might need and then you have something that works and you look to deploy it and I want to sort of walk through the tools that we've built and how we sort of use those internally in building your own model and creating the data that prepared data phase we talked a lot about the system that we've called cosmos and cosmos is a massive MapReduce system it has exabytes of data and it receives you know millions of events every second from you know billions of devices across all of our different products and platforms and of course it's has been a fun project for me it's gdpr compliant and we've learned all about the privacy rule and regulations and controls super important for our company and for enterprises around the world to make sure that we're handling data in a respectful manner and so being able to have this as one of the first building blocks and go to any group in the company and they're they just they get so accelerated by not having to think through all of these questions and concerns the next step they need to build and train and we talked a lot about ether this is a system that we use for managing ml pipelines optimizing prototyping managing your data use you know one of the main things that we do as computer scientists is we create abstractions you have something that's really complex and you wrap an abstraction and youngji talked about we can have 500 modules and you sort of wrap an abstraction on that and that's you know extract data from this and get it in this particular format and run all these transforms on it and that just simplifies the workload for people and we have millions of pipelines that we run on this lots of active data sources this is a tremendously valuable tool that we use and virtually everyone who's doing machine learning at Microsoft is using this tool and then the next phase is deploying and we've had to work a lot on how to deploy in a really smart and sophisticated manner a lot of our you know if you send a query to Bing about 600 machine learned models need to fire and it needs to fire in milliseconds you know we have about 50 milliseconds to go to the index pull all the relevant results out and give them back to the ranker and you know so the system we use is a system called the deep learning inferencing system and so it really takes all of those deep learning models and can run them really really fast it works at 600,000 requests a second which is really fast and run it's super low latency and it deploys constantly it deploys something like six times a day and so anytime a data scientist has a new model they deploy it onto DL is it will automatically run your tests and sort of verify that hey this model doesn't break sort of if you search for something obvious that it gives you the obvious answer for those things if you search for Microsoft it better give you Microsoft comm as a search result it'll also look at sort of the core metrics and sort of compare is this performing better than the old MA that you're aiming to replace it with it'll flight it it'll start at two percent of traffic and ramp it up through the stages and make sure that we're not breaking things or doing things that we didn't intend and so just taking all of that complexity away from the developer and saying you can just push the button and deploy this again dramatic acceleration started in being and now used widely across the company additionally we've built other things to sort of accelerate the work that people do in deep learning and all of machine learning but particularly in deep learning managing the number of hyper parameters is a really challenging thing for people to do hyper parameters for those who aren't tremendously familiar with machine learning if you think about what does deep learning traditionally do you sort of come up with some error function and then you do gradient descent to sort of take steps down it and get closer to the optimum performance one of the key questions is well how big a step should I take and there's no real science to that which is one of the interesting things in machine learning it's basically guesswork and so you try generally ten different parameters how many layers should my convolutional net have there's again no real science to that so you tried different numbers to see what's going to work best and so those are all the different hyper parameters and to really have an effective model you need to try and sort of explore the space but it's very expensive you have to train a model each and every time you do that and so we've built a system that we'll walk through you know the the hyper parameter space and we'll try all the different options and we'll even quickly prune jobs that have tried some job and it's you say this is not converging as fast as the other things that I've had it kills it which saves you a lot of time on your GPUs and manages that and doesn't much you know it's it's a machine learned algorithm itself and so it makes much better predictions than just sort of randomly walking across or through it so another really important piece of technology that you know we've learned and now are bringing to all of our different teams and you know bringing it together it's hard to do machine learning it's hard to do it at scale and we've had to build a lot of really rich infrastructure to make it easier to make it approachable for developers across the company to really go and do this collaborate on large models to do their individual models and really pull all that together and you know now it's providing tremendous acceleration to all of our different teams and I wanted to talk about you know project connect Fraser which we announced again this morning Sachi talked about that as another area where you know they've taken this really cool fourth generation Kinect camera and you know it takes an image but the cool thing that it does is with each pixel it can calculate the depth of that pixel and so not just the brightness but the depth and so now you can do things like change your point of view and actually get a 3d model from a single camera that's sitting in a single place this is amazing this is really cool stuff and they want to figure out all right we want to use this in a lot of different scenarios but having the picture on the left which shows you just sort of the point cloud and the depth cloud but it doesn't tell you what that is and so now you need to you really want the picture on the right where the walls are labeled the floor is labeled the chairs are labeled the table and now I can sort of take this depth cloud picture that I just created and now try and do something more interesting I know all the information about it but to do that effectively you have to figure out how can I go and train the model to take this image on the left and the depth and all those things with it and actually now put the label labels onto the different pieces and so what the Kinect for Azure team did is they first took sort of the input from the camera and then they built their own custom labeling tool where what they needed to do is they would take a particular image and scene and create the labels for it and just really this is the ground truth and then they take those labels and then they split them traditionally as you do a machine learning the training set and the test set and then you train your machine learn model again on Philly the GPU cluster that we have and they produced a CM TK model that then they transfer to an onyx model and they used to deploy and go from there and then they take that same scene TK model and they validated against the test set and they built an analytics client on top of it which will help them understand where the different places where it's performing well does it understand that this is a table really clearly but we're missing that it's the floor or things like that and really put all of that together into a rich model and so I wanted to invite Michelle to come and walk you through some of the things that they've been doing in the project Connect for a juror I am perception team I'm gonna tell you a little bit about what we've been doing here which one you want sorry of course there you go so like Eric said we have been working on training a DNN to take the raw output from Project Connect forager and build essentially environment understanding on top of that so this example is focusing on common building and furniture elements like chairs tables floors could be used for things like navigation and obstacle avoidance but as we walk through this you can imagine using this for any type of object object detection so the first thing we had to do is actually build out that training set traditionally image classification is done in 2d over the entire image but what we're looking to do here is actually classify individual components of this image so we had to produce an image like what is on the left or sorry on your right my left here right so that we can actually tell the DNN what parts of the image we actually care about and what their labels should be one of the biggest challenges we had was the amount of data that's required to actually train your DNN reliably so our training set right now is approaching a million labelled frames obviously we can't do that manually for every image so I'm going to show you how we scaled that process maybe okay so this is a fully reconstructed 3d scene from a real room scan and what we're able to do with this is actually create a tool that let us go in and label this in 3d basically selecting areas of the image and labeling those you then go from many different perspectives and project this into 2d for a scan this size we can get approximately a thousand images that have now been labeled that we can plug into the DNN once you have all that data ready to go we use Visual Studio tools for AI to help manage some of this experiments that we're running so we've got a main driver script all of our parameters in one configuration file some utility classes for shared functions we start out running this locally on a very small data set and when we're ready to scale we can take advantage of some of the tools that Eric was talking about specifically Philly which lets us actually manage GPU and scale out so I can deploy to Philly directly from visual studio I can pull it up select the cluster I'm interested in and actually scale out how many GPUs I want to use right here once I submit that job I can actually manage and monitor everything directly from visual studio as well I can see my full job history and I can actually watch in real time as my job is running and that models being produced the other thing Philly provides for us is Elastic Compute it's very important for our ability to scale you can see even for this job this was running across eight GPUs and it still took almost two days to process so with Philly we're able to run many of these jobs all at once or when we're not using them share them out with other teams at Microsoft who might want to be taking advantage of that compute and last but not least once you actually have the model trained we actually run the test image through the DNN and we've produced our own tools for running metrics on top of this so once you run the test image you want to visually compare the original GT or what we wanted the result to be with the actual predicted image from the DNN so having this visually side-by-side lets us actually drill into issues that are identified and be able to see exactly where our models failing we can that this tool helps us calculate metrics across the whole data set or we can also look for class so you can see for this model ceilings doors did pretty well chairs we have a little bit more work to do you can check out other AI ready IOT devices in the expo hall today and I'm looking forward to seeing what else gets built on project connect Fraser from you guys thanks Ari thank you so so yes a lot of very interesting exciting things happening really all across the company as we build you know AI models in almost every product so we started with Bing we expanded across the company we've now built a platform that we think works really well across our internal company and the next thing we wanted to work more on is making sure that we had this open and interoperable AI as we think about you know the world of AI it's really gone fast it's that mean so developing so rapidly and one of the things that's powering that has been this commitment across the industry to keeping things open not lock-in not competing vendor platforms and things like that but really making it available really to every developer and you know we started to face this problem internally as we started to we had several different internal platforms you know with all the different teams across the company everybody kind of made their own decisions and I sort of made allusion to this earlier that we support just about every single platform and every single combination because someone's using it somewhere so from tensorflow to CM TK to PI torch really across the spectrum and then you know what my team is expected to do is to take all those different platforms and make them work really really well on all of our different hardware and accelerate and all the other you know as we need to go and deploy or do training or all these different things and you know what that leads to is really something of a mess where each framework now needs to connect to each of the different hardware platforms in really just a combinatoric explosion kind of manner and you know we were really starting to struggle with how can we manage all of this complexity and we started talking to other company and found that they're facing the same challenges as well and so you know the approach that we want to take is to just wipe all this mess away and come up with a much cleaner and simpler model of let's bring everything into a single format and then have that that single format connect to all the end devices and you know this is informed a lot from compilers if you think about compilers that have multiple front ends for different languages that all then compile down to the same intermediate representation then all the optimization on the intermediate representation the common sub-expression elimination all the things you do there all happens at that intermediate representation and then you know how to go from that IR into each of the hardware layers machine learning is actually fairly similar where each of these frameworks will output a graph and the graphs are all relatively similar and can be represented by a common framework a common model description you can then do a lot of optimizations even a lot of the same compiler optimizations you would do and then now deploy it directly on all of the different pieces of hardware that you can in the most effective way that you can and so this is a project that we we've called project onyx and we've partnered with other companies across the ecosystem to make this happen but the simple perspective is tools should work together everything should just work and it shouldn't be made a particular choice and that locks you into a particular direction and so as we built this project we really are looking for interoperability and there are two key dimensions we look at on the one side you can take any framework and now connect it to any particular hardware layer on the other side and make any connection through there that you need and do that really effectively and performant lis and really you know making that really simple the other challenge that people run into is they develop a model in you know say PI torch and they want to make it run in cafe 2 and or even just convert two different models from one place into another and so how do I really bring that together and again onyx provides you a nice you know way to sort of take a model in one framework and then effectively show it up in another framework and make that work very well and this has been a broad based industry cooperation you know we started with Facebook and created the project and then we invited Amazon and they are now participating and then we've got a host of hardware partners as well Qualcomm and Vidya Intel you know basically basically the whole industry is seeing this is really valuable thing that they need to go and do and and it's good for everybody it's good for the hardware vendors because they want to optimize for all the frameworks and they don't want to have to provide 20 different libraries it's good for the framework makers it's good for developers and this is a community project it's it's open source and so please feel free to get involved its github onyx and feel free to use it and use it in all your tools and all your platforms and then you know there's a lot coming this is a very active development project one that we're very committed to as a company we've basically said all of our tools are going to standardize on onyx and Windows ml is a runtime in the April release of Windows that you can now get that onyx runtime and onyx hardware accelerated machine learning runtime just baked right into the operating system and so you know how can you use onyx and how can you sort of use that in your applications well let's sort of talk through this a little bit there was some pretty interesting work done at Stanford where they took a publicly available set of chest x-rays and they trained a machine learning model on top of it to predict certain diseases based on the checks s chest x-ray and they got really very good results high high quality prediction high fidelity on that and it was a pretty interesting you know research paper that they wrote and the data set was public so a researcher at Microsoft went and just tried to reproduce it let me reproduce their results and actually was able to perform even better than the results that he got from Stanford building on sort of everything that they'd done but so now I have a pretty interesting tool I have a tool that given a chest x-ray can advise a doctor hey maybe these are the diseases that you should go look in a little bit more deeply but one of the key challenges we often see in the machine learning ecosystem is I've got a data scientist who really understands the model construction and the model building and how do I just you know make it tune and work really well and then I have developers whose job it is to take this model and then actually deploy it to lots of different platforms make it work performant lis and so there often is this divide that we see between them and so we really want to make sure we've got the tooling to just really simplify that so if we think about again our standard data prep build train deploy the standard model that we're talking about across all of these systems we had you know a publicly available National Institute of Health's chest x-ray data set that we then built a model a very rich convolutional dense net 121 a whole bunch of jargon that you can go and read a paper if you want to know exactly how they did that and then we trained it and we trained it we use Visual Studio tools for AI to build and create the model we used a dremel to go and manage the training and our deep learning GPU clusters and then now it's up to the developer how do I get this thing into a lot of applications and you can see on the far right hand side how do we want this to work we want this to work on everything from the cloud to a Windows PC to an iPhone or an Android device to an IOT edge device I don't want to have to train and rebuild this model every time I just want it to work and so coming up with the tools that can really manage all of the life cycle of a machine learning developer how do I take this model how do I put it into a docker container how do I convert it through the different models and really make it work and so I wanted to invite Chris Lauren up to come and walk us through exactly how we did that and where that model is and how it works thanks Eric so what I have here is an iPad where we've taken that train model that he mentioned and you can imagine a doctor somewhere in rural America or somewhere else in the world where they don't have ready access to trained radiologists leaving access to a portable x-ray machine maybe with Doctors Without Borders who's traveled somewhere they're able to take chest x-ray images and now they're trying to triage and diagnose and determine whether it's worth so a patient actually taking time out of the day and traveling to the big city to get more the expert care that they need so we've deployed the model on this device here and we can simply click import image and select one of the x-ray images and it done and so sitting in a doctor's office then you can imagine going through and actually looking at and having that the trained AI model on device providing guidance to the doctor as to where look how to interpret this it might catch some other diseases where doctor might find something pretty obvious in one part of the x-ray but then there's of course maybe some other things going on in that x-ray as well and so augmenting the skills and capabilities of every human through AI and devices that are scoring locally or in the cloud is something that I'm super passionate about but as a developer I want to show you how you can actually build something like this so let's toggle over to my Windows machine here where I can show you we've got that exact same model in the Windows ml app and so we can again detect the diseases that might be affecting this patient now to build these applications use visual studio to both train the model as you saw earlier we can work together with the data scientist you see I've got a Python project down down in the lower right here where the data scientist is training using kiosks and tensorflow and they can right click they can submit their job to as your batch AI or as your GPU VMs and use the power of Azure machine learning to keep track of their experimentation but as a developer I'm going to let them worry about that stuff I'm going to collaborate through kits like I do for all of my other projects with my other developers and show you how we can take that train onyx model and incorporate that in my application now not all data scientists start with training their models and understanding what onyx is but it provides a mechanism by which the data scientists and developers can work together in a common way so for if they've used another framework like tensorflow or scikit-learn etc right here in tools for AI you can convert from core ml tensorflow scikit-learn etc to produce your onyx model additionally you can create a new model library project which will take in a trains tensorflow or onyx model and using our new Microsoft ml scoring library wrap that in a consistent way to provide an interface where a service like the deep learning inferencing service that youngji mentioned earlier can load balanced lots of models across a cluster like an azure so regardless of what type of model is being used the way that you can exercise or score those models is always common now I'm going to show you how to just add the model to an existing project I have here I'm going to import my model and I'm going to import this trained onyx model that detects diseases from the chest x-rays now when I do that I'm going to traverse the graph and I'm going to look at all of the the inputs and outputs and give this a name here and I'm gonna give this a description so I can remember what this does now because this is generating code I want to be able to control the the name of my classes and methods so I can change my my input names and my output names and hit OK and now this will generate c-sharp code in my project that I can use now this particular project contains a model as a reference just like it would any other asset that you would add to your project and I've got the model code here as well you can see it already imported the Microsoft ml scoring that I mentioned earlier and you can see all the code is here to take the inputs and the outputs and actually score and provide a prediction now as Eric mentioned I need to actually take this model and deploy it not only in this particular application which is an azure function going to be used in a jour function but also on IOT edge so that we can perhaps do the scoring right on the edge in an x-ray machine itself so I'm going to show you how to deploy using the STS which as a developer I'm using all the time anyways and notice that here I've got a dependency on the onyx model file so when the data scientist iterates on the model training and they have a new version of that model then they can simply do it get commit and push to the repo and VSDs will watch for that change you'll trigger my test cases and it will then subsequently go ahead and do the next great stuff which is prepare the docker image using that ashram machine learning CLI will go ahead and prepare the docker images and roll those out to the IOT edge as well as update my new get package that I had created in Visual Studio and then subsequently set up my build process to update every time the model is updated and so this is monitoring that inferencing project and updates my asha function so at you that as your function uses that NuGet package so just like any other NuGet package that includes my libraries my DLL and my onyx model and this will distribute that across a sure in a serverless manner so it will scale up all the inferencing power that i need to score those x-ray models in the cloud so regardless of whether I'm taking the x-ray on the device out in the wild or I'm attached to a data center or a collection of hospitals that are on network where we're uploading all the images to the cloud in a secure manner then we can do scoring all in Azure so I'm gonna go ahead and show you how we can upload these images that have already been captured and I've got them on disk here and use cosmos DB to store these in my V net and trigger as your functions to score these in parallel so I don't have to manage my own cluster of machines when I'm not using them this will simply spin them up and do the scoring so we can go ahead and refresh and we can use the azure function monitoring screen we can see that in fact the data is in fact being uploaded to the cosmos DB and the scoring is happening a little bit slower today than I would hope get updated and you could take my word for this MS brand an hour ago you can see each one of these is running in milliseconds that's time taken to spin up the compute distribute my code and actually do the scoring and we can see that from the cosmos TV query that it's actually taking in the the input PNG images and it's applying the probability of each one of the diseases in the cloud so then I can build a thin client or access in a consistent manner over time and understand what did the doctor see as of that point in time rather than each individual doctor needing to rerun the model on their own device and and potentially have an updated version of the model which is helpful for certain situations it's good to have the latest model sometimes it's great to capture a point in time so I have that flexibility to manage how my model rolls out so you can see using a combination of visual studio tools for AI and I sure you put these together and you can enable data scientists and developers to work together to build more intelligent applications please become an AI developer today thank you Chris so great so we've built all these wonderful tools internally but none of you guys get to use them so what do you care well that's the beauty of it is we've learned all of this stuff and we're figuring out how to bring them forward into our AI platform we've learned a ton and running AI at scale is hard and it takes a lot of investment to get it right but we've done all of that learning in that investment for you and now we get to bring the benefits of our learning into our AI platform and so we think about our platform in several different stages if you're just getting started with AI and you're a developer the place to start is with pre-built AI Azure cognitive services these are models that were built and trained all across different products that Microsoft speech-to-text is used in many different products including things like Cortana vision API is used in a lot of different products across the company we take these models we wrap them up and we expose them as cognitive services that you can then take advantage of in addition sort of the next step with that is you can customize those so you can take a vision model that we built and trained it's really great at identifying images and you can bring your data to really help learn all of the additional images with it we've got the conversational AI in the azure bot platform if you're building a bot and you want to be able to communicate with users natural language processing is it really hard in our cane field to really get right and what you really just want to do is use a bot platform to focus on I want the bot to be able to answer these types of queries and here's where I want it to get the information from so again using all of the information that we've learned from building these services at scale making that available to you the custom AI side which is what a lot of this talk has been of hey I need to actually go and build my own custom models you can start with Azure machine learning packages which are effectively scaffoldings of here's how you can go and start with a model it's sort of looking inside the source code of some of the pre trained models and and customized from there or you can just use it straight natively as it is we talked quite a bit in several places about visual studio tools for AI this is a plugin for visual studio that's available today that integrates very simply with Azure machine learning and you can basically go from your laptop scaling right to the cloud without you know any effort that same model I trained it locally and now I add more data and I want to train it in the cloud and I can just go do that I can monitor my GPU utilization use things like tensor board and get all the results and data back we talked earlier about TLC which is the internal c-sharp library of algorithms the visual studio team is announcing today that they're releasing this as ML dotnet in preview and so all of the accelerated algorithms and work that we've hardened over years in all of our different platforms is now available as c-sharp libraries available for people to go and you can go to github net machine learning and you can go and check that out as well and there's a whole host of things that we've built that we're bringing and making available and so you can I hope you start to see the theme here if we've learned all of these things we've expanded them all across Microsoft and now we're gonna make them available to developers across the world we talked about cognitive toolkit the framework that our speech team built if you're doing an RNN and doing speech recognition this is the fastest framework you'll find for that ml dotnet the TLC the c-sharp algorithms that we built onyx we talked about Azure batch a I we kept talking about filly what's this filly it's an internal system that manages all of our GPU all of the learnings that we've we've had from building and managing GPUs at scale we're now making available in Azure batch ai and so you can go and you can spin up your own clusters of GPUs and manage those effectively the hyper parameter tuning that we talked about making that available on Azure ml project brain wave which was trained and learned on being available in Azure ml and visual studio tools for AI and there's much more coming we talked a lot about ether and a lot of the features and functionality of ether some of them are starting to pull through and there's more to come on that so really a lot of things that we're doing to take all of what we've learned and really bring it and make it available to you on our platforms and so this really brings together like the reason that you build machine learning the reason that we build these applications is the power that they can have in people's lives and so I want to show you a quick video about seeing a I'm Sachi Shaikh I lost my sight when I was 7 and shortly after that I went to a school for the blind and that's where it was introduced to you talking computers and that really opened up a whole new world of opportunities I joined Microsoft 10 years ago as a software engineer I love making things which improve people's lives and one of the things I've always dreamt of since I was at university was this idea of something that could tell you at any moment what's going on around you I think it's a man jumping in the air doing a trick on a skateboard I teamed up with like-minded engineers to make an app which lets you know who and what is around you it's based on top of the Microsoft intelligence api's which makes it so much easier to make this kind of thing the app runs are on smartphones but also on the pivothead smart glasses when you're talking to a bigger group sometimes you can talk and talk and there's no response and you think it's everyone listening really well or are they half asleep and you never know I see two faces forty year old man with a beard looking surprised 20 year old woman looking happy yeah can describe the general age and gender of the people around me and what their emotions are which is incredible one of the things that's most useful about the app is the ability to read our text thank you I can use the app on my phone to take a picture of the menu and it's going to guide me on how to take that correct photo move camera to the bottom right and away from the document and then he'll recognize the text read me the headings I see appetizers salads pani nice pizzas pastas years ago this was science fiction I never thought it would be something that you could actually do but artificial intelligence is improving at an ever-faster rate and I'm really excited to see where we can take as engineers we're always standing on the shoulders of giants building on top of what went before and in this case we've taken years of research from Microsoft Research to pull this off I think it's a young girl throwing an orange frisbee in the park for me it's about taking that far off dream and building it one step at a time I think this is just the beginning so a really fantastic app it's available you can go and get it and use it built by people who are really excited and passionate about what they're doing and so that's what I want to leave you with become an AI developer now we talked a lot about all the different things that we're using in our systems if you'd like to race back across the street you can watch Joseph Soroush and he's going to talk about the state of the art of many of those different platforms that I just talked about and how you can get using them today and thank you I appreciate it and thanks for your time [Applause] 