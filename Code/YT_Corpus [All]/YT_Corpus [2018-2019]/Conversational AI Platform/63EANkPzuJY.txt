 [Music] on the back-office perhaps we can even create a net new capability for HR where we collect feedback on our colleagues and employees as they do their work and we can collate that we can just ping that information to a bot and collate that information as we approach the review cycle so these are these are ways that chat BOTS are actually being used today okay so hopefully hopefully you would agree with some of these use cases so next I want to go through a framework that might help developers and executives think about chat box chat BOTS so first I want to just explain what the different axes represent so on the y-axis you have the domain where you start with closed domain chat BOTS that handle one task or are very specific to one one thing versus open domain chat bots which ostensibly can talk about anything under the Sun so it's almost a philosophical notion in a way and then on the modeling front along the x axis you see retrieval based BOTS versus generative based BOTS and so retrieval based BOTS retrieve pre-canned responses or they retrieve information from a database while generative bots have actually learned something about the data that it was trained on and they're actually generating text or perhaps you might say generating speech is a generative process but let's go through these one quadrant at a time and try to understand a couple examples so in the first quadrant let's take ordering a pizza as an example this is pretty straightforward there are only so many ways that you can order a pizza there are only so many different toppings that you can put on it has to be delivered somewhere and so the point here is that it's it's closed domain it's a pretty well-defined task that needs to that needs to occur and it's retrieval based because these are well-defined conversations and we can retrieve an answer to everything that the user says moving along the x axis toward something slightly more complex you have a generative model and so you may have heard about google duplex announced at Google i/o a couple weeks or months back and this is using mostly machine learn deep learning and what it's doing is it's Google duplex is able to make restaurant reservations and hair appointment bookings for you and it's also able to do this in a very dynamic way so if you heard some of the phone conversations you heard about how dynamic it was in being able to book that for you so I'd consider that a generative process but it's closed domain because just because it's able to make a restaurant reservation for me does not mean that it can help me assemble my IKEA furniture and then in the lower left hand corner this is a bit of a thought experiment and so I hope you'll you know hope I can convince you that this is actually impossible and the idea here is that something that's retrieval based cannot be open domain and the idea here is that you can't anticipate everything that the user is going to ask you and anticipate every response that you would have to provide for any conversation right open domain really means just that we can talk about anything and so I hope I don't offend anybody by using expert systems here as an example because the idea of hind expert systems is to encode information about the world and then be able to reason about that and use that in a productive way but there's always something that's not encoded in the database now to make the point really clear so perhaps I go to fathom and say hey can you get me the bumblebee cat gif and in the bot says I don't think I've seen that one and I say you had you never have the latest ones you never have the latest cat gifs and it apologizes because you can't possibly keep up with everything that a user might ask for or every conversation that could take place so hopefully that makes the point clear and then moving on from there is something that I think a lot of people in this room and and perhaps at all the research conferences are interested in which is artificial general intelligence or AGI this is the hardest and this is where research is taking place and this would involve a generative model a generative base bot that's open domain now we don't really know how to imbibe machines with the same qualities that humans have we're very deft taking new information incorporating that into our knowledge and an understanding of the world and then in using that in new and innovative ways and for any non-native english-speakers here maybe you've never heard the word deft before but you can probably already start to make some predictions that it means in a clever or skillful way and it's this quality that we don't know how to encode into machines yet where you take something you've never encountered before you reason about it and you extend your understanding of the world okay and so an example of this for any Isaac Asimov fans out there is Robby the Robot and so what we're talking about is the positronic brain and I say that with a little tongue-in-cheek but but yeah this would be an idea of something that's artificially generally intelligent so just to be clear this is where fathom bot sits you might argue that something like Google or any kind of search based system is more open domain but I think that needs to be reserved for only the most advanced systems okay so hopefully this is helpful for again both developers and business thinkers alike and thinking about what capabilities they need to build into their next chat bot okay so the bottom line here is bots are really powerful in well-defined domains we're still figuring out how to extend them to many different tasks now the second section is around how BOTS are being built today so as promised we just talked about some of the use cases and now we'll talk a bit about how BOTS are actually being built but first I think we can go ahead and do a demo ok so this is the fathom bot and so we can start by saying hey fathom and we'll get what we'll get back is a response to kind of corral users and start to educate people about how they can use the bot and what it is useful for I think it's important to educate users but they're definitely going to test your BOTS and you're going to get questions like how old are you and bought yours I'm just a kid and then you'll get questions like can you get smarter yes but it depends on how much PwC invests this is currently under active development and then you'll probably want to do something more useful like search for one of your colleagues profiles so let's take Adrian boda as an example and this is actually his profile so please don't spam him Adrian's actually a really nice guy and so then we may want to retrieve some information about one of our clients or a company and so we can search for alphabet and we can pull back that company card not sure if the Internet's slow here there we go so if we wanted to we could then click into that that company's profile and I think the end state of something like this would to be able to would be to reason about that information so what projects is adrianne on or what projects do we have going on with alphabet right now and so that would be the end State and this uses some mild machine learning and I'll explain that when we get back to the slides but now I want to show you some more explicit machine learning that we've used so if we engage the FAQ model I'll show you how we're and I should be clear that we've just engaged a certain intent and I'll explain that when we're talking about building these BOTS but maybe we have a question that you might find in an FAQ sheet and so what we might ask about is CEO action on diversity and inclusion because PwC has been really focused on this year our CEO has been really like front and center on this and so what you see is that we're matching on a number of similar questions and answers in our question and answer base and so the idea here is that we can directly address users questions without having to have them click around in the search portal and so this is using machine learning we've matched on this question it gives you the answer sites its source and then this is really an artifact of development but it just gives you this notion of there is a a goodness of fit or a goodness of match that you can discern how how confident you are in putting forward the answer so I'll mention that when I talk about thresholds but what I'm really excited to show you guys today is something that I think really moves the user experience and if I do a general search for something related to international tariffs because that's making a lot of headlines lately what it's going to do is it's going to go and retrieve a bunch of articles from the fathom database it's not able to connect to the fatum database I think you ever try hmm sorry you guys what's the first rule of live demos don't do them I'll give it one more try but I don't want to belabor the point because I'll talk a bit more about it later in the presentation I'm also thinking that this is public right now and I'm wondering if there are people in the audience or remotely that are actually hitting the spot and just overloading it right now so I thought that was a possibility I was just testing this before I came up here and let's try it one last time no well apologies for that so what I was going to try to show was the ability to do a general search we retrieve some documents and then we ask a follow-up question about that documents and so this uses a machine comprehension model and so I'll talk a bit more about that as we move through the presentation so I think with that we should probably just move back to the slides thank you okay so a bit about what you just saw these are the four things that you're gonna have to solve for no matter what bought that you're building and so this is a bit of a conceptual framework and I think it's helpful for any developer to to understand so starting with intent detection what I'll do when I when I go through these slides is I'll talk a bit about the challenge that we're talking about a bit about show you an example and then talk about a few solutions using vendor solutions or open source solutions if that's more your speed so for this example the intent detection is what is the user want so if we're looking for the whether it's just that the intent is to get the whether if you look at this example on the screen info on Adrian boda is intent classification where you're trying to determine what the user is looking for and it's not a company search it's not a people search but it's a it's not a company search or a general search it's a people search and so if you think about that it's just a text classification task and you can use your favorite text classification pipeline TF to random forest or if you have more data you might go with something more sophisticated like an LS TM or if you don't want to build something like that you can use something rules ish based approach like dialog flow or the NLP API which hopefully I can explain on the next slide so the second challenge is going to be entity extraction what information do we need to extract from the text in order to be able to fulfill the request and so what we see here is and then one sub bullet on that is can we rely on dialogue flow for proper nouns so people places and things like that because you have to train the the dialogue flow agent to be able to detect these proper nouns and it's very difficult to enumerate all of the employees at your company in dialogue flow and so here are two ways that this may go off the rails in the first example somebody might say info on Adrian boda and maybe you get the intent which is to query for a person but you don't extract the entity which is underlined in red and so one solution here is to use follow ups so you might say hey I think you're looking for a people of people search but I didn't get the name so what what name are you looking for and then you can proceed that way when the user provides the name another situation is where you don't have a lot of context at all and you're you're working with proper now and something like alphabet you might not get the intent or the entity and so one solution here is to use something like the NLP API so you head out to the NLP API determine what entity was extracted and if it's an organization you now have the company that you're searching for as well as the intent which is a company search and so what we're talking about here is named entity recognition models or any are models and dialogue flow does this natively the NLP API does this natively but if you're more into the open source scene the Stanford NER tagger or the source Pacey or any number of open source tools will allow you to allow you to accomplish this task now the next challenge is state management the idea here is where are we in the conversation and anytime you have a conversation that takes on more than turn in other words the user asks a question like and by the way this was what I was going to try to show was where you have a search like international tariffs it finds some documents and asks do you have any additional questions I say yes and then we asked about I don't know what are the implications of International tariffs on manufacturing and then it will you know give you a direct answer now I also want to mention that machine learning hasn't really been able to solve this yet and so I'll link you to a paper here called end to end task completion neural dialogue systems where they attempt to use a fully differentiable deep learning model to be able to solve the task of booking a movie ticket and it's you know with mixed results so I guess that brings me into the solutions dialog flow is terrific for this and architecting these conversational flows and then another thing that I'll mention is or some sort of external store you can imagine interacting with your user across time and across multiple platforms and so you seem you need some notion of cache or persisting that users profile across interactions and then the final thing is hopefully machine learning will make some advances in this in the near questionmark future the final challenge to be solved for any bot assuming it's informatics retrieval based right so the distinction I made earlier would be would be retrieval based versus generative based models when you think about information retrieval there are two sources of information one is structured sources the other is unstructured and so structured sources are things like CSV files you can think of them like sequel queries or API calls but unstructured sources require a bit more deep learning machine learning data science and that's the stuff that I get excited about and so what you see here is an unstructured text document and that was really the focus of our project which which was to retrieve information from unstructured text sources so similar to the way that I was showing the FAQ model were able to answer questions as such and so some solutions here are to use machine learning models and deep learning models or even something more naive like a tf-idf model if you have less data and then some of the easier ones or sequel queries and api queries and I'll also call out Q a net because that was the model that we had running in our fathom bot which is a machine comprehension model Google published that paper a few months back and I think it's still number one on the squad leader board and I'll explain a bit more about all that in just a moment so this is probably why a lot of you guys attended this talk and so I want to take a moment to just talk through the architecture and sort of what this actually looks like on Google cloud starting with the user in the upper right corner the user is trying to interact with dialog flow they make a hop to App Engine which makes the call to dialogue flow and dialogue flow evaluates the users request determines how to route it determines what information needs to be extracted out of that and then sends that to what I call the broker which is this guy right in the middle and this is a bit of a Russian doll we have a Python flask app running in docker running on kubernetes and so I'm sure this is a pretty common design pattern for some of the folks in this room but what I really like is the elasticity of this and so the idea is here is that you get all the benefits of the horizontal scaling is more users come online and so the idea I guess I should be clearer and say that the brokers responsibility is to ping all these different API is to go out and do fulfillment as it's called in the dialogue flow documentation and so one source of fulfillment is the PWC database where we're retrieving some of that you know information about our colleagues and companies and clients and then we also have some GPU enabled instances running on kubernetes so we've got some tensor flow models running on GPU enabled instances for some of these tasks like information retrieval question-answer models and then finally you can in you can use external storage like or you might even hack it with something like cloud storage where your retrieving information based on conversation IDs so this is in short what you know what what is architected on the cloud platform and a couple tips that might be might be useful to somebody that's going down this road once you understand what dialogue flow does I would encourage you to look at the Python dialogue flow package once you really understand what dialogue flow does you can and should script just about everything with respect to bot creation and updates g-cloud is obviously a great tool it's your friend you can use it to to build these these kubernetes clusters you can basically use it to build your entire infrastructure and so I would definitely encourage that to be your source of documentation versus you know creating documentation sheets where somebody has to go into the user interface and click around it's just too prone to error as we found establish a robust testing framework so this becomes a quite complex system what happens is you've got the you've got a noisy input coming from users dialogue flows evaluating that you're trying to understand how dialogue flow is going to behave under various circumstances and then you're trying to understand how fulfilment is going to behave insert under certain circumstances so you need to really have like a robust testing framework to understand where things might be going off the rails if and when they do and then finally I'm team docker and kubernetes I probably don't have to say much more on that but I just like it as a development pattern alright so the the key things here our machine learning is already pretty present in most chat BOTS today and GCP has all the necessary tools to do this so now let's move into some of the intelligence layer as I was discussing before with machine learning and again as promised we've talked about some use cases we've talked a bit about how bots are actually built and now we're gonna talk a bit about how machine learning is layered into this bot so these were three information retrieval tasks that we thought about during this project and and so what are each of these in a word so document retrieval you've got a query and you're looking for relevant documents it's basically what Google does we have a query like International tariffs we should be able to retrieve documents at PwC that are related to this query the second task question-answering is taking questions that you might see on an FAQ document and being able to take a query and find a similar question or find the answer to that user's query so an example would be what should I know about international tariffs and the response would be the matched question so what tariffs have been proposed with which it was matched to and then ultimately the user is looking for something like this like the answer and then machine comprehension which I think is the most sophisticated of all these is where you're answering a user's question by retrieving a very specific span of text from some unstructured text document so an example was and this is what I wanted to show during the demo but the query would have been something like what should manufacturers prepare for after tariffs tariffs and the response would be something like higher material costs so again these were three of the tasks we focused on the latter two so question and answering and machine comprehension so we can double click into each of those to get a better sense of how we're actually building that so question answering there's two ways to think about it one is question to question where you're using something like a tf-idf base model and you can just measure the cosine similarity between two vectors and you'll get something like a match score and the idea here is you take a question like this and you match it to all of the questions that you have in your database and you look for the best match and then by default you just have the answer as your lookup value right so that's hopefully pretty straightforward now the other way of thinking about it and this is often when you have more data and we've trained models you know with with hundreds of thousands of examples and we've seen deep learning models start to really excel here where you go from a question to an answer so the user says something like what should I know about international tariffs and the match and you're trying to match to an answer and so the deep learning model is going through the answer column now and trying to match that query to the appropriate answer and and I think this is probably obvious for a number of folks but you might want to cite your source and any other information that might be helpful to the user to be able to do their own further investigation now the final piece of this talk is around machine comprehension and I really like the way that the one of the co-authors of the squad paper described it and he says it's the ability to read a piece of text and then answer questions about it and this is really challenging for machines and to be clear the squad paper refers to the Stanford question-answering dataset which has become the de-facto benchmark for machine comprehension models and so QA net as I mentioned earlier I think is still ranked number one for that data set and so again to be abundantly clear we're talking about selecting an answer span from a provided text passage and so if we look at the picture on the right which is taken from the squad paper you see there's a passage these are Wikipedia articles and then the users have posed a bunch of questions against that article and they've also highlighted the answer and so the first question is what precipitation to fall answer being gravity and you can see that it's listed up above and so that should also tell you how these datasets are structured where you have a passage query and an answer and with that we can build machine learning models to take in passages and questions and hopefully put forward what it thinks is the most likely answer and so for anyone that thinks they've solved the task of machine comprehension I would encourage you to take a look at this question here which says which technology was developed most recently in to us the answer is obviously a the cellular telephone but for the guy on the left this is a difficult task we don't really know how to imbibe machine learning models with this notion of common sense or real-world reasoning and the information is there we just don't really know how to encode it into these machine learning models whereas it's very trivial for the guys on the right which is a picture from the movie Dumb and Dumber and I'll leave you with one little nugget here if you're interested in these topics I really enjoy listening to a podcast called data skeptic hosted by a guy named Kyle Pollak and he did an episode called the imitation game where he interviews some folks from the Allen Institute and talks about this data set and this task fresh off the press last month squad the the squad team released a second data set and I think they're addressing what I think is a pretty important problem which is to abstain when presented with a question that cannot be answered in other words if the models not confident enough or couldn't find the answer in the provided text passages we need to know that we're not confident enough to put forward the answer and right now I'm having to use things like thresholds to be able to determine if I've got enough information to answer the question or not or I'm confident enough to answer that question so for any practitioners out there you know that transfer learning can only take you so far Squad was from the Wikipedia domain I'm working in the PWC domain so transferring a model trained on Wikipedia will only take me so far for PwC labeling data is expensive so as much as I'd love $20,000 to go out to Mechanical Turk and label a bunch of data I'm not gonna get it proof for that and so we need to think about how we generate data and either a semi-supervised or a fully unsupervised way and so what I want to do what I want to share here is something that we would be exploring if we had more time this paper is called sin net and if you look at the one two and three on the screen here the idea here is that we can take the squad data set we've got the passages the questions and the answers and we can train a model to generate interesting questions and answers given the passage and we know what those questions and then and answers are because we have labeled data so once we train a model to do that we can feed in pwc documents and take those passages and generate questions and answers in the PWC domain and then you close the training loop by going back to the machine comprehension model and fine tuning that model on a combination of the squad data and the PWC data to be able to bring that model into the PWC domain and perform a little bit better so these are the things that you have to think about on a day-to-day to be able to actually get these machine learning models up and running so I think hopefully that was convincing that machine learning models can and should improve information retrieval especially for unstructured documents so some final considerations here what data can you collect so for the FAQ documents we had to round up all these FAQ sheets and then parse out those documents I extract out the questions the answers and I think we wound up with something like 20,000 question-answer pairs that we can then train our machine learning models with I just talked a couple slides back about the need to generate or label data and circumcircle stances these are some things that you'll have to think about and then how can we improve the quality of our results from user feedback so for open to capturing user feedback was this useful or not how do we then close the model training loop and incorporate that into our model improve results on the user front how confident are we that we have the right answer and this comes back to the notion of fresh holding where you have a probability score associated with a lot of these models and that should be tuned to determine the number of false positives and also the amount of volume that you can handle for incoming queries be clear about what the bot can and can't do so when I first started interacting with the bot it said I'm helpful for people queries company queries general searches and so you kind of have to coach users because I think the expectations are set very high and I think you're definitely set up to disappoint users if you're not clear enough consider edge cases so your your users will try to push the bots to its limits and you just want to understand how it's going to behave in those edge cases and this is for a number of data scientists out in the room I think we all do a good job defining measures intrinsic evaluation so accuracy score recall that kf1 scores exact match all that fun stuff but we also need to think about measures of extrinsic evaluation so how have we moved the needle for the business in terms of user time save number of clicks reduced to find a piece of information or potentially even revenue so if I've done my job well this is where you are we've talked about some use cases how bots are being built today and a bit about the intelligence layer that concludes this talk and I'm happy to take questions at this time thank you [Music] 