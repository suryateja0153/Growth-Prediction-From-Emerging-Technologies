 good morning and it's great seeing all of you here and thank you thank you for coming to this session I'm Ted I'm a program manager on the ashram machine learning team and Doug here is a distinguished engineer at Microsoft Research and I think you've heard a lot about the announcements yesterday with project brain wave and what Doug will be going through will be a deep dive into what project brain wave is and then I'll talk about how you can use project brain wave so we'll start off with Doug and just giving you an overview of project brain wave and and how you can use that to accelerate everything great Thank You Ted good morning everyone thanks for coming so we're gonna do 25 minutes for my half and 25 minutes ish for Ted's and this can be interactive so if you have questions at any point feel free to put your hand up we have-we have enough time I think for it to have some online discussion for those of you looking at my right eye and wondering what happened it wasn't a bar fight and it wasn't a technical disagreement I'm a squash player and I took a racquet to the face Saturday morning and fortunately I had protective glasses on so there's no problems here but it does look a little a little grim well they looked a lot worse Saturday morning okay so this is a really exciting moment for us because we are taking this work that we've been doing across the company to accelerate our internal AI and bringing it to our customers and so you know I run a team in research that built a big chunk of the brain wave architecture which runs on top of the FPGA infrastructure that we've built and Ted is the the product owner that's that's really running the service and so really Ted is your your right point of contact I'm always happy to answer questions in forward email to Ted and since he's the hard product truth guy he's going to give you some serious technical depth you know I was a professor for ten years and so if I flip into professor mode you know Ted may come up with the hook could pull me off okay so so what is project brain wave it's a hardware architecture think like just like the Nvidia Volta Google TPU Nirvana's chip there's a zillion startups building their own what we call NPS for neural processing units so so project brain way there's a hardware architecture the reason we call it project is because we don't have an official product name for it yet so it's still it's still in the research project nomenclature phase and so we've chosen with the hardware architecture to synthesize it down to FPGAs rather than building our own custom chip and that's a deliberate choice and we think at least at this point in time it's been the right choice for us and it's given us a tremendous amount of flexibility and also the ability to get into a performance leadership position for DNN inference and we've had a great collaboration with our partner Intel who's been working with us to optimize the system and we've really benefited from their technology so I know I have a bunch of bunch of friends from Intel and in the room here so thank you all for for the great partnership and support so I think what I'll do to start off is play a short video that that we had produced and to give you a sense of our overall fpga project and and and really what we've done over the past few years it's been a really interesting ride because we've we've been able to leverage this for all sorts of different data set of workloads project brainwave is the latest attempt or the latest iteration that drives it for deep learning and so let me start with that and then we'll in this era of exponential data growth data centers need a faster more intelligent cloud to keep up with growing appetites for compute power since the earliest days of cloud computing Microsoft has been innovating with specialized processors to give CPUs a boost for critical workloads among accelerator options FPGAs offer a unique combination of speed and flexibility ideal for keeping pace with rapid innovation on FPGAs data flows through programmable silicon level logic blocks their process instructions in parallel a perfect approach for big data our unique board level architecture uses Intel FPGAs to augment our data center with an interconnected configurable compute layer Microsoft leads the industry in transforming data centers with programmable hardware we were the first to prove the value of that PGA's for cloud computing first to deploy the McLeod scale and first to use them to accelerate enterprise level applications beginning with Bing our leadership been accelerated networking delivered the world's fastest cloud our pioneering with FPGAs for distributed computing pave the way for breakthroughs in artificial intelligence our FPGA is enable real-time AI with industry-leading price for performance and this is just the beginning Microsoft has an aggressive roadmap of platform architecture and algorithmic innovation ahead Roger will remain the world's most intelligent cloud [Music] okay so that's a little bit of the project history and I think you saw from 2016 to 2018 we were building a lot of this AI capability on to our network of FPGAs that we've provisioned within the company and really what the announcement today or this week has been is about bringing that to our Azure customers and so we'll talk I'll talk a little bit about the architecture and why it's disruptive and and and how we generated these levels of performance and then 10 will go into what the service looks like how you can access it I think one thing that's really important to understand is that when you when you work with raw FPGAs people typically write something called HDL which I'm sure many of you have heard of hardware description language in a language like VHDL or Verilog and really these are languages where you code like you're designing a chip and then you will take that that hardware code and run it through a different tool chain or a different compiler you know hardware compiler and then you can either take that to a fab and produce your own custom chip or you can run it through a different flow and synthesize it onto an FPGA and of course the FPGA is you can change the image you know every second if you want to although it's hard to generate programs that fast and so the the what's really nice here is that we've built an engine that's what project brain wave is that has its own language so you so we can write down to it in a higher-level language right so when you have a DNN we can work with customers to map that DNN to the engine rather than dropping all the way down to the the bare metal of the FPGA so it's a much easier developer experience than working with the raw FPGA in some sense we're abstracting the complexity of that programmable Hardware chip from our developers and users but the really nice thing about that that model is that we can actually keep turning the engine and turning the crank and improving that Hardware continuously under the hood almost like you know you get a noose like when you're using CPUs you get a new optimized CPU every month without actually changing the chip in the socket so as we generate more optimized versions of the the architecture we can slide those in and they'll remain software compatible with the models that you port and so you just get this could ually improving level of performance I mean Ted knows that we we brought up you know the convolutional networks the resident 50 model that's our launch offering and three or four weeks ago I think it was maybe ten times as slow as it is today because the team was just tuning it and they brought the latency is down and so we're now at about one point one point eight ish milliseconds per image and so as customers you'll see those you know those those improvements come in a steady stream you know we Rev the hardware more than once a month and one of the really neat things about that model is that when there's a discovery in the deep learning community so we see some research paper published with some new operators some new activation function some new algorithm that's substantially similar but different from what we're currently deploying we can actually pull those innovations in very fast and then roll out a new image so it's like your hardware infrastructure that are running the models and your enterprises for those of you who are you know enterprise customers will will have a steady stream of advances you know following closely behind their discoveries in the research literature so it really keeps you up-to-date and keeps you in a super compelling position all right so I'll go I you know go on with a few more details here I don't think we'll see the video again ok so we talked a lot in this release about real-time AI and I'd like to be very precise about what that is so I think many of you know this so apologies for the redundant information what many chips today do that in the DNN acceleration community is something called batching and the idea behind batching is you take n requests where and it could be 2 or 4 or 8 or 256 or 512 or a thousand 24 doesn't have to be a power of two but it often is and you lump those individual requests together into one big batch or package and then you ship that off to the hardware all at once and the reason that that that works with a lot of the chips like GPUs and TP use today is that the chip doesn't have enough bandwidth enough memory bandwidth to keep one request busy so you get low utilization of the chip if you only send it a single request so by sending in a thousand requests the the chip can work on the first chunk of each of the thousand requests and then the second chunk of each of the thousand requests and in all thousand finish at once the aggregate performance you get out of the chip is thus much higher but the latency you see is actually much lower or much higher it's much slower and so today in typically people have to make a trade-off between do I want to run with low latency or do I want to run with high throughput which means low cost and so what we really tried to do with this real-time AAA model is eliminate that distinction and so we got rid of batching and so the chip runs it nearly full throughput with a single request so you send one request to the project brain wave service the chip gets very high utilization and it sends you the answer as soon as it's processed and then you can send another request and so in some sense you're sending a stream of individual requests and you're getting a stream of responses back and so you don't have to play games and software where you're batching up a thousand separate requests waiting for them to arrive batching them together doing the data marshalling shipping that whole big package over then waiting and getting the results back so it really simplifies the way you use this system you don't have to think well what's my latency down how much should I batch how much will it cost me to do that we just say we provide a single interface you just send requests as fast as you can and the responses come back really fast and and that's really what you want in real time scenarios when you're when you're on a we've talked about Jabil as our lead customer or one of our lead customers they have a manufacturing line they don't want to see 256 boards flyby on the manufacturing line before they shift the did the images off to a DNN to be processed and figure out which of the boards are bad because by then they're they're down and you know way down the line in the next stage being incorporated into products right so they send a look at the they look at the circuit board they send an image it comes back good or bad if it's bad they pull it off it's good it keeps going and they can keep up with the rate of the line in fact they can speed up the line now because of those capabilities so for deep learning the deep learning is starting to cover many many areas and initially it's being integrated in workflows for things that are pretty common in terms of people's capabilities vision language speech question-and-answer knowledge but what's happening more and more is as we're discovering new uses that humans aren't good at those are a little bit harder to find things like sales lead generation malware detection bug finding there's just many many many applications of deep learning that we're discovering work well that we didn't we just didn't know because we haven't evolved to process those workloads as humans the way we had with speech and vision and so this these capabilities are growing and becoming broader and broader we're seeing them get incorporated gradually into many of Microsoft's products and our competitors are doing the same thing and then it turns out that many of these use cases end up being real time on inference and so if you're having an interactive session or you're integrating with some live process like a manufacturing line or you've got a thousand cameras in a retail store and you want to know what's happening at any given point you know is somebody shoplifting is some product out of stock on the shelves has a refrigerator door been left open you know these are all things that you wanted and find out as soon as they happen and so that low latency is really critical and by eliminating that trade-off between cost and speed we can now integrate real-time AI into our processes with really no no no consequences or no trade-offs so we think it's a pretty exciting pretty exciting offering now previously to this release the DNS have been pretty constrained by performance and power which is why we're starting to see all these new accelerators be generated and because these accelerators tend to be pretty memory bound like I said that's why you have needed batching up until now but with this release now we can give you a hardware accelerator that you can get real-time a on get responses quickly and get very very competitive cost so really those trade-offs go away one other interesting thing that we've announced and I'll touch on this a little bit later is that we're going to do this both for the cloud in the edge and so a lot of customers have real time in their factories and their warehouses in their oil platforms and their oil wells all you know in their in their ships and their drones and their airplanes in their in their office buildings I mean all of these things have servers and very often you just the the data you're processing is so intensive you don't want to send it all the way to the cloud or you don't have time to send it over with their cloud and so that's where real-time a on the edge and here in the edge I mean in servers that are on customer premises is really important there are also scenarios in the cloud that people care about like when you're big datasets or in the cloud you've got multiple datasets streaming in from multiple places like things satellite imagery and you want to correlate that with other events and so there's definitely real-time uses in the cloud as well but there's also a lot of real-time uses on the edge and so when we develop this capability we thought it would be really important for customers to have both now the the cloud offerings a little bit ahead so we have people in production today thanks to the great work of Ted news team and and we are working through the details of the the edge offering but we are in preview and we are working with partners so now I we I'd already talked a little bit about why we chose FPGAs rather than synthesizing a custom chip but I want to go go over those advantages again because there's a lot of noise in the community I mean there's startups that have raised hundreds of millions of dollars and so they really want you know they're talking about high levels of performance and there's a lot of one-upsmanship with metrics and benchmarks and and just a lot of chatter because the economic stakes are so high so we really wanted I really want to cut through that and so the first thing we did with the project brain wave architecture said we wanted we want to do this real-time thing so we're gonna get rid of batching and that means you both want to have the high throughput that I mentioned and very low latency and so performance there on the left was was one of our primary goals and so what we've been able to show is that we're actually we have industry-leading latency for the requests we're sending over to the FPGA the launch mawla we have his resonant 50 which is a general image classifier a very popular image based DNN and of course we were working with lots of partners on other models and we've got a whole ton of models within Microsoft and so we'll be rolling these out in pretty rapid succession but on rezident 50 no-one has actually shown levels of performance and any batch size comparable what we're achieving on FPGA and we're right now we're hitting about you know from a local host to the to the chip and back about 1.8 milliseconds per image and that number is continuing to drop there was a little bit of a because of all this the economic stakes and the numbers flying around people have been pushing a meme that well if you build a custom chip it's going to be faster and they talk about Asics application integrated circuit which people typically think of as being very efficient because they only do one thing but it turns out that all of these DNN accelerators are actually programmable they're all programmable engines they all have they all have an architecture that you can program to because you want to run different models and that program mobility comes at a cost you want to be able to do multiple things and you need to make design choices when you build the chip so TP use GPUs other NP use what I call neural processing units coming in from all these startups and the project brainwave engine on FPGAs and other neural network implementations that people are doing in FPGA s are all examples of programmable accelerators and really the other way you have to look at is the architecture at that top layer not what is your deployment strategies FPGA or custom chip or something else and so what we've shown here is that we're able to achieve industry-leading performance with the fpga by choosing the right architecture and a lot of people had thought that that wasn't possible but i mean i think that a lot of that just misconceptions and fun ok and then i talked a little bit about the flexibility in that we can incorporate new discoveries quickly we can tune the engine continuously so you get this rapidly improving stream of innovation really without changing your software model so we pour it we port a model and then your engine gets better and better over time now of course you know we we can do things that break the architecture if we get radical improvements and that's always a discussion with customers you know it's just opportunity but we will support backwards compatible models another really interesting thing that we do though and this is something that we haven't talked about very widely yet is that these DNN models are actually very different now a convolutional network for image processing is very different from a recurrent network for say text processing or language processing and there are many different kinds of networks now coming out of the research literature these different networks actually have different requirements and so what we do in the project brain wave system is we will actually generate images of the hardware accelerator that gets mapped on the FPGA that are customized for individual models or classes of models and so if you have four big buckets of models we can actually generate four variants of the engine that are tailored and streamlined to those models as opposed to giving you a single fixed architecture that has to serve all types of models and where you have to make design compromises and you have to pick those when you freeze the architecture and then that has to work for two three four or five years so we're able to to shrink-wrap the engine to fit models and in fact we even do things like pick the data types you know instead of 32-bit floating point or 16-bit floating point or 8-bit integer you know we actually might pick nine bit floating point for this model an eight bit floating point for this model and that all happens behind the scenes you know we have in our tools we have a flow right that will take your models and cast them into the right data type and then pair those with an image of the project brain wave architecture that's been optimized for that model so when you think about the edge offering what you want to do and we'll I think I know we'll get into the details later but you have a you'll be running an azure IOT edge client on the server it will connect up to Azure ml where you've trained your model and it will pull down both your model and a version of the project brain wave engine that's been optimized for that model and so really the FPGA images then implement that architecture and the models are paired tightly together and so you can optimize it on a per model basis that's a capability really that very few other offerings have and that will also give you another boost in efficiency up beyond what we're doing just with the raw architecture and the real-time AI and the elimination of batching and then finally in in our cloud for the cloud offering we have scale so Microsoft has been putting one of these FPGA boards and just about every new server it buys for three years so as you started in 2015 Bing has been doing this in about the same timeframe and so we've deployed massive numbers of these things we have more scale than anybody else in the industry for this technology and so there are some demo there have been some demos this week the ESRI demo in AI for Earth where we're I mean it's been shown already right mark russinovich will show it oh okay all right so I won't I won't give it away but show you should I say it okay go to Mike's talk yeah go to Mark's talk and see the demo but that emphasizes the the very large scalar which were operating so for customers that want to integrate very cost-effective real-time AI into their into their businesses you know at the business site we can do that or often will do that if you want to go into the cloud and intersect flows and do AI on those flows we can do that but then if you want massive scale we can also do that and that's one of the other advantage of this of a cloud you get massive scale and that elasticity so if I want to do if I want to be processing hundreds of millions of images in a very short timeframe we can scale out and get that job done for you and so that's a that's a really compelling capability that I think we're just even now starting to understand what that really means because they're all these big data sets and and but people haven't been able to operate at the scale on and these speeds before so it's gonna make new things possible like just continuous monitoring a very large data sets very large facilities geographic regions think about the power grid and what you might be able to do there with you know millions of producers and millions of consumers as opposed to a few centralized producers like coal plants that you can move up and down so just this worldwide scale is an opportunity to really start building control systems but you need very large scale to run those control systems with AI and that's something that we offer okay and then we don't really talk about cost compared to all the other technologies because you know there's there's a base cost and there's margin what we've just been talking about is the cost to you the customer and so when you come to Asher for this offering and you want to get on Ted service his team service and start classifying images for your business for your research you know for your personal life or whatever the current cost of the current offering is under 20 cents per million images and so that's what it costs you to rent the service process a million images you'll pay about 20 cents actually little bit under I know it says 21 on the slide but because we've been tuning the engine the costs have come down but because the throughput has gone up since we made the slide okay and and I'm hopeful that you know in a few weeks it'll be 16 cents and then 15 cents I can't keep up so we're just gonna leave it at 21 cents and and speak to it alright and actually I do think 10 we're gonna be at 15 cents and you know in a few weeks month okay so that's also that's also a really amazing thing and and this level of capability hasn't been available before go ahead so the low cost is that okay okay yeah so the question I'll repeat the question for everyone the the the low cost is at what latency and the it's a little bit of a nuanced answer so I'm gonna give you an honest answer rather than a marketing answer so the the low cost is in is at that real-time AI batch one offering okay and so today on the on the from the from the service hosting the chip like I said it's 1.8 milliseconds currently we haven't tuned the software stack as much as we might so it's still a few milliseconds over to that server and back okay so we're seeing today six to seven milliseconds end to end and we also haven't tuned the flow of images so if you're using all 24 cores for example on a host machine driving all that data to the FPGA and then back there's a bunch of queuing delay in that host machine that's driving up and you know another 10 so there is actually a some additional latency when we're driving it very high throughput but that latency is not fundamental and we're gonna be we're gonna be driving it out you know over the next few months so for example like that the time to the server and back is gonna go to near zero because we have this accelerated networking capability and I might be getting over my skis but I'm gonna keep pushing on that so the I think the marketing answer which is also true is that there's no at that cost there's no inherent extra latency built into the system so it's still that industry-leading latency of the chip it still baptized one it's still real-time AI that's all true just today when you're driving it at maximum throughput there's there's a bunch of queuing elsewhere in the system not on the chip itself that's that's adding some latency but that's going to come down over time okay so like I said before really we what we want is no compromises we want to give you real-time AI very high performance at that cost structure which is continuing to drop and I'm confident we'll get there and it's already really good Ted do you want to add anything to that yeah so I think the question from latency like Doug said if you were to send an image in the same data center as the FPGA we're seeing about six milliseconds and 2n and so basically the service offering is about 42 cents an hour and to run 1 million images takes about 31 minutes and that's where we're coming up with that 21 cent mark yeah and it's continuing to drop I have to keep emphasizing next our team has been burning so hard up till build right every every Center we're squeezing down I get excited by it's kind of a sad life ok so I think I've actually talked to a lot of this I'm going to say a little bit about our internal technology because we don't have it quite ready for for the public offering so today what we've deployed in Azure our servers that have four of Intel's really amazing Aria 10 FPGAs on individual cards ok so that's a you know fairly standard high-end server and it's got four of these cards you've already heard the price structure to rent a single FPGA and of course you can rent multiple ones if you want more throughput multiple boxes what we've actually built within Microsoft and we haven't brought this technology to customers yet but I want you to see where it's going and you saw it in the videos this notion of a configurable cloud so what we actually do within our infrastructure is the FPGAs talked directly to the network and so they are network attached devices which allows us to scale out with very low latency and so what you can see here for very large models and this is something we do in worldwide production today already is if we have a really large model we'll take it and then we'll just stripe it across those F's there those FPGAs that are talking to the level zero switches in the data center up to the level one switches and so this is something that aspirationally I would like to bring to our customers because then you get the ability to run even larger models at even greater speed so I think the point of this is that we do have a roadmap where there's some very advanced technology coming down the pike we just have to figure out the cadence to get it into Azure and bring it to our customers and then of course on that each FPGA is that you know is that project brain wave hardware architecture that synthesize down onto the programmable Hardware that you can see over there on the right of the chip and so we've talked about the speed and this trying to eliminate this trade-off between cost and latency to first order although we still have some tuning to do and we've talked about the flexibility which is another advantage where you get this continuous stream of updates and improvements and you also get the ability to customize the engine for different models which gives you an additional bump of efficiency over what you're able to do with other technologies and then the last point that I really like to emphasize and I think you've hopefully heard this is the fact that we want to support many of the frameworks we want people to use any framework that's popular that they bring to the cloud and so we're we're doing models and tensorflow today in fact we'd like to run tensorflow and lower latency than Google does with CPU I think we're already there but we want you to be able to bring C NT K PI torch many others and then we have this open source open neural network exchange onyx format that we will also be supporting and so that is that many people in different frameworks will be able to argue that's our this infrastructure and and get great benefits without being locked into a vertical yes it's question the back so the question was do these Asics become more cost efficient than the FPGAs and I think may maybe I'll use that and I'd like to address that question now because I think it's a really important to address so I'm not I mean I'm a longtime computer architecture researcher and I'm actually a CPU architect by training so so maybe this is a new adventure but in in the if you want to be sort of academic and nitpicky ASIC and I'm sure you know this as many here may not it stands for application specific integrated circuit and typically what that means is a fixed pipeline that only does one thing all of these neural architectures that people are building like I said before programmable architectures and they can be synthesized they're not really Asics they can be synthesized down to a custom chip which is kind of how people are using ASIC today has become popular or an FPGA so now if you decide hey here's here's a a neural processing unit NPU architecture that i want to run at very large scale and I'm synthesizing to an FPGA but I'm convinced it's not going to change and that the models the class of models that it supports as opposed to the other variants is so big that I want to optimize it for cost I might be willing to pay the 50 million bucks and stamp out a hardened version of that where you know the area is slightly lower you know you can always take something softs into the eyes of an FPGA and make a more efficient copy although it you have to freeze the design and it takes several years to do that and get it in deployment and it costs some number of tens of millions of dollars so there are scenarios where I think this makes sense and but but at least for us you know every quarter we ask ourselves the question is it time to maybe take one of the engines of the many we synthesize on the pages and harden we haven't hit that point yet we're still iterating it pretty fast so in theory it's possible but right now all the economics say no this is the the path we're on is the right one and and right and really you know we're right now we're at I think a very strong performance leadership position in the industry while retaining all that flexibility so it's a pretty good place to be yeah yeah I mean I again I think what really matters is the architecture you pick above that custom chip FPGA deployment vehicle and so I'm just as a as an ex academic and Ted you might want to stop me when I'm running over five minutes okay you know what's really fascinating right now is that all of these different designs that people are building are all fundamentally different architectures you know brainwave is actually a sort of an out-of-order vector dataflow thing that looks under the hood more like a superscalar made of vectors but it very much wider ILP or VLP you know the other side of startups have massive numbers thousands of statically scheduled tiny processors which so all the smarts is in the compiler Google's got a systolic array I mean it's just you know all these old debates we had in the 80s about yeah it's all being replayed now for neural networks and no one knows what the right answer is so the people that take the right architecture are gonna do well regardless of how they do how they deploy it again but you know we're iterating and learning very fast so I'm pretty optimistic about where we are okay so we talked about scale so I'll skip over that and I think this is slide 15 isn't it one more okay well I'm going to turn it over to Ted because I think this is where he can pick it up okay cool so I got done three minutes early you see this is the first time you've seen it three minutes early so I hear first thank you Doug I'm tired so I'm a p.m. on the Azure machine learning team and and Doug essentially described to you the engine of a Lamborghini you think about it that way and it's a beautiful engine it's super ly highly tuned and optimized and it just tongues but it's an engine so from the product team I'm here to give you the keys to that Ferrari and and show you how you can drive it and so in terms of project brainwave that we as we talked about the first thing is just resonate 50 and so so ResNet 50 is a steep neural network the way I like to think about deep neural networks and and and how they can be flexible is in the context of a bomb-sniffing dog so let's say you wanted to train a bomb-sniffing dog at the end of the day a bomb-sniffing dog is a german shepherd right so so imagine I'm a 3 year old German Shepherd German Shepherd has the infrastructure if you will to be able to take in smells and to be able to distinguish among smells in a very very sensitive way so that's what a German Shepherd is good at but a German Shepherd is not a bomb-sniffing dog so what do you do you give it some smells you say this smells like a bomb this doesn't smell like a bomb this smells like a bomb this doesn't smell like a bomb three weeks later you know 20 boxes of kibble later you have yourself a bomb sniffing dog so training that German Shepherd to be a bomb sniffing dog is not that difficult building the German Shepherd that's the hard part so essentially ResNet 50 coming out of resonance coming out of microsoft research is this german shepherd so when we launch with resin at 50 now you have the capability of having a German Shepherd that would be able to take in data image data in this case take out those features and then to be able to train those features to do the things that you want to do so as with a German Shepherd now you have a mom sniffing dog now you have a food sniffing dog and a fruit sniffing dog for your airport now you have a skin cancer sniffing dog right all these different things you can do so and and I can't emphasize enough for Doug and his team the kinds of world-class researchers we have to be able to to be able to do that industry-leading performance and so in terms of what we have from the azure machine learning integration I think it also Doug talked about no batching required I'm gonna now talk about what that looks like from Azure machine learning connecting with people from different cultures finding and treating cancer earlier making the world accessible to everyone today's breakthroughs and artificial intelligence come from deep neural networks using very large multi-layered models that need amazing amounts of computing power running these models at high scale low cost and ultra fast speeds has always been extremely difficult not anymore project brain wave unlocks the future of AI by unleashing programmable Hardware using Intel FPGAs and this delivers real-time AI at blazing speeds with no batching no compromises and no need to choose between high performance and low cost our machine learning accelerated models powered by project brainwave enable data scientists and developers to easily train deep neural networks and deploy them to the world's largest configurable cloud with record-setting performance ResNet 50 is an industry standard image classification model processing one image requires nearly 8 billion operations project brainwave leads the industry in speed on ResNet 50 models with under 2 milliseconds per image with image classification that can be customized with your data table one of the most technologically advanced manufacturing companies on the planet maintains rigorous standards of quality control today Jabil identifies manufacturing defects in electrical components using human judgment one set of photographs at a time what if cable could analyze thousands of quality control images in seconds using deep learning to reliably identify anomalies for people to examine more closely making humans more effective and making the whole process faster and more accurate now they can by using Azure machine learning accelerated models to train and deploy a model whether in the cloud or on the edge what can you build with accelerated real-time image processing detect spills or open freezer doors in your retail store conduct real-time medical image analysis inspect critical equipment track endangered species and this is only the beginning accelerating with programmable hardware means project brainwave can evolve quickly to keep up with rapid innovation in deep learning Microsoft is already using many models for text audio speech and natural language and will bring these models do you soon you'll even be able to accelerate your own custom models and design the next generation of real-time AI applications start accelerating with Azure machine learning and project brainwave today [Music] all right so let's talk a little bit about Azure machine learning and in terms of how it can integrate with with project brainwave so Azure machine learning is an end-to-end data science platform and you think about what data scientists do today data science is the sexiest job of the 21st century but you talk to a data scientist and they're probably spending 50 to 80 percent of their time on very boring tasks like data cleaning and a lot of data janitorial tasks so starting with the from the from an intelligent data preparation perspective just having tools to be able to help you get that data into a cleaner form that then you can process terms of model training giving you the flexibility of training your model on the compute that makes the most sense to you maybe you want to train a model very quickly on your local workstation trying out different models and and trying different frameworks just to see how you're able to get something that seems to be promising you want to do that quickly on a local machine then you might want to scale up to a spark cluster maybe run a cluster on CPUs now submit jobs to this big cluster that then you can train on to be able to train your model or maybe for deep learning models you want to spin up a GPU cluster using something like as your batch so now you have a GPU cluster spin up submit your job to train and then spin down again so the integration of all of these various compute contexts and now recently with the integration with data breaks to to be able to train these models very very quickly from a model management perspective this is also starting to be very important we were talking to the CIO of a Wall Street firm and he was saying we have 10,000 machine learning models in our firm we get data from them we make decisions from them but I have no idea who created the models I have no idea what their training results are no idea of the performance so what happens in that situation so it'd be great to be able to get the results from the model trace that model back to the source code know the data scientists that created the model know the training results and all of that so that's the becoming very very important from an enterprise perspective we bring in the best of open-source so whether you want to use Python Microsoft cognitive in Python Microsoft cognitive toolkit Google tensorflow cafe all of these different frameworks you're able to use in Azure machine learning because we know there's so much innovation happening in open-source to bring you the best of open-source a lot of great features and functionality and open source which is great but some of the things that open-source does not care about are the boring things like compliance and security and Venus and all those things that an enterprise cares about so what we do without your machine learning is that we give you the best of open-source the best of Microsoft and then we package it up into an enterprise-grade end-to-end data science platform for you to be able to build and train your models and so the preview offering for Azure machine learning hardware accelerated models is incorporated into this entire infrastructure into this entire platform so the same platform that you're using to train your models deploy to CPU deploy to GPU deploy to the edge you are now integrating to be able to now also deploy onto an FPGA so so this is Python this is tensorflow and then you're creating that model and then deploying it in a server list architecture so from an infrastructure perspective just to give you a quick overview our first region is East us so East us is what we're operating with a launch if you were to deploy a model today it would be in each in East US each of these stamps have 20 racks each of these rafts have have a lot of boxes and each of these boxes have four FPGAs this is like that story I met a man coming from st. Kitts and he had seven wives and each wife had seven kits and each kit had seven kittens and how many were actually going to st. kids but basically this is the number this is how we basically have all of these in in the data center we're going to be adding new regions very soon in West Europe Southeast Asia and South Central u.s. so these will be coming in the next few months so that you'll also be able to deploy models on FPGAs in these regions and then what will expand out from there in the actual Asscher host itself basically we have the Intel Arya 10 FPGA we have four of them and there's a wire service that basically is a thing that programs the FPGA so so Doug talked about this image that you flash on to the FPGA so until today is that highly optimized resonant 50 image and then in this host we have our VM so this is an azure VM currently this is available on to us as Azure machine learning so we give you back an API but we would we would we as a our machine learning we provision the VM and then we flesh the FPGA and then we give you back that that API so on this on this VM we have our VM extension so when the VM is provision the VM extension would talk to the wire service and the wire service and we tell it to flash the fpga and then that brain slice images flashed on to the fpga this is the way that we talk to a brain wave talks through the vm to the actual fpga and on top of that runs our Azure machine learning and so basically then we expose the GRP capi ER pcs used for tensorflow serving as justice of a way that people are able to to expose these today and that's essentially what happens underneath the covers and when we give you this api we also have monitoring etc and all the all the goodness that you would expect from an azure service the pipeline which i'll be talking about the actual model that you're deploying will be running in the Azure machine learning runtime and so part of it will be running on CPU so for example maybe your pre-processing you're converting a JPEG picture into tensors that's going to be running on CPU and then when those tensors are going to be run on the FPGA that will go to brain wave which will then process that on ResNet 50 on the FPGA for ResNet 50 and then you might have some post class post processing so once you get the extracted features you're going to be running that classifier and that classifier then runs back on CPU so the idea here is we have highly optimized operators that can run things on CPU and FPGA and then and then accelerating that on on FPGA when you actually deploy a model so basically again using Azure machine learning I now go through the code and the notebook for this in a minute but you're just rewriting tensorflow you're using you're using Python and tensorflow that's doing that pre-processing you know converting those JPEG images into tensors and then and then deciding whether you're going to run the FPGA and then you might train a classifier at the final stage and then after that what what happens is this service definition goes to our model management service and this is the same model management service in Azure machine learning today meaning that if this model if this destination is about to go to an FPGA great model Management Service knows about it maybe you created another model and it's going to be running on a CPU cluster Model Management Service knows about it so in this way you can manage all of the models in your enterprise you know which models have been created and you know where those models have been deployed so one centralized location to be able to manage your models the Model Management Service talks to our control plane service this is the control plane that will spin up and provision that model and this is just another view of that VM where we have that Orchestrator on the orchestrator might run the pre processing on on the CPU and then it runs the resident 50 on brain wave and then for a classifier then that you have essentially again tensorflow classifiers that you might want to run we can also front this with a software load balancer so right now in terms of the in terms of the throughput that we're seeing at about one point eight milliseconds we're getting about 530 images per second but let's say you need you know two thousand images per second so what we'll do is we'll just make lots of copies on this on these over multiple VMs and then front it with a software load balancer so that you can get the throughput that you would need so essentially this is the architectural diagram of what happens when you when you click deploy and I'll show you the next that when you when you run that one-line to deploy a model all this is happening underneath the covers to give you that to give you that API so let's run to that right now to our to our cloud service okay sorry this let me see if I cannot apologies for this maybe Doug did you want to take any questions let me let me try to remove back into my machine all along here yeah [Music] so let me see if I so I you you said that there's a bunch of different architectures like we discussed it which one do I think is the right one in the future well it's a great question I wish I knew the answer we've taken Microsoft ok the the the trade-offs are really I think three threefold one is how complex is the software interface and so for example if you're providing a thousand cores and the schedule has to be static that's one approach some people are taking you're presenting it's like the old vlw titanium world but sort of in a much broader sense it's a very very software intensive interface and those architectures I'm not a fan of those architectures they haven't worked super well in the past but this time might be different and maybe the DN ends which are linear algebra are much more tractable for the software to manage so I'm just kind of waiting to see the the systolic array approach is really a throughput approach right so that's not one where you would you you know that's that's probably actually pretty good for training because you know you can you can just bang through a lot of requests and work on slight is the and then the approach that we've taken is is actually we expose brain way to expose a single threat of control with vector operations so you have you know annotated see that gets compiled down to assembly instructions that are vector operations and then those get taken and broken up into many many many sub operations in the hardware and scheduled on this distributed fabric and run as a dynamic data flow graph so the advantages of that are that you the the programming model is pretty simple the the disadvantage is that you need a pretty sophisticated hierarchy of decoders which we've built and is working so I mean you've seen the results and then one another in advantage you get is software compatibility so if I want to roll out a new generation of FPGA or a new image I don't have to change the program I can just change the microarchitecture under the hood and so I don't think we know how why that can scale but in the brainwave architecture we actually have single instructions that are matrix vector operations that generate over a million math operations per instruction and then those get fanned out to 130,000 parallel units that run for ten cycles so we do a you know 1.3 million operations in ten cycles from a single instruction okay so now I won't say which one is right are you good but I do have my biases and those are the trade-offs as I see them now you know if you can if you if the compiler can schedule all that stuff really well then you can really lower the complexity of that decoder hierarchy that I talked about in hardware okay and maybe will maybe I'll go back to ten take a couple of these questions once we have a ones for once we're through the demo okay cool Thanks it's a great question oh thank you all right cool so just to walk you through a demo in terms of what the experience will be we have our gig github repo and so when you clone the repo basically what you can do is run our QuickStart notebook and you can set up an environment everything is packaged up in a nice condo environment and by just activating creating this and activating this this will install all the dependencies for you in your environment that you need and so next step here as you can see here again just Python tensorflow the image pre-processing this is where you are defining how you want to convert those images so basically we have just a bunch of nodes and the output of one node and the input to the next one will just be tensors so you'll be taking your JPEG images or your PNG images or whatever they are converting them into tensors in the pre-processing step the next step here is where you are loading a quantized version of ResNet 50 so so this super highly optimized version from Doug's team what this means is that this a quantized version of ResNet 50 gives you the same results from CPU and GPU as you get on an FPGA so now you can feature eyes your data on CPUs or GPUs when you're training maybe you want to spin up a GPU cluster and feature eyes all your data so you feature eyes it with this feature Iser the features will be the same as if they were running on FPGA so that when you train your classifier the classifier will be will be a will be performing very well and so this is that quantized version of ResNet 50 and moving forward we're going to be enabling different types of models so you can think about all the various models out there in a snit dense net yellow and inception inception ResNet you know plethora of models but will also enable more models in the model gallery that you'll be able to bring in and then after that you'll be training a classifier and a classifier is essentially the thing that takes these features and then you train that classifier and then we package up all of these into what we call a service definition so you're a pre-processing step the stuff that runs on an FPGA and then the classifier all of this is in a service definition file and and and that's what we take when you deploy this is just your as your subscription subscription ID with your model management account and again the same model management account that you you're using for Azure machine learning the model management account that you're using to deploy models onto CPUs or GPUs same model management account everything is incorporated into this end-to-end data science platform and here's that one one line that I talked about this create service line right here and all that we saw in the previous slide when you click this when you when you run the it's going to take that service definition it's going to register it with model management account it's going to go on to the FPGA BM that model will now be on that VM from a client perspective now that you have this API that's expose you can call it and so here I have a picture of a chip that that J ball might be interested in analyzing so you think about a circuit board and you think about all the various components of a circuit board and you want to determine whether that that passed or failed the inspection so let's click run here and you can see just how fast it came back so this is a 14 milliseconds so there's a little bit of overhead but the time that it takes for this picture to leave my client machine go to the FPGA API get converted from JPEGs to tensors run the two milliseconds of resident 50 those eight billion calculations or resident 50 run through the classifier get the results send the results back to my client machine in about 14 milliseconds do you think about doing something like this on a CPU machine we have a similar model running on a CPU that end-to-end would be about a hundred and fifty milliseconds so 10 times faster than a CPU implementation and so this is just the end to end on how you are able to now easily create and deploy this model and so for those of you who are at the talk yesterday in a Joseph serozha's session we built an app essentially that looked at the performance of a CPU running this model on a CPU and running it on an on a box that has CPUs and FPGAs so in this case you'll see that the the needle kind of barely moves but we're getting about four to six images per second on a CPU so there's a there's an that there's a model we deployed it on a CPU machine it's running in the same region there's our client app here and we're sending images we're using four concurrent threads to be able to send images as many images as we can to the CPU model and here's the media latency and here's the and and the throughput is about six six images per second so let's now start sending images over to the API that's running the model on FPGA and so you can see just with one thread I'm getting medium latency of about six milliseconds so six milliseconds to go end-to-end from image to API and back again with the results and and the reason that the throughput is actually pretty low is because I'm only using one thread to send one image one after the other I can't send images fast enough to this API so let's slam this API and kick it up a notch or eight notches here and and you can check out that kind of throughput that we're seeing so the hit on latency is actually due to again what Doug was talking about those queuing Layton sees but if you had essentially eight you know eight machines that were sending images to this API you would be getting about six to seven millisecond latency and about 530 to 540 images process per second on that FPGA so again this is the type of performance that we're seeing 42 cents an hour 21 cents for a million images right that's that's what you get for one chip and that's that's that's everything from our cloud offering so this is the thing that you'll be able to do when you when you clone that github repo and deploy your own FPGA service this is the type of performance that you'll be able to see so remember that the FPGA is live in the east us data center so your client machines should also live in East us otherwise the network just just sending images across the network to the API which would would be would would add a lot of latency there so now the next thing I want to actually that can I introduce a little mint so that that six to seven that we talked about you know several milliseconds going to the across the network and and back and in the video we talked about our accelerated networking program which we're actually not using but the I mean we've announced that the VM to VM latency when you use accelerated networking and we're going to be turning it on you know is about 25 microseconds so just and actually it's actually quite a bit lower than that but I don't think we're supposed to say that so it's gonna be dropping dropping quite a lot and so you start to see how with this hardware accelerated stack going going in and you can just ring credibly low latency s all the way through yeah okay sorry for absolutely the next thing I want to talk about is AI at the cutting edge and this is the integration with IOT edge maybe you've noticed this up here this this the the box right here here I have a server rack with two servers there's a there's a Dell and a hewlett-packard server there and so basically now what we can have is putting these FPGA chips into these edge devices and all integrated with Azure machine learning and azure IOT edge so let me take a moment to talk about what azure IOT edge is azure IOT edge enables you to compose the same type of pipelines you would have in the cloud and bring it to an edge machine and so why would an edge machine be important so as Doug mentioned earlier maybe you have an oil you know an oil platform out in the middle of the ocean maybe you have a super-secret nuclear research facility and you have data that you don't want to touch the internet maybe you have you know those thousand cameras in your facility that you want to process without having to send all of that data to the cloud so what azure IOT edge enables you to do is to compose various services and to containerize them in docker containers so your stream analytics your Azure functions or your own custom docker containers can be incorporated with the IOT edge runtime and that would just be containerized and then you would be able to bring that down to the edge and now this is running on the edge device you manage everything from the cloud so for example maybe you have 11,000 retail stores so from the IOT hub in the cloud you would manage all 11,000 of these edge devices you would be able to configure and say these are the models I want to run in Northern California these are the models I want to run in the eastern United States you can configure that in the cloud is just a JSON document and then you push all of that to the edge the edge devices will then pull the containers from their respective registries to be able to bring down those containers instantiate them and run it and so this enables you to be able to manage from the cloud in disconnected or non connect or scenarios in which there's very very little connectivity so you're saving on bandwidth you're saving on latency you're saving on cost when you can bring this down to the edge and so what this means now is the same types of integration could work and so let's jump over to a different VM and I know all the VMS are looking the same but I'll show this VM right here and and this is an actual edge machine so let me just pull up the device manager and just prove to you you can see here that we have a catapult FPGA device so right here right here in this machine and one of these edge machines we have FPGA cards in there with an FPGA and this is an actual FPGA in an edge device that you would essentially be able to deploy onto your onto your on-premise locations there and then the same type of app that I can run so I'm going to open up this config file real fast and just show here this is this is the IP of that VM here 10 123 88 134 right so so it's not I'm not I'm not making a call off the box I'm making a call on the directs machine that's happening right here so let me run the demo again and it's the same app except now it's running locally on the machine and let's just start that running again right so same idea so what's happening here is the same thing except now the the images are being sent the process locally and then let's kick it up a little bit more at the number of threads there's still some things that we're trying to work so basically what's happening with IOT edge because everything is container based we're sending images from a container or to that API and getting it back and so the latency ZAR a little higher latency ZAR and throughputs a little different on this edge machine but same same aria 10 fpga same type of architecture and the ability now to be able to process mass amounts of data locally on your edge device so this is something that we are working with an in private preview and so if you're more if you're interested in this type of scenario where you want to put have that fpga on an edge device on prem then feel free to reach out to us and we'll be able to chat with you more about it so for us I mean like for the cluster without 8 and then edge device maybe we should just kick it up a little bit more there's there's some overhead with the containers that's that's the main reason that we have to we have to work through and so really just to summarize everything we've talked about in terms of Azure machine learning Azure machine learning hardware accelerated models powered by project brain wave I'm really good at naming here at Microsoft so you know exactly what you're getting with our names but you essentially the models the you know as I as we had covered easy to create just using Python and tensorflow an easy to deploy into the cloud right once deploy anywhere everything that I ran through in that Python in that Jupiter notebook with that tensorflow code the model that you create can be deployed to the cloud and also deploy to the edge without changing anything so write once deploy anywhere I manage and update your models using Azure ilt edge so from that IOT hub you can basically say oh data scientist created a new optimized model works better I'm going to push it to my edge devices now those edge devices have an updated model that are running on the FPGA and so in terms of next steps we'd love for you to just go check out our github repo to just give it a shot and be able to deploy your own fpga service and really just unleash the power of real-time AI so thank you very much so we have about eight minutes left to take any questions I'll invite Doug back up here and to be able to address any other questions that folks might have yeah okay let's start with it - yeah I got a Mac already the brain wave I really like the product but the question is really about the I travel so for XML although I believe that public cloud is the future right in the next maybe a few years there are still a lot of customers using primary cloud so do you think a dremel to support them in some way if es was the roadmap if not was the blocking easiest yeah in terms of you're saying just a completely private environment that does not touch the climate point a dremel at the private environment yeah so so this is where we might want to talked about a ml net ml net and also our server offerings so our ml is a cloud offering that enables you to connect to experimentation services and to mono management and different types of service that will help you manage all of that but we have a whole suite of on-prem offerings that we'll also be able to be able to run these things without ever having to touch the cloud so those are things that we can talk about Shrikant here's our engineering director for Azure machine learning so I'm sure he has more context to be able to speak to that so you know in terms of working things on premises certainly we have the ml server offering that has high performance algorithms so you can use that also the models you build in the cloud in Azure ml they're all containerized so you can actually take the container and run them on premises and actually you can choose to send the telemetry into the cloud if you want or if you choose not to so those options also exist as Ted said now the azure IOT gateway and the edge devices are another place where you can run these models thank you for the talk for workloads that are bursted in nation in nature how long are we looking at to scale out and add new instances on the fly if I realize I need more I have more demand how how long does it take to increase capacity yeah that's a great question and so this is on the order of seconds so basically what we have is a we have a VM pool we have a VM pool today it's you know it's already flashed with a resident 50 image and it's just a matter of copying your model file to it spinning it up and adding it to that to that load balancer and so it is somewhat of a manual process today we don't have the automated way of being able to detect higher traffic so there will be some manual work that you'll have to do to basically increase that pool but once you say I want to spin up some more then you'll be able to add more fairly easily yeah a great question and and and and to to that point what I also want to say is that from from the experiences that we've had working with some customers in our preview before GPU machines and cpu machines are so expensive that you want to be very very cognizant about you know how many you're actually provisioning at a given time and to be able to try to scale up and scale down as you need that's because they were just so expensive in a way the FPGA machines are so cheap one one customer they just provisioned for their max before their max throughput even though they may be only using this much at a time and they may be using this much on bursty and provisioning all of that was cheaper than doing it on a CPU or GPU machine or GPU clusters so so that so that way they didn't need to worry about it at all it was even cheaper that way so maybe the elephant in the room maybe not I don't know if you addressed brain wave addressing scaling for training to models is that also when is that coming and get my my mic on here so I don't know that if it's an it's an elephant in the room because as your offers the leading edge nvidia gpus today which you can rent a train yeah I mean it was from the perspective of different silicon yes that's right that's right we we haven't announced anything with respect to training at this date so if there's an elf in the room I'm ignoring it they're certainly free to do so but you know so today the answer is you know read NVIDIA GPUs or Intel CPUs in the cloud to do scale out training and when we if and when we have offerings that are that add value for our customers will roll those out well if you had something that was provided good cost value then we would bring that out to our customers so those so the question is what kind of hardware was on that drone that was introduced yeah so we're actually working with a bunch of different hardware partners in terms of when you talk about IOT we actually have a session this afternoon at 4:30 where we're going to be focusing on IOT and Azure machine learning and all the various deployment targets for that so FPGAs and also we just also announced that partnership with Qualcomm and Qualcomm and their chips the 605 to be able to containerize the models and deploy them and accelerate them on the Qualcomm chips and then in the drone itself I think I think they were the GPUs that were running in the drone as far as I know yeah any video GPUs yeah yes PCIe I think it's Doug PCIe yeah yeah PC are you to connect sorry maybe I'd be happy to take this so what were ya there I'm happy to the question is are we planning to take this to lighter-weight IOT edge devices you know with potentially smaller FPGAs and I would say that if you look at the other partnerships that we've announced in the containerized ml really you know asher is one are going to be one of the very small number of global computers with just massive number of cloud hubs talking to you know billions and billions of IOT devices and AI nml or central part of that and so I don't think we want to be tied to one particular technology what we want to do is be able to push the best AI we have to all of the end points with this offering what we're what we're showing is that we can hit really differentiated high end performance and cost in the cloud and on what I what I like to call the heavy edge I don't think this technology is really settled and then when there for other lighter IOT nodes there's going to be a I think a lot of churn and different technologies and whatever ends up being the vector there or the set of when things stabilize the set of technology to get deployed we will serve SML models to those so if we think we can do it with smaller FPGA in partnership with Intel for example that would be a great great offering if it's other technologies you know that may also be good so we're I think will be agnostic what we'll try to do is just make all of that better yes yes yes thank you Rashmi so there's a there's a booth with working working servers with real FPGAs and the servers that are actually working in the IOT edge booth and in the expo hall so so I think thank you watch me for reminding me something to see and so we're running out of time but though the thought I want to leave you with is in terms of the direction that Microsoft is going with all of this you probably saw the mail - two months ago from Satya I think you probably saw it on Twitter sooner than some of us saw just because of that for all the emails to get through through Microsoft one of the main things that came out of that was the the combination of a team and being the AI platform team and being and the cloud AI platform team from asher typically you know microsoft is fairly schizophrenic or maybe even with 27 different types of personalities and what happens is you know the Bing team are dealing with a they're being scale problems and they have a set of tools and things that they're using for them and then we make some other set of tools for our end users and customers and that just didn't make sense and I think we're finally starting to see the light and organizationally combined so that this is one of those things where the the same problems that the Bing team have been trying to solve the tools that they're using the technology that they're using we are enabling and bringing to you and so this is this is one part of that you're going to see that more and more and more and so we're super excited about just all the great things that we can now bring to you as the end customer and as the end user so starting with something like brain wave and ResNet 50 to really help you accelerate and get real-time AI so thank you so much for your time today [Applause] 