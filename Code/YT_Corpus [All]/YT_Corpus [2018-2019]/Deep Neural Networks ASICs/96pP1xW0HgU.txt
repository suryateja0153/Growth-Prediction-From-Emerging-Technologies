 you welcome to cloud on air live webinars from Google Cloud roasting webinars every Tuesday my name is John Barris and I'll be talking about cloud CPUs you can ask questions anytime on the platform when we have Googlers on standby to answer them so today I'll be joined with by Jax can you talk by Zack Stone the product manager at tense flow and cloud tea views from the Google brain team so I want to talk a little bit about machine learning and cloud tea views and new opportunities that are available now due to machine learning and also the central computational challenge related to machine learning and what we're doing about it so I have a couple of examples I'd like to mention first you know digital cameras were created 40 to 45 years ago and then about 20 years ago they became popular even for consumers and if you had a digital camera 20 years ago you might have been taking pictures taking the memory card out of the camera and sticking in your computer and just saving the pictures on your hard drive and if you wanted to find a picture from an event or of a particular person you had to kind of either have good organizational skills of how to keep track of photos on your computer or you could remember when the picture was taking kind of search by date and try to find that photo well things have changed a lot nowadays I keep my pictures on Google photos and if I want to find for instance a picture of the Golden Gate Bridge or the Eiffel Tower I can just type in Golden Gate Bridge your Eiffel Tower into the search bar and because Google photos has used machine learning tools to identify objects in those photos it can call up all the pictures that I've taken to the Golden Gate Bridge or the Eiffel Tower and present them to me so that I can go through them very quickly it's made my life so much easier in fact if I have a friend that I'd like to find a picture of if I've already tagged that friend in one photo I can easily find that friend and many other photos as well so that's super convenient so that's an example of how consumers can save time using machine learning but perhaps you'd be interested in another example that are that is good for enterprises and I'm thinking about automobile manufacturing lines it turns out that when an automobile manufacturing line goes down it costs about twenty-two thousand dollars a minute and lost revenue or in costs and that's up more than a million dollars an hour and so having a machine fail on that manufacturing line is actually quite expensive and so it's much better if the company can identify when the machine might fail and then schedule a maintenance repair to that machine during a time when the line is down already for maintenance and so what a company might do in that case is fit those machines with sensors maybe audio sensors pressure temperature vibration or anything like that and then look at that data and use machine learning and use the computers to identify patterns in that data that might indicate when a machine might be ready for maintenance and by doing that they can schedule the maintenance on the machine instead of having the machine fail at an inopportune time so here's some other examples too for that for instance the Google Arts and Culture app that runs on your phone can use a selfie and identify a picture or a drawing by a famous artist that most closely matches your selfie so our colleagues at eesh did this here and hitch turns out he looked most like Chuck Berry in this drawing by Gilbert Shelton there's another example that's very useful when you're traveling if you have if you're in a foreign country and you can't read the language that's presented on it on a street sign or a menu you can actually use the camera to take a picture of that menu and then Google uses machine learning to identify the text convert that text to machine readable format convert them to a language we understand and insert that language in the text that we can read in the same place on the on the image so that we can read a menu even in a foreign language it's incredibly convenient now here's another example of making life easier is smart reply in inbox who are when a piece of mail comes in instead of having to type with your thumbs the whole response smart reply offers a few responses that might be appropriate for this message and you can just press a button and then that response is inserted into the email then you can add your own comments and send it off it's a nice time saver now it turns out that making all this machine learning work effectively and quickly requires a substantial amount of computation power because of the increasing complexity of machine learn models are in use so for instance image recognition and object detection models that are run for on images often if 50 or 100 or even more layers and billions of neurons that can be activated by the information coming in to the model so in this example you can see the input layer at the bottom perhaps that input layer is taking the data from the machines that have been instrumented in a manufacturing line and neurons are activated based on the input data and then that information activates additional neurons in layers above that until finally the final output layer might only have two settings either needs maintenance soon or doesn't need maintenance soon and so that's an example of kind of how the neurons work in a machine learning model now what we're finding is that current solutions haven't provided enterprises with enough computing power and recent AI advances are almost all attributable to advances in compute our and in fact we surveyed attendees at a recent tensorflow event and found that most of them felt like their compute power was inadequate they needed much more computational power so our mission at Google is to democratize AI make it available both through services and infrastructure making accessible fast and useful for enterprises and developers we have over 10,000 customers taken advantage of our AI solutions we've launched 12 products last year with dozens of features related to a AI and we have over 30 cloudy AI partners and in fact we have over a million Kaggle members that are using AI on our platform so now here to tell you more about cloud T P uses axe stone Zak thanks John hi everyone I'm really excited to talk to you today about cloud GPUs as you can see here this cloud CPU board is the latest offering in Google cloud and it's Google's second generation machine learning accelerator so as a product manager for tensorflow one thing that john mentioned earlier is that most tensorflow users who are working with state-of-the-art deep neural networks are hungry for more compute and tensorflow it's open source it's free it across a huge variety of different platforms from state-of-the-art CPUs to GPUs to cloud CPUs and on to mobile and embedded devices so across all those devices were committed to tensorflow providing fantastic performance today we're focusing on this cloud GPU this new addition to the lineup in google cloud so what's special about this second-generation TPU well first of all on the reference workloads that we've open-source this provides differentiated performance per dollar we think this is a really economical way to get work done especially on some of these incredibly intense computational challenges associated with state of the art machine learning systems each board delivers up to 180 teraflops per device which is a lot if you've ever built a system yourself and by hosting these in cloud you can scale up and scale down easily as your needs change instead of having to put in the capital and expertise and data center cooling and power and so forth to build a cluster like this and manage it yourself right now these cloud TPS architected for tensorflow so you can program them with tensor flows high-level api's instead of having to do bespoke supercomputer programming as you might have in a previous era let me tell you a little bit more about the motivation behind these cloud GPUs first of all it's really important to be able to train and run complex machine learning models faster as John mentioned one of the trends that we've observed over the past several decades really but especially over the past five years is that increasing computational power and scale of these models has led to much higher accuracy and unlocked new applications that just wouldn't have been feasible previously also we find that it's really important here at Google and elsewhere across the world to improve the productivity of machine learning teams when you are training machine learning model it's common to wait for hours or days or even in some cases weeks for the model to converge and be ready to deploy in production that's like waiting weeks for your code to compile and so what we're trying to do both with tensorflow across all platforms and then this cloud CPUs is to reduce that time to hours and perhaps some day to minutes so people can iterate faster and be more octave finally as I mentioned earlier it's really valuable to be able to scale up and down easily with on-demand machine learning infrastructure in the cloud you don't have to fight with provisioning machines configuring drivers you can just connect your virtual machine to a cloud TP you start running open-source tensorflow and train faster than ever before let me say a little bit more about ml teams of ML engineers ml researchers data scientists one thing that you're probably aware is we certainly are here is that the talent to build these state-of-the-art machine learning models is in very high demand right now and very scarce and so it's really important for the people that you have in your organization who have the strongest expertise in this domain to be able to move as quickly as possible and so with these higher performance accelerators and great high level software on top of them you can enable your ml teams to accomplish a lot more than they could otherwise furthermore I'd like to emphasize as I mentioned before that this is now the second in a whole line of machine learning accelerators that Google has developed in-house we had our first generation tensor processing unit that was optimized for inference and used internally this is the second generation tensor processing unit built into a cloud CPU board and even into larger systems that I'll tell you about in a minute so every time you run a search your search results are ranked in conjunction with some of these tensor processing units here at Google when you use Google photos as John mentioned earlier some of the organization of those photos is facilitated with these tensor processing units and and across many other Google products you're interacting with these accelerators every day and now you have the opportunity to incorporate the same accelerators into your own businesses through Google cloud one thing that we're trying to do is to make it easy for people to get started with cloud type views and the way that we're approaching that is by open sourcing reference workloads that are optimized and ready to go on these cloud Tipu's initially we've concentrated on a few categories of reference workloads on visual recognition so image classification these are models like ResNet fifty or inception or for lower resource devices mobile net and squeeze net and this is really useful if you have an object sort of centered in a frame you know where you're looking and you need to tell is this a cat or is it a dog is this you know a vehicle in this closed crop of an image or is it pavement right for an autonomous driving application so we have a lot of models there that are ready to go well optimized and we're continuing to improve their performance further over time secondly we've got reference models in machine translation and language modeling these are built around this state-of-the-art transformer model that's part of the open source tensor two tensor library and transformers been really useful for both machine translation from one language to another also for language modeling where you're trying to predict the next word in a sentence as you might if you're building a messaging app and you want to make it easier for people to input messages to one another but transformer is actually more general and useful for other sequence to sequence problems as well finally related to the the small image visual classification I mentioned earlier we've got a reference model for object detection called retina net so this is useful if you don't have a small crop but instead you're searching for small objects in a large field of view like you might for autonomous driving but also for medical imaging you might need to look for regions of an image that medical practitioner should focus on in a large scan that may be many megapixels and so retina net might be a useful starting point if you're interested in those sorts of applications just to give you a sense of how broad the application domains are even with these building blocks of these reference workloads that I mentioned earlier I'd like to call out a few other applications one that's quite surprising is genetic variant calling so when you're sequencing a genome there's this final step when you're reconstructing the sequence where you need to call variants you need to decide ok the reads are suggesting that there's a there's a base pair modified here is that base pair really modified or not is this is this a real variant or not and so a team here at Google has developed an algorithm called deep variant that recasts this problem that additionaly takes hours or days on a cluster of CPUs as an image recognition problem that you can now run on these state-of-the-art machine learning accelerators whether GPUs or TP use and so we think this this dramatically speeds up the process and we think this will make genetic sequencing faster and a lot more affordable to be used in many new ways as I mentioned before autonomous vehicles are really important domain for visual recognition and object detection and there are many aspects to that problem there's some teams that are considering running machine perception on vehicles detecting pedestrians and street signs and road markings and obstacles and so forth those are all visual perception problems but also visual recognition and processing is valuable for the mapping that often goes on beforehand before the vehicles even go out on the road you need to understand the world in great detail so they have an accurate map of where they can drive and not drive and finally obviously there's a huge amount of opportunity in retail retailers everywhere are experimenting with new kinds of customer experiences checkout experiences in-store experiences shopping experiences online and all of these experiences tend to be highly visual and require detailed understanding of what's present in an image in a scene in a live video stream and so we think these these state-of-the-art cloud GPUs are going to be helpful for rapid prototyping and then deployment of these new retail experiences so I mentioned earlier that these cloud GPUs on the left are the ones that are available today in beta and Google cloud but they're also designed to be connected together into even larger systems like the TPU pod that you can see over on the right now these pods are amazing this has a total of 256 of these TPU chips 64 boards and can deliver up to 11 and a half pedda flops of computing performance for a single machine learning problem and if you want to train a model in half an hour instead of in days and these are coming to cloud later this year we think these cloud TPU pods are going to provide a great solution for Polk's folks who are starting their development now on cloud GPUs with no code changes to then scale up by more than an order of magnitude without any code changes to law as long as you can tolerate larger batch sizes so one thing that we've announced recently is that our own internal experiments have shown that you can take a model that would train in about a day on one of these boards in a certain configuration and scale that all the way down to thirty minutes training on a cloud TPU pod and we're continuing to optimize these systems further so we're looking forward to bringing these to Google cloud so you can enjoy them as well so if you want to get started since a demand is intense we ask that you visit the page here cloud google.com slash TPU and request quota and we're doing our best to process those quota requests as quickly as we can thank you very much we'll be taking questions in just a moment we'll be right back after a minute break thanks again you welcome back we have a few questions we'd like to answer so we'll mention the question then give the answer first of all someone said I've applied for TPU quota during Alpha and again at beta and I haven't heard anything when will I get a chance to try cloudy fuse well I have to say they've been very popular we just ask for your patience you'll get a chance soon but thanks for for your patience and waiting totally again we wish we could give these to everyone right now so we're trying as hard as we can there's a second question here I saw an article indicating the cost of training on GPUs is much more expensive compared to Nvidia volta GPUs so while we won't make direct comparisons today one thing that we'd encourage you when you get access to cloud GPUs is to run benchmarks yourself on your own workloads it's actually really subtle to run a benchmark accurately you need to decide are you processing real data and measuring the full performance of your input pipeline are you running on synthetic data are you testing that the model converges all the way to the right accuracy as we do with our reference workloads are you just looking at the step time performance we're pretty confident that we offer differentiated performance per dollar for our reference workloads and we're already starting to see some of our early users running their own benchmarks and talking about them online so the performance will definitely be different from one workload to another and it's really important for you to run your own experiments to decide what accelerator works for you the important thing to recognize is that Google clouds goal is to offer the best experience for machine learning across all workloads and that means we're hosting the state-of-the-art CPUs state of the art GPUs and state of the art cloud GPUs so we don't want this to be an either-or kind of a thing you should be able to test on all of the state-of-the-art machine learning platforms and on a workload by workload basis figure out which ones are most appropriate for you ok the next question is can I use PI torch cafe or other frameworks for programming GPUs so today we offer tensorflow for program GPUs and that's so that's the only thing that's available right now we've heard people request pie charts and cafe we don't have a solution for you today but I'd say stay stay in in tune you know and if we have anything to offer will let you know but today it's tensorflow only great there's a question here can I do inference on TPS or only training so that's a great question so TP use support training and inference at the moment in bata the best support is we spend a lot of time optimizing training we've also got some early support for batch inference if you have some huge corpus of images let's say that you need to process with a trained model that's something you can do today we are working on more optimizations over time to support low latency inference but we wouldn't recommend that right now in production okay the next question is this is a second generation of TPU as act mentioned earlier will there be a third generation and can you tell us anything about it well we gue Google Cloud has not announced anything about future generations of TPU so we don't have anything to say about that right now right all I can comment is that broadly speaking hardware architecture is interesting again so there's a whole variety of new hardware platforms that are becoming available both here and elsewhere let's see if we have any other questions yeah we don't have any other questions so stay tuned for the next session showcase your data stories online with Google Data studio thanks for hanging in we appreciate it 