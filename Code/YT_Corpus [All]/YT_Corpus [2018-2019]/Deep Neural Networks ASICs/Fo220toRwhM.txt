 so I gave folks a couple of minutes I saw the escalators were broken downstairs which is never exciting for conference conference activities but welcome if you are here for DevOps for AI you're in precisely the correct place and I am Paige Bailey so if you're not interested in learning more about machine learning or data science or best practices for dealing with them dealing with them both then I will not be offended if you flee out the door I promise and hopefully you've been enjoying build so far this is my first one and it's been absolutely delightful so with that let's get started so this time this talk is a variation of one that I've given a couple of times before mostly because of personal frustrations but it's about DevOps for data science or alternately why life sucks if you're a data scientist while your life sucks if you have to work with us and how DevOps can make everything a little bit easier and help us work a little bit better together so to get a feel for the room how many of you are machine learning practitioners or data scientists okay we have some hands how many people are DevOps practitioners okay so about half and half that's really interesting so there hopefully there's their tools and and best practices for both of us I'll be starting with kind of a theory based approach so what it machine learning actually is for folks who might be a little bit fuzzy how machine learning projects are architected and how they're operationalized and then also walk into some of the trips and struggles that most data scientists seem to have because most of us don't really come from a software engineering background or a sysadmin background and we don't really learn all of the software development lifecycle best practices that software engineers and developers have kind of ingrained and they've seen the truth and now it's time for data scientists to see it too so I'm dynamic web page via the Twitter's so if you feel if you feel the yearning to go and tweet things then please please use my handle so part one why data scientists get paid so well or why we would like to get paid so well and this is kind of an overview of artificial intelligence in general so AI gets thrown a lot thrown around a lot now especially in you know developer conferences and in terms of building functionality into products but artificial intelligence by a textbook definition is really just any technique that enables computers to mimic human behavior and if you go by that definition AI has been around for a long time like ever since ever since you are able to explicitly program it felt statements for a computer - so you say hello to the computer and then the computer responds back through the terminal and it you just have this iterative cycle so that's been around for a while and the components of neural networks they've been around for a bit to machine learning though is a little bit different and that's the ability to learn without being explicitly programmed so what does that mean traditionally in programming you're tasked with data so you have data and you define rules on that data or on those inputs and then you get back to some sort of expected output and that means that a lot of the responsibility for software engineering is on the computer programmer right like you are expected to be the old controlling master who knows all the answers and can explicitly incorporate every single step of the process every a little bit of the recipe machine learning takes a completely different approach so you have the data you have what you expect to see as an output and then you give those to the computer and the computer gives you back the rules so instead of you having to explicitly program everything into your software application the computer does it for you which is kind of awesome right and there are a couple of different methodologies for this this is highly simplified there's also another category called semi-supervised learning but the tomb major categories of machine learning are supervised and unsupervised supervised is what you do if you already have tags for your data so you already know the answers you know that this picture is of a cat this picture is of a dog this person defaulted on their loan that's an exoplanet something of that nature and you can either be predicting a category so again cats versus dogs red versus blue Republicans versus Democrats or you can be predicting a numeric value and that would be a regression task and regression just think of it again as predicting how many widgets you're gonna sell in July of next year or it's going to be 88 degrees tomorrow so you're just trying to predict a numeric value as opposed to a discrete categorical value unsupervised learning is less fun because it's harder it's a lot harder there's actually a tweet recently that it's much better to hire an intern to tag data manually for a month and then do a supervised learning task than to spend six months trying to figure out clusters and an unsupervised task and build a terrible model so unsupervised learning you've probably seen it a lot in clustering but that the thing about clustering is that when you get into higher dimensional spaces and think of higher dimensional spaces as just like a spreadsheet with a whole bunch of columns right yeah it's really really difficult to pick out where clusters should be it's easy to spot them in two dimensions it's easy to spot them in three dimensions but when you start getting out into that higher dimensional space it gets it gets a little bit more difficult so for all intents and purposes most of the algorithms that you'll probably see and that you'll probably be working with are either classification or regression problems so that's a lot of words on a screen and not really intuitive if you don't see an example so let's try one right so say your boss comes to you with the CSV file because it's always a CSV file and he says alright we've got our customers dot CSV and you work at a bank and we know some things about them we know their age whether or not they have a job whether or not they own a house their credit rating their education and then whether or not they defaulted on a loan so whether or not they paid us back the money that they owed us and really if you're my boss all you care about is that last column right like did they pay me back my money or not and ideally you would like to be able to find the particular the potential likelihood of a default from this potential new customer so he's 35 he has a job he doesn't own a house he has bad credit he did go to college what is the likelihood that this guy is gonna pay me back my money so you could do something you could set up a decision tree which looks a little bit like this and you have a question or a specific category at each node you answer the question and based on those answers you kind of follow it through and eventually you reach a decision node where you are there grant someone a loan or you don't and if we followed this through for that example I mentioned before our person who's in his 30s who doesn't have a house who is educated should be given a loan because there's a high likelihood based on all of that historic data that we have that that person will pay us back so again examples are great much better to show code so if you go to aka dot M s 42 because 42 is the best number and then also because all the other lower numbers were taken then you should be able to find this ipython notebook so there's a lot of stuff in this repo a lot of it is related to the tensorflow workshop that I gave yesterday but this is a decision tree model implemented in R so who is seen as your notebooks before cool so as your notebooks is an online implementation of Jupiter Jupiter is an interactive programming environment for data scientist so you can type in commands you can run it in a variety of languages and you can get back a response so you can see visualizations you can play with data you can manipulate it quite easily yes so the link is aka MS build 42 excellent and each one of the each one of the cells in this workbook is interactive so if you double click you can automatically edit it and the syntax works the same as any programming as any other programming kernels that you have loaded you can also introduce markdown into some of these cells so they build really nice reports the most often the most often used use case that I had when working as a data scientist for Jupiter notebooks is being able to export them as HTML so you can export them in a variety of formats PDFs HTML whatever but it was great to be able to set up a job to just run the notebooks and all of the code inside and then to generate an output that was then placed in a URL for business reports so if one of managers wanted to see metrics for the last month all they would do is go to that particular URL and the data analysis would be periodically refreshed because all of the connections to the data sources were live so it was a really great way to generate dashboards for people to see that word that were kind of more dynamic than you would expect from a typical business dashboard but the first step we have here we would bring in the CSV file and you can see it automatically displayed with 17 different variables some of them are factors some of them are integers and of on these variables on these columns and our data sets so just think of these as columns in a CSV file that's what we will base our analysis on for that particular machine learning problem so we bring in the values we divide them up into test and training data sets I don't recommend using 90% for training ever but just for the purposes of this demo that's what we're going to do and then actually creating the model itself so we had substantial amounts of code to understand our data to bring it in to get it into a state that we wanted to but to actually build the model itself it's very simplistic it's actually one line of code right so you're saying here train a model on everything except that last column and then use that last column which is or the yeses are nosed for when something defaults or not as the as the data for the predictor so we build it out and then we see our summary of the model itself so checking account balance dependence and this is just a less pretty version of that tree diagram that I showed you before so each one of these branches would be one of the nodes and less than or equal to one greater than one those are the the decision boundary so you just follow it through and you get this summary you get this summary for the model after that's done you can evaluate performance and we can see here that we got most of the most of the data pretty well classified so we had thirteen point nine percent error which means that we're probably a little bit over fit but the but the decision matrix looks pretty good and you can see that our attribute usage it makes kind of intuitive sense as well so if somebody has a checking account balance that's negative they're probably not going to have any money to pay us back right that makes intuitive sense if you wanted to improve model performance you could take a whole bunch of decision trees and add them together and you do that by just introducing this new hyper parameter sorry my screen is apparently diving around lots of ways that it wants to but you introduced this new hyper parameter called trials you run ten trials and it's basically the same the same sort of mentality as a Monte Carlo simulation you add a whole bunch of models together and it improves the performance of all of them collectively more heads are better than one and here you can see that our that our performance boost is a little bit better than what we had before so it runs through all of the models iterations and we get we get a boost performance of I think it's three point four percent yep awesome so this is a very very simplistic example of building a machine learning algorithm and you would usually do it if you were a data scientist and either a Jupiter notebook environment or you would do it in PyCharm potentially or you would do it in our studio if you are more of an are focused developer but for the most part you aren't using Visual Studio you're using a very specialized tool for data science purposes and you're pulling in data from a variety of sources so here it's just a CSV it might also be streaming sources than others so if we go back to our slide show it's problematic in a lot of ways though so it sounds a little bit like magic that you're able to get such high predictive accuracy so being able to adequately say yes these people are going to default on their loans or not with just a thousand samples and just one algorithm and probably less than ten lines of code but it's a little bit trickier than that in actual practice and it's because your data usually looks a little bit more like that right like it's never in a standard format it's usually you have floats mixed in with integers you have you have text values that aren't standardized you have all caps versus lower cases you have tons and tons of missing values and data is never never a fun space to play with it also comes in a variety of sources so you might have XML that you need to parse or JSON or you might have tables in a database and nothing has a standard key so you have to figure out what you should be using as opposed to what's already been given to you and then also what about other features so for the first instance we were given a CSV file but it didn't initially include this account information and that was apparently the best trigger as to whether or not a person defaulted or not so if that's the case then you would have to go to your boss and say I need additional data there's some feature that I think should be incorporated into this model that I don't currently have and I need to find a location for it I need to find a golden source of truth for it and I needed to have a way to incorporate with the rest of my data sets an also model building we tried one iteration of one model with very little hyper parameter tuning and we had great performance at the very beginning but who's to say that you couldn't try something else and get much better accuracy it's never straightforward to build a machine learning model you go through lots and lots of iterations you might want to try branching off of different nodes in the tree you might want to try a support vector machine or you might want to try you know logistic regression or something else so lots and it's very interactive it's never straightforward and it can be effective but it's also best suit for well defined very specific tasks with alphanumeric data where your data is in a great format and where you understand it pretty intimately so machine learning primary takeaway you have to be conscientious of your data it's best for out the numeric data and its best on usually small data sets and machine learning like I said it's been around for a long time like the vanilla machine learning packages that you see for Python and are you would probably have seen very very similar ones in your 90s textbook on data mining or machine learning so that hasn't changed much since University but the new buzzword the new hotness is deep learning right and what is deep learning actually and it's a subset of machine learning so it's using deep neural networks whereas machine learning was using things like I mentioned before you know decision trees support vector machines kind of shallow neural networks and deep learning is learning underlying features and data by using those neural networks so how is it used it's used in a variety of ways mostly with images so very dissimilar to the example that I showed you before you're able to automatically detect any sort of entity within an image so how many of you have played with cognitive services excellent so they're awesome I think they're very cool and they're backed with deep they're backed with deep neural networks so we've had some sort of model that we've trained on top of and we can we've packaged them up into a containerized service and you can ping them just like you can ping them as a REST API and the way that the custom services work for cognitive services is that you're building on top of these deep learning on top of these deep learning models that have already existed that have been trained for weeks and sometimes sometimes months on millions of images and you just get to add a couple of additional shallow layers on top of it but they're incredibly effective and they're really really powerful you can also do specialized people detection right so detecting cific entities in your images you can train it against a corpus of all of your employees and be able to pick them out in video footage right you can even architect a system where you're grabbing you know video footage from surveillance cameras you only pluck out the frames that have OCR done so so you have a model based on the camera and you just kind of wait for people faces to appear and then whenever a people faces appears you send that to Azure to get classified as a potential employee so you can architect really powerful solutions with just very straightforward deep learning algorithms you can also do things like disease detection so here I have an example of spotting a tumor and a lung and this is done by simply just having two directories of images one that's you can call it whatever you want but it's basically cancer and not cancer and you point your algorithm at it and it basically cycles through every single image and determines what constitutes a tumor without having without you having to do really much of anything deep learning removes a lot of the struggles that you have with feature engineering and machine learning because a machine learning you have to decide what's important you either have to have domain-specific knowledge or you have to kind of think outside the box about what could be impacting your problem but for deep learning you throw the data you throw in what it should be and you get back the the answers and the classifiers that you would expect it's also great for text so this is what happens when you throw every single Harry Potter book at a recurrent neural network it gives you back Harry Potter and the portrait of what looks like a large pile of ash which sounds which sounds reasonable right like that that's very similar to most of JK Rowling's Rowling's books but you can also do things like language understanding so we saw great examples with some of our Louis BOTS there have been a lot of advances recently with tensor flow in terms of being able to to understand natural language even with ohms and us and have your neural network respond with those similar speech patterns and then for sound you can do things like create music or you can have a drag and drop situation where you create a melody for folks who are familiar with with kind of music composition you create a melody and then all of a sudden you can click drum or any other instrument and have other other aspects of the song complement the initial melody that you created you can also do things like speech to text or speech to speech and we saw a great demo of that the first day when Harry Shum was speaking in Chinese and his colleague understood everything he was saying even though he was deaf so it's huge advances and really really sort of complicated data transfer format so we've seen images we've seen video we've seen text we've seen sound and there are a whole lot more so autonomous vehicles being able to automatically spot other cars and other potential driving hazards being able to do sign up kind of artistic transcription or translation so you get starry night juxtaposed on top of Mona Lisa and then alphago which is too generative adversarial networks which are too neural networks kind of pitted against each other and finding out the strengths and the weaknesses of both until you get an optimized an optimized neural network that can they can power through and and complete the game so deep learnings become really popular very recently and when I say recently I mean like in the last two years for a number of reasons it's now more accurate than humans so there was this guy named Andre karpati who was a grad student at Stanford and he basically sat in his dorm room and classified hand classified this image set of hundreds of thousands of images so this poor guy was just sitting in his dorm image popped up on his computer that's a cat that's a dog that's a book that's a cup and that's all he did for like a month but that's that's also one of his claims to fame so Andre carpet the-- who is now the head of data science at Tesla he is the human benchmark for computer vision so Andre carpathia was not 85% accurate so we just assumed that humans are 95% accurate at image classification but now neural networks are more accurate than he was at classifying those images we're also better at doing translation so being able to translate between Japanese in English it's now more effective to do that with a deep neural network than with a human even with all of the nuances of language and speech and you know kind of cultural cultural hunting it's still easier to do that with a neural network there's also a great anecdote translate.google.com it used to be a computer program about 500,000 lines of code very statistically focused lots and lots of very specialized heuristics so very specialized rules that dealt with these one-off cases they took that 500 thousand line program and they turned it into five hundred lines of tensor flow and that's what runs it today and it's much more effective we also have much more specialized hardware so FPGAs you heard about on Monday and they're really cool they're very speedy at inferencing and they also allow you to do very very effective training TP use which are tensor processing units are a kind of custom ASIC so actually you're burning an algorithm on to silicon to get the best possible performance TP user mostly specify kind of specialized for tensor flow or tensor flow ish use cases but you can use FPGAs for a variety of other frameworks and a lot of other algorithms and then we also have lots and lots of data so obviously you know we're pulling in sensor data from everywhere we have this wealth of information and data is the fodder for neural networks and for deep learning we also have wonderful open source frameworks and I call out here tensor flow especially because it was it was I think released two and a half years ago we've only had a couple of tensorflow dev summits and it's grown immensely you know tens of thousands of contributors lots of quick starts and tutorials and all with kind of the mentality of let's create this open-source software ecosystem to make deep learning as straightforward and as developer friendly as possible so just like you can code a machine learning algorithm in less than ten lines you can create a deep learning neural network in less than ten lines as well so getting started with artificial intelligence you can either create your own model and you can do that with a variety of things like tensorflow cognitive toolkit scikit-learn and you know hundreds the deep learning community kind of gives javascript to run for its money and how many frameworks we create and thereof and they have not as good names though they I think there was a lawn chair Jas recently and that's kind of awesome but but yes it feels like there's a new deep learning framework every week and you can also deploy somebody else's model and that's just using cognitive services or if you have ml studio creates some sort of some sort of service that you can connect to that's a much more straightforward way but if you wanted to create your own model it would go something like this so you would have a use case this is one we like to use a lot you would have for example people working on an assembly line for circuit boards instead of having people there you might want to be able to do kind of the the detection of outliers the detection of M precisely connected chips with computer vision so say this right like this is automatically this is obviously created the way that it should be it's very securely fastened it's a it's a correctly placed chip this the chip has gone on vacation right it's no longer there how would we go about how would we go about identifying that this chip has been that it isn't it isn't placed where it should be in this system so if this is the case if your boss came to you and said hey software engineer build me an application that automatically detects whether or not a chip has been placed correctly immediately you would start thinking in terms of if-else statements like I could sample the space in the middle of this image I could if I get back orange pixels then I could say you know that's incorrectly installed if I don't see orange pixels then obviously it's okay so we're just gonna go with that sample orange pixels were good to go and that works for that particular use case but what about that one right like that chip is obviously missing as well but there but there are no orange pixels for you to sample and in that case like what would you actually do so and then you would start thinking about how many components there are on a circuit board actually and that you would have to define custom logic for every single one of those components as to whether or not they've been installed correctly and then you would get really sad or you would get really excited because job security but like that is not an ideal situation for a human to kind of go through and determine what constitutes a correctly placed part versus not so there are better ways and there is an example of one that I can show you here so let me dive over to another Jupiter notebook and this is this is a way to solve that problem so to use Python as part of a jupiter notebook to determine passes or fails for specific components on that circuit board if I start the notebook server you can see that it's pinging a Linux machine that we have on Azure I just click through and go shift enter shift enter shift enter shift enter shift enter to run the code it comes through and we look at the first image just to prove I'm not fibbing I'll change it to the that one and then detect as to whether it's been installed correctly or not let's try it with the second one so that second image and it's been installed incorrectly and if you wanted to see kind of what the what the neural network is seeing through each one of its rep one of its layers um it looks something like that so it's picking out edges like low-level features and then mid-level features and then high-level features but none of this is explicitly programmed by you your neural network is the only one that's picking out each and every single aspect of this of this process and then you can monitor your accuracy over time so maybe as you makes my new changes in your data or my new changes in your algorithm it gets better or it gets worse and then once you get a performance that you're happy with so 94% accuracy feels pretty good will package this up in a docker ice container with a schema JSON to define inputs and outputs and a Python script to initialize and run the model and that's in a container that's an artifact that's easy or relatively easy for a software application to connect to and that's a that's an ideal situation for for a data science model right for a predictive model you want it to be packaged up so that's kind of the the overarching data science process that's what happens whenever somebody goes to a data scientist and says hey solve my problem there's the you grab data you start thinking about it you test out some algorithms you train some with some hyper parameters and then you tune it as needed and some popular machine learning packages and scikit-learn for Python carrot for R and some deep learning frameworks tensorflow a max net cognitive toolkit and pi torch are some examples and then Carris is the developer facing front-end or that you should be using if you're using tensor flow tensor flow is excruciating ly painful to write it's like it's got a lot of boilerplate and it looks incredibly unfriendly and then all of those tools that I just mentioned they're automatically installed on our data science and deep learning virtual machines so if you provision one of these assets which you can get started with for free on Azure all of those tools that I just mentioned are already there for you to use and what's more all of the package management is taken care of as well as the driver the driver configuration so if you're using a deep learning virtual machine the Nvidia graphics card is is ready for you to go with tensorflow or MX net or whatever you choose so hopefully I've convinced you that creating your own model takes it takes data it takes experience and it takes a lot of hardware as opposed to just using a cognitive service or using something that you've trained in ml studio so that's part one part two why people who deal with data scientists are superheroes mostly it's because our code looks a lot like that right it's never never well-documented you have a lot of data analysis components which means that there are a lot of feels like unnecessary bracketing happening especially if you're using pandas often they're like double brackets to indicate columns and then you're usually cycling through and kind of doing lots of visualization steps and your jupiter notebooks so you do visualization steps you might get some statistics you you try and you try out a lot of different things and there that's just kind of the reality if you've heard of the obfuscated C competition every year I think that was floating around hacker news a couple of days ago this is this is almost feels kind of similar and the naming conventions look that like that too right so you might have like bad data test or Friday just trying something out or untitled untitled ipython notebook 25 or something like that the naming conventions are just atrocious and also version control is never a thing either usually data scientists are kind of operating in localized environments without using git or subversion or anything similar and that's not obviously not an ideal situation and it's really difficult to convince someone who's never worked in kind of a team environment of why version control would be important Jupiter notebooks are also notoriously bad at version control and it's one of the reasons why I love using data breaks notebooks because they have much better much better version control processes also the languages that your stack is in JavaScript HTML c-sharp Objective C I get whatever Java that's not what we use so it's usually Python are sometimes Julia but mostly Python and R and there's no real there's no real support and other languages for for those machine learning frameworks there there was a new binding situation for tensorflow announced recently called tensorflow j/s which is very interesting it actually it has node bindings which means you can leverage the GPU in your laptop in the browser to do training for deep learning neural networks but for the most part other than ml net which was just announced other than javascript which was or tensorflow JS which was just announced there's not a whole bunch of support on the on the other language side and then also for the tools that you use there's visual studio there's IntelliJ there's whatever on the developer side and then for data scientists it's everything's in Jupiter notebooks or Jupiter lab or data breaks notebooks pycharm spider our studio MATLAB something very visual that makes it very easy to connect to data sources and very simple to bring in the packages that you love to do the data analysis itself and the list goes on right like the not really understanding security best practices for data not really understanding what a test constitutes or why you would need one not remembering to turn off virtual machines I am super guilty of that and then also asking for support on you know maybe machines that aren't standard to your workplace so lots and lots of struggle buses for people who are familiar with SDLC best practices and data scientists often you know they're not even co-located usually late they might be on a different floor they maybe have never sat with the software engineering team and they might not even know what kind of end point would be useful to have for an application usually data scientists outputs are like a CSV file with a bunch of IDs and yes or no or true-false true-false true-false true-false and it's really difficult to explain why that's a problem like you can't build an application on top of the cs be file but try explaining that to people who who are responsible for doing just the data analysis component right so part three DevOps why this is so impactful and why this is so potentially useful to this situation in particular so usually one doesn't simply deploy a model to production rightly so friends don't let friends deploy models without an update strategy I mentioned before that that you know you would go through the process of selecting an algorithm training hyper parameters deploying it as needed but then you would also need to check it periodically make sure that the accuracy is what you're expecting make sure that the assessments that you're making make sense over time if you're predicting how many surface notebooks were sold in downtown Seattle on a Monday it's probably different numbers on builds day than on just some random Monday throughout the year right or different numbers around christmastime then some random than some random time throughout the year and you would need to understand historical data in order to incorporate that in and so Visual Studio Team Services is very very effective at doing all of the steps that I'm about to mention but you can use whatever you'd like obviously you know visual studio team services is very near and dear to all of our hearts and I'll show you an example of it in a second but for the most part whatever tools you'd like to use feel free to so best practices document each step in the data engineering process and the data engineering can again be done with a variety of tools so sequel server often pandas the tidy verse tools if you're working with are but every single script every single bit of data engineering should be well documented you should also ensure that your data scientists are writing tests and these might not look like the tests that you would expect they're often going to be checking the data that comes into a model to make sure it's consistent with the data formatting that you had to create the model itself so making sure that the sampling rate is consistent so if you're sampling every one second and it changes to every three seconds you would need to retrain your model to incorporate that or if the type changes for your piece of data or if you suddenly have five additional categories instead of just two or three you would need to make those you would need to make those allowances in your model and you would need to change change things to reflect that and then also ensure your data scientist you're using version control and again I don't really care what that looks like you can use the automatic version control and VST s you can use get you can use github I don't care but make sure that they're using it and make sure that you can see kind of the history of a models creation over time and then also ensure that your models are portable and scalable so the ideal situation that I mentioned before packaging up a model with a schema dot JSON for inputs and outputs and then some kind of script to initialize and run it in a container that that's ideal and that's what we should all be kind of moving towards there are a lot of tools for that so docker is obviously one choice but there's also you also have the ability to scale up models with kubernetes and using aks and that's that's very powerful as well and you also need to set up cadence for model and data refresh so being able to retrain based on data being able to to update your model based on whatever your business requirements you have you also need to define which performance metrics you want to capture so accuracy might be the best choice for you you also might look at something like an F statistic because accuracy isn't always the best answer if you were if you had a data set where 99% of your values were fine and then just 1% were the values of interests you could create a model that identifies everything is fine and it would still be 99% accurate but that's not really giving you any sort of insight into that 1% that you're interested in so again you can do things like F statistics get area under the curve do accuracy and then also define security and compliance standards for data access so if your people if your data scientists are analyzing data make sure that it's data that they should have access to that was agreed to be used for that specific purpose and that you also have allowances built in for gdpr compliance if that's if that's impactful to you and then also define a schedule for decommissioning and shutting down Hardware so that's kind of the the overview of all of those bullets right so documenting version control writing tests making these portable doing model and data refresh model performance metrics security and compliance and decommissioning and turning off assets and most importantly understanding that all of this is a continuous process so it's all CI C D it's not just a typical waterfall project where you create a model you go about getting all of the artifacts done you deploy it as like this happy model out in the world and then and then you decommission assets that's never the case you're going to have to constantly make sure that the model is kept up-to-date it's like any software application any software application that you work with so data science DevOps best practices they're just DevOps best practices so hopefully all of this kind of made intuitive sense that nothing sounded nothing sounded unfamiliar it's just now there are more things to apply more things to apply DevOps best practices to and if you want to learn more about artificial intelligence if you have that that inkling those are some of the the best examples I can get so and rue-ing's courses on Coursera introduction to deep learning from MIT OpenCourseWare data camp is a fantastic source of modularized machine learning and data analysis there are a lot of textbooks out there and then also Kaggle is kind of an online machine learning competition platform that i highly recommend you go take a look at if you if you'd like to kind of battle your friends with those but yes so those are the recommended resources let's actually take a look at what a machine learning project looks like Envy STS and I there's no Jordyn Edwards out in the audience right I don't see him cool so I was supposed to have a co-presenter today who shall remain nameless but but this is kind of VST s so visual studio team services for folks who haven't seen it before who has seen it before excellent so V STS is kind of an online or you know you can I think you can implement it locally as well but it's a standardized repository for all of the code oh the files that you're working on you can track commits pushes and even builds kind of project management logic into into this solution so it's a great place to set up to set up checks for specific components of your application just set up checks for deployments and then also to set up notifications and logging strategies and everything that I just mentioned before V STS would support so for files being able to to make sure that all of the files that are used for data engineering are incorporated during every single model refresh making sure that everything is documented and then for build and release pipelines being able to add definitions to get started to make sure that if a accuracy isn't quite what you need it to be being able to to monitor that over time and be able to to stop a build of of an application if your models not giving you the performance that you need so that that is kind of a whirlwind overview of what you would need to do as a data scientist or a deep learning practitioner if you'd also like to see what the artifacts themselves kind of look like let me let me pull up a virtual machine really quick and also if anybody has any questions feel free to feel free to ask him I'd be delighted to take them as we're pulling up the as we're pulling up the virtual machine environment how many of you have seen and you'll also find out what I was doing with my time how many of you have seen a deep learning virtual machine before a few okay cool so this this is a Linux virtual machine running on Azure it's pretty performant it has it's enabled with a GPU that that allows me to do kind of more more rigorous deep learning best practices but you can leverage CPU as well for most of the sources and it comes with a variety of files so if you go into the desktop or into kind of your home directory you can see lots of different notebooks have automatically been added for many deep learning frameworks and then also machine learning frameworks so if I go to deep learning tutorials we have things on framework comparisons we have things for LS TM and product detection from images we have let's see there is something I saw just recently yeah so we have six different architectures for neural networks in deep learning virtual machines and all of these are just running in Jupiter notebooks the same as what we saw before I can also use something called tensor board and let me I think I've already launched it so I can pull up and pull up a terminal and Firefox because Linux and it should be this guy so this shows the performance of one of my neural networks over time it's a sort of a metrics dashboard for specific deep learning deep learning architectures so you can see that the the metadata that I'm grabbing is associated with accuracy cross entropy and some other some other performance metrics that I would probably want to log and track and check to make sure that the my model is performing the way that it should you can also see graphs so this is this is kind of a it's difficult to make out but since this is a transfer learning model we're doing a lot of very very specific layer building on top of a much much broader model this is built on top of something called Inception v3 which was trained by Google on I believe 14 million images and it took a week but I was able to retrain a model in just about six minutes on Azure which is which is pretty cool and you can also see distributions of your data and histograms and the histograms I think are some of the most fun for for understanding model performance over time and whether or not you're accurately classifying so ideally I'm not sure if you can see it but when you start when you start with models you you have validation accuracy you have training accuracy which is usually very very high and then you have cross entropy what you want to minimize over time so you want to get it the lowest possible that you can get with each of these steps each of these epochs as I cycle through data sets you can see here it's like step three eight six zero so there were a whole heck of a lot of steps you you get your training accuracies still high but your validation accuracy is also still high and ideally your entropy goes down as you keep retraining and this the final accuracy for this classifier was about was about ninety I think eight percent which is kind of remarkable for what I'm doing perfect there's also like I mentioned before Jupiter notebooks weeka which is kind of a very similar to ml studio it's a drag-and-drop machine learning environment and then also spider which is very similar to MATLAB in configuration it looks it's an interactive programming environment for Python for data scientists in our studio which I love it's it's kind of a one-stop shop for all of our development and you it has a lot of additional extensions to pull in data from a variety of sources including cloud based sources so I think that's all so I'd like to open up for questions and also hopefully this was useful to folks in the audience to kind of get a flavor of what DevOps folks would be looking for for data scientists and what data scientists should be kind of training themselves to give to a DevOps practitioner so thanks [Applause] 