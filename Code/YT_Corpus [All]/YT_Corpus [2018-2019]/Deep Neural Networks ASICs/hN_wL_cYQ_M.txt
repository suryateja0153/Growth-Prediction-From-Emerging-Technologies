 so what I'm going to be talking about is how to train image and text models classification works faster with TP use tensor processing units on Google cloud primarily using ml engine which is our managed infra platform but I'll also show you how you can directly connect to a TP you from your Jupiter notebook and you can run things directly from there as well so what exactly RTP use right so TP users are essentially hardware accelerators right mostly I think all my career I've dealt with software but hardware is this can cool thing right that's out there and it we have been fun basically getting to work with T peers to basically the key thing about TP use is that unlike most processors that we are all familiar with this is not general purpose software when we take up CPUs you think of GPUs these are general purpose you can basically do graphics on a GPU you can do rendering on a GPU you can do now gaming you could Bitcoin mining you get a variety of things on a GPU there's only one thing you can do on a TPU machine Roenick right so it's back it's up its up basic it's an application-specific chip that's meant for one specific task but what that brings you is that it is the fastest Hardware out there for machine learning so there's the benchmark thing that's run by Stanford called at dawn benchmark and basically the TPU reached like they have a desired accuracy of 93% on imagenet how long does it take to get to that and it's basically reaches that in less than 30 minutes ok so what I'll basically what to talk about here is I'll talk about image and text models the two major kinds of models that people train and how you could do this on your own data using our DPS okay so it also finally talked about this final thing about if you wrote a custom model how would you adapt it so that you could run on a TPU so I'll talk about why we do NTP use how to develop with them and then talk about state of the art image models state of the art text bottles and a spoon it said in the intro I really get excited when people can do this without writing any code or writing minimal amounts of code so the two parts about the state of the art models are about how can you take something that is bleeding edge right and basically just go use it and finally I'll talk about custom models and the custom models is for the geeks who basically said no no I want to I want to write my own I want to actually do it myself and we talked about how that as well so first of all ytp use right why do we why why do we go around and develop a custom ASIC chip and we have three generations of it already right but no at least right we're not a chip company why is it that we care and it basically comes down to a few things that are interesting from the AI development point of view the first is this is a graph that basically shows how the size of your data affects error rate okay and this is common to a variety of different problems this particular graph shows natural language but whether you take images you take time series you take text the graph looks very similar the numbers are different but the graph shape looks very similar so what is this this is the error rate and the error rate drops as the size of your data goes up all right fine but let's look at the drop it's linear right the error rate is dropping linearly but what's happening to the data what's the x-axis here exactly exponential right between this grid line and this grid line what's happening to your data doubling right so for every small decrease in error rate when we say we need a lot of data for deep learning what are we talking about we're talking about twice four times 8 times 32 times as much data right we're not talking about 10% more data we're talking little small data right so that's a first graph the second graph here is year-on-year so Alex net which basically jump-started the hole deep learning idea right with eight layers and they basically know how much compute they need that some some level of compute and neural architecture search that is Google's Auto ml that's basically go ahead and searching for appropriate models using reinforcement learning so that's the amount of compute it needs and then alphago which is even higher than that okay so again this is every year and this is the amount of compute that is needed and what's happening to the compute it's going ten times right so pretty much you're on here so if you then put these two graphs together it turns out that we now need about three hundred thousand times more compute that we needed just a few years ago okay so that's the that is the big thing that staring us in the face we need huge amounts of compute to be able to get to better and better accuracy and better accuracy is hugely important right the reason we all speak into our phones now is because the accuracy dropped below a certain threshold until it dropped below that threshold they it wasn't good enough we couldn't actually embed it into products and now we can right and so it's hugely important that we train these models and we train them on our data with our jargon with our industry jargon etc so this is Jeff Dean who's like the lead of AI at cocoa so in 2014 in at the back of the envelope calculation and said if everybody speaks into their phone for three minutes will exhaust all available computing resources on the planet so what do you do you develop a TPU right so basically the other thing to do as well with Moore's Law save us right you're on you're of course right computing power keeps going up and these are all the different revolutions that have happened in computing with a number of cores the different power and so on and long story short it turns out that pretty much everything has started plateauing around 2010 it's a moore's law is not going to save us we need a lot more compute but Moore's law isn't gonna save us right so that's basically why it now is forget about general-purpose processors if you have a need for something that no has to be developed then you go develop it right if it's often enough so in 2015 a year after Jeff deeds back-of-the-envelope calculation we came with TPU version 1 and remember what his calculation was it was basically around if everyone speaks into their phone so what was he worried about was he worried about training or was he worried about inference inference so TPU version 1 was about inference was about basically making sure that our models that were trained could basically handle the load of all of our users basically using it right so prediction the training can take care of it softly thought let's get the inference out so that was TPU version 1 version 2 onwards it does both training and inference so and then the version 3 currently in alpha right it's all all of these that we've developed I mean the one of the things about an ASIC chip is now you've got put in a lot of investment to develop it so there's a lot of incentive for us to basically open it up for other people to use and the way Google opens up for enterprise users through Google Cloud okay so what is the GPU this is basically the schematic of what a TPU looks like ok so you basically have two cores per chip and each core basically contains a matrix multiplication unit that's a heart of it it also contains scalar and vector units I'll talk about those and it contains about 8 gigs of memory I'm talking about a TPU version 2 version 3 has more memory it has 32 gigs but version 2 has 8 gigs of memory per core which is the largest kind of thing seems tiny right why would you build a chip with just 8 gigs of memory but it turns out that it's not about this one core it's about basically how many you can put together right so per chip right you basically have per core you basically have no all of these things and you have about 8 memory course the four chip four of these chips per tip you each chip contains two course so you have total of eight course on it they are on one of these TPU boards but then you take these TPU Birds and you put them together into a rack right and that's what we call a part and a part has 64 TP use and each of those TP uses four chips has to course so that's basically getting the power out there so great so you have 64 TP use per pod and the matrix multiplication is where all of the power comes in the idea is that you go look at machine learning and what is all of machine learning is essentially linear algebra matrix multiplication so you build a system that does matrix multiplication very fast and nothing else right and that's basically the idea of identity to you I just do matrix multiplication really really really fast okay so the idea is to basically pack a lot of power and density come because it doesn't have to do anything else all that does is matrix multiplication and if we can do that the idea is it's a fixed function architecture and the earned brow if you were to think of a CPU as doing scalar but a GPU of doing vector math ITT use doing tensor Matt it's basically and just focusing on matrices and matrix multiplication okay the second part of this and is is that all of those chips they aren't individual they're all connected by a very high speed toroidal mesh and what this essentially means is that they can function as one unit so in either you're not actually treating one core you actually you're treating all of them together so that's great right what that actually means is that as you increase the number of course the number of images that you're processing or the number of texts that we're processing grows linearly this is a very very very hard graph to get because this basically means that we all familiar with it right you distribute your code what happens to your code it doesn't scale linearly right it slows down you're the fastest if you use just one with the TPU the idea that one is a part as long as you're within the part is just as fast so you're basically getting that high speed interconnect so that you're getting the scaling that is pretty linear for the number of course yeah so all of those things we are concerned about and those are optimized but the blick every chip is optimizing it right the key idea here is we're focused on matrix multiplication and we'll make sure that this thing will we'll focus on the networking so that this thing's case scales linearly it's this it's I think it's a Google philosophy right where don't do it on one machine doing on multiple machines scale them out and you get scale through numbers it is that same philosophy of the whole MapReduce ideas and all of those apply to compute I have no idea what the clock speed is I'm a software guy I'm very good server what I could get out of the TP use I don't I don't I can find out if I don't know even if we release them if we do I can find out for you yes oh the Matt's if there's a 64 TP use in a pod okay you can go up to 64 you can also get multiple pods but at that point the linearity starts to break down right as long as you're within a pod you're basically getting this amazing network once you're once you have a second part it's not as much it yeah and for you right okay cool so we have a TPU so this is why we developed the TPU how do you develop with the TPU right so that's basically now on to the software side okay so first thing is we are developing the TPU it's kind of helpful to have a TPU that just attached to your computer how do you attach a TPU to your computer it's sitting there in Google Cloud so what you do is then you basically spin up a virtual machine in Google Cloud and that virtual machine is connected with behind the Google firewall right to TPU so you basically gonna basically have you know your cloud tippy and you're connecting to it and essentially what you do is if your CTP your cloud tpo up so if you're on if you're in cloud shell or you can actually download the CTP you program and you can install it so you can basically do CTP you up give it the name of the TPU and then from your local machine you basically do SSH into the cloud TPU with a port forward basically i'm port forwarding port 8888 to my local machine let's open now when then I start Jupiter with port 888 I'm actually connected to the TP right so anything that I run in that Jupiter notebook is going to be running on the TPU that's that is basically the idea or I can run something on the TP so how does that work so what I did was just before we started the class I basically they talked I went ahead and basically did the CTP you up and I started Jupiter so let me just go ahead and do Jupiter here and at this point I'm doing localhost eight eight eight eight and I now have a Jupiter notebook and let me go ahead and start a terminal and what I'll do inside the terminal is that I will do a git clone of on my repo Google cloud platform training data analyst all right so it's been get cloned and I can go back here and the repo showed up and let me go ahead and pull up its notebook dpu and so this is all on github it's a TPU fundamentals so now I have a notebook so let's look at the so this notebook now when I run it right it's running on this CPU or this VM that's attached to the CPU so most of the stuff is happening on the VM but if I say run this function on the TPU to run that function on the TPU so first thing that I'm doing is that my name of my TPU was locti PU so I'll do this and it was in US central c so if i do that demo gods eight this thing still running place it's too reload it okay there much better okay so let's run that and oh there is no I need to change this it says there's no such TPU so if I run that again so there it is so that what I first thing I did was that I said go ahead and find a DP you with this name that's attached to this machine so it now found that TPU so at this point I have a TPU so let's go ahead and say that we have a function and very very fancy function I just want to add x and y so have a function I have that function and normally I would just add x and y but what I'm going to do here is that I'm gonna say go ahead and call the add function passing in x and y and run that function on the detail so as long as you have a Python function right as long as you have a python function all that you have to do is basically say TPU rewrite here's my function these are the arguments it takes run it for me and that function is gonna run on the TP so that's essentially what programming a TPU looks like is basically writing some function that is basically gonna get done so that is a TPU ad and if you know how tensorflow works that is created the graph and if you want to run it I initialize a TPU and then I say please go ahead and run the TPU ad and we should get the result of adding X&Y right there X is basically numbers from 0 to 15 and I'm adding once to it so I get numbers from 1 to 17 right 1 to 16 X is 0 to 15 so I get 1 to 16 yes this will also run in the Eagle correct you can also do eager mode right now but the eager mode makes it harder because I want to do a TPU rewrite right so so it makes it much easier if you're doing it with this create the graph rewrite the graph and then run the graph yes the parameters have to be tensors correct they have to be tensors and I will also talk about ideally these tensors have are huge and they basically have bad sizes of like hundred twenty-eight but I know I also because that game it's this fixed function that is the optimal behavior on it so as long as you have the does that's essentially it now we can of course take this and put it together into something a bit more interesting so what I'm doing here is that I'm gonna basically get the M nest data or hello world and then basically create two numpy erase off of that so that's basically what I'm doing in this step and I'm downloading the data and having gotten it I have X train and white-white test and I then have a function to do my fit my fit in my train function so what I'm doing is I'm taking the image that I get in and basically flattening it out I have a 28 by 28 image an M nest making that a 784 array and then get take my weights which is a variable taking my bias which is another variable and then doing a very fancy logistic regression model so x times W plus B right that's my model and then my loss so this is a one change that you have to make so my loss is normally my cross-entropy and my optimizer basically optimizes the loss I can use an atom optimizer but you don't use the atom optimizer directly what you do is that we take that atom optimizer and you wrap it up in what is called a cross shard optimizer remember what I said about ATP you basically scales linearly with the number of course the cross shot optimizer is what does that right so it instead of it optimizing either one core it's gonna take this batch of images that's gonna come in it's gonna spread them out on how many other calls you have and each of the course is going to be is going to do this optimization on us on it's part of the batch and then there is a barrier and everything hits the barrier all of the gradients get averaged and then you're on to the next step right so the cross road optimizer is the one change you typically have to make to your code to basically take advantage of this linear scaling so we do this and then we say okay I want to go ahead and optimize it so at this point then we can basically say I want to take my fit bat function and rewrite it TPU rewrite of the fit batch function so my original function would run on a CP or a GPU rewrite it so that it can also run on a TPU and run it for one step actually run for fifty steps range of 50 and then you basically get your loss so that that's pretty much it so you can do iterative development with the TPU attached to your machine and you base almost like now you're writing you writing a jupiter notebook the way you're familiar with running things within with the GPU is it's very similar except that you now have the the ability to scale this thing out to really fast hardware and really fast processing yes which optimizes beside atom is supported almost all of them right there may be some and you can also take your own optimizers and you can wrap them up there's but we're getting disappointed almost all of them intends to floor support there's no there's some list of apps that are not supported but as a first approximation assume that it is right and if it's not like holler yes will this interface shift now this is all this is TF not contributed TPU so yes it is gonna change yes but basically the way it works is everything starts with contrib we get the api's and everything correct and then we fold it then this particular API is not going to change much I think it's just one function rewrite but later on Manish when I talk to you about how to do an estimator which is this higher level thing that is going to change significantly the idea being that out of the out of the box we want our estimators to actually work on a TPU without you having to change your code today because GPUs are new and the hardware is coming up so no there is this process by which you write your estimator code and you have to modify it you have to add in the cross-cut optimizer those kinds of things but we want to get to the point where it's just automatic right you run on it run on a TPU and this graph modification just happens on that device before it happens right does it so that's that's our end goal we want to get there but right now until we get there this is the few changes you make but the part what I'm hoping to say is that it's not this magical thing these changes are very minor you're essentially using crusher optimizer and you're taking this function that you want to write and saying call it with a TPU rewrite that's it yes can we use deep use with Kara's yes right absolutely yes you can use TP use with Kara's and your multifunction can be any tensorflow function including agarose function but you don't use the inputs and outputs I'll talk about those remember that the TPU does only one thing and does one thing well which is the mat so you want to make sure that that math can be written in Charis Charis on tensorflow it works yeah you don't have to convert your Karras model to TF layers no yeah it just works yes no fight not currently yes what about eager execution so that's one of the things that's on the roadmap it's something that we want to be able to support but right now there is this rewriting phase so it doesn't quite work with that but the idea being that you you develop with eager and then you basically in production time you run it on a TPU is probably the more more acceptable way to think about it so and no that's your training and then again we have a predict function and that's just a traditional function and you want to run it on a teepee you do teepee you rewrite of that so again same thing so having done that that will basically do the prediction and you can you can plot your prediction so but I didn't install my plot like never mind okay so that was that was the demo right and that's the link to the notebook that I was running okay so essentially you know follow those steps and basically run run the notebook and you can run it on on Jupiter so all of my demos everything is on github already so you should be able to try them out okay so that was cool so I showed you how to develop 180 tu from scratch you basically have some tensorflow function something and you just want to run it but let's lick step a little bit higher now let's say I want to do a state of the art image mod right well we could write it but why were you writing it there resonate 50s around can we just take a resonate 50 and run it right and so now so there is a variety of reference models available or for a cloud TPU so if you're thinking of image recognition right you basically have your resonate 50 101 200 etcetera there's a variety of different resonant models available ameba Netta that pretty interesting choice what a me Burnett is this is a neural architecture search done on the image net with the constraint that it should train very fast on a TPU right so it is fast it is accurate and it's trained on a very large data set so amoeba net is a pretty good choice if you're saying I want to do image recognition right but resident is the one that more people are familiar with I'll just show you ResNet they all work very similarly very interested in the object detection Retina net if you're interested in running things on our own of phone mobile net or machine translation I'll talk about the transformer model right all of this up transformer based and I'll show you an example the exact time I'll show you an example of of training something on your own problem and the speech recognition image generation they're more of these reference models that are coming up all the time they're all in this github repo ok so if you want to go ahead and do a state-of-the-art model it's all there so let's look at how to take one of those state-of-the-art models and just run them on your own data so the one I'm gonna pick is the resonate 50 right so let's say I only train resident 50 and that's the one that basically was at the top of the dawn benchmark and it's in the reaping the report that I just talked about Dan's referral slash TPU and there's a models of facial resonance so that's the model to run it first thing that you do right how do you run it what I just showed you was basically starting a TPU on a compute engine that is useful for development purposes do you want to DP you attach to your desktop machine so you can basically run things while you're developing but now I want to train resonate 50 on my large data that's no longer development that is now production and for production ization you want something that is fully managed the idea being you submit a job and m/l engine starts up a TPU trains the whole thing shoves the TPU down it's a bandages the entire life cycle without you having to start and stop it yourself so all that you have to do is then you onto the job you basically scales a scale here is a basic dpo and see basically specify the base the base gear you can also do the cuban at this if you are basically interested in having full control over it so you can have a Cuban ettus thing with cube flow and you can have TP use as part of the kubernetes flock I won't talk about that so demo number two right so I want to basically train or deploy ResNet on my own data using a Mellinger so the links here basically take me to a code lab how many of you are familiar with code labs right so a set of instructions that you basically follow that essentially tell you how to do it so the first thing is to basically set up our environment so this is basically telling you to do a cloud shell and then convert so first step is to convert all of our JPEG data so I I will have data that I would want to convert so let's see so okay the data looks like this so I'm going to be training on a data set of flowers and I want to Train it to basically say these are the flower categories in these images the images all look like this and the CSV file that I'm training on looks like this okay everybody can see right so it's essentially the name of the file or the location of the file or cloud storage and the category okay so you basically have a bunch of images each of which has this is the file and this is the category so you have that as a CSV file you have a similar file for evaluation so you have two files one for training one for evaluation and that's all you need okay from this point onwards it just follow these instructions train your model okay so first step is to go open cloud shell and make sure that these things exist so I can go down here I can go to cloud shell and do that and make sure that the the input exists and the next thing is to find the dictionary of labels and what I'm doing here is that I'm basically taking those files and doing a sort and finding a unique of all of the labels to basically make sure that I have the canonical list of labels so do that and so there those are the five types of flowers in my data set so that is now my dictionary and at this point I'm ready to basically train on the TP you now can I provide my JPEG files and my CSV files to my trainer if I were doing it on a GPU that's exactly what I would do right I would write an input function it would read the JPEG it would train it and I would be done but when you're training on a TPU there's something to be very careful about which is that the TPU is extremely extremely fast and it's good at only one thing what's good at matrix multiplication is it good at reading JPEGs No so what do we have to do we're going to do a pre-processing we're going to take our JPEG files or CSV files take all the things and basically put them into TF records right so instead of it reading one file for every input then a bat if you has a processor bat there does a read all these files ins will be really slow right the TP is gonna be sitting there waiting for this input our input pipeline to finish reading the data so instead what we're gonna do that we're gonna take all those files and basically dump them into a single TF record file so that we can read in a huge batch all at once and that way we limit the i/o overhead that's being paid does this make sense it's a first step let's convert it into into TF records so to convert those into T of records right there is in the TPU repo itself so tensorflow github turn through flow TPU so in there in the tools in the datasets there is a lot it's a nice little program called JPEG to TF record so we're just gonna run that okay so it's gonna basically go ahead and take all of our JPEG files and convert them into TF records so this is an Apache beam program how many of you've heard of Apache beam beam ok beam is this is a distributed execution environment ready you basically write your code in Python and you can you can run it on a variety of different runners including flink and including spark but also including cloud dataflow and GCP so what I'm gonna do in the next step of this is that I'm gonna basically take my code and run it ok run this pre-processing code and at this point it's gonna shoot this code off to the cloud it's gonna get distributed onto a bunch of different machines it's gonna process all of this all these files and create tensor flow records and then we can train off the transfer flow records this is going to take a little while because it has to process all this data so I'm gonna do the Julia Child think I'm gonna say I put that at the oven here it is it's already done right so I already have in my cloud storage bucket I hope I have in my cloud storage bucket training the most ml there should be the the pre-processed data is already present so there is there it is flowers and in there okay it's my pre-processed data somewhere anyway it's there so what I can then do is to basically go ahead and do G cloud ml engine ok pointing at the code which essentially comes from the repo right that I just showed you saying that big please go ahead and run this on a teepee you using tensorflow version 1.8 because i was when i tested it on and with these many images so this is the one thing that you have to change figure out how many images you have and pre-process it run it you have a trained model and then once you have your trained model you can deploy it on ml engine you can deploy it anywhere you want and then you can just invoke the model it's just a Python program and you should basically get back the classification it's sending a jpg file so when you're invoking the program you don't need tensorflow records they'll just send in a jpg file you get back the classification so questions so bottom line then state-of-the-art image model pretty much all done right so essentially run a few scripts take your data make sure your data isn't that CSV format write name of the file category and follow the code lab and you should have it done yes we don't actually expand on the JPEG image as a JPEG images in the pencil flow record remains compressed it remains the JPEG bytes data what the only thing that we are doing is we are taking the JPEG bytes data that's in those files and instead of it reading individual files it's gonna read one TF record it's gonna get a thousand of them along with the label right so it's good that's essentially what what we've done we basically minimize the amount of i/o hit that has to have this man yeah it's it's not uncompressing the data yes exactly it is yes absolutely as you pointed out even though you have to do this conversion it is so much faster because you do the conversion on the ones but you train so many epochs read rather than doing the conversion seventy five thousand times because you're doing 75 thousand no whatever you get to do the conversion on the one so pay the penalty once and that and that penalty also you don't have to do you can scale it differently you can run the dataflow job on fifty machines where you may be training it only on two TP use so instead of you doing this thing on 16 cores you now suddenly did it on many more cores because a cheap CPUs are cheap so do it on a lot of sea leaves so I let's flip from state-of-the-art image model so you want to train an image model pretty much they take the cord run it on your data and it's strained right so you basically the choices that you have to make are essentially how big of an image model do you want do you want resonate 50 or resonate 18 or ResNet 120 something right depending on the number of images you can go deeper and deeper right so that's essentially the only thing that you might want to do you might want to hype a parameter qunit and that's fine as well right all of these models support hyper parameter tuning so you can basically say don't just train the model once train it with all these different parameters including the depth of the resonant modeling so let's change a little bit and say how would we do state-of-the-art text models unlike image models with text it really matters right you can't judge an image is a consistent format everything is the same you just basically know you're training you're training an image partly just training an image model good text there is some difference or some things that you have to deal with so basically when you're doing there are text models the modern-day text models essentially consists of four steps they do an embedding where they're basically taking all of those words and converting them into numbers into vectors the second thing they're doing is that they're now encoding these using an LS TM right and then while you're training you're basically shifting the attention right during that sentence to basically focus the the alastair model on a certain length so that it learns to basically expect what's going to happen there and then finally in this combination of this embedding it in coding and then attending and then you have the prediction phase where you're basically predicting the next word of a sequence but you're predicting that in a conditional way and then you're playing a beam search during prediction to choose among those different possibilities so you have to basically write those models and deservedly these have this reputation of being really hard to train and really hard right so it's really really cool that we have a library to answer to tenser which basically provides this best of the breed language model and luckily enough it also runs on a TPU so let's but unlike images it's not just scripting we have to do some work here so let's talk about what kind of work we have to do so let's say we want to basically write a machine learning model to complete our lines of poetry so given this line we would like the model to do this is a model going to do that no obviously not right the model is going to do something but the idea is basically have it create plausible next lines and how do you train a machine learning model to create a plausible next line of poetry what do you train it with real points you trained it with real points so what am I gonna do I'm gonna go look at all of the points that I can find I lay my hands off in Project Gutenberg and basically slurp them all in and my input is gonna consist of one line and my output is gonna consist of the next line that's my label and I'm gonna basically give that to my model and say this is my input this is my output text input text output tray fair enough so in order to do that what we do is that we write a problem and the thing is that I recognize that my input is text and my output is text so what I have is a text to text problem intense returns so I have a text to text problem and I'm calling mine a poetry line problem because I'm predicting a line of poetry and then I basically specify how many words I want this model to remember now I thought it would get lots and lots of times but it turns out people don't write that many poems so all i altom utley ended up with was about sixty thousand lines of poetry so it wasn't a very big data set so I had to kind of limit my vocabulary here right so I did to power 12 words just learn the most common 4,000 words in this poetry database right so that's the size of my vocabulary and then I need to write a generate samples function got given the data directory it's going to basically yield a dictionary that consists of two things the input and the targets it may case the input is a previous line and a target is a current line fair enough so that's essentially what that's code I write basically saying that what my vocabulary like what kind of problem is it text effects problem and that I want to read in something I want to write out something and then I basically they go ahead and generate data for me so this is the game very similar to what I had to do in images I don't want my TPU sitting there basically reading right up 20 line poems everywhere I want to basically create this data in such a way that my input output overhead is minimized and so what I'm doing is I'm basically running this data gem for point yeah this is the problem this is my data directory right so basically go ahead and read it and write tensorflow records for enough and the reason it can do this is because I told it given my data files how do I create my input and my output from a machine learning model really when you think about it that's all it needs to know how do I create my input how they expect the output and then it basically slurps through all of you all of those all of the data that you specified in this directory and creates a tensor flow records once you have the tensor flow records we are ready to train the model but then I realize that hey my model isn't going to be great it's not going to be great because I don't have enough data remember that first graph I showed you how much data do you need for a from scratch model hey typical you know the 10 standard thing by default is set up for English to German from all of this all of the speeches ever given at the European Parliament or something right so humongous humongous data set and I'm gonna give it sixty thousand lines of tree it's not gonna be big so what I want to do is to basically make the model small so what the so this is my hyper parameters I'm basically saying just use two hidden layers just use 128 nodes in them and filter them with five 512 and my attention heads how many words do I look at at any point of time look at only four words at a time and do a lot of drop out what does drop out do it drop out shuts down the neurons right and the basic idea is it limits overfitting the smaller your data set the more dropout you need right so because my data set is so small I'm going to basically increase my dropout and I'm going to slow down my learning rate okay because I don't have great amount of data so use the lower learning rate and that's per half of parameters so other kind of like small tuning that I do to my model and at this pump from point I can just do go ahead and train it okay and the model is a transformer model and I'm having hyper parameters that is transformer poetry because that's the name of my Python function that returns this hyper parameter set so I'll go ahead and use that hyper parameter and run it on ml engine with one GPU or run it on ml engine with four GPUs or eight GPUs or whatever if I wanted to run this on a TPU kind of wasteful because this is a small data it it trains in now a few hours on a GPU but if I said I want I don't want it to train in 20 hours I wanted to train in 12 minutes I would race at least flip that remove the GPU part and add minus - are you sleepy and that's pretty much it now that then what transformer is gonna do underneath is if this flag is set take this thing that I'm doing and give it to TPU rewrite that's all it's doing right underneath it's basically quality - you rewrite with all of the code that's already been written fair enough it you can specify you can specify how many how many cores you want let's see I don't know if it's still running okay but it that that is a cool thing I for a while I had it running poetry let's see never quite I think all ml poetry nothing is still running so that was a blog post and there was a link somewhere in there I don't never shut it down but if it's not okay okay so give me a line you I'm sorry when we meet again we trim it again okay so again it's not going to give you the exact same no for him you want it to be a different point so this is a no App Engine thing so it's gonna no one's run it for a while but it should come back in a couple of seconds let's see what it does a water by a strained sweet soul poetic at least let's do this let's take that line and make it do the next line to it we asked each a song uh that is terrible poetry okay let somebody give me another life the woods are lovely dark and deep some Robert Frost efficient arrow here Oh yet long yet long not died and views on how long delays that sound pretty good yes can can it look at a picture and do a poem absolutely that would be an image to text model and you can train it as long as you have enough of a data set of images and images and life I'm sorry have I done it no I have it I don't again it was it was easy enough for me to think of Project Gutenberg and going and getting lines of poems and training it so as before this thing is this no I don't want to lose track of the fact yeah it's a fun thing but this notebook is also available on github so you can go look at it you can try it out try running it you can run it on a GPU run it on a TPU right and all of the code is there go ahead and please do try it out so last bit custom models okay so we now know how to create an image model yep just take an image model take it take a text module we know where to go find it and how to run it the last video I want to talk about is how but if you wrote your own custom models and I'm gonna assume that you wrote it using estimator if you'd ordered with Kerris you would basically do a Karass after the model you do Kara's model to estimator and you will get an estimate route so it's one function call on your Kara's model after you give you model that compiled and then you do model dot to estimator and you have an estimator so the idea is that now you want to basically run this on a TPU so what do you have to do you have to enable nerve replication how do we enable replication the cross run optimizer business right we also want to avoid post latency how to be a white host less latency converting to T of Records put them all in one place you want to do static shapes now that's something we didn't quite talk about we said we had a matrix multiplication unit what does it work with it works with matrices of a certain size if you want to go multiply a three by three matrix it's kind of wasteful right so what would what were you gonna do is that we're going to basically bring in a huge matrix to multiply and that matrix is a systolic array it cannot be this variable shape to be very aesthetic shaped so what we'll do is a normally intensive flow programs we read bad sizes with a question mark in the shape we won't do that in a teepee you program we will have static fixed shapes normally it's not a problem but if you have things like language models for example you might be padding each batch differently based on the longest sentence in the batch that's not going to work you have to pad it to the longest sentence in the corpus okay so yes this restriction is because of the D because the fact that the speed-up happens because of the mxu the matrix multiplication right and the matrix multiplication it's a systolic array it requires a static shape so that it can optimize the speed so you would have to do padding of the dancers so how do you do this well what you do right is you basically use the TF data API to read your data and when you're doing a TF data rate API during training you repeat forever so that you never have any left over badge batches all the batches are of a specific size and then you read TF records we talked about that right you read T of Records don't read anything else and you basically go ahead and do a you know parallel interleaf so you basically read them so that every worker gets its own copy of the data okay and when you do your pre-processing etc you do them in a parallel punch so you call the prefetch and then you make sure that you always have static sizes so if you have an evaluation data set that is 315 and you basically want to make it divisible by 8 you dropped at 3 images or whatever right so you do that and then you make sure that the first dimension is your bad size okay so you transpose you input if necessary ah right and then you basically make anything that has a question mark in the tensor flow shape you actually give it the actual so this is basically what this does it basically sets the shapes anything that's a partial unknown you set the shape and then you basically create a TPU estimator instead of a regular estimator spec again I'm running through this but just we've tried to make the deck available so you wouldn't get that okay and then what you saw in May Jupiter notebook you find the TPU with a cluster resolver and having found the TPU then you basically go ahead and you train and evaluate the way you normally do your train and evaluate okay with this thing there's this idea that there are some functions that you execute on the TPU and there are some functions that you execute not on the TPU in general all your input and output is not on the TPU everything that is on the TPU is all the matrix lat so in with most machine learning models the model function is on the TPU yes yes there is a TPU profiler so when you run it it will tell you what fraction of of the functions that are running on the TPU are optimized for the TPU and what function what percent aren't optimized we try to shoot for about 70 percent it's it's really hard to get to 100 percent when you try to shoot for about 70 percent and the more you move all of the pre-processing all that stuff into the input function and you leave your model function pristine the higher that fraction is going to get and finally you save the model and the cool thing is I want to save the model it's just a normal model and you can serve it either from a CPU or from a GPU or from a TPU ok so you can export the model and you export the model it exports it both in a TPU serving format and in a GPU serve requirement so you can serve from it either one so all of these steps that I just rushed through there no explained in that blog post and there is a github repo as well so that demonstrates about that CODIS with that I just want to only do a plug we have this promotion going on we have a bunch of training courses so on Coursera if you go to Coursera that arc slash next extend it don't tell anybody that next is over just go there next extend it and you will be able to basically get a free I think one month free ok so you can take any of our Google cloud courses for a month for free with that one with that link and cool talk what courses there's a bunch of courses that you can take any of them for a month for free so that's the link Coursera coursera.org slash next extended [Applause] you [Music] 