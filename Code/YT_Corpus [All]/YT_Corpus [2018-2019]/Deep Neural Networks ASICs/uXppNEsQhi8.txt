 - [Announcer] Ladies and gentlemen, please welcome former dean of the San Jose State College of Science and currently special advisor to the provost, Michael Parrish. - Good evening, and thanks for coming to the third in our series of Industrial Revolution 4 talks. Tonight's subject is artificial intelligence and convolutional neural networks. If there's any area in this series that really emphasizes the necessity and importance of interdisciplinary cooperation, it's this one because in order to really understand and develop both neural networks and true artificial intelligence, you'd have collaboration between people from areas like neurobiology, psychology, obviously, network design, programming, and it's something that we really emphasize at San Jose State and something that I think that we truly value in our collaboration with these groups. I would like acknowledge our sponsors tonight. We have ESDA and Team Hogan. And I'm very thankful for them and their involvement. What we're gonna do next is, I'm going to introduce Sean O'Kane, who is going to, in turn, introduce the panel of experts. This is an area that's rapidly evolving in Silicon Valley and we have really an outstanding panel of people who have the expertise in this field. Without any further ado, I will hand it over to Sean. (applause) - Thank you, thank you, Michael. Welcome, everyone. How's everyone doing? Good to see you. Good, what a great crowd for a Monday. Hmm. My name is Sean O'Kane, I'm with Cadence Design Systems. I'm the marketing director there, and also I'm the president of Big Kahuna Productions, it's a video production group that works with many of the high-tech companies in the valley here. So, once again, welcome to Jim Hogan's Fourth Industrial Revolution: The Cognitive Era speaking series. Tonight's panel discussion is on artificial intelligence and convolutional neural networks. So this series helps unlock really exciting possibilities and change the way we make decisions and interact with people to solve our biggest challenges. So tonight the panel is a whole lot smarter than I am. My job is to keep it very simple, keep it at a thousand-foot view. They're gonna take a technical deep dive and give you more of an understanding. I'd like to give you more of an understanding how AI and neural networks can be applied in our life. The capabilities generally classified as artificial intelligence includes successfully understanding human speech, autonomous cars, interpreting complex data, including images and video. So the term artificial intelligence is applied when a machine mimics cognitive functions that humans associate with other humans, such as learning and problem solving. Now, if you think about convolutional neural networks, it's like using multiple copies of a same neuron in different places. It's like writing a function once and using it multiple times in programming. So the network is better able to model data when it learns how to do something once and then uses that in multiple places. So we're gonna talk about that as well. So here's some examples that I'd just like to share with you. In some of the emerging areas, putting substance behind some of those billion-dollar projections. So the first one is something near and dear to me and to my heart is advanced melanoma screening and detection. So my early detection was my wife when she saw something, spotted something on my chest and she goes, "That doesn't look right." Timing is everything, and now I have a foot scar from here to here, and I get checked every six months. So she was my early detection and I was pretty darn lucky. So researchers at the University of Michigan are putting advanced imaging recognition to work detecting melanoma, one of the most aggressive types of cancer that is treatable at the early stages. So today high-resolution imaging is so sophisticated that we're relying on it for recognition for security systems, traffic recognition for our vehicles, and autonomous, or the autonomous vehicle of the future, and there's a little pitch here using a 10-silica vision processor, which Chris Rowen might mention or talk a little bit about during the panel discussion. So this is a great opportunity to use neural networks, neural network techniques to enhance computer vision, applications with a very high level of accuracy. So neural networks, I'm sticking with healthcare. Neural networks for brain cancer detection. A team of French researchers note that spotting invasive brain cancer cells during surgery is very, very difficult in part because of the effects of lagging in the operating room, which is interesting. So they found that using neural networks in conjunction with biomedical optics during operations allows them to detect the cancerous cells earlier and easier and reduce residual cancer post-surgery. Pretty good. Here's another example, and this will prime you for thinking about some of the questions you might want to ask, how that may apply to your life or career when we bring the panelists up and we'll have a Q and A. Energy market price forecasting using neural networks. So researchers in Spain and Portugal have applied artificial neural networks to the energy grid as an effort to predict price and usage fluctuations. So a lot of different examples from weather forecasting to disaster event detection, civil and mechanical engineering to sociology, psychology, and the humanities. Many different disciplines this touches. So AI and neural networks are paving the way to solve our biggest challenges through innovation saving time, money, and lives. So this is not a comprehensive list, but we will talk about many, many different areas, and many different disciplines have been added just this past year, and we're just at the beginning of a mainstream applications for deep learning. So once again, I really thank you for joining us tonight. Right now we'd like to welcome to the stage our host. Mr. Jim Hogan has held the senior engineering and marketing and operational management positions at Cadence Design Systems National Semiconductor and Phillips Semiconductor. Serves on the board of advisors at San Jose State's School of Engineering and is currently the managing partner of Vista Ventures, Mr. Jim Hogan right here. - Thanks, Sean. - This gentleman is a Silicon Valley entrepreneur and technologist. We'd like to welcome Mr. Chris Rowan. (applause) He's the co-founder and CEO of Sonics Incorporated, Mr. Drew Wingard. (applause) He's a registered patent attorney experienced with mobile hardware and software architecture. Mr. James Gambale Jr., right here. And easily, our last panelist is the president and CEO of One Spin Solutions. Please welcome Raik Brinkmann (applause) Thank you, Jim. - Well, thanks for coming, everybody. I appreciate you being here tonight. So this panel grew out of a panel we actually did back in the middle of summer in Austin, Texas at a design automation conference. We wanted to expose that audience to ideas around AI and machine learning, convoluted networks, and it surprised us, right? That we got through the presentation, did the questions, and people just wouldn't leave. It was like 90 minutes. So I know better. No, people didn't come to see me, that's for sure. They came because they have an interest, a curiosity about the subject matter. And the subject matter is hard to get your head around because there's a lot of noise going on, right? So, it was back then that we actually conceived, that was the beginning of this Industrial Revolution 4.0 series that we've been running for the last few months here at San Jose State. Thank you to the university for letting us do it. So tonight what I'd like to do is not try to solve every question in the universe, but at least expose you to these gentlemen, to get an idea of what practitioners in the art are doing now. Maybe get your interest, you know, I'm sure there will be a lot of questions. A lot of you want to talk after the pitch. I'd be happy to do that as well. So I like to move on and we'll go from Chris, to Drew, to James, to Raik. And there's an order that's implied in that because Chris has been a practitioner for a long time, he's got his own venture firms, got his own company that he started recently that he may talk about. Also, if you look at the history of (mumbling), one of the first solutions out there that was dealing with a lot of the image technology. So, without further ado, Mr. Rowen, would you please start. - All right, thank you very much. So I'm gonna take a perspective here of thinking about some of the broad impacts of this class of technology on key applications and even on the structure of the technology that we're creating. Drew, I think, is gonna do a little bit more of an interesting exploration down into the underlying technology. So our two talks in particular are sort of a pair, and hopefully, together we'll give you something about how it works and something about what its technical and business impact will be. So a place that I think is really interesting to start is to think about cameras, to think about image sensors. And in fact, we know that the rate of shipment of new image sensors is really spectacular, and we can therefore, assuming, say, a three-year lifetime for an image sensor for a camera in the field, we can look at the population of cameras versus the population of people. Now, you might say, well, what does that matter? Well it matters because the old, conventional view of what cameras were for is that they were to capture pictures that people looked at. So if you get a lot more cameras than you have people, it begs the question, who's looking at the pictures? What are those pictures for? And we see that we've just hit a magic crossover point where we really now have significantly more cameras than people. So if all our cameras were on and all of our people were looking at the output of those cameras 24 hours a day, seven days a week, 365 days a year, they could not keep up. So we really have to think about, what happens when we have to do something to filter all that down to find the useful stuff? And that's really an important theme here. It's also worth noting that of all data that we capture from the natural world, virtually, all of it is pixels. Motion sensors are important, microphones are important, humidity and temperature sensors are important, but all of them are very low data rate, or very low volume compared to image sensors. And so 99% of all new data that we're capturing is pixels. And so it has a profound effect on what is the shape of computing, what is the shape of storage, what is the shape of networks. And just to give you a little bit of a challenge here, if you take the assumption that these are high-resolution cameras and that they're on all the time, that means that you're capturing something like 10 to the 19th pixels per second, and that's far more than any network we can build or any storage medium we can build. Another way to look at it is to say, okay, well, what does that mean economically? And I recently looked on Amazon for the cheapest camera. The cheapest complete camera system I could buy, which turns out to be $11.99 for this nice, little, high-resolution security camera with power supply, and electronics, and network interconnection, and even infrared lighting. - [Jim] And free shipping? - Free shipping with Prime. And it sort of allows you to start thinking about, well if that's what the camera costs, what does everything else have to cost to make that make sense? And I think what we find is that we are going to see not just what we do with images, but because images are so important, everything about the cost structure, the technical structure of networks, and computing, and storage will be shaped more than any other single force by the needs of cameras. And one thing at the very heart of what we're gonna do with it is apply these more and more intelligent algorithms in order to interpret the data on behalf of the humans because either we need to get the cameras to be smarter or we have to make a lot more babies. So we can ask the question, where does computing happen? And we have, in fact, lots of choices. If we think about some set of monitoring cameras out in the real world, we could say, oh, well, we're gonna capture those images and we're going to do the processing, possibly some sort of neural network that will find interesting events taking place right in the camera. Or we could have that computing taking place somewhere upstream in a local network. It may be happening across the wireless and fiber and DSL channels in some cloud edge serving? Or it maybe happening all the way up in the cloud. And there are very important trade-offs taking place between these two ends of the spectrum, because the cloud is very flexible. You have infinite amounts of computing capacity theoretically available, you have lots of storage, it's a very convenient place to merge together the data from many different sources. But it's actually very expensive, and if you add up the costs of storing there, transporting all those pixels up there, it's prohibitively expensive to consider taking all the pixels and putting them in the cloud, at least from all these $12 cameras. And so, in fact, what we're gonna find is, we have to make some very smart choices, and these are not just technical choices, not just economic choices, but these are social choices as well as to what happens where. So the question of system responses. Generally, making the decisions close to the camera is a good thing. If you're driving your car, you do not want to be waiting for AT&T to decide that you can stop. The scope of data analysis on the other hand gets much better as you move towards the cloud. Protection of privacy is generally best if you don't let most of the pixels out, if you just get the necessary information only being shared up into the cloud. And the costs are dramatically affected, that when you look at what the cost of a unit of computing is if you do it in a specialized system on chip device close to the camera, versus doing it on some GPU or CPU in the cloud, you're talking about a couple of orders of magnitude of cost difference and power difference and carbon footprint difference and everything else. So one of the things that happens is that this move toward neural networks has made everybody in the hardware business extremely excited. It really is a fundamental technical discontinuity in computer architecture because we've taken a whole wide variety of different kinds of algorithms that need a very wide variety of different computing models and replaced them with a computing model that basically says, if you can do a lot of multiples and adds in parallel, you win. And so a whole new class of computer architectures is emerging. And in this little graphic here, I look at many of the choices plotted on a horizontal axis, which is computing capacity per chip, and a vertical axis, which is energy efficiency, both of them representing in billions of multiplies per second, or in the vertical axis, billions of multiplies per second per watt. And what you find is, if you track out what's happening, everybody is moving up and to the right. But we see the data center GPUs from NVIDIA, who have been extremely successfully and extremely dedicated to this segment over the last few years. You have new kinds of architectures from new players like Google, who really understand this problem well, introducing new platforms. And then you have the evolution of highly efficient architectures like vision DSPs to be even more efficient in this specialized class so that the state-of-the-art solutions are dancing around a trillion multiplies per watt, and that's more than two orders of magnitude. More efficient than what we have of our general purpose, X86 processors. And that two orders of magnitude is a pretty big deal, and it's really gonna change the landscape. And we're gonna see this kind of computing perhaps as special purpose subsystems entering into almost every kind of computing platform, from small chips down in security cameras up through the largest cloud servers, and certainly occupying your cellphone in a significant way. There's even people working on some really interesting low-precision analog methods to do the same thing. And so my particular interest over the last couple of years has been on startups, both because I've done startups, but because I think it is one of the fundamental engines of innovation, not just in technology, but in business models and use models and impact on society. And there are a lot of AI startups. In fact, it's a basic intelligence test for startups. If you have any way that you could possibly call your company an AI startup, you should, and everybody did regardless of what they're actually doing, other than they probably are processing a bunch of data in some way. And so there's a lot of sorting out to do because if, the best working definition of an AI startup is founded since 2014. Bu within that-- - [Jim] Close, 2015. - But within that, you find there actually is five or 10% of them that are doing something serious within neural networks. And we can break those down in lots of different ways and it becomes a very interesting way to understand where innovation is really taking place. And so some really basic statistics here. Two-thirds of these startups are in the cloud. One-third doing embedded systems. Half of them are doing vision. Only a small fraction are doing new chips, but the 15 or 20 companies doing new chips around the world, and a lot of them actually here in, of all places, Silicon Valley represent a big step function in terms of chip startup activity compared to what we've seen if you look back just three or four years. So that's kind of encouraging. There's also really interesting to look at it geographically. This is an area, for example, where China is quite active but by no means dominant. There are, by my measures, significantly more deep learning startups in Israel than in China today. There are more in the UK than in China today. And there are far more in the US. So that this isn't one of those things that's sort of fulfilling some of the worries that we've long had. There's lots of good work happening in China, but also in a lot of places around the world. And so it is a truly global phenomenon, and it's one where this kind of technology disruption really is putting us in a position to do, who expect a lot of deep innovation. - Thanks, Chris. Yeah, to give you an idea, you know, as Chris was saying, there's a lot of startups going right now, we'll get to this in the question period. So many have tried and put it on. What, 5% or something like that, or actually probably need to test whether they actually do it. So in terms of innovation, at least from my perspective, those are the ones I want to try to find and foster, and we'll talk a little bit about more in the question-answer. So Drew, I'd love to hear your thoughts, my young friend. - Well thank you very much. I am, as Chris mentioned,, gonna try to dovetail on his presentation a bit and take it a little bit more into the technology only because I think it's interesting, and secondly, I think maybe it will help you better understand some of the trade-offs that are going on out there. So there's a pretty famous diagram, that if you start to get into this space, you will no doubt see it was published by someone as the Asimov Institute, that tries to summarize all the different commonly described topologies for neural networks. And it turns out there's a really large number of them. I won't go into this. I don't understand them all. I'm not even sure I understand what all their colored dots mean. But what I think you can see pretty quickly is, these all have a relatively similar shape. You see these circles and they're connected by these lines, right? There's a network effect that's built into them. There's a couple really interesting things about this. One of them is, for a lot of these topologies, we know much more about how they actually work than why they work. And so when you hear terms like data scientist being thrown around that has a lot to do with the people who have the intuition that's say, if I'm trying to solve a given problem with a neural network, which one of these topologies should I pick? And please understand, a large number of variations in exactly how you apply one of these. The number of layers between the yellow and the purple and the green and, can vary quite a lot based upon the dataset. So the one up here that I'm highlighting is the one that we call the convolutional neural network, which is kind of one of the focuses of the panel today, it's largely because that one has been shown to do some very interesting things, especially with image data like what Chris just talked about. So if we take a look at that one in a little more depth, what we found out is there is a lot of math in making one of these work, and that's kind of interesting because people who've got a background in computers are used to trying to balance things like decision. So the if statement in most of our programming languages and things and these don't really look that way. It really is a lot more math. So if we take a look at one of those circles here, that represents the neurons because we are talking about cognitive computing is kind of biology-inspired, and so we're trying to mimic, in some way, some fashion, what we think goes on in our brains. And really the mathematical operation of habits inside of those is fundamentally a weighted sum? You take a look at the layer before you and you take the values that are output from that layer, you multiply them by a set of weights, and each one of the weights is specific to each one of the arcs. You come up with a sum, that's the closest thing we get to an equation here, probably the closest thing we get to an equation the entire series of lectures, which I think is a good thing. - [Jim] It's a great? - Exactly, the big sigma? - [Jim] It's a fraternity? - And then depending upon how you're doing this, you take that sum and you come up with an output value, and that's the activation function, and then people argue to have lots of different ideas about how you do it, but typically they end up being nonlinear. So as we look the ones that we use for things like image processing like Chris talked about, they're much deeper than this, and the order of each of the layers is much larger. They end up being not just, those look like one-dimensional layers, each one of the columns there, they end up being multidimensional layers. And so the total number of nodes it takes to get one output is very large, and if you think about each one of the inputs ideally being connected to each one of the guys in the prior layer, you find that the total number of weights is massive. And so if you think about that mathematical operation, the weighted sum, it is a set of multiplies followed by set of ads. That's this famous operation that is how we've characterized digital signal processors for many years that the multiply accumulated? And when Chris talks about trillions of multiply accumulates per watt, that's what we need. These networks can absorb as many of those as you can throw at it. Now there's a bunch of optimizations of people trying to apply because the energy and the amount of hardware it takes to process one of these networks can be really large very quickly. So one of the things they play with is, how precise is the arithmetic. We started off using general-purpose computers, and we used floating point numbers with a large number of bits of precision. Then people started doing analysis and say they could use much, much less. There's actually plenty of research work that try to use one bit values. We also recognize that by the time you're done going through the process that's called training of the network, a lot of times, a lot of the arcs have a very, very small weight on them. You can replace that by multiplying with zero, and I know how to multiply by zero, it's a really easy thing. And so you can build structures and take advantage of the fact that the networks, while, ideally they're fully connected, they don't necessarily need to be and so they can be sparse. Then as Chris mentioned, the sum of products function is relatively expensive in the digital domain, but if your precision doesn't have to be very high, it can be implemented much less expensively in the analog domain it turns out. So there are plenty of people that are looking at interesting approaches there. - Well, and the currency is power, for example. - That's right. If you're trying to get a certain amount of work done on a certain budget, sometimes the analog domain can be the right way to go. So through all that people have tried a whole lot of different ways to go. Maybe the most famous early work here was the chips that IBM built, including the very famous True North design, which where they were really trying to more closely mimic the behavior of the brain. And they built a chip with our governments funding that implemented a million neurons and 256-million synapses connections between neurons. The most commonly deployed technology in this space right now are of general purpose or graphics processing units like the one that NVIDIA just came out with. I think their most recent one is the Volt V100, which has a set of dedicated cores that now add it in to make this type of multiply accumulate more efficient. And this one is quite good at both the step of actually coming up with answers to neural network problems, which we call inference, or the steps to try to decide exactly what weights to put in our neural network, which we call inference. - Training. - Training, thank you. Good Lord. Chris also mentioned this interesting design that Google did. The first design they did, which I think they talked about last year. They called a tensor processing unit, or TPU. It is really a machine that is really, really, really focused on doing matrix multiplies. Most of the area on the chip is a big parallel multiply accumulate array. They can do 64,000 multiply accumulates per second, which is a pretty darn big number. But because of some of the limitations in the design, it was much more focused on inference and wasn't very good at training. And another design I know a bit about is a startup company in Campbell called Wave, who took a data flow processor architecture that they had and adapted it to this space. So they've got a design that has 16,000 eight-bit processing elements all connected in a hierarchical network approach that they've applied with a massive amount of memory bandwidth. If you look at these four designs, they could not be more different. And what that says to me is it's a very rich space right now for the exploration of architectures. But there's a bunch of implementation limitations. So first of all, as Chris notes, the type of system you might want to build, and therefore, the type of chips you might want to apply may depend upon where in the network you're trying to do the computing. Another big question is, what types of services would you want to provide with your neural network. Right now, inference is everything. Being able to process those mages right now with a network that somebody else trained is the way it needs to go. But as we look at the expectations that people have for where things are going to go, it appears that there are a number of interesting applications where being able to update our weights in realtime based upon what we see at this specific node becomes much more interesting, and so it looks like architectures that support some amount of continuing updating would be more important. These weights end up being a big barrier. So we have problems with where we keep the weights. We keep them on chip, then we can't have very many of them because we run out of storage space. If we try to keep them all off chip, well then we run to energy problems and just fundamental bandwidth problems of getting them onto and off the chip on time. And then so what some people do is apply techniques that we use in general-purpose software all the time, like you take a series of computations and you unroll the loops so that you can batch things up. That's a good way of producing the amount of bandwidth you need for weights, but that increase the overall latency. And then there's this overall question in this space of how do you manage the overall communications. The communications that you need are dependent upon structure of the network you're building, and if people keep coming up with clever new ways of doing this, data scientists keep coming up with new network topologies, the new ways of hooking things up, then you need more flexibility in the communication system than you might have immediately imagined. - Thanks, Drew. Well this is gonna be the transition. We just talked about the possible architectures, the importance of weighting information on models, for example. The fact that you have sparse matrices that need a lot of compute power. And we're gonna shift a little bit, and purposefully, we have James going next. James is a good friend of mine and somebody who's taught me a lot about this. Just so happens he's a lawyer, and don't hold that against him please. I asked James to kind of give us some thought about the problems we're gonna have, just not from a technical standpoint, but also sort of, what are we walking into? What kind of hornets nest are we gonna see once we get to the other side? And then Raik's gonna follow it up with that, says, yeah, well how do we manage to keep track of everything James is talking about and ensure that we don't run a car into some school bus. Sorry, I had to come up with an image that wasn't too bad. But anyway, so, James, kick her off. - So I wanted to talk a little bit about the value proposition for a cognitive science department here at SJSU. And I think it's important to think about the roles that a multidisciplinary department can play in contribution to thinking about this space. So if you think about it, it's not limited to these four silos. There's many, many different departments here at the university that will contribute in this space. Engineering, of course, we've seen the technical, the highly detailed nature of these hardware devices that are designed to do neural net processing, and the complexity involved is pretty significant. It's important to think about, well how do non-engineers contribute in this space? What's the role of philosophy, and psychology, and business, and the other departments in a cognitive science program? I decided to address that problem by giving two examples and talking to two different faculty members here at the university who are involved in thinking about problems that actually turn out to have relevance in this space. The first person I talked to is Dr. Evan Palmer. He's doing some very interesting research here involving cognitive psychology and gamification. I talked to Evan, and we actually agreed that this research could have application to neural network training. We'll talk about that for a moment. And then I talked to Dr. Daniel Susser from the philosophy department. And we have a very interesting discussion about accountability of machine learning decisions and who should be responsible in issues like accountability versus transparency. And the point of all of this is, there are a lot of folks here at the university. - Well, how do you depose of machines? Sorry. - Yeah. That may happen some time. - I know. - So, let's go through this. Let's talk a little bit about some work that's being done here at the psychology department. Gamification is a way of using game-like elements in non-game situations to make people perform certain tasks better. For example, people who are scanning, looking at X-rays at the airport to scan baggage or technicians who are looking at radiological images to try and detect disease. What Dr. Palmer is doing is he's applying game-like elements to determine whether or not these tasks can be performed better in some way. And so, it actually occurred to me and by talking to Dr. Palmer, that this is not that different than what is going on with training systems of neural nets, most notably, Google's Alpha Go, which has a training scheme involving multiple neural nets. There we go, now I'm back. - It's alive. Yes. So, anyway, this is not meant to be a detailed, substantive example, but clearly, there is a role for non-engineers in departments like a psychology department to play in the cognitive science department, and I think this is a great example where research that's not directed directly at the details of the hardware or the details of the network topology, perhaps, could potentially be applied to make neural nets or help neural nets think more like humans, which really has turned out to be the key to making some of these systems of neural networks like Alpha Go perform more like humans and perform their tasks much, much better than if we were just thinking about the math alone. Another way that I think the non-engineering departments can contribute is thinking about issues of ethics and policy and technical policy. And so I talked to Dr. Daniel Susser from the Department of Philosophy. I had recently seen an article in the Wall Street Journal, maybe other folks have seen it, published last month. It was written by an early pioneer in the neural net space, Kurt Leevee. He was with a startup in the '90s, HNC Software, that did a bunch of very early machine learning systems that are fundamental to fraud detection. And the gist of the article was that, we need to think about how we're going to explain decisions that are made by these emerging machine learning systems. Why do machine learning systems reach a particular decision? You get into these issues of opacity. Not only how do we explain, is it even possible to understand the decisions that these networks are making by analyzing the weights alone. Or even going further, the network topology, the code, the training dating development, the methodologies, these all may affect the decisions. So in the Leevee article in the Wall Street Journal, the issue was accountability verus transparency. When you start thinking about the policy around analyzing the decisions that are made by these neural nets in the case where something might go wrong, is transparency enough, or rather, do we need to have other standards of mechanisms or technical solutions to provide for accountability? Accountability, perhaps, may mitigate the insufficiency of transparency. You may not be able to understand precisely why a neural net is making a particular decision by looking at the weights or the topology or the code, but maybe by developing certain factors, for example, explainability, to ensure that non-technical reasons can be given for why an artificial intelligence model reached a particular decision, or developing confidence measures that communicate the certainty that a given decision is accurate. Procedural regularity means the artificial intelligent system decision-making process is applied in the same manner every time. And then thinking about responsibility to ensure that when something goes wrong, there's appropriate means for those adversely affected to have a recourse. And so this is really going to be a very important area, and having active involvement from the philosophy department, many, many folks across the spectrum here at the university is going to be very important. I just wanted to highlight some of the thoughts in the conversation with Dr. Susser that I think are very, very interesting with respect to the non-technical analysis of some of these emerging issues. One issue is, do technical solutions that provide accountability eliminate the need for transparency? In other words, can we come up with a way to explain the decisions and eliminate the need to look inside the network? A related kind of corollary is, is it even possible for technical solutions to keep pace with the increasing complexity of these machine-learning systems. The devices that Drew was describing are only gonna get more powerful, and the networks are gonna become more complicated, and the weights and the data, it's going to continue to increase. So what does that mean for our ability to explain the decisions that these systems are arriving at? Some important questions posed by Dr. Susser. Who are the machine-learning algorithms being explained to? Do we need someone like Drew and Chris to understand exactly what the explanations mean? Or can that even be communicated to policymakers and decision-makers effectively. Which aspects of automated decision making are being audited in the explanations. Is it simply a function of the weights and the different variables as suggested in the Leevee article. Is there some kind of bias in the training set that is affecting the decision making? Who's responsible for problems? These are all complex questions. We're not gonna answer them here today, and again, I think you can kind of see what's involved here and why you need a robust university with a multidisciplinary team to address some really, really challenging questions. - Thanks, James. So this is a great transition. Raik's company is involved in formal verification. Formal verification is a method that ensures that we have coverage, and where they're successful in particular is it's a Munich-based company. No accident there that they're very successful. In the autonomous car world, the safety world. So it would be great to hear from Raik and hear his thoughts about where we're going on ensuring and auditing this. Please, Raik. - Thanks. What I want to do is want to bring the discussion to a particular context and try to take convolutional neural networks and its prime application, which is division and decision-making, plus some safety critical aspects in autonomous vehicles and try to understand and look at a few questions of what it means and as we have heard before. We're doing a good job in understanding how these artificial neural networks work. We can explain the weights and the mechanics behind it. We can do a great job on verifying the mechanics of it, at least to small scale parts of the designs, and we have some issues maybe verifying the connectivity and all the interconnects, but that's something that we can solve. But that's not explaining how things work. It doesn't explain why. Verification is basically for us a situation of providing a convincing argument why things don't go wrong for a wide range of scenarios. So it's not just a question of whether it works for the cases you have studied. Whether a training set is good enough, whether your test set is performing well. That's only one aspect to it. Even for this it's hard to say why it works. There is more to it. There is other questions that you need to look at if you bring these systems into a safety critical context. One is, for example, they're pretty hard to train but they're easy to break. There's quite a few examples where you can easily manipulate images, and you will see that the network will come to some really interesting conclusions, and I put one here that is a study from, it was published in IEEE, where people have put some markers on some stop signs, and it would actually let the neural network drive that this ais a 45-mile-per-hour street sign instead of a stop sign, which is not really something you want to have in practice, I believe. The second point that you may look into is-- - [Jim] Although I know people that practice that. - It's hard to defend. That means hen we look at machine-learning applications compared to more standard ways of engineering, if you look at how people built cars and things like that before, you could go back to the engineer and ask him, why do you believe your system works? Explain it to me. What did you consider? What can go wrong? You cannot ask a neural network to do that. It's not explaining itself. That means it's not inter-- - [Jim] Just like Trump. - Yes, it's a bit like it. He may be a neural network in some way. (speaking with strong accent) - Maybe some neural networks, sorry to interrupt. - [Raik] I wouldn't say it. Sorry to mess you up. - At least quite a few of them. So the causality of the models is the big question here. So you cannot actually audit the decision trail. You cannot say why a certain decision was made by the network. Today it's not possible, and there is no theory behind that that we can use in order to answer this question. Also because of the complexity box and the operating process are also difficult to find. That's something that we can work on, but now solving the whole problem. And there's the motive. I call it what I picked up from the literature, the recent studies. The answer and uncertainty. It's about you don't know what you don't know. It's the fact that the training takes place on a decision you make. It's the trainings that you put in. And will only be answering questions similar to the trainings that you have considered. You can make a risk analysis on a training set. You can, you know the probability distribution maybe for a certain amount of parameters. But you don't know that for all the parameters because you might not know all the parameters that are relevant in the system. So there's an uncertain risk here. You cannot actually measure that and you cannot take precautions other than trying to understand the environment as closely as you can and trying to model all the different ways the system might be used or the situations that might be exposed to. There was one study that I saw that said, hey you should actually make a collection of all the different scenarios that get into. Including the car driving on a motor way close to the scene, it's an army sitting the coast. There was one video of a poor guy who actually survived the tsunami in Japan making really interesting and very successful decision on how he was maneuvering his car and survived the blast because he was actually seeing the situation, making the best for him and could get away. There was no way an AV would do that if it hasn't been exposed to the situation before. Obviously, this is maybe something that you say okay, this is really reckless. Who's telling your system engineers what cases to consider? What's the autolesion, how do you find out? And last, but not least is a really complex problem because there's so many interdependent sub-systems that it's pretty hard to get the connection between all of them and make a full verification for this. So it is said that there is a no one knows what they're defending against Some of those are real examples, this was the first case. It's actually soriatically possible or hopeless. So we're really in the early days of understanding the impact of this technology. And what it means is if you want to apply it, you need to at least find some good practices of how we tackle this. And there's some interesting ideas of how we can do that. The main point I wanted to make was the answer in uncertainty is the training data forms the requirements. This is the data processing. It's not an engineering science that we had done so far. So box are the ones that kill you. It's the things that were omitted in the training set that actually will cause fatalities in the end. It's not the thing, I believe that we can make these works on the things that we know. But the things that we don't know are the problems though. One way to do it is to insist on models that can be interpreted by people. That was going into the question of accountability. So if you need to find ways of doing it, as a practitioner of you may want to exclude features from your training set where you don't know how they relate to the outcome. Just don't do it because if it's in some unknown way related to what you want to do, you may not want to do it because you don't control it. There's failures of these networks that you might be able to detect. Or when your network is less confident than the predictions, you may want to actually put some precautions into the system. It can also look at the cycle that it puts you on the right. How common factors want to learn from the experience in the field. Verification becomes implicative for the whole cycle. It's no longer just designing the system, verifying the system, deploying the system. Because the data's getting feedback into the system. From the field, you need to have the verification in the loop the whole time. So you need to re-verify the whole model. You need to recertify what it's doing every time you deploy that otherwise you run the risk of data spoofing as I mentioned on our first leg and other things. So last but not least, it's the systems verification question and you need to have interdisciplinary views on this. It's not as efficient to just as the machine guy to come up with the verification. And we need to employ a lot of non-machine methodologies in order to control this. So as one of the last points here, someone made a comment that the ID verification is more complex and interdisciplinary than the chip verification in the street and that's a complex one if you, from sitting in your industry, you know what this means. It's really a very, very difficult thing to do. - Yeah, thanks Raik. Hopefully what we gave you so far tonight was a overview of from the hardware to the sort social issues and verification issues. And then, what's a method that we can actually design these things. Now I think a common misconception is big data is often said in the same sentence as neural nets and machine learning. And we had a pretty nice discussion right before we came on, it's about the training data. People kind of know that data scientists have know what they want because just turning a convoluted network on a big data set, who knows if you get a result at all. So Chris, why don't I start you, asking you what do you think about that comment by me? - It's an interesting perspective because it's clear that having a lot of data, big data is one of the prerequisites for applying this class of algorithms. In fact, a good way to think about all of neural networks and deep learning is that it is a statistical method, which is applicable in problems that have gotten so complicated that we don't know of any conventional algorithm to do a good job on them. And so we really use this learning method to get a statist, a good statistical guess, the best guess we possibly can get based on all of these examples about what the right answer is or the right behavior. So you need big data to do it, but the mere existence of masses of data isn't very useful because the real purpose of the big data is to use those examples to train the system. And that means you not only need to have big data, but you have to know what the right answer is for all of those examples. And so there's a higher, there's a higher standard or an additional need besides just having terabytes of data. You have to have useful data. - Useful data or apply it usefully. Yeah so a quick comment on what Chris just said. In semi connected manufacturing it's a continuous process so more akin to the pharmaceutical industry than building a car. So it's statistically driven and used to operate in the three sigma range and in terms of distribution thought we were geniuses. So with the smaller, smaller geometries, the variance is so high that I have to be in seven sigma range. So back to statistics for a second, we don't have enough time to run the statistics. So what the machine learning's allowed us to do is reduce that time and hopefully find that variance. So along those lines, I'm the investor, Chris is as well, but and I'm the investor tonight. And so one of my commies just exited Friday and it was all about getting to the seven sigma distributions and we did that through AI techniques. So when I hear these guys talk, you can just imagine that I'm thinking company, company, company, right? Drew what do you think about the training data? How hard is it? How important, for example, Wave, one of the companies you've illustrated, getting training data for those guys must be really hard. - Okay well so a company like wave is trying to sell the computing solution part so they don't need the, it's their customers who need the training data. But as Chris mentioned, the data by itself isn't valuable until the, as he said, the answer's been plagued. Now for a lot of data sets that is often called labeling. And to label a data set is actually an awful lot of work. And so one place where people might apply their gamification strategies that James was talking about is in trying to incentivize people to go through data sets and as humans and label them. So that's like in some of the famous image, that's a cat, that's a dog, this is a stop sign. - [Jim] That's cancer, that's not right. And so the quality that labeling is incredibly important to the success of the training process. The training process itself is complex. The math that I showed was the inference math, but the mathematics and the communication associated with action training involves them trying to calculate, okay I ran this training data set through my network and I didn't get the answers I wanted. Now what do I do? And then there's a whole lot of different kinds of math and communications that go on in trying to come up with what's the best way to update my weights so that the next time I push something through it has a reasonable chance of getting better. - Or you can multiply by zero. So now let's see how comfortable James is with this question. So we have this training data, it's got value, commercial value, right? Actually the training data is IP. I don't want to give my IP up to anybody. Right? - Yeah I mean that's really kind of the central issue I think that's posed by the Levee article that was in the Wall Street Journal. I mean he's suggesting that we can circumvent that issue and not disclose the training data, not disclose the weights because we can develop technical measures of accountability. And the philosophical question and the thing for the technical ethicist and the lawyers to straighten out is accountability versus transparency. Is accountability alone enough or do we have to open up the black box and disclose its contents in order to develop technical policies around how accountability is going to be developed for these. - Yeah, yeah for me it's standard. It's all business, all over that kind of thing. And so yeah, I can see a commercial offering at some point. - I wanted to chime in with just one additional thought associated with training databases. And that is not only are they valuable, but they potentially capture bias of one sort or another. And it's pretty hard to look at, say, a million photos and say, is there something wrong with this statistical distribution here? Am I over represented you know-- - Cocker spaniels. - Cocker spaniels over a Norwegian Elkhounds. - Bulldogs. - Well it's kind of hard to tell just by glancing at it. And in more profound ways we actually need to think about the robustness of our systems as being bias free, whatever that means. But it means something in order to be able to use these systems effectively. - So you think that's a great lead in for Raik, actually. - Well I wanna lecture on the question about the data and where it comes from and that's actually the gold of the next machine age. It will actually be probably one reason why people want to push computing to the edge. Because if you control the data and you actually do the computations and you learn from it and you control the whole flow and it's part of your IP, it's part of your system, you're not gonna, you don't need to give it up. It doesn't go to the cloud. No one else needs to know what it's doing so it can protect your IP. If you can get away with accountability then this is gonna be a really good thing for people doing that. But if it needs to be transparent, this is gonna be challenge and it goes the other way around. If you have these trade-offs that you have heard of about the the data identity and the storage that you need, how do you put any form of transparency in place if you have so much data. It's probably not going to work. So it actually makes it even worse. - Let's see if I, yeah there's some. Hey can I invite Sean to come back up? We're about 50 minutes into the hour and I'd like to get Sean to help me out, invite questions from the audience if possible. - Yeah, so we have one mic set up in the back here. Don't shout your questions from your chair. Just feel free to line up and ask a question. I have one for James. I don't know if this was discussed earlier, but what about computers as inventors. And in other words, what are the legal and policy implications of a computer actually being an inventor? - Yeah, that's, I don't think we have thought on that yet. I haven't researched any case on that specifically, but I think it would require modification of the US code to, only people can be inventor as this point. So-- - I'll go ahead-- - I think to the extent that a piece of AI came up with an idea and that piece of AI was developed by a person, it would likely be the case that the person would be considered the-- - The inventor? - Yeah. - That makes sense. I mean otherwise we'd sort of have to say, well my pencil invented it, I didn't. - Well maybe cyborgs. Any questions from, it looks like Graham's ready. - Graham, do you have a question? - [Graham] On, Sam can go first. - Oh Sam. - [Sam] This question is for Chris. - Well, you need to turn that on? - The mic out in the audience is not working, fellas. - There's switch right on the end. Could you help him out there? - Tech support's coming. There it is. Nice ring to it, thanks. - So this question is for Chris. What do you think is the role of open source processor like risk fibers in AI? - That's a good question. Two thoughts. One, I think there is this open source movement which is touching many, many different areas of intellectual development. I think it is most profound in software and most likely to be successful because if you, you can replace a piece of open source software relatively easily if you find it isn't everything it was meant to be. Open source so far has played a very minor role in hardware, partly because it's really the case that things like processors are such essential technology that people want a robust architecture with great support. And here are at two four, there has not been open source solutions that have really good support. Now risk five does seem to be a departure in that it appears that there may be enough of a critical bass in the ecosystem to have it survive. On the other hand, the risk five architecture is more like that intel X86 at the bottom of the chart than any of the other architectures that is shown there. And that these new architectures which are extremely parallel machines are going to be the ones that do essentially all of the evaluation for inference and do all of the training. And conventional risk four six architectures are really going to be supervisors on the side. So I think that open source is becoming relevant, but there's nothing about this revolution that particularly favors intel versus armed versus sci-fi architectures. They're really next thing to being irrelevant to this computational revolution. - A couple things on risk five. Risk five event was last week in. And WD's gonna use it and their systems make sense from WD's standpoint because gotta integrate a whole lot of companies. So it cleans up their integration path, but as Chris says, it's for control. The computational engine is this big massive parallel process and unit. There's a reason in video's good at this, because they've been moving a lot of pixels for a long time. So we're gonna see something on your, I believe, out of application processors that are working in tandem with control stuff. So that might be a good segway. Drew, what do you think? - Well I mean, as Chris said, I think the reason that open source, when open source it definitely works because you get a network effect, different kind of network. But you get a network effect. You get a critical mass of people who are willing to contribute and care enough about the result that they themselves or their company allows them to invest some time for the benefit of the community. And so the whole is worth more than the sum or the parts. So that's been a big challenge for hardware type structures because typically, semiconductor companies don't like to let their engineers give their time away to such tasks. People try to keep things to themselves. And when you're pushing really hard on numerical metrics like the number of things you can do per second, people get all excited about what they believe to be their competitive advantages. In this space, as Chris pointed out and I tried to elaborate a bit on my slides, it really is all about a specific branch of math. And I see a very, very rich idea of places where universities could play, putting together ideas for technologies that potentially could be non-differentiating open technologies that people could use as accelerators in some of these systems. I think that's a great area for academic research. - James, what do you think about source here? - I think-- - Or on the hardware I think. - Yeah on the hardware side, I think risk five could play an important role as a supporting technology. I tend to agree with Drew's comments on the IP issues. I think to the extent that hardware companies have technologies that confer a particular advantage in the neural net computation training, arms race, they're gonna wanna keep those close to their vests. They're gonna wanna file patents around these things either as a protective or a source of licensing revenue and it, that's problematic in the context of a risk five type model. - Yeah, I mean we didn't disclose, but you can Google James and you will see that was, he worked for a call calm down in San Diego so he's well versed in this. Raik, what do you think? Did open source give you a good headache, buddy? - Open source hardware, I think not really because as we've discussed, the application companies for open source hardware are actually in those upper companies so it's not really the same environment as if you had this open source software. And the other reason that I would like to add is that potentially there's a difference between software and soft course. It's not really soft that we want to do in hardware IP. And there's also the cost for the hardware itself. So it's not like you have the NIE on the software and then you have a community and it distributed and copied as many times as you want. There is a substantial cost in the hardware itself. And that's making big part of the cost for the system that you're open source. It's not gonna come for free even though it's open. - Yeah, free is never free. A great example of that is check out the market cap of Red Hat. It was free sport at Linux and they're like a $30 billion company for God sakes. How'd that happen? So we got some more questions? - Hi, I had a question on the social side. It seems like with all of these sensors that are available, I can capture all of my data, all of my experiences as a human being from birth onwards, including my biometric data. And it seems to me it's almost like a privacy issue. Shouldn't I own all of the training data that can come from me as a human being? And shouldn't it be only as an opt in that you can collect any kind of training data about me? And maybe there's a business model for managing people's personal data. I don't know, but it seems to me on a social level, people should be able to have complete control over their data, or at least of complete visibility in what's happening with their data. - I think that's a really important distinction between visibility over and control over. And I think it's always useful in these cases when we're talking about automatic data collection to substitute human data collection. So in society, in the absence of technology, do you have complete control over what everybody thinks of you? Other people are observing you and forming opinions about you and you don't. Especially if you go into public places, people are going to be sensing you, learning about you, training their brains about your behavior. And it seems implausible that we could say that we own everything that comes out of an interaction with other humans. However, I mean clearly this is data collection on a massive scale and data collection with a possibility of reuse in purposes that we never had in mind. But we need some practical ways to draw the line. I mean today I think the law generally is if you go do something on the public street, then you are by actively going out in public, you're giving a kind of permission for other people in that public space to sense you. But what you do behind closed doors remain yours. But we have kind of this very crude separation. But well we better ask the lawyer here. - I think that to the extent that you're interacting on a public street, as Chris used in his example, is and you're being sensed that you're not gonna be able to control the use about what others learn about your interactions with others or things in the public space. - But what about in the work place? Do I have control over the biometric data that I'm emitting in my work place? - Yeah, that's a more complex question that would probably be governed by contract. I mean it really, you'd have to look at the contractual agreements between your employer. I mean that could be something that's covered in an employment agreement. It might be an issue for an employment agreement in the future. And the disposition of that data and the ownership of that data is all something that would have to be straightened out via contract. They have to contrast that private setting with all of us in a public setting. And I think the law is pretty well developed with respect to privacy issues or your lack of any clear right to privacy in a public setting. In a work place or a private setting, it depends. - The only example I can give you that, where there's process or policy in place is on health records. So one of the companies I'm involved with has contract with the Veterans Administration and we've automated an aggregation of their health data. Which is useful because it's in despair in places and different formats. So we get all that VA data into a big data set. We start looking to transfer the veterans. Now what you wanna do is ensure, and what the law requires by the way, there's laws, HIPAA laws on this, is that we don't ever, we deal with the patient themselves. We can talk about behavior, trends, things like that. So we gotta really abstract the data. So I would argue that at least in the case of Veterans Administration and healthy records, there's thought and law there. Now is that gonna, the first place it's gonna migrate to is the rest of us. So I think health is probably the first place. - I think it's really useful to also recognize that different parts of the world are wrestling with this very differently. Europe has it on a trend towards much greater protection of privacy, much greater assumption that people retain rights and the other end of the spectrum may be China. Where surveillance is-- - [Sean] Everywhere. - Everywhere. And the idea that somebody's going to claim that they're behavior on a public street is private information, I don't think is gonna play terribly, terribly well. - I think it might be important to think about who's collecting the data too. Think about your interactions in your vehicle and you've got waymo driving around, sensing. I mean is your behavior in your vehicle immune from capture by those companies that wanna understand driving behavior to fill their autonomous vehicle systems to put them in. I don't think so. I mean I think that data's out there for everybody to collect, but distinguish that with perhaps the government trying to collect information about the way you interact in your car. So I think who's collecting the data and what the purpose is these are all very intertwined prognosis. - Well the scientist in me, let's talk cars for a second. So he was saying give me the data set, give me all the the assertions that I can plug into my verification system. And if I have everybody's driving behavior, then I have a big database that I can utilize to ensure I save lives. So who makes that decision? That's above my pay grade. But it seems to me, yeah I'm not particularly happy with them knowing where I am all the time, although I never go anywhere because I'm an old guy. Because you just sit home, and Friday night's Denny's so I'm pretty predictable. But you know, I think, the special and 6.49 for anybody over 55. But with that said, I'm kind of okay with them taking all my data, that data, and plugging it in to an autonomous thing. Raik, what do you think? - Well I think as European and a proponent of privacy, I think it's one of the most important cornerstones of democracy and freedom. And you shouldn't give it up too lightly. And in this particular case, going back to Graham's question I think you may want to have some control over where the data goes that you use. So it is the first stage that you talk about the cars collecting data about you. And there may be some reason for doing that in order to provide some service to you that you actually want. Now leaving the government outside and the Chinese government particularly, it is then the question, where does that data go from there and who is collecting data from multiple sources. And Jim, your health insurance might still go up because people are collecting some data from other sources making connections which actually identify you as Jim Hogan, even though you're record was not private. But because nothing personal, it was anonymous. But there's enough study out there that makes clear that it's, from enough data you can still go back to the source and identify individuals. And you will all have all this data reconstructed from multiple sources. - It looks like they're piling on. I mean that second scoop of ice cream might show up on your health record. - The third sure does. But okay, who uses Waves. Anybody? Yeah man, if you live in Santa Cruz you do. And so that's a great example of everybody using your data. Now it was really great before everybody was using it. But okay, that was kind of useful experiment, but I don't know, it's lost its utility I think. So another whole lecture. - [Sean] We should take another question right here. - Hi, I'm Edmond. It seems as the most AI projects will run on new type of chips, so it's silicone. Silicone is hard and of now we have a big wave on consolidation in the chip industry. So my expectation is that in five years there will be two electronic chip companies left. So my question is to those who are in the startup industry, what's the point of having a chip startup in that kind of environment? - So I mean if you saw Chris' data, actually we're seeing an expansion in the number of small chip companies because of these opportunities. The trend 15 years ago was absolutely in the direction you're talking about. We had everyone trying to chase this one socket in that phone thing that was going into our pockets and it required huge investments. Hundreds of millions of dollars and a thousand plus person design teams, all chasing this unicorn of an opportunity. For better and worse, that market changed a lot. Large semiconductor companies started scattering those design teams into different application areas. And some interesting things happened. So we have this set of emerging technologies. Today we were talking a lot about the neural network space. There's been some fasting work done in trying to exalberate things like crypt to currency, blotching activities. There's a lot of small companies, incredibly economically focused companies in that space. And so actually there are, from my perspective a lot more small semi companies today than there were five years ago. - Yeah I mean let's stay on the crypto currency sort of the second right. So I got a company, it's an Asic company, builds Asics for people. And they got a pipeline of 20 Bitcoin like companies. They're bring their FPG algorithms over to build in Asic because they can do it 100 times faster. And that 100 times faster means they have that much more revenue they can produce. So economic, it's worth while. So we're seeing a lot of little guys, I mean a lot of small volume processors. - I think there are really three, three trends that are woven together. First of all I actually agree with the basic premise of the question. I don't think that there is something about neural networks that say, oh yes semi conductor consolidation is gonna be reversed. Because that's much more about manufacturing scale and distribution channels, which are not fundamentally changed by neural network technology. And so I think there are a lot of startups. I think some of those startups will fail. I think some of those startups will be acquired. A very small number of them may survive over the long run, but we're not gonna see 17 of them as major semi conductor companies 10 years from now. Impossible. But I think there's also the trend that says that a lot of semi conductor innovation is now taking place in systems companies, not in semi conductor companies. And it really reflects the fact that due to integration and due to the cleverness of software methods and higher level algorithms, like deep learning, that system companies often have much more of the really important rare insights compared to chip companies. The chip companies are good at producing chips for the problems that are already very well understood. But I think we're enjoying a period of very rapid change in how you solve some of these really big problems. And that advantages the system companies. And some of them, like Google most recently, will choose to leverage that know how in their own silicon designs. So it's true, we do have this consolidation taking place in the chip companies, but in background you have this re-vertical integration taking place in lots of other places. Often with very particular applications in mind. And I think it is in fact this phenomenon where more and more of the real value add in systems is happening in higher layers in software. And partly because more slog deceleration. Partly because there's just so many interesting problems out there to solve, that can be solved at the software level. And so just as a very personal dimension of it, I've been a semi conductor guy since I was working on 1k dynamic rams. A very long time ago. And I really reached the point as an investor, I don't invest in semi conductor companies because that's not where the action is. I really believe with my heart and my wallet that the innovation opportunities, the pace at which you can do things is much higher in the software domain. So I am starting a new company in speech processing applications. It's a very technically intensive, it cares a lot about high compute, and it's never, I hope, going to do chips. - Okay let's just look at the Google account. So they're actually going to be a merchant supplier semi conductor. They're gonna consume all that in all likelihood insight. Well maybe they will, a chip android on it or something. But with that said, it's unlikely that they'll end up being a big volume semi conductor guy. So the systems folks are realizing that they get a lot more cycles and a lot less energy by getting to an Asic. So all this is true. This is a constant debate. - [Sean] Jim, this was an observation from Twitter from Catherine and she feels like there's a disconnect between Silicon Valley and Humanities because she basically says, tech people think we just sit quietly reading a physical book. Not a lot of what we do is construct, analyze narrative. Who reads labels, code, humanness. So that was just in her observation, but would that be a topic of social engineering? - Well I'm gonna olay this to my panel. I'll do it to my person to my immediate right. - Thanks a lot for that question. What was the question? - [Sean] It wasn't a question. It was an observation that there's a disconnect between the technical community and the humanities. - I see. So in some sense that's true because we still keep trying to understand what we do here on the technical side. As I mentioned earlier, it's like we don't even know why it works. How are we supposed to explain that to someone else. And then understand the consequences of what we're trying to do. I think that a bit early. This discussion will come, I think, and we have it and there will be discussions about implications of what the technology will do. The question really is when is the point to have this discussion. What's the thing we need to fund out. And I think right now, we are still trying to understand the consequences, the potential ones of what we're doing and obviously we need to be careful with what we're trying to release. - I am struck with the fact that the premise of the question is sort of that we are just specialists in Silicon Valley. But in fact, we are humans first and specialists second. We have families, we have all of the usual problems. The question of our ultimate demise weighs on all of us especially as we get older. And so we're living the human experience first, but like so many people in so many places in so many parts of the world, the best way for us to survive, feed our families, do something interesting, is to specialize in a particular way. And we specialize around electronics and software here in the valley. Just as brain surgeons specialize and farmers eking out and existence in some demanding ecosystem, also are very highly tuned specialists for that. But we are, I would claim humans first and specialists second. - Yeah, just pick up on that. Thanks for giving me some cover while I came up with an answer. But the way I see it is, rarely can I fix anything. I mean I'm just, you know, I'm a math guy. But I often can make things better. And that's the specialization side. I can bring our skills and our knowledge for a tool for everybody. That's our value proposition, I believe. - [Sean] Question here. - Hi, I'm an old guy and I'm looking at the neural networks and think, those look like analog circuits and all the math is like fast pace. Which is something that most of the digital guys I see in the panel don't really deal with. So is EDA ready for this? - Actually I think, thanks for bringing it up. People that know me say you turn everything into a spice problem, Jim. But I kind of see it where as far as metrics. So metrics solvers, et cetera, et cetera, et cetera. Analog's great because you just cycle. You only use power when you cycle. So you put it on the edge where you have a battery maybe and you can get 10 years worth of battery life out of it. So I think the analog, me personally, that's one of the things I'm gonna work on. Is I'm interested in that, as well as all of the other things, but the analog solution errors is a good one. You're bringing up a great point. - I think it's still the case with lots of people having tried to make it otherwise, analog is still more art than science. And that's a little bit like the whole neural network problem today where as we say, we don't know why the things work. So people come up with an insight of oh gosh, if I add another layer of this shade here maybe I'll get better results. Let me throw a bunch of data at it and back propagate and trade and my gosh, it's worse. And so it really is that domain. (speaking over each other) Pardon me? - It's good fit. - Yeah. - Two drops of black magic together. - So I don't think it's an EDA problem yet. It could become one, but I don't think it's gonna be a problem yet. - I'm gonna take issue with the premise of the question because I don't think the interesting dichotomy is between analog and digital. I think the interesting dichotomy is between programmable and non-programmable. Because non-programmable fundamentally assumes knowledge is stabilized, we are able to freeze this problem in a way that we are either gonna go build the digital circuit or go built the analog circuit that does this one thing. And that we're willing not to really change it in important ways for the next two or three years that it takes to get that, either analog or digital chip, out there. I think that if the large arc of technical history tells us anything, it says that software wins. That of course everything's running on some hardware deep down inside, but the layers of innovation and the layers of value add that are accumulating in software are so rich and so deep that we really need to be ready to leverage those things. And yes, of course, the non-programmable system is always more efficient than the programmable system. But the non-programmable system is really a bet against learning. And history says, people are gonna go on being creative. So I bet on programmability every time. Programmable analog, great. Programmable digital, great. But just not non-programmable. - There's a fair amount to start us going into the analog space. And in machine learning so that's an irrelative point. But I think that the question earlier connect to the systems question and if you look at the most profitable systems companies in Silicon Valley, they actually do have some hardware business inside. It's actually a good thing to have it. So I don't fully agree with the software. Only thing if it comes to making a valuable business-- - Now the panel's getting fun. - I'm not saying that there isn't a role for hardware, but you think about the software experience first and that's where the innovation, I think had predominantly been driving value for the last couple of decades. - I think it's actually not even software anymore. It's the ideation from various startup experience that you provide to the user. And it doesn't really matter if it's hardware or software. It's only meets to end. So if you look at it from this perspective, maybe we can say that it doesn't really matter how you implement it, software is maybe the most flexible thing do it and get to the money quickly. If you make profitable, you want to add some hardware. But the user actually doesn't care as long as it works. - I'm really glad we invited you. Because we would have never got that out of any of us. And so yeah let's think about Chris' chart. The edge, the cloud. Over here, maybe some analog. Not much programmability. If you have some programmability you can afford it, okay. But if it's gonna be low energy and you want it to last a long time, you know software take a low energy. And so that might, there might be a place. I don't disagree with you that software, this stack of software has allowed us to have a platform for multi-generations without changing the hardware, which is really cool. - But Jim, changing a million devices costs a little bit of money. So if you got it wrong and you deploy it, then having some programmability is incredibly valuable. - Oh now they're piling on. Bring it on, it's great. - Well and I think there's ample evidence that to some extent you can have it both ways. I mean you can have things that are very highly energy efficient and which are highly programmable. And most of the really significant hardware innovations have taken place in things that are pretty programmable. That's the only reason why we're so fixated on processors along the way. - Actually I don't know that I can even care, as long as it meets the parameters of the system problem. And so you know, darn it we're out of time. - [Sean] Yeah, we're running a little over, but could you just do one question here. This gentleman right here, he's been waiting. - Hello, so I wanted to know what your views regarding artificial intelligence for securing networks and infrastructure. There's a couple of startups which I actually work for it, but all of this kind of debate going on. My second question is securing your artificial intelligence systems. - So I mean I think in defending networks, pattern matching is fundamental. And that's since we know artificial neural networks have a leg up because you can feed a lot of example, a tad factors at them, and then they will, they will recognize the no ones very, very quickly. How good are they going to be at predicting other ones? I think there's a lot of interesting work going on there now to figure it out. - Yeah it's an immune system problem, right? You inoculate yourself with a known virus and the CNN's will work well. Are they gonna figure out new viruses? - I mean there are a half a dozen security companies using pretty serious deep learning on my list. And so I take it as a proof by example that it must work to some degree. Though how you get enough examples, I mean the number of examples you can get relative to say what you can get in areas like vision or speech, has got to be a lot lower. And it makes it much tougher to generalize if you don't have a whole lot of data. - Kind of back to the training question in a way. Well, did we get to your second question? - [Man] Not yet. - Okay, not yet. Somebody remember the second question. - Yeah. How do you secure these artificial intelligence systems? At some level at the sort of, it's a piece of software running on a piece of hardware. The issues are not fundamentally different from any other software running on a piece of hardware. But there are of course, some additional areas of exposure. One of them is, as I think Drew eluted to, oh and no, and Raik eluted to, spoofing these systems is actually a big issue. And so there are ways of getting at them through their back door in new ways and getting them to do things that weren't expected by giving them inputs that they never expected. And so I think that actually that robustness of the problem definition is leaving lots and lots of big white open doors that do represent kind of serious security and robustness questions. - Yeah, yeah. I mean that's, at least my few projects I'm involved with it's finding the data, excuse me, the behavior that is mal-behavior to breach. So that is a big challenge. So we'll see where it goes. If it's okay. I'd like to conclude if I can. - Yes. Yes you can and the next event will be in the new year. February 23, right? - [Jim] Yeah and while-- - [Sean] 21st. - 21st and while we're on the subject, as I tried to explain in prior once, we set this up eight or nine months ago and I've been fortunate enough to have August panel volunteer their time and join us. And we've taken a little bit of criticism, a lot of criticism for not having females on our panels. Obviously there's females that can help us on this understanding and I can guarantee you that next year as we work on the agenda and the panel discussions, we'll have representation for sure. Hopefully you understand that the planning of this took a while and I wanna, while I'm here, I'd like to thank my August panel. You guys did fabulous. Thank you so much for giving us of your time. (applauding) I really appreciate the support of San Jose State at San Jose State University. I hope and wish for them as practitioners that you guys, you all can develop the cognitive science curriculum here and let's see some students graduating with master's degrees and undergraduate degrees in cognitive science. That would be our reward and I would really appreciate it. And I do appreciate the time that the university has given me this year. I'm a, what do they call it, a estrous alumni. I wasn't when I was here. So you gotta, so there's some kids going to school right now that are probably, Jim Hogan said. And if it weren't for San Jose State, I certainly wouldn't be here today. So I wanna thank the university for not kicking me out when they could. And making my life really, really making a difference in my life. One of the people that's not here today is Emily. She's gotten pneumonia for God sakes. And Emily was the real driving force behind this. Anybody that's ever tried to manage me, it's been pretty hard and God bless her. She managed to get all this thing done. So God bless Emily, I hope she well sooner. Anything else? - No, I just, one more time for Chris Rowen, Drew Wingard, James Gambale, and Raik Brinkmann. And the big kahuna, the riddle one, Mr. Jim Hogan right here. (applauding) - Thanks for hanging out. - [Sean] Yes, we'll see you on the 21st of February. Thank you so much. - Thank you so much. 