 so hi everyone um I'm Frank and I work on the Google brain team working on tensor flow and today for the first part of this talk I'm going to talk to you about accelerating machine learning with Google cloud abuse so the motivation question here is why is Google building accelerators right I'm always hesitant to predict this but if you look at the data this has been you know the end of Moore's Law has been going on for the past 10 or 15 years where we don't really see the you know the 52 percent year-on-year growth in single-threaded performance that we saw in the from the late 1980s through the early 2000s anymore we're now single trial performance for CPUs is really growing the rate about maybe 3 or 5 percent per year so what this means is that I can't just wait 18 months for my machine learning models to train twice as fast this doesn't work anymore at the same time organizations are dealing with more data than then ever before you know you have people are uploading you know hundreds and hundreds of hours of video every minute to YouTube people are leaving product reviews on Amazon people are using you know using chat systems such as whatsapp you know people are talking their personal assistants and so and so forth so more data is generated than ever before and organizations are just really you know are not really equipped to make sense of them to use them properly and this and the third thread is that at the same time we have this sort of exponential increase in the amount of compute needed by these machine learning models this is a very interesting blog post by open AI where you know in late 2012 where we just had where deep learning was first becoming useful we have like Alex Ned and we have dropout which used you know a fair amount of computing power but not that much compared to you know in late 2017 where deepmind published the alphago zero alphago and alphago and alpha zero paper we see in about six seven years we received a compute demand golf computer man increased by three hundred thousand times so this you know puts a huge strain on companies um a compute infrastructure so what does this all mean the end of Moore's law plus this sort of exponential increase in computer requirements means that we need a new approach for doing machine learning and the same time of course everyone still wants to do compute do machine learning training faster and cheaper so that's why Google is building specialized hardware now the second question you might be asking is what sort of accelerators is Google building so from the title my talk of my talk you know that Google is really Google's building type of accelerator that we call tensor processing units which I really specialized Asics design for machine learning this is the first generation of our TP use we introduced back in 2015 at Google i/o the second generation of TP use claw now called cloud GPUs version 2 that we introduced at Google i/o last year and then these cloud TPU version tools can be combined into pods you know come called cloud TPU b2 pods and of course at Google i/o this year we introduced the third generation of cloud TP use now it's big from air cool now its liquid cooled and of course you can link a bunch of them up into a pod configuration as well so what the differences between these generations of GPUs so the first version of TP use is really we it was really designed for inference only so it did about 92 Terra ops of innate the second generation of TP use did both train does both training and inference it does it operates on floating-point numbers it does about 180 teraflops and it has about 64 gigs of HP m and the third generation of GPUs it's a big leap in performance so now we are doing 420 teraflops and we Double Dhamaal memory so now is 128 gigs of HP m and again it does training an inference and of course we see the same sort of progress with clout GPU parts as well of our our 2017 parts did about 11.5 peda flops that is 11,500 teraflops of compute with 4 terabytes of HP m and our new generation parts does over a hundred peda flops with 32 terabytes of HP n and of course the new generation of parts is also liquid called with a new chip architecture so that's all well and good but really what we are looking for here is just not just peak performance but cost-effective performance so take this very commonly used um image recognition model called rest at 50 if you train it on again a very common data set called image net we achieve about 40 100 images per second on Braille data we also achieve that while getting the getting of state-of-the-art final accuracy number so in this case is 93 percent top five accuracy on the image net data set and we can train this um resonant model in about seven hours and 47 minutes and this is actually a huge improvement if you look at the original paper by climbing her and others where they introduced the resonant architecture they took weeks and weeks to Train one of these models and now you know with one GPU we can train it in seven hours and 47 minutes and of course these things are available on Google cloud so the current training so it takes about you know if you pay for unpaid for the resource on demand it's about thirty six dollars on and if you pay for it using Google Cloud preemptable instances it's about eleven dollars so it's getting pretty cheap to Train and of course we want to do the cost effective performance at scale so if you train the same model rest at 50 on a cloud TPU version two pod you are getting something like two hundred nineteen thousand images per second of training performance you get the same a finer accuracy and training time goes from about eight hours to about eight minutes so again that's a huge improvement and this gets us into the region of you know we can just iterate on you can just go train your my go trainer model go get a cup of coffee coffee come back and then you can and see the result so you it gets into almost interactive levels of machine learning of being able to do machine learning research and development so that's great then the next question would be how do these accelerators work so today we are going to zoom in on the on the second generation cloud abuse so again these this is what it looks like this is a one entire cloud TPU board that you see here and the first thing you want to you want to notice that cloudy views are really network attached Isis so if I want to use the cloud TPU on Google cloud what happens is that I created I go to the Google Cloud console and I create a call TPU and then create a Google compute engine VM and then on that VM I just have to install tensorflow so literally I have to do pip install tensorflow and then I can start writing code I don't have drivers to install you know you can use a cleaner bundl image you can use the machine images machine learning images that we provide so it's really very simple to get started with so each TPU is connected to a host server we have 32 lanes of PCI Express so each TPU so the thing here to notice that the TPU itself is like accelerators like so you can think of it like Jeep it's like GPUs so it doesn't run you know you can't run Linux on it by itself so it's connected a host server about 32 lanes of PCI Express to make sure that you know we can transfer training data in we can get a results back out quickly so and of course you can see on this board clearly they're very for fairly large heat sinks underneath each hits heatsink is a cloud TPU chip so zooming in on the chip so here's a very simplified diagram on how on off the chip layout so as you can see each chip has two cores is connected to 16 gigabytes of HP M each and there are very fast interconnects that connect these chips to other chips on the board and across the entire pod ok so each chip that's about each cord that's about 20 2.5 teraflops so and each core consists of a scalar unit oh a vector unit and a matrix unit and we are operating mostly on full30 tools with one exception so zooming and a matrix unit this is where all the dense matrix math and dance convolution happens so the matrix unit is implemented as a 128 by 128 systolic array that does be flow 16 multiplies and float32 accumulates so there are two terms here that you might not be familiar with be float16 and systolic arrays so I'm going to go through each of these in turn so here's a brief guide to floating-point formats so if you are doing machine learning training and inference today you're probably using FP 32 or what's called single precision I Triple E floating point format so in this case you have one sign bit 8 X 1 in bits and about 23 and 23 significant bits and that allows you to represent a range of numbers from 10 to the negative 38 to about 10 to the 38th so it's a fairly wide range of numbers that you can represent so in recent years people have been trying to train neural networks on FB 16 or what's half precision I Triple E floating point from it and this and you know people I tend to flow and across the industry have been trying to make this work well and seamlessly but the truth of the matter is you have to make some modifications to your to many models for it to train properly if you are only using FP 16 mainly because of you know issues like managing gradients so you have to do loss scaling and you know all sorts of things and the reason is because the range of representable numbers for XP for FP 16 is much narrower than for FP 32 so the range here is just from about 6 to that 6 times 10 to the negative 8 to about 65,000 so that's a much narrower range of numbers so what did the folks at Google brain do so Google brain did is that we came up with a floating-point format called be felt 16 so a b-flat 16 is it is like 32 except we dropped the last 16 bits of mantissa so this results in the same sign bit the same exponent bits but only 7 bits of mint ISA instead of 23 bits and in this way we can represent the same range of numbers just at a much lower position and it turns out that you don't need all that much precision for a neural network training but you do actually need all the range so and then the second term is systolic arrays so rather than trying to describe what a systolic array is I will just show you a little animation that made up so in this case we are computing why it was very simple you know matrix times vector computation so you are computing Y goes w/w times X where W is the 3 by 3 matrix and X is a 3 element vector and we are computing X with a batch size of 3 so we have already loaded all the weights into the matrix unit and if we start a first clock cycle you see that the first element of the first the first vector is loaded into the matrix unit and then we multiplied the we multiply the position 1 1 of W with the first element of the first the first element of the first vector and in the second clock cycle what happens is that more weights are loaded so we are doing more we are doing more multiplications by the same time we are pushing the results from the previous round of multiplication right words so that in the case of the in the case of the yellow box right there we are not just doing the multiplication we are also summing the result of the multiplication that happens within the box with the result from the box to the left of it and then this continues and so you can see you know you're utilizing a lot more compute now until you get there get the outputs out so what this effectively is is a 2d field of compute so it allows us to put a lot of compute units within a very small amount of chip area so if we optimize on we optimize on you know cause of the chip because the bigger the chip the bigger the the higher the cost and with a chip architecture there's also build for pipelining that is we can fill the in so in in this previous example we only had a you know a batch size of 3 but if you have bigger batch sizes if your chip architecture is built for this you can just you know always fill the matrix units and this means that we get very high throughput for our matrix multiplications which is really at the heart of you know a lot of these deep learning models so ok cool how do I use these accelerators so our recommendation is that we start that you start with a cloud TP reference models these are high performance open source models they are licensed on the I think the apache license they implement you know very common and also cutting-edge modern architectures internally we test them from performance and accuracy and you can use these and get up and running really quickly and you can modify them as needed so you can train and run of course on the sample data on your own data and so on and so forth and we have a lot of reference models so I gave you examples of you know ResNet 50 and other image recognition networks but become you can also do things like machine translation language modeling speech recognition image image generation we have all these models just as sample models for cloud GPUs if you want to get started but it started with them great so remember these remember to remember those parts it turns out for a lot of our models we've not only optimize them for single TP use we've also optimized for TPU parts for instance take the rest at 50 take the rest at 50 example that I quote the performance figures for earlier in this case if you're training on a single cloud TPU this is really literally all you do you download that you start at TPU you download tensorflow you clone the git repository and then you just basically call Python and just say point it to the TPU pointing to Rio data is he'll tell me what the batch sizes tell me how many steps you want to train for and then BAM you off you go it turns out that training on the cloud TPU pod is not that different instead of starting a cloud TPU you start a cloud TPU pod and they're really the only things you have to modify is the name of the TPU the training batch size and the number of training steps so the reference model so in in this case the reference model for rest and 50 uses you know like fairly recent techniques such as the lares optimizer and label smoothing to achieve the target accuracy so that you don't have to re-implement all these changes we have already done it for you so a lot of our reference models scale up from one TPU all the way to a pod so of course you're unlimited to reference models so what do you build our own model our models with of course we build them as tensorflow so and when you build models do tensorflow there are really two things that you have to think about there is there is the thing that most people focus their energy on which is the network architecture itself which is running on the on the accelerator but a lot of what people neglect is the input pipeline so basically moving our training data reading them parts decompressing them parsing them performing data augmentation batching them and then sending it into the accelerators a lot of people don't think about this as a problem but really this for these sort of high-performance accelerators this sort of limits your limits performance you live because if your training pipeline slow then accelerators just I know half the time so face one build an input pipeline so this is a every simple input pipeline for a rest at 50 so you have an input function you list a bunch of files you shuffle them you repeat them and then you send it out so this this is great guess what the performance of this is this is 150 images per second so even if your R and s on the call T P you are getting 150 images per second for training which is not great because college GPUs can do 4,000 images per second so what do you do you have a bottleneck so how do you improve performance you find up at an AK you optimize the bottleneck and of course you repeat until you get the desired performance and quality Buzek Chua li provide a fairly comprehensive set of profiling tools so in this case you can see what's in this case this is 10 support so you can you can bring up a profile what's happening on on your TPU on the host and so on and so forth and then you can see that oh there are large gaps so this means that the CPU safe idle waiting for data and this is not great so a simplified like representation of what's happening on 10 support right now is something like this so in this case we have extract we have a transformer for load and then we have the training on the accelerator and they're all happening sequentially and this is not great right because what really happened what is really happening here is that you are leaving the CPU idle and you're leaving the accelerator accelerator idle and these two things are the biggest you know cost factors in your training pipeline so what you really want to do is to do something like this right you're overlapping every single step and you are utilizing all of the all the expensive bits in your computer to the full X fullest extent so the accelerators 100% utilized the CPU is only I dough slightly and the discus idle but that's fine so you know and to do pipelining is really easy so you just have to really modify one thing so you see the second last line instead of doing just do data set up prefetch and this just ensures that everything above is pipeline with accelerator training and of course you also want to do you know parallel reads because you know reading from many files is faster than reading from one and there are many other techniques that won't go into today because I don't have time so you know you can use sloppy interleaf use data set operators that we have a good performance guy intense on the tensorflow website that tells you how you can optimize your input by blinds I encourage you to take a look but this is sort of a partially optimizing pool pipelines as it's slightly longer than our simple one but this does over 2,000 images per second and if you want a fully optimized image pipeline please take a look at our over TPU sample code okay cool now comes the fun part building your model so the first way you can build your model is actually with Cara's so we have experimental Cara's integration available starting with tensorflow 111 which will be coming out in about two to three weeks so you can build so you can write your models in Cara's as per normal and the only real thing that you have to modify is basically create what's called a cluster resolver give it a name create a distribution strategy and call the carrots to keep you model function and this will transform your model to something that's compatible for the TPU and then after that you can just do the simple sort of you know model the compound model that fit and order you know the carrots goodness that you know and love and we are in tensorflow 1.12 which is the release after it is we are going to make it even easier so you don't even have the call carrots to TPU model anymore you can just call a model that compiled directly and then this will work great you don't want to use camera so you want to use something lower level so we also have a solution for that you can use something called tensor flow distribution strategy I think there was a talk about distribution strategy yesterday so if you missed that I think the video will be online soon so you should take a look at that so in this case you know this is using estimator with distribution strategy so you can write your model function like you see on the left you can write your input function like you see on the on the top right and again the only thing you really have to modify is you know a couple lines again creators create a cluster resolver create a TPU strategy and then you can just pass it into the estimator function train distribute so this will let it work on GPUs so that's all great and so are people using these these TP use people are in fact so here's a case study of an architecture search project that's done from a done by a group from Stanford MIT so they did parallel runs using hundreds and hundreds of cloud CPUs from the tensorflow research cloud program which is where we are providing a thousand free GPUs to academic researchers so if your academic researchers I encourage you to look into this program so each blue dot in this image is a run it's a is a run on the GPU training an image net scale a convolution RNN so these you know each run used to take hours and hours to train on you know other hardware but on GPUs because you have access because they had access to so many GPUs they can do you know hundreds and hundreds of these runs so whatever you do was that you're trying to search for a model that was a better fit for the data that you record say if you put electrodes in my brain and look at what my visual cortex is trying to do when I look at things so they are trying to find analysts trying to find a neural network there was a closer analog to the primate primary visual cortex so it turns out that so here's a diagram of the space that they were searching and it turns out that across a population of you know many different models they found that the red connections were sort of selective for the search versus the others and what happens is that they went back and compared two models to some of the signals that were they were recording that the biologists were recording and they found that a convolution are in ends were a much better fit for neuro signals for instance in v4 in IT than in ordinary like convolutional feed for Burnett with feed-forward models that you see in the literature today so this is a really new and exciting direction that a research group was able to do from scratch with access to lots of compute so you can not only train just train models on GPUs you can search for them basically automatically too and so finally of course cloud GPUs today call TPU version 2 today is available it's generally available on google cloud if you want to learn more about them go to google google google comm / TPU to get started alright so now Alex will present some new functionality that lets you write these accelerated code more easily Alex 