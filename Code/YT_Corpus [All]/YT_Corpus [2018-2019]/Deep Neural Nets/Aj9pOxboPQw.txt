 Anna Choromanska: Okay, so we're gonna be starting. Welcome to the Seminar Series on Modern Artificial Intelligence in the NYU Tandon School of Engineering. This series begins a new tradition in New York and neighboring areas and aims to bring together faculty and students to discuss the most important research trends in the world of AI. The speakers include world-renowned experts whose research is making an immense impact on the development of new machine learning techniques and technologies and helping to build a better, smarter, more connected world. I know I don't have to convince anyone here that AI is already having an enormous impact on society. AI researchers are helping make autonomous vehicles, our defense system stronger. Autonomous vehicles safer, our defense system stronger, and the world, in general, a better, smarter place. That's why it's so fitting that this new Modern Artificial Intelligence Seminar is happening here, at school whose mission is to create technology that can be used for the benefit of society. NYU Tandon is fast becoming a global center of influence in the field, thanks to the entrepreneurial efforts like our AI Nexus Lab, an accelerator program that supports AI focus starts up and academic centers such as AI Now, which is bringing together experts across computer science, economics, law, academia, and other sectors and which is incidentally the first AI Research Institute founded and led by a woman. I would like to thank Dean Katepalli Sreenivasan as well as my own Department of Electrical and Computer Engineering for hosting this series which promises to be a significant venue for those of us working to develop new machine learning techniques and technologies. Without further ado, let me introduce our esteemed speaker and tell you a little bit about him. Yann LeCun earned a PhD in computer science from Pierre-and-Marie-Curie University in Paris. Following a postdoc at the University of Toronto, he joined AT&T Bell Labs and in 1996, he became head of Image Processing Research there. In 2013, he joined Facebook where he is now the director of AI research. He is a member of the National Academy of Engineering and I'm happy to say he's also affiliated with NYU Center for Data Science and Courant Institute, where he's the silver processor of computer science. I'm proud to call him a colleague and hope you will join me in welcoming him for his talk: Obstacles to Process in Deep Learning & AI. [clapping] Yann LeCun: Thank you Anna, It's a real pleasure to be here. I was also affiliated with CSE actually - CE - I'm sorry. The computer engineering department here and it's always a pleasure to, you know, cross the river to come here. You know, as Anna, remarked the machine learning and AI is having an increasing effect on society and a very interesting development over the last few years is that NYU, as a whole, as you know, taken a big role in that revolution and has become kind of a focal point of a lot of interesting things happening in that area, partly because of the Center for Data Science, you know, to which many members of Tandon are affiliated and also because, you know, there's a lot of people working on deep learning and sort of the new AI including here at Tandon, and that being one example. So, you know, I think there is a huge amount of opportunities particularly in the engineering. So, my background is actually Electrical Engineering. I studied actual engineering as an undergrad. My specialty was actually chip design. It's very odd, but that's what that's what I studied at school and only later kind of shifted towards things like optimal control and then kind of led me to neural nets and AI. So, I view myself as an engineer, as well as a scientist and always like to build things which is really the purpose of engineering; inventing new techniques and building things. Okay, so let me start by kind of a summary comment that I heard from Josh Tenenbaum who's a cognitive neuroscientist or a cognitive scientist at MIT. He was at a conference I was at also a few months ago and he said "All those AI systems you see nowadays, none of them is real AI," and what he means by this is that none the AI systems that we have today can match the ability of biological systems, animals and humans to learn, you know, how to deal, how to survive, deal with variability of data in the world. So, we can do a lot with current techniques but we're very far from really sort of reaching the goal of making machines really intelligent and so, the first part of my talk is going to be a bit of a state of the art, and a little bit of history as well, and the second part is going to be about the challenges of the future and I think areas like robotics, a big important role to play there, because that's where the important problems pop up. I'll come to this in a minute. Okay, so AI today really is supervised learning. What supervised really means is that you have a decisionary tool so I use an analog synthesizer as a model for a learning machine. It's a learning machine really it's a parametrized function. You show it an image and you know run the calculation through it and it produces an output. If the output is what you want, you don't do anything. If the output is not what you want, you tell the the machine here is what the correct output is and what it does is it figures out how to adjust its parameters so that next time around it sees the same input, the output gets closer to what you want. Okay, so let's say you want to classify cars from images, or cars from images of airplanes, you know show your image of a car and the machine doesn't say car, just you know, adjust all the knobs, you know, which they could be millions, so that the output gets closer to what you want and the  interesting thing about this is that if you build a machine in such a way that it has enough power, if you want the machine will eventually figure out what the concept behind a car and an airplane is and and kind of will be able to classify any car and any airplane, you know, including if there is a lot of variability, you know the example of chairs here where there's a huge amount of variability between the appearance of chairs and the reason why that is is because of the, essentially the, appearance of deep learning over the last, of the popularization, the deep learning will last 10 years or so, even though work work on this started about 30 years ago and we started more recently about 15 years ago. The world kind of started learning about this, you know, about seven years ago or so, not even that. So, classically when you want it to do pattern recognition, you would feed the image to a feature extractor that would generally be built by hand and then you would plug a trainable classifier on top of it, so those kind of rows painted pink painted boxes are supposed to mean that they are adaptable; that you can be trained. Alright, so the top, at the top is kind of the classical model of a pattern recognition and it's used still very very much in a lot of practical applications of machine learning today. At the bottom, it's kind of the deep learning model and the idea of deep learning is that you you you build your model as kind of a stack of adaptive, trainable parametrized modules and you train the entire system end to end, so as to minimize some objective function that measures the discrepancy between what you want and what you get. But, the point is the entire system is trainable and that allows it to kind of process the data in sort of a hierarchical way and I'll come I'll come to why that's a good idea. So the next question you have to ask is what you put in those boxes and it's deceptively simple, essentially a simple neural net is a succession of such boxes and there are really two types: one type is just a linear operation, right. So imagine it's  text, an image or an audio signal is coded into a vector, you take this vector and multiply it by the matrix, you get another vector possibly of a different size, okay, that's a linear operator and the coefficients in this linear operator are going to be the knobs that we're going to adjust by learning, so that produces an other vector you can think of each component of this factor as being a weighted sum of the component of the input vector weighted by the corresponding row in the matrix by which you you multiply this vector. Then the second operation is a pointwise non-linearity, and in recent incarnations of neural nets very often this point- wise non-linearity is the one shown at the top, the top right, is called a value until you have wave rectification. The nice thing about talking to engineers is that in order to explain what half wave rectification is not so for computer scientists. So you have linear operation, point wise nonlinearities, which are very simple and then you stack multiple stages of these pairs of operations: linear, non linear, linear, non linear; you can show that with only two layers of these, so linear nonlinear linear, you can approximate any function you want as close as you want as long as the middle layer is large enough, okay and that's something that people have relied on for many years, arguing for the fact that you never need more than two layers because you can approximate any function you want with two layers, that actually is not entirely true because for most functions you want to compute the middle layer might need to be extremely large and so classical models like or what I've become classical models like sycrono- machines, super vector machine, things like this are limited to two layers, in fact, only the second layer is really active and that's why you miss a power. It's the fact that it can only do basically two steps of computation and one nonlinear step computation so it turns out there's a lot of interesting functions that require multiple nonlinear types of computation and that's why deep learning is really interesting and it's very difficult to kind of turn this into a theorem and sort of have some theory about why you need multiple layers but intuitively makes perfect sense. Most functions require multiple steps, computer scientists know this right. It's very rare that you can write a program to compute a function that will only require two instructions, right, generally you have loops yeah things like that right. Okay so training supervised running really comes down to minimizing an objective function. You show an example to the system, compute the output, compare with the output you want, that gives you a value for the cost function/the objective you want to minimize, the error between the what you want and what you get, adjust the parameters a little bit then show another example adjusting parameters a little bit and show another sample etc etc and if you do this what you're doing in effect is what's called a stochastic gradient optimization algorithm shown at the bottom, where you basically replace each parameter by its own value minus the partial derivative of the last function you want to minimize with respect to this particular parameter multiplied by some constant/a step size and because you do it on the basis of a single sample that's called stochastic gradient because you get an estimate of that the gradient of the objective really the object you want to minimize is the average of all the samples but because you do it one sample at a time you get a noisy estimate every time or you do it with a small batch of samples at a time, you get a noisy estimate so it's called stochastic gradient for that reason and so pictorially you can think of the cost function as being you know some sort of landscape in a high dimensional space and what this algorithm does kind of find the minimum of that you know the valley, the bottom of the valley and that landscape but kind of stochastically, you can stochastically every time being even kind of a noisy estimate of the direction of steepest descent. So next question I ask is how do you compute this gradient and it's the so called back propagation algorithm which I'm not going to go through because it would take too long and many of you I'm sure know about it it's basically based on chain rule so it says if you have if you know you know your your network is basically a graph of computational nodes and you know how to compute the output of each of each node of computational blocks, here in this case you know how to compute the output of each block as a function of its inputs and its internal parameters and it turns out if you know the gradient of the loss function with respect to the output of that module you only need to multiply by the Jacobian matrix of that module with respect to its input or with respect to its parameters to get the gradient respect to the input respect to the parameters so that gives you the suggestion that you can have some sort of recursive algorithm that goes backwards you know through the graph and kind of propagates gradients this way and it's called a back propagation algorithm it's basically just chain rule but it turns out you know if you express it graphically you you express your system as a graph of networks you don't have to figure out how to compute the gradient as long as your system knows how to compute the gradient for each module you can assemble very complex networks and not worry about how to compute the gradient; that will be done automatically by the software you used. So, all the deep learning framework, basically what they do for you is they allow you to build a graph of those networks and then they automatically compute the gradient for you. Okay so, linear operators are good but if you have an image, an image might have you know a million pixels and you know if you multiply an image of a million pixels by a matrix and the output is also a million variables let's say, that's a big matrix right? 10^12 terms so what we need to do is it's actually reduce you know put some constraints on this matrix so that it becomes manageable and cognitional nets which are used somewhat universally now for image recognition, as well as for all kinds of other applications like speech and even text translation, things like this. Our, the main idea behind that is to basically constrain the matrices so that they're basically two piece matrices and that they can you know have fewer parameters and reduce the amount of computation. So again, the nice thing about talking to electrical engineers is that I don't know to explain what convolution is but but the basic idea of a cognitional net is you take an image and you take a neighborhood of pixels and you have a set of coefficients that you multiply this little patch of pixels by and I give you a weighted sum okay that's one term in the output so that gives you one output value and then you swipe that little window of pixels over the entire image and every time you compute the dot product with the coefficients and record results so, that's the discrete 2D convulsion and you get the you know if the input is here at the bottom the bottom right you see the little completion kernels, the set of coefficients and you see the result here that's called a feature map that you pass result to a non-linearity, let's say value or Sigmaoid or something like that So at the bottom here on the on the top left you see the image, then you see the four feature maps, a lot of result of convolving the image with four different kernels and the coefficients of those kernels are the result of learning right, along with with back propagation all the way through and then there is a second operation called pooling which consists in sort of aggregating the response of the filter of a small area by a max or an average or an L2 norm or something like this and then reducing the resolution and the reason for this is to impose a little bit of shift and distortion invariants in the system and this is very much inspired by biology you know we talk about convulsions and invariance and blah blah blah but in fact the architecture of this is completely suggested by the architecture of visual cortex. This classic work in the 60s in fact Nobel prize-winning work by Hubel and Wiesel about the architecture of visual context where what they showed that neurons in the visual cortex of cats look at local areas in the visual field and and are basically repeated all over the the visual field so that's where the idea comes from and there were models that try to emulate this know back in the 80's. Fukushima's new neutron, for example. So, neuroscientists called this simple set of complex cells the the sort of convulsion like operation and the pooling. So here it's a cognitional net in action and you know each vertical column is a is a different layer; it's an input on the on the left and you know if the input shifts by four pixels then the third layer after pulling shifts by two pixels because it's a sample by factor of two and then you know two layers up it's shifted by one pixel and then couple of years up and maybe 1/2 clicks or something like this so as you go out the layers the amount of distortions caused by shift in the image or distortion in the image is reduced and it's easier to model which is why the systems kind of more amenable to do to do image recognition or kind of invariant somewhat variant recognition. So this seems like an obvious concept, but but back in the 90s very few people were convinced this was this was a good idea. So for example, this is a picture taken in 2005 which was the dinner resulting from a bet between Larry Jackel sitting on the right and Vladimir Vapnik who is in the back here whom I think you will hear talk at this seminar in a few weeks. So he's a former colleague of mine at Bell Labs and the bet, Larry Jackel was the head of the department at the time, and Jackel bet one fancy dinner that by March 14, 2000 (this was in 1995), people will understand quantitatively why big neural networks working on large data sets are not so bad. Understanding means that there will be clear conditions and bounds and things like this; the kind of theoretical arguments that Vapnik likes. Vapnik bets one fancy dinner that Jackel is wrong but if Vapnik figures out the bounds and conditions Vapnik still wins the bet so this was a silly, failed attempt by Jackel to convince Vapnik that you should do the theory for neural nets. He completely failed and and he also lost the bet because we still don't quite understand really kind of the generalization properties of neural nets. I mean we understand they work, you know, there's no problem with this but but not to the same extent as simpler models just because you know they are much more complicated and so we don't have simple theory for it; we have theories but they are not that useful. There's a second bet Vapnik bets one fancy dinner that by March 14, 2005 no one in his right mind will use neural nets that are essentially like those used in 1995. Jackel bets one fancy dinner that Vapnik is wrong so Jackel lost the first one, Vapnik lost the second one and so in 2005 there was, there was a dinner and they shared the bill and Louboutin and I, Louboutin is the man on the left here, Louboutin and I just enjoyed the dinner. Okay, so you know since the Scotia people here I see fortunately here the first row working or working robotics, work a little bit around, talk a little bit about some robotics project that we did and this is a little story called Green Back. You know about, ten years, even more, the end of the project was about ten years ago, this was a project called LAGR (Learning Applied to Ground Vehicles) sponsored by DARPA and the idea was to use machine learning to derive robots of the type you see on the top left off-road, so system should be able to you know, through vision, kind of figure out what's an obstacle and what's not and you can do this with stereo vision for short range but it doesn't really work with long range because with stereo vision you can only you know, depending on the baseline of the two cameras you can only figure out it's something sticks out of the ground about ten meters out but not much more than that. So what you see here in the middle column are the result of applying stereo vision to this and you know it starts after 10 feet meters whereas if you apply the neural net its monocular so it works all the way and the cool thing about this is that you can train a neural net with the data produced by the stereo vision system so you don't have to exclusively supervise it, you basically let the robot run, cutting data from stereo vision and then use that to train the monocular system to tell you whether something is an obstacle or not and so it was an accomplish on that, which you know the idea of this was that you would apply the cognitional net basically on small patches over the entire image and you would classify every pixel if you want. So it's a kind of a task in computer vision that we call semantic segmentation which consists in categorizing every pixel in the image by the category of the object it belongs to. So this was running on, this was a, you know, robot built by N Reich at CMU around 2005 and so the processors were not what they are now. Nothing new or anything. So we could run the neural net to the back one frame per second so we had a short-range stereo system for kind of avoiding unexpected obstacles and then a long-range vision system, we run at about when one hertz and so this is the robot; this video is accelerated twice and these are annoying PhD students making the life of this poor robot impossible but they are allowed to do that because they actually wrote the code and this is Pierre Simon who is now working on robotics at Google Brain in California and Raia Hadsell who leads the robotics research group at DeepMind in London. So after this we decided that we can do semantic segmentation for real so instead of, whoops, I'm not sure why my slides are shifting without telling it but anyway, so we decided that we would work on semantic segmentation for real and essentially have a system that labels every pixel in an image not as to whether it's traceable or not but with the category of the object it belongs to. If you have an image like this and you'd like to be able to tell whether it's the road or a car or building or the sky or trees or whatever pedestrians, that kind of important and so eventually we build such a system. It's again a convolution net, kind of swiped over the entire image, you can run this very efficiently. With claim off our bay, who is now the VP of Machine Learning Services at Nvidia. He implemented an FPGA board that runs a system at about 20 frames per second and this is kind of the demo of this. This is state of the art at the time. Down at Washington Square Park, some of you might recognize Washington Square Park. So it's making stupid mistakes, like it's classifying you know some areas like Washington Square Park has desert or beach. There is no beach I'm aware of there but, it would certainly make the campus more attractive. So you know it's not perfect and since then those technologies based on some convolutional nets have a huge amount of progress. But you know, this was 2010 roughly and we could run this at 20 frames per second. So I gave various talks about this around at that time and it gave some ideas to people working on self-driving cars; they say they realize perhaps they could use this for self driving cars so in particular it is the company called Mobileye in Israel that was working on vision systems for cars. Created by Amnon Shashua, who's a professor in computer vision at Hebrew University and they started playing with convolutional nets and were getting much better results with whatever technique that were using before and they had a problem because the chip they had designed wasn't really designed to run convolutional nets. So they kind of had to shoehorn convolutional nets onto their current chip. Now they have new chips that do differently. But, it went, it worked well enough that the Tesla cars that appeared in 2005, 15, 2014-15 actually used their chip for autonomous driving. After that, the divorce was various reasons That also prompted Nvidia, who built the GPU cars, to get into the business of self-driving cars. They realized you know they could you know build hardware for this and they also realized that you couldn't just sell the hardware; they had to build software on top of it. In fact, one team that does research on self-driving cars at Nvidia is, you know former colleagues of mine from Bell Labs and worked in New Jersey, in the same building where I used to work at Bell Labs and they are doing pretty amazing stuff there and Anna's husband actually works there too. Okay, so what happened in 2012-13 is that people figured out, I mean, first of all, there was you know much bigger data sets that became available like ImageNet for image recognition. So ImageNet is a dataset with 1.3 million training samples, it's got 1,000 categories of objects you know, the dominant object in the picture and this is the kind of data where convolutional nets really shine. Before that, the datasets were so small that you know, techniques that build more things by hand were actually more successful than convolutional nets. We couldn't really get kind of you know record beating performance with convolutional nets before that. But large, with large datasets that really kind of blew everything out of the water and the second reason was the efficient implementations are convolutional nets on GPUs which allowed us to build much much bigger commercial nets. So the work by our friends at University of Toronto and Krizhevsky, Ilya Sutskever and Geoff Hinton really kind of open the eyes of the computer vision community on this and sort of created a kind of a revolution which is funny because in 2011, we submitted a paper on the semantic segmentation to CDPR and it was rejected mostly because people had no idea what a convolutional net was, the reviewers and they didn't believe that technology they'd never heard of could work so well. Three years later, maybe three four years later, 2015-14 you cannot get a paper accepted as a CDPR unless you use convolutional nets. So, you know it's been a big revolution. Now what's happened over the, over the last years is that so many people got interested in this that they all came up with really cool ideas, new architectural ideas on how you kind of assemble those functional blocks into, in various ways and the architectures of convolutional nets for image recognition have something like 50 layers or 100 layers. The user site called ResNet originally proposed by Kaiming He from Microsoft Research Asia, he now works at Facebook in California and they you know, it kind of, this revolution has led to incredible  improvements in recognition rates. So, on ImageNet for example, before convolution nets, the best error rates were around 25-26%. This is, you count an error when the correct category is not in the top five among the 1000. Then when one year later, the Krizhevsky paper brought this down to about 15, one year later it was around 10, one year later it was around 5 now it's below 3 and that's super human. So if you ask people to categorize those images, they'll make just about as many mistakes and most of those are images are ambiguous, in you know, in the first place; you don't know what the relevant object is. So one question we might ask is what is it that makes convolutional nets and those hierarchical data structures actually good and in my opinion it's probably because they reflect the compositionality of the real world. So the real world, particularly the perceptual world, is compositional in a sense that an object is formed of parts and parts are formed of motifs and motifs are formed of collection of edges and so if you have the low level sort of oriented edge detector, which is what those convolutional nets end up learning and what your brain is doing and what every actually even handcrafted computer vision system will do, then you can detect combinations of those edges to detect you know common shapes like corners, circles, grating, things like this and then those assemble to form parts of objects so you detect combinations of those and that's kind of what this architecture convolutional net reflects. With a pooling operation, that allows the position of those little features to kind of vary a little bit and so you get this sort of it elastic perception and so you know, it sounds like a conspiracy that the world is so designed that it's easily understandable by architectures that happen to fit in your braincase which led to Keemun with a applied mathematician at Brown to say "The world is compositional or there is a God." This is kind of a play on a something that Albert Einstein said; he said "The most incomprehensible thing about the world is that it is comprehensible." The world could be completely random and there was no way we can make any sense out of it and so it seems like a conspiracy that there is you know some understanding we can have of it. Okay so moving on to a slightly more recent work, this is a little bit of a snapshot and state of the art of what people can do in computer vision and this is some work done at Facebook research in California, in Menlo Park. A technique called Mask R-CNN and this is again a semantic segmentation system but it doesn't just label each pixel with the category it labels it with the instance of the object and it gives you a mask for the object so I'm not going to go into the details of this architecture but it's conceptually quite simple. You can do things like this so it will you know again it's kind of a sliding window convolutional net. It looks at multiple scales and it you know it kind of focuses on different areas and the output is not just a category for the for the pixel in question it's also a mask of the object that it believes it's looking at. Okay, for every location so you put this together and you get a you know a mask for every object in the picture, the visible dog at the bottom, the people, etc. There are individual people, you know, the wine glasses, wine bottles, all the computers in the back, the tabl,e you can count the sheeps, etc. I mean it works amazingly well and this is in my opinion really one of the most kind of impressive progress of the last few years in computer vision, showing what people can do with this. We can track key points on bodies and sort of reconstruct the pose so this is the result. This code is open-source by the way so, if you go to this URL you can just download the code, play with it, you can you know train it to your purpose. It's taught C++, SQL. You can apply convolutional nets to 3D data. This is some pictures from also semantic segmentation competition actually run by Stanford I believe, where a team from Facebook in Paris actually won that competition using 3D convolutional nets and the problem with 3D is that when you have no compancy map or a point cloud of 3D points from LiDAR for example, or a depth sensor most of the voxels are empty and so if you run a convolutional net that on the entire volume you kind of waste your time just multiplying zeros with you know with all the numbers. So they figured out a way to kind of just follow the areas where there is voxels that are non-empty and and you know there by kind of increasing the efficiency and the results with that competition just to show that convolutional nets can be used for something else than pixels. The one of the systems that Facebook uses for translating languages from when one language to another is actually a convolutional net. It's a gated commercial net so it's all more complicated and than traditional convolutional nets. The architecture is shown on the right. I'm not going to go through it but it basically takes every word as a vector essentially and it learns the mapping from words to vector as well and then that goes to a convolutional net and then this convolutional net has some gating mechanism that can sort of you know regurgitate words in a different language in the right order. So if you translate you know English to German, you remember to put the word verb at the end and stuff like that. So it's, it's interesting that something that was designed for perception and vision can actually be used for you know producing sequences and that are not you know pixels. But you know, text has a common property with images which is that this kind of local correlations, right the you know the one word can follow an other and there is some correlation between neighboring words but faraway words are less correlated so it's kind of like pixels and images. Now a lot of the things that we do at Facebook and you know I I wear two hats, I have you know one foot at NYU, one foot at Facebook, I should say one and a half foot at Facebook and one half foot at NYU, is is that all the research we do is open. We publish everything and we distribute a lot of code and open source so the one that's probably most useful at the moment is Pytorch which is a framework for deep running; it is very flexible. And it would be nice if my laptop didn't die, okay apologies. Alright, okay, now deep learning is this idea of assembling modules in kind of a fixed graph but really what people are moving to its something some of us would call differentiable programming and the idea there is that you don't want this graph of operators to be fixed. You want it to be defined by program and you might want it to be dependent upon the input you're looking at so for every new input you're seeing the architecture of the graph will change and so why is it called differential programming? It's because basically you write a program that computes the ooutputs and this program calls functions that could be the equivalent of those modules, functional modules, I told you about and there is you know in the background something that figures out how to basically backpropagate gradients through your program. So that whatever program you write, whatever or I should say sub-gradient for the mathematicians in the room because a lot of those functions are not entirely differentiable, so lets call it sub-gradient and be done with that. So, but the idea that you know the program is data dependent and you know builds the graph dynamically and you can still propagate gradient through it. So let's call this differentiable programming, you can think of this as a slight generalization of deep learning, the same way probabilistic programming is a generalization of probabilistic graphical models for example, and people are interested in this particularly in the in the context of natural language processing or reasoning because we think that's kind of required for certain types of reasoning. So, an example of this is neural nets that are augmented by modules that basically act as a scratchpad memory like a RAM, if you want. And so that's kind of a concept that popped up in a few years ago with the memory network idea by Jason Weston who is at Facebook, stack-augmented RNNs by Joulin and Mikolov, also at Facebook, neural Turing machines and differentiable neural computers by Alex Graves from DeepMind. Those ideas kind of popped up more or less at the same time, within days of each other on our calendar which is funny and the basic idea is that you have a recurrent neural net or a neural net that can feed its output back to its input if you want so we can have some dynamics and it's you know accessing a memory on the side. So, a memory is just a different type of set of modules that behave in a particular way and this, you know, again several architectures which I'm not going to go into the details of but just talking about this idea of differentiable memory. So think of the circuit of a RAM, a RAM chip. Okay, a RAM chip has an address decoder which basically compares the input address with all 2^n possible bit configurations and activates one line in the in the chip memory and then what you read out is the sum, the weighted sum of all the memories except all the weights are zero except for the one who is you know well is addressed. So, the differentiable memory used in those memory networks is exactly the same idea, the input is a vector, you compute the dot product of this vector with a bunch of key vectors which are that's like the address decoder. So the dot product gives you, you know, how well the input vector matches each of those keys. You get a bunch of numbers correspond to dot products, you run that through a softmax which turns this into a bunch of numbers between 0 & 1 that sum to 1. It's kind of like a gives a distribution. The formula is on, is on the left and then you use those coefficients to you know, in a weighted sum of values which themselves are vectors produced by the memory. So, what you get is a weighted sum of the values and the memory weighted by those coefficients resulting from the dot product of the inputted address with each of the keys. So it's kind of a you know, soft associative memory if you want and the cool thing is that you can propagate gradient to that, you can learn the keys, you can learn the values, etc, so you can build a neural net. So this is kind of a recurrent neural net and fold it in time which accesses this memory three times in this case, if you look at the the right part of the picture here. So you get a, for example, you want to build a system to answer questions, you put a question at the input which is really encoded as a vector and then the system sort of computes an address, accesses the memory to see if the answer is there then the gets the answer, runs the recurrent net once or twice, then access to the memory again, recurrent net access once or twice etc, and that would allow the system to basically access the memory multiple times if it needs to connect multiple facts to answer a particular question. So my colleagues at Facebook came up with this thing called the bAbI task. You can, it's kind of hard to read but in the middle at the top there is something, a little story. Sam walks into the kitchen, Sam picks up an apple, Sam walks into the bedroom, Sam drops the apple, where is the apple? And the answer is in the bedroom so after kind of track you know what goes on, etc. You have to make simple inference. Brian is a lion. Julius is a lion. Julius is white. Bernhard is green. What color is Brian? If you assume all lions are white, Brian is white. So, we came up with 20 different types of questions of this type that people might want to answer and or you know, want machines to answer and training a memory network to answer those questions, they they can basically solve 20-19 of the 20 types. There's another type of network called entity network that can solve all 20 that we came up with more recently, but which I'm not going to it's very similar it's also kind of a memory augmented neural net so that's really nice because that's the idea of kind of marrying reasoning with with learning and reasoning is something traditionally I was very strong at so traditionally I, you know, expert systems, world based systems, this is basically completely concentrated on the whole idea of doing reasoning properly. But, then there is the problem of knowledge acquisition. How do you feed a machine with enough knowledge to reason properly and that was always the main problem, the main issue with expert systems; it's how much effort it takes to actually reduce everything to rules, etc. So what you'd like is machines to be able to reason but at the same time learn how to, how to do that and learn facts. Here's another example also done at Facebook in Menlo Park by Justin Johnson, who was at the time an intern and a whole bunch of characters from Facebook and here the problem is to answer questions like you know, the the picture at the bottom here, is there a mat cube that has the same size as the red metal object? So to be able to answer questions like this, you kind of have to you know count the number of objects of a particular type, detect them first, etc. So they came up with this architecture and people since then have come up with even better ways of doing this but the idea is a little similar so imagine you have a question that the system needs to answer, something like are there more cubes than yellow things. So you take the image, you run a convolutional net on it which you know we're going to train for this whole system then what you'd like is, you'd like a little block, a little block, one of the green blocks here that filters colors, another one that filters by shape and detects cubes. The first one filter is the yellow objects, then there is a block that counts those objects, then it's a block that compares those two numbers and you get the answer to your question. The problem is it's one of those examples where this graph of operators needs to be dynamic, it needs to be determined by the data and so what I have here on the left is the question goes into a recurrent neural net that then spits out a specification for a graph essentially, you know, it's a kind of a tree type I mean it's sort of a graph like syntax that specifies a graph and it's it's kind of hard to backpropagate through this operation. More recent work on this by others, by Aaron Courville at University of Montreal, for example have done this in a fully differentiable way so, here there is a bit of hocus-pocus you have to do to train this system end to end. But, the wonderful thing is that the only way you have to train this system is you give you a question, you give it an image, you give it the answer and it basically figures out how to kind of organize all this. It is a little bit of prompting at the beginning to tell it what the graph looks like for a few examples, but it's able to generalize pretty well so that's one of those kind of really eye-opening examples where you can have essentially, the neural net producing another neuron dynamically and then have this neural net solve a particular problem and it seems like, it sounds intuitive that we might have something like this in our head you know head, right. We face a new situation, we kind of configure our you know reasoning engine to kind of figure out you know. analyze the current situation. Let me skip ahead a little bit. When I'm skipping ahead I have to apologize to an abacus, it's work that he and I did together a few years ago but I want to get to obstacles to AI so as I said before animals and humans learn much more efficiency, efficiently than all the machines that we have, and you know, why one may wonder what kind of learning algorithm does the brain use or what kind of principle, what kind of paradigm, you know it may not be just a question of algorithm and one thing that humans and animals seem to have is some sort of common sense ability, right. The fact that we kind of know how the world works and so we can fill in the blanks for a lot of things, we don't have to be told everything, you know. Right now, most of you are seeing my left profile but you can figure out what my right profile looks like because you know that faces are mostly symmetric, you know you have a blind spot in your visual feel where your optical nerve punches your retina and you're not even conscious of it because your brain kind of fills it up. You know, we have a very good ability to predict the future and predict the consequences of our actions. That's what allows us to plan ahead, in fact, you could say because of this that the essence of intelligence is really the ability to predict; it's the ability to build models of the world right, by learning, by observation, by interaction. So, perhaps what we did, our mission is to do is it's kind of predictive learning. It's a little, the phrase here is not exactly, it does not exactly reflect what it means. Perhaps a better name would be inputative learning but that sounds a little pedantic. So, it's basically the idea that the system should learn to predict every variable it doesn't currently observe from all the variables that it currently observes, so we're predicting the future from the, from the present and the past, predict all the things you can't directly observe right now but you observe by you know moving your head so right now some objects are, you know, the front of the stage is hidden behind the lectern but you know I can move and figure out what it looks like so I can sort of you know, train my brain to predict what it looks like and see what it is. My brain certainly can predict what the view is gonna, is going to look like if I shave my head 20 centimeters to the right and in the process of learning how to do this, we probably develop the notion of depth right. So, depth can be learned naturally by predicting how the world changes when you move your head or if you look look at the world with two eyes or so but even if you had one eye it would still work and so you know we, we learn this as babies right. We look at the world, we can hardly act on the world when were babies but we learn a huge amount of stuff about the world just by observation. We you know, we learn object permanence, we learn you know intuitive physics you know, baby orangutans also learn things. Here is a baby orangutan, it's playing, it was being played a magic trick, put an object in a cup, we move the object without him seeing and then showing the anti cup and he's rolling on the floor laughing. That's a baby orangutan. They are almost as smart as we are and, you know if you know in my lifetime we could build machines that were you know just a little bit you know as smart as orangutans there would be complete success. In fact, I think if there were as intelligent as a house cat I think would be complete success, you know. So if you show the the picture of the top left to a baby, I stole it from Emmanuel Dupoux, who is a cognitive scientist in Paris and if you show the six month baby or you know four month baby the scenario on the left here, you push a little car off of a support and it floats in the air, of course is a trick. Baby said "sure, that's the way the world works, no problem." After six or eight months, you figure out what gravity is and they look at this fact just like the baby on the bottom left. They're completely surprised by this because they figure that can't possibly happen. Every object is not supported should fall. So in fact, Emmanuel built this chart of at what age babies learned through different concepts like object permanence, the fact that there are animate and inanimate object that's about two months, three months stability and support, at five months gravity inertia, conservation of momentum that's about eight months, you know things like that. So you know the object rigidity things like, so basic you know concepts about how the world works are learned in the first few months of life, mostly by observation with very little interaction with the world and we don't know how to do this with machines but that's what we should do. So that led me to this, you know, there is those three modes of learning that people use in the context of machine learning. One is called reinforcement learning, so reinforcement learning is when you get a machine to do something, you don't tell it the correct answer, you just tell it you did good, you did bad, right. You give it one scalar reward or you know feedback once in a while and it could be at the end of a long sequence of actions which makes the whole thing very difficult. The supervised learning where you tell the machine where the correct answer is. Well, we talked about this and then this kind of other modes of learning which generally we can call supervised learning but really that doesn't really tell what we do, but maybe predictive learning that what machine learns to predict every variable it doesn't currently observe from the ones it does observe and in that platform there's a huge amount of feedback that the machine gets, right. If I, if I put it if I try to predict what the world looks like when I move my head and I do move my head the amount of feedback I get is all the pixels in the world, it's not just one scalar value once in a while; it's all the pixels in the world. So, it's a huge amount of information that the machine gets and that's according to various people like Jeff Hinton, this is the only way we can train very very powerful learning machines to learn anything about the world is, is if we give them a huge amount of feedback every time we train them. So that led me to this slightly obnoxious slide that I'll show every time now, because become a meme in the machine learning community where if intelligence is a cake, the bulk of the cake version wise if you want, is unsupervised predictive learning and the in terms of the amount of feedback you give to the machine, the you know icing on the cake is supervised learning and the cherry on the cake is reinforcement learning because you're only giving a very small amount of feedback classical reinforcement learning and that's, that's why you know classical reinforcement learning doesn't apply to the real world. The world it works really well for things like go and chess and things like this but for the real world if you want to learn to drive a car and you have to drag the car off a cliff 50,000 times before the machine learns not to drive the car off the cliff, you know, we seem to be able to do much better than this because why? Because we have a model of the world allows us to predict what's going to happen if we run off the cliff so we don't actually run off the cliff; I don't need to do this to predict what's gonna happen. Our model of the world is good enough to prevent us from doing this. So what reinforcement learning systems need to have is a model of the world, they need to learn a model of the world and that's called model-based RL. But model B is learning in general it's not just limited to reinforcement learning, it's it applies to all kinds of things. So we first knew RL works well for games because games, the world is very simple. You can simulate it very quickly, you can, you know, crash your car 50,000 times doesn't matter and so it works really well. For group of course it's amazing how it works. It works for you know games like Doom, it's starting to work for games like Starcraft, although it's still very preliminary. But this whole idea that you should have predictive models is very classical in optimal control. There's a few people here working in optimal control and the way you do an optimal control system is you you have a model of the thing you're trying to control that predicts the next stage of the system from the current state and the action you're taking or the command and then you optimize the trajectory, this way this those techniques exist since the 60s. So why can't we do this with with learning machines basically having learning machines learn the model of the world and so clearly the next revolution in AI will be unsupervised and the concept for this like on some earlier choice from Berkeley so you know will will need AI systems I have you know in the AI system tries to optimize some objective does perception it it acts on the world etc but inside the agent you need some sort of world simulator some way for the machine to predict what's gonna happen next and I'm just going to quickly show you one example of this it's the don't be scared by the fact that I'm skipping over a bunch of stuff and an first point point to towards a problem the problem is that the the is that the the world is not entirely predictable. So if you have if your world consists of two variables y1 and y2 your entire world is you know consists of two pixels and the only dependency between those two pixels the data points you observe you know obey this law so if I give you the value of y1 you can to some extent predict the value of y2 but you know if I give you the value of y2 - it's probably 2 values or maybe not at all for y1 that are possible and I priori you know I don't you know you're not going to know if you observe y1 or y2, you're gonna have to predict the other one from whatever the one you predict. What is the the best way to represent the dependency between y1 and y2? Possibly through a contract function. So this is the idea of an implicit function, you don't have the function that computes y2 for y1 and another one computers y1 from y2, your function that takes y1 and y2 and tells you whether those two values are compatible or not so call this a contrast function, an energy function and negative log likelihood whatever you want to call it but it's a something that will take loop values on the data points and take higher values outside the data points and the animation here describes a house with the learning process for such a function with me right so it's a function with two inputs y1 and y2 and it produces a single output which is the whether y1 y2 have compatible values so it's easy enough to train a model to take low energy values for points you show it but it's hard to actually push up the value for points you don't observe and why don't we made a list of you know seven different methods to do this which I'm not going to go through, be reassured and I'm only mentioning one which is actually not on this slide. I'll call adversarial training and that's because I think it's one of the most exciting developments in machine learning over the last 10 years, adversarial training. It allows us to learn predictive models under certainty so so here is a situation for example we have a predictor. Predictor looks at the past so it looks at the segment of video for example it's trying to predict what's gonna happen next right so they'll be a good way for a machine to learn intuitive physics for example so let's say I put a pen, I hold a pen on the table and I tell you I'm going to lift my finger you can predict that the pen is going to fall but you can't really predict in which direction if I if I do it right. So the way we will build a system this time it's at the the projector is going to make a prediction and to make a prediction it's going to have access to a source of random vectors called Z here in this case and so it combines the x and z, run this through some neural net and makes a prediction for what the world is going to look like half a second in the future and we can train the system because we can just let time pass or observe the result and then you know train the machine to produce this. Problem is of course is that if the machine predicts that the pen falls to the back into the left and what actually occurs is the pen falls to the back in the right; machine was not technically wrong it's quite you know quantitatively wrong but you know it produces the wrong picture but you know you kind of predicted the right thing so how do we tell it okay you do it wrong, but really I'm not going to punish you for it. So what you'd like is among the set of all possible futures represented by this red ribbon here in the the space of predictions you'd like to tell the system if you made a prediction that is on the red ribbon of plausible futures I'm not going to punish you even if your prediction is different from the thing that actually occurred it's only if you make a prediction outside of this red ribbon that I'm gonna punish you, okay I'm gonna make you pay you know the objective function now the thing is you don't know what this red ribbon looks like so the idea of adversarial training is you train a second neural net to learn the location of this red ribbon and it's you know it's very much like this contrast function I was telling you about earlier and the predictor is actually used to produce hypothesis samples that the other one is going to tell whether they're real or not so that's the idea of adversarial training. The this contrast function, the normative predicts is from contrast functions called discriminator and we have a generator that makes predictions so initially the generator makes bad prediction and we train the discriminator to produce a high output, high energy output and then we show the discriminative what actually occurred in the world and then we train it to produce a low output okay so the discriminator basically learns the contrast function between things that actually occur and things that for now don't occur because they predicted by the generator which initially doesn't do a good job so if you turn to see the generator gets the gradient from the discriminator and so it knows how to change its prediction so that it's communicator will think it's real. Okay so as we train the discriminator, the discriminator learns this contrast function and as we trained the generator, the generator trains to produce those green dot closer to the real dots and learns to do good predictions. That's basically a virtual training, you train two networks. So it breaks all the things we do in machine learning because instead of now minimizing an objective function we try to find a Nash equilibrium between two functions, one that is used to train the generator and the other one used to train the discriminator and you know that turns out to be a lot more complicated than  minimizing functions so people are working a lot on trying to make this process stable essentially. But when you manage to make it work it works amazingly well. Those are non-existing bedrooms that were produced by a generator that doesn't actually look at anything other than random vectors and is trying to produce images of bedrooms so those are randomly sample images bedrooms. This is work from a few years ago. Soumith Chintala is at Facebook, the other guys are Google I believe. There is a huge amount of progress there, I'm only sure you kind of early work on this and this is trying on image net so you can't really actually recognize the objects these are trained on dogs you can you know it's kind of funny dogs. More recent work on adversarial training by various groups across the world have produced images that are kind of amazing, high resolution megapixel images of the faces that are incredibly realistic trained on celebrities so you get a non-distinct celebrity that kind of looks nice. This is work at Nvidia I believe, it's a lot of really really cool work there in this area but we are interested using this for modeling the world so if you use the square to train the convolutional net to let's say to predict the next frames in the video you get blurry prediction because the the system cannot really predict what you know what is going to happen in the world and so it produces the average of all the possible things it thinks might happen and that's a blurry image so that's the you know mean square or you know traditional criteria produced the kind of result you see at the top right here but if you use a bit of adversarial training you get much sharper predictions which may or may not be right but they look reasonable. So those animations there are six frames, the first four frames are observed and the last two frames are predicted indicated by the red contour and the predictions look okay. These are video segments shot in various New York apartments and as the camera rotates the system is to invent when the apartment looks like and so it look you know it figures out what the bookcase is supposed to look like as the camera turns, the couch is supposed to continue, you know that there are pictures on picture frames on the wall things like that. More interesting if you work on self-driving cars you'd like to be able to predict what the other cars next to you are going to do before they do it so that it will allow it, allow you to drive defensively so these are examples again. A few frames observed and that you predict three frames and in future, there are spaced from I think a 6th of a second apart so you predict half a second and the system predicts that you know if pedestrians start crossing the street they're going to keep crossing the street, if the car starts turning life is gonna keep turning, that the scenery is gonna keep moving. So slightly more interesting with a model that I'm not going to explain it's a little video so this is a little game the paper is gonna be or not on archive in a couple days. This little game where you pilot a spaceship and you're supposed to go to one of those colored space station and there is you know a planet with gravity and you know you don't have enough threats to actually you know go against the gravity so sometimes you crash but you have to figure out you know it's hard to figure out the trajectory and what it does is that it builds a model of the dynamics by just learning, observing what happens you know with sort of various random actions and then it uses this model to do planning so it's you know very similar to what people do with system identification and optimal control except here it's everything is learned with a neural net. Essentially you're counting on that and this works much better than sort of classical reinforcement learning methods that require on the order of 4 million interactions with the environment to get anywhere and don't actually work really well whereas this system you know with 800,000 actually works. Ok ok, I'm going to stop here and thank you for your attention and I'll take your questions. Audience: Thank you Yann. Since we have you here I was wondering if we could comment on max pooling operation and the work of George Hinton in terms of capsule networks and see if you could comment and then it's good to use max pooling where it says capsule networks and going back and forth basically Yann LeCun: Right so there is this new paper, recent paper by Geoff Hinton and a couple of his students on something called capsule networks and it's a you could think of it as a new type of convolutional net where the pooling is replaced by another operation that it's kind of more explicit and tries to essentially directly manipulate the geometric parameters of parts of objects into objects so normally in a classical convolutional net, you do max pooling so you take the response of the filters over an area you compute the max and you can think of this as kind of a switch that chooses one of the activations. There are other types of pooling that people have used but capsule is a particularly sophisticated kind. So, so far it's been demonstrated that this works really well on things like Mnist when you have a lot of when you kind of need a lot of invariants to recognize handwritten digits it's not been really demonstrated on large data sets like Imagenet or at least not demonstrated as particularly useful. I don't think the experiments have been done yet it's a little difficult so it's it's a little up in the air whether this will really change the way we do things, on whether this is sort of an interesting idea that needs more maturing. Geoff has had ideas of this type for about 30 years almost. Rich Zemel was a student with him back in the late eighties actually worked on a model called Traffic that had a kind of a similar idea of explicitly manipulating coordinates of parts of objects. I think they applied this to that constellations or something recognizing constellation stars and so you know it's taken him quite a long time to figure out how to sort of use this to practice to something that actually works on a list. He had an earlier paper four years ago on you know subversion that wasn't nearly as good so I think you know it's gonna take a while before before we can this one out. Anna: Okay I don't think we have time for any more questions but the Yann's gonna spend the entire day with us. Speaker Two: Okay well thanks Yann for that enlightening talk and what is arguably one of the most exciting areas of research today in artificial intelligence and it's applications. Also as a colleague in Electrical and Computer Engineering so you're affiliated with our department and on behalf of the Tandon School of Engineering, I just have a couple of other words to say one is I would really like to thank Anna Choromanska and Raquel Thompson who's here somewhere in the back. They put an immense amount of effort to put the series together and so I'd like to thank you Anna in particular and back here. And for those students or as to make a very bad joke I would say who's supervised learning we are in charge of, we are you know I hope you take the right takeaway that this combination of mathematical insight, data computing power and good coding skills on your part can lead to immense benefits so please keep that in mind and of course very hard work so as we saw the presentation today so again thanks Yann and thank you very much and we have this plaque for you. Anna Choromanska: Thank you, thank you very much. 