 all right ty I'm Josh and today I'm gonna be talking about tensor flow probability which is a project I've been working on for two years now so what is tense flow probability so we are part of the tensor flow ecosystem it's a library built using tensor flow and the idea is to make it easy to combine deep learning with probabilistic modeling we're useful for statisticians and data scientists to whom we can provide our like capabilities which take advantage of GPU and TPU and 2ml researchers and practitioners so you can build deep models which capture uncertainty so why should you care so a neural net that predicts binary outcomes is just Bernoulli distribution that's parameterised by something fancy so suppose you have this you've got your sort of v1 model looks great now what that's where tensor flow probability can help you out using our software you can encode additional information in your problem you can control prediction variants you can even possibly ask tougher questions no longer assume that pixels are independent because guess what they're not this is what we're going to be talking about so the main take-home message for this talk is tensorflow probability is a bunch of low-level tools a collection of low-level tools which are aimed at trying to make it easier for you to express what you know about your problem to not try to shoehorn your problem into a neural net architecture but rather describe what you know and take advantage of what you know and these sort of images over here we'll talk about a few of them but each of them represents a part of the tense flow probability package okay so in the simplest form how would you use tense flow probability sort of like get our feet wet type example so we offer generalized linear models think logistic regression linear regression very boring stuff maybe but it's a good starting point so you'll see this pattern throughout the tensorflow probability software stack and how you use it but basically you specify a model in this case Bernoulli corresponding logistic regression and then you just fit it and in this case we're using l1 regularization in l2 so you can get sparse weights and why you should care about this is it's using a second order solver under the hood which means that up to floating-point precision you would never need more than 30 iterations of this and in practice maybe three or four is all it takes and since you can take advantage of GPU it's like a drop-in replacement it takes advantage of GPU drop-in replacement for say R in this case okay so that's just kind of the canned like an example of some of the canned stuff we offer the where things really get exciting are this sort of suite of tools so first we're going to talk about distributions which are probably what you think they are we'll also talk about by jekt airs in this talk hence low probability provides probabilistic layers things that wrap up variational inference with with different distributional assumptions we have a probabilistic programming language which is the successor of Edward that's also part of the tense flow probability package and then on the inference side that's kind of for building models on the inference side we've got a collection of Markov chain Monte Carlo transition kernels and tools to use them diagnostic criteria that sort of thing tools for variational inference in a numerically stable way and various optimizers like stochastic gradient lon German descent BFGS now they're Mead sort of the stuff not stochastic gradient descent maybe some of which is more useful for single machine settings others baking in probability with optimization okay so a distribution I hope this is boring because nothing here should be really fancy capability of drawing samples you can compute probability CDF one minus the CDF mean variance all the usual stuff a little more interesting here at the bottom the event shape and you can't see it but it says bad shape so tensorflow probability distributions to take advantage of vectorized Cindy hardware you specify you call the distribution once but you specify multiple parameters so here's an example we're building a normal but we're passing to location parameters so when you call sample on this it's going to return two samples every one time you call sample that makes sense one will correspond to the normal distribution parameterize with mean minus one the other with mean one it turns out this very simple idea is extremely powerful and lets you immediately take advantage of vector computation so not only that not only do distributions have this sort of like small tweak from other libraries or packages but we've got a bunch of them and you can combine them in interesting ways so it's not super important what distribution this is the point is we're making a mixture combining categorical distributions with multivariate normal with a diagonal parametrization and it all just kind of fits together and you can do sort of cool things using simple building blocks and that's a theme that's pervasive intensive low probability simple ideas scaled up to two to be a powerful framework and formalism so here's another example of a distribution we have Gaussian processes I think this is cool because in a few lines you can learn uncertainty so notice that the model has sort of different beliefs in areas where there's no data and it's tight where there is you could easily turn this into a layer in your neural net if you wanted to okay so distributions there's a bunch of them they have these sort of batch semantics they're cool on to our second building block by jecht urse so a by jekt arisia for transforming a random variable it is think like log and X you may on the forward transformation take the the exponential of some random variable and then to reverse it you take the logarithm so the forward is useful for computing samples and the inverse is useful for computing probabilities so a by jekt ER is a bijective diffeomorphism a differentiable morphism between two two spaces and those spaces represent sort of an input random variable and an output random variable and because we're interesting computing probabilities we have to keep track the Jacobian this is just change of variables in an integral and and so that's where this implements we also have notion of shape because here again everything supports these sort of bat shape semantics so what would use a by director force so this is a behind the slide is an amazing idea you can take a neural net and use it to transform any distribution you want and sort of get an arbitrarily rich distribution so this little piece of code here really is just two hidden layers to dense hidden layer neural net and then it's wrapped up inside this autoregressive flow by jekt ER which transforms a normal now here's why this is amazing you could plug this in as your loss in the output of like this could be your loss basically the this final line here whoops I should have done that on the final lines just this distribution dot log prob that's an arbitrarily rich distribution capable of learning variants not prescribed by like a Bernoulli the variance is P times 1 minus P and unless your data actually is generated by a Bernoulli distribution that's a fairly restrictive assumption in part because any time that's not case it's very sensitive to miss specification so this is like a much richer family and it sort of combines immediately neural nets and distributions so another cool thing you can reverse by jek tears and this little one-line change was a whole other paper and we see this phenomenon intensive flow probability a lot because everything's low-level and modular one little change brand-new idea okay so that's kind of some background let's like go through an example of how you might use this so this is from a book Bayesian methods for hackers which we'll talk about at the end and the question is so I guess the guy who wrote this book he got a girlfriend and at some point his text messaging frequency changed so the question is like can we find that in the data and maybe you'd guess 22 days or May forty some days I don't know let's see so here's a simple model we'll pause it that there was a rate of text messages in some pre period and a rate in some post period and the question is was there a changeover and that's the sort of math or statistical program as I like to call it that statistical program translates into tensorflow probability in an almost one-to-one way exponential uniform flip it over final poisson and to compute the joint log prob we just add everything up in log space and using that we can sample the from the posterior and so what we find is yes there was one rate around 18 text messages today I guess another around 23 and it turns out that the highest posterior probability was on day 44 so how did we get these posterior samples from the joint log probability we used MCMC so our MCMC library has several transition kernels I think one of them were powerful ones because it takes advantage of automatic differentiation is Hamiltonian Monte Carlo and all we do to use that is take our joint log prob what you saw in the previous slide and just pin whatever you want to condition on so in this case we're going to condition on count data and we want to sample the Tau and the two lambdas the rates and the changeover point so we set this up we whoops we ask for some number of results burnin steps sort of usual MCMC business something a little different here is this transformer the transformer takes a constrained random variable and unconstraint it because HMC is taking a gradient step and it may step out of bounds and so since the lambda had the lambda terms our rates of a Poisson they need to be positive so the X by director goes to and from positive real to unconstrained real so to with tau that was a on the zero one interval and so using sigmoid which you can't see here we transform to and from and forty day 44 it turns out that really was when he started dating and so it seems like Bayesian inference was right okay so super hard graphical model which we won't talk about but the point is there's a whole lot of math here and it's really scary not really each line basically transforms one to one so you pull out some graphical model from the literature before neural nets got really popular again and you can code it up in tensorflow probability and where things get amazing is you can actually parameterize these distributions with a neural net thus getting the benefit of both and you can differentiate through the whole thing so it's really sort of what's old is new again yet in a way that you can take advantage of modern hardware so just one-to-one between mass and TFP okay so we did see a little bit of the deep learning the mask auto regressive flow and I mentioned you can Reaper ammeter i stuff so here's sort of the idea of Reaper amortization so as we know probabilistic graphical models tend to be computationally very intensive there'll Nets are really good at lowering the at embedding data into a lower dimensional space why not take your complex computationally intensive probabilistic graphical model and parameterize it with a neural net and that's kind of what this slide is saying we should think about doing so you've heard of Gans so variational autoencoders are kind of the probabilistic analogue a began that's the adversarial networks trying to fight each other to come up with a good balance it actually has a probabilistic sort of analogue and this is it so in this case the posterior distribution takes say an image and outputs a low dimensional is a distribution over a low dimensional space C and the likelihood is a distribution that takes a low dimensional representation and outputs back the image and using variational inference which really just consists of ten lines of code you can take these different distributions which are themselves parametrized by neural nets and just fit it with Monte Carlo variational inference taking advantage of ten suppose automatic differentiation so it all kind of fits together nicely ok so that was a lot of information that we kind of breathes through quickly we are in the process of rewriting this Bayesian methods for hackers book using tensor flow probability it already exists as I think there's like a PI MC version of it and so we've started all the chapters 1 & 2 are in the best shape so definitely start with those and chapter 1 you'll find the text message example but that's that's basically yet so in conclusion tense flow probability helps you combine deep learning with probabilistic modeling so you can encode additional domain knowledge about your problem pip install easy to use and you can check it out as part of the tensor flow ecosystem to learn more thanks and I've got a few minutes here for questions anyone has any yeah yeah so the question is can I quantify uncertainty in an ER lab basically using this stuff and the answer is absolutely yes that's why you would use this stuff in fact the larger question of why would you even use probabilistic modeling is probably because you want to quantify uncertainty and so I pulled back to this variational autoencoder slide because what's happening is this like it's a little hard to see here because it's just code but this low dimensional space is basically inducing uncertainty as a bottleneck and all of your neural nets do this often you'll have a smaller hidden layer going from a larger hidden layer to a smaller back to a larger so the point with this is just do that in a principled way keep track of what you lose by sort of compressing it down and and in so doing then you actually get a measure of how much you lost and so while this is variational autoencoder the supervised learning sort of alternative this would be variational information bottleneck and the code for that is almost exactly the same the only difference is you're reconstructing a label from some input X so you go from X to Z to Y so image low dimensional back to the thing you're trying to predict okay so I'm out of time and with that I will 