 [Music] please settle in I see people are still coming I see empty spaces make yourselves comfortable while you settle in can you tell me has anyone built a neural network in this in this room raise your hand okay quite a few and among the others if I say things like that's a convolutional layer or cross-entropy loss if it rings a bell if you know what I'm what I'm talking about raise your hand okay a few as well all the others you will not be able to follow no I'm joking no this talk is specifically designed to take all of you developers through the learning curve and today we will build a neural network together from scratch to the to something that I believe is I mean a good quality neural network and if some of you have have seen other talks from the tensorflow without a PhD series today we will be focusing on the latest advances in visual processing neural networks and what are the architecture ideas that go into a good neural network today so let's start and for that we need a data set so what can we do let's head to Cagle the data science community I went there and I found this data set of little twenty by twenty tiles with airplanes and non airplanes I actually trained plane spotting us with quite a nice activity why don't we build something that can recognize airplanes from not airplanes and then continue building an an airplane detection neural network so how do we start well let's take the neural network manual 101 on the first page and on the first page it says this is fully connected or dense neural network so the image comes in as a set of pixels and we flatten all the pixels into one big vector and we will be processing that big vector the white circle you see our neurons so neuron in a neural network will always do the same thing a neuron does a weighted sum of all of its inputs and then feed this sum through what is called an activation function that's just a function number in number out but in neural networks this activation function is always non linear this is what this is the key thanks to which neural networks can solve non linear problems so you can layer those layers of neurons the the second layer instead of doing weighted sums of pixels does weighted sums of the outputs of the previous layer you could have as many layers as you want and then at the very end I added a layer with just two neurons and here since I'm building a classifier something that will classify those images into airplane non airplane my hope is that with the correct weights these two neurons one of them will will have a very strong output when this is a plane and the other one will have a very strong output if this is not a plane so we will need to choose the activation functions correctly and again looking at the neural network 101 manual most of the time the activation function you use is called a Rayleigh and the manual says if you're building a classifier there is one exception on the very last layer you will use a different activation function which is called softmax so I will dive into those two on the next slide but first let's write the code for this so this is what it looks like in tensor flow the first line just flattens all the pixels as one big vector of pixels and then in tensor flow you have this high level API which can instantiate an entire layer in in one line so here I have instantiated by three layers the first one with 200 neurons then 20 and the last one with two neurons as you see the intermediate layers are activated with this Rayleigh activation function and at the end need to do something different so if you are building a classifier let's follow the recipe the recipe says if you're building a classifier the last layer must have the softmax activation function and you will apply you will compute now some distance between what your network is predicting and the correct answer we are doing here supervised training so we don't know initially what those weights are we are doing weighted sums but we don't know what those weights are and we will be feeding examples into this model where we know the answers in comparing what the model is predicting against our answers so we need a distance function and in a typical classifier the distance you will use is called cross-entropy i don't even necessarily need to know at this point what that is tensorflow actually has a function combined function that applies this loss softmax activation and computes the cross-entropy distance between what the network predicts and the correct answer this is called a loss or error function and that's what we want to minimize so as soon as you have built your layers and computed your loss an error function tensorflow can take over you pick any of the optimizers that are available here I chose Adam optimizer and you can't be asked it to minimize this loss this will give you a training operation which tensorflow with and will then repeat in a loop and what happens in this training operation is all the training magic tensorflow will look at your loss will differentiate it relatively to all the losses sorry to all the weight in the system obtain something that is called a gradient mathematically and apply an algorithm gradient descent that figures out how to change the weights in your network so as to minimize this loss so as to make this loss smaller and so you will be feeding in images and correct answers in batches and at each batch tensorflow slightly adjusts the weights in your network so as to make the loss the distance would be between what your network is predicting and the correct answer smaller and smaller and smaller that is how you train a neural network so just a little look at those two activation functions so here's again just one neuron ok weighted sums of all of its inputs usually you add something called the bias that's an another degree of freedom something that will be determined by training and you feed this through an activation function so this rayleigh function that we have used on the intermediate layers this is what it looks like it's a very very simple function 0 for all negative values identity for all positive values the softmax activation function that you use on the last layer here I have represented a last layer with 10 neurons so that's if you are doing a classification into 10 categories so that softmax function is slightly more complex well actually it's just an exponential so you compute your weighted sum and elevate it to the exponential and then you normalize across those 10 neurons well the output I I put in a little animation here is what it does boom it pulls the winner apart ok but without destroying it's not max it doesn't completely nullify all the bad answers so you still have signal if your network is miss recognizing something ok that's why it's called soft max it's it's max but in a soft way it pulls the winner apart but it doesn't discard all the bad all the negative information which is still useful for training alright so we have all that we have built our network now we can train it we have our data set during training well so here is what goes in in the window which is brew so really one soft max the two activation functions the cross-entropy was the distance function we used ok and it's ready okay let's test actually before we go there a word about the the the tooling were using here so this is the code is in tensor flow the tool I'm using to actually run the training is ml Engine on Google's cloud it has one super useful feature that I love is that whatever infrastructure I run my training on when the job is done it shuts it down okay that's not rocket science but I just can't be bothered shutting down my machines because and figuring out if my jobs I've done ml engine gives me the job based view I can show you here that I need to work my jobs running finished and when they are finished the the the machine or the cluster goes down and finally tensor board that's a visualization tool that you have in tensor flow where you see all your curves and and you can see what's going on okay so I will oh no I can't skip this we're handling images so I will have to introduce another piece of technology here those dents neural networks that you have seen they work well for a lot of things but for images you need something else and those are called convolutional neural networks so bear with me here in a convolutional neural network the what is called a neuron behaves a little bit different a neuron this little cube here that's the output sees only a little fraction of the image right above him okay it doesn't do weighted sums of all the pixel of the image just a little portion and then the next neuron actually does a weighted sum of again a little portion just and above him but using the same weights so it's actually a filtering operation you pick a set of weights as many weights as I have highlighted cubes in the image over there my image has red green and blue channels because it is a color image okay so you have red green and blue and I'm doing weighted sums of all these pixels but using the same weights at each position it's a filtering operation and once I have moved this filter across my entire image with correct padding on the sides I have as many outputs as I had pixels initially so how many weights did I use well as many as highlight as the highlighted pixels so that's 4 by 4 by 3 which is 4 1 4 16 48 48 weights typical neural networks have something in the tens or hundreds of thousands of weight so we need a way of giving this more freedom more weights to play with in a good way is to pick another set of weights and repeat the operation so you pick just another set of weights repeat the operation and you obtain a new channel of data in the output and you can do that again and again which means that a convolutional layer in a neural network will transform data cube into another data cube and this is the shape of this convolutional the matrix of weights so the first two digits the first two number is 4 by 4 that's the size of the filter in pixels the third digit is how many channels of information here you were reading in the input image so 3 channels red green blue that's this number here and then you repeat this operation 4 times with four different sets of weights and as a result you obtain four different channels of outputs and the four is this last number here so this is a convolutional layer and since it has a number of input channels and a number of output channels you can chain them and a convolutional network will be a sequence of those layers transforming the data data cube into another data cube then filtering that a direct you began transforming it into a new data cube and so on so this data cube can grow in both directions vertical direction or the horizontal direction so we have seen here in the vertical direction that's just a number of times you repeat the filtering operation with a different set of of of weights but how do you adjust the size in the in the horizontal direction usually the idea is to go from an image and boil the information down into something smaller like recognizing what is in the image so two two choices for that actually we have many options the first one is to play with the step of the convolution instead of doing those weighted sums pixel by pixel you jump every second pixel and well mechanically you obtain twice fewer results in the output that's the stride parameter that you see here stride 2 or stride 1 and there is a second option which is actually more used and that's called maxed pooling and here the idea is interesting these are filtering operations so as the network trains you will those filters will train to pattern match or recognize certain features in the image let's say there is one that trains to recognize little horizontal lines another one that specializes in vertical lines and so on so the output of the filter is basically something like here I have seen a little horizontal line here I have seen nothing here I have seen nothing and so on the max pooling operation takes four of those in a in a square and just keep the max which makes sense because you are interested in the value of the filter that was maximum at that point because that is where the filters I have actually seen something the other ones which you can which you discard or I've seen nothing and that's not really interesting so this is a basic subsampling operation you take your image you take it was not your image but your data cube you take the the data points four by four and squares two by two and just keep the maximum and again that reduces the size of the data cube horizontally and the little guy he points something out that there is something called a one-by-one convolution well if you're a mathematician that doesn't make much sense like one by one filter that's just multiplying by a constant right that's not very useful but again we are doing this filtering multiple times with a different constants so one by one convolution actually makes sense it's the weighted sum of this little column of data points and that way yet some might be interesting so we will see later that one by one convolutions actually in convolutional networks make sense all right so this is what we will build there are three convolutional layers we have our image here three convolutional layers and then at the end we need to connect this to our softmax layer which will do the airplane non-arable and classification so the last data cube we reshape it we flatten it out as one big vector here and this we apply normal dense layers to this vector and end up with our softmax softmax activation and softmax cross-entropy loss because this is a classifier after that a lot of experimentation and a lot of regularization so I will not go into this you have many other talks in the tensor flow with our PhD series that focus on what is regularization for now all you need to know these are techniques standard techniques that can improve the convergence and I am quite good at those techniques so plus another one which is hyper parameter tuning as you have seen there are many parameters here the number the size of the filters the number of layers the strides in so on and so forth you can well if you if you if you know what you're doing you know the acceptable ranges for those parameters but still it's a lot of work to explore this parameter space so that's why am L engine gives you this hyper parameter tuning module where you just define your space and you and say go ahead try all the combinations so there are a couple of ways of trying out all the combinations the basic one is just grid search and you know that's where you would start just you know map out all the possible combinations of parameters and search throughout the whole grid it's actually slightly faster to do a random search than a grid search that's a bit counterintuitive we are rational people I like the grid but it turns out random is slightly better but ml engine does thread algorithm that is called Bayesian optimization and I won't go into this one it's it's something where from one set of runs it can mathematically determine which part of the parameter space has been mapped and where is still missing information and focus on that other part of the parameter space in an optimal way so that's what that is the best way of doing hyper parameter name and m/l engine does that how well this is how you package your files to go to ml engine just you your your Python code in an inner folder and a config file which usually has just this in it scaled here basic GPU which means one machine with the GPU and then you go you go run the run is here you use the g-cloud command line g-cloud ml engine jobs submit training and you train if you add these lines here to the config file you start instead of starting a normal training job you start a hyper parameter tuning job so what do you what did I do here I said I want to maximize some metric there is a way of specifying what your metrics are in your tensor flow code so here my accuracy I want fixes 50 trials 10 trials in parallel again be inching a Bayesian optimization so it it derives useful information from trials for the next ones so it's it's better not to run all the trials at the same time even if you have the necessary hardware and then you say which parameters you want so I have one parameter called LR - which is an integer min/max values the scale this other one is a categorical parameter and it will try all those parameters in some optimal way so using all this the network you have seen here this one plus my best knowledge of regularization techniques plus hyper parameter tuning I was able to bring this network to an accuracy of 99 per 6 percent and I was very very proud of myself until I tried this network in real life let me show you a demo well first of all just a little trick how do you transform a classifier into a detector it's actually fairly easy if you have a big image just cut it out in 20 by 20 tiles slightly overlapping and maybe at different resolutions which you resize then to 20/20 run the classifier wherever you see a plane you put a box there and you have a detector so here we have that San Francisco and this was my very first model this one not yet very good just the data I had not not much regularization not much high / / I'm tuning I guess it's it looks like it's doing something it's not completely horrible so it's encouraging then I used my best skills to hyper parameter tune and regularize the hell out of this and I obtained my ninety nine point six percent accuracy model which I will now run for you and and wait for it it's absolutely horrible you see here let me show you lots and lots of false-positive everywhere it's it's it's noisy it's not clean so here is the first lesson of neural networks there is your training data there is your evaluation data on which you compute your accuracy of course computing your accuracy on your training data would be cheating and then there is a real life in real life has nothing to do with either your training or your evaluation data real life is hard so with a lot of effort actually augmenting the data set adding tiles of non planes hoping that this would make things better I was able to increase the accuracy of this model a little bit but yeah as you will see it's it's still not great and that was the end of my neural network 101 handbook so from now on you've got to read papers all right that's scary so before we go there let me give you a couple of just tensorflow tips to use ml engine really to the best of its capabilities I advise you to wrap your model into what is called an estimator API that's just because in in estimator we have written for you a ton of boilerplate code that is not interesting to write things like checkpoints regularly outputting checkpoints so that if you're training crashes after 24 hours you can restart from where you were you were you where exporting the model at the end so that you have something that is ready to deploy to a serving infrastructure or or distributed training the distribution of algorithms of distributed training also baked in into estimator and to wrap your model in an estimator you need those oops sorry those four things here and and then you can run train and evaluate which will alternate training and evaluation phases so that in your output you get nice curves with your training data training metrics evaluation metrics you can compare the two and so on mostly what I what you need to provide are those four functions here so let's go through quickly through them it's it's it's really nothing fancy the model function its your layers okay that's your model and then it returns whatever a model is supposed to return the predictions the loss you you you put the loss into into an optimizer and you've got this training operation which the estimator will run in in a loop and whatever evaluation metrics you care about so that's your model then the training input function and I'm putting code on the slides here okay you don't try to read all the code I will give you the the highlights of what is there in the code you will not be you not have the time to see all the syntax so the training input function that's the function that will define how your data goes into the model and I use the this data set API and that's really good because this data set API is designed for out of memory data sets you define what it what your data set is and then as you as your model is training the data is loaded and the loading triggers the loading of additional files from disk if the data set is does not fit in in memory and by the way no data set ever fits in memory I mean no real data set so here for it for example I'm reading images and well focus on this here the data set is initialized from files get matching files in a directory all the files in a directory and then I like this syntax it's really the workflow I'm used to I apply some loading operations which will load those files and and decompress them and so on I usually need to shuffle my data to batch it into batches because the you know the training always proceeds by batches and usually I repeat it and definitely and my data set is done and finally the serving input function so estimator also saves periodically a snapshot of your model which is ready to be deployed again on ml Engine ml Engine has those two parts one is for training the second one allows you with one click to put your model behind the rest API but for that when your model will will be listening behind this REST API it will be receiving data in a certain format and usually you wants to do stuff with that before feeding it into your model so if you don't this is the do-nothing pass-through starving input function but if you do this is the serving input function which I used to first decompress the incoming images from JPEG to pixels and then I also implemented right in this the the scanning operation you remember to transform a classifier to detector you need to cut your image into little 20 by 20 tiles overlapping at different resolutions actually I was able to do this on the fly in the deployed model this is the code so it's about 10 lines don't read it but I find it interesting that I was able to do this on the fly in the deployed model and the demo I was showing here this is a JavaScript UI which is calling into ml engine sending a part of the image to ml engine and then getting the results back so this is actually live alright so we need to read papers there are many papers for detection and image image work in detection there are many papers I want to talk about these two and a little bit about the others but mostly focus on the big ideas and the first bigger yes and I need a new data set unfortunately because now I will be doing a real detection those 20 by 20 tiles of planes are in airplanes that's no good any longer now I have to handle I want to handle big images indirectly output a square box around each airplane so I had to build my own data set in this case it was actually possible I beat I built myself a little JavaScript UI and then I went clicking on airplanes and and well I you know it was about a day day of work so not so bad here all right so let's start with this an inception this is the paper that really brought to the table some of the big ideas in this space on this side you see what the inception model looks like so all those little squares are covering convolutional layers and what you see is that it's weird I told you before that convolutional layers should be sequenced just piled up one layer next layer next layer next layer and here we see branches and then things coming back what is that so the big idea here is that you are somewhere in your convolutional neural network and then you ask yourself the question what is the what is the best here should I now add a 1x1 convolutional layer or maybe a 1x1 followed by a three a three by three or maybe something else what is best and they had the idea that you can actually do all of these things in parallel and simply concatenate in the very vertical direction the results and they call this a module basically during training the network will decide which is the best path to use for a specific image and a specific task so this is this module based approach that was one of the big ideas the second by big area is called filter factorization so what you see here are is a sequence of two 3x3 filters you see let's look at this this bottom one so this piece of data is produced by some weighted sum of this 3x3 square here and if you look where those data points in this 3x3 square are coming from they actually come from some combinations of this white data data points in this five by five a piece of the previous data so it looks like two consecutive 3x3 filters do some combinations of a 5x5 zone in the same way as a 5x5 filter will be doing combinations of data points in a 5-11 5x5 zone it's not the same combinations but let's count the weights 5x5 filter has five by five by one by one that's the depth I mean the number of channels that's 25 weights now if we count the weights 4 3 2 consecutive 3 by 3 filters it's 3 by 3 plus 3 by 3 which is 18 hey 2 3 by 3 filters are 30% cheaper in terms of number of weights than one big 5x5 filter they're not doing the same thing but it's worth checking if for our purpose it's it would it wouldn't be enough that's the second big idea and the third video a big idea are those one by one convolutions which again could sound funny to a mathematician but once you realize that you are applying many of them and that this one by one convolution is actually a weighted sum of all of the pieces of data in this little column it's it's like saying well I have many different filtering results like I have filtered my image for horizontal lines and then vertical lines and so on and maybe the feature I'm looking for is some combination of those filters sometimes I'm looking for only the horizontal line a little bit of the vertical ones the one by one convolution can give me the right combination for that purpose of course the weights defining the combination will be trained and one more this one is a bit of cheating so at the very end you do your convolutional layers and at the very end you want you classify if you're building a classifier so let's say classify in five classes typically you would take the law datacube here reshape it into a vector apply a dense layer and then and then softmax activation and you've got your five classes but if all you're trying to do is obtain five numbers there is an easier way those dense layers they connect everything with everything they tend to be heavy in terms of weights there is an easier way you need five numbers okay let's take this this cube of data just slice it up horizontally like a piece of bread in five slices average those five slices and you've got five numbers you can apply softmax activation on it if you want and you've got the same thing with zero weights so it's a lot cheaper but warning it only works if you care about global information and only global information if you want to classify an image as dog versus cat versus sunset yeah this will work if you actually care about the localization of stuff in your image like we do you are averaging your filtering output across the entire image so you will lose the the that information so not good for our purpose here but still an interesting idea to put this all together into a simple architecture the inception architecture is a bit complicated for my taste but this squeeze net paper actually put all of this in a you know in a very elegant architecture they said let's build everyone out of these kinds of modules so we start with the one by one convolution where we usually reduce the horizontal size of our data cube and then we have a parallel module which is one by one convolution and a three by three convolution in parallel we stack the results and the stacking usually increases the the number of channels in the data cube again so they call it a squeeze and expand layer and when you put those two together they call them fire modules and your full model becomes a nice succession of fire modules max pooling operations which decrease the size of your data cube horizontally then again a sequence of fire modules then you decrease the size again horizontally and so on it's simple and I find it elegant so let's build it but I wanted to test this squeeze net idea against a more traditional you know layer layer layer layer layer architecture and we'll go into the yellow paper you look only once that's the detection paper which we will use to actually produce our squares around airplanes in that paper they had this this this very simple architecture that they called darknet and I thought well maybe we can compare the two so you see on the squeeze net size we have this alpha this these squeeze modules here sorry no here one by one followed by in parallel three by three and a one by one convolution and then the max pulling operation and then it continues so you see the data cube is a small one squeezed then it's expanded that is squeezed and it's expanded and so on on the other side is just a sequence and actually the the depth of the cube is roughly constant until the end and we just have these max pooling operations which restrict the size of the image horizontally so we'll see which one fairs better now how do we actually detect airplanes this is what the yellow algorithm does and it's you look only once it's a one shot detector it's not you live only once it divides the image into a grid and it will say well each grid cell will now produce a certain number of boxes so each grid cell will designed to output four values which is XY the center of the the box the the yellow box here and the center can be anywhere in the grid cell so x and y are between minus 1 and 1 relatively to the center of the grid cell and it will also it will also compute size of this box the size can be anything okay this the the box can grow to the to be as big as the image itself it's not constrained in the grid cell and also some confidence factor which tells us whether we if there is a plane here or not okay and you can adjust how many of those boxes each grid cell is able to generate 1 2 3 we will see what's best this is the loss they have in their paper okay so let me please allow me to simplify it a little bit the first line actually is it's kind of okay it's it's the error on the position of the box so you compare the Box you have against your ground truth and you see this a squared square of differences of the center so that's the error on the position I like that the second one is the error on the size of the box well they had let rectangles I have only squares so let's kill the hate only with and for some reason they thought it was oh no they in the paper they argue why they put in the square root of the size and not the size itself I didn't understand that so I remove this the the next thing is the error so this is on detected airplanes what is the country so do the error on the confidence factor where you had an airplane here I just replaced the ideal value by one when you have an airplane that ideal confidence is one the next one is the same thing for four four boxes where you do not have an airplane so they are the the ideal is zero and the last line they were detecting many categories they were actually detecting cars and dogs and and blah blah blah I have only airplanes the last line for them was the missed detection error detecting a car as a dog I have only only airplanes or not airplanes I don't care about this they also so this is a multi a composite loss so they had this idea that maybe the the different parts of this loss should be weighted different again I didn't understand why so remove them and and that's about it so the only thing that is still left and there is a hard problem is this you know operator they called one which is actually the assignments between the boxes your generate in the ground truth and that's not an easy problem so of course for a grid cell you know that if you generate if you have a ground truth box that is centered centered in one of those grid cells you will want to pair it with some box generated by that grid cell but those grid cells are allowed to generate more than one box and you're allowed to have more than one an airplane you know in a grid cell so you might have more than one ground through the box how do you do the pairings yeah well there's a lot of choice and when I looked in the paper they said oh well we did something simple and actually our algorithm is not very good for swarm type things when you have you know a flock of birds or an airport for a full of airplanes it's not very good for that thank you guys so well we'll see you will try to play with those parameters how do we build this so very simply you've got the end of your convolutional Network you've got a data cube split it horizontally in your end-by-end grid and then split it vertically in four or maybe eight or twelve depending on how many boxes you want to generate per grid cell and then the red boxes will become X the yellow box is why the green box is the the size of the box and and the blue ones will will become the confidence factor you just average all the values in them and for all of those that you need to put between minus one and one you feed them through a hyperbolic tangent that puts them between minus 1 and 1 and all those that you need to put between 0 and 1 you feed them through a sigmoid and that puts them between 0 and 1 and you have generated from an image 1 2 3 4 boxes per grid cell which detect airplanes that's it we're done the only thing now is the work of the data scientists now start a slow but steady process of grinding through all the hyper parameters of this model and trying to improve the accuracy initially this is what my first training run looked like actually let me show you my my my real numbers because I have them here so this is initially what I had I wasn't sure it was actually doing something at all you know but why not initially I tried a four by four grid and generating just one box per grid cell so not very good I realized that shuffling data is actually super important big progress in so iou that's the intersection of a union it's a measure of how accurate those boxes are relatively to the ground truth so I tried with more boxes per cell like 4x4 but generating four boxes per grid cell that doesn't work so well again it's a hard problem to assign them to their ground truth control counterparts you've got four boxes per cell for ground truth which one is which it's hard so this was not very good so then I went to 8x8 and then generating only one box per cell yeah that's better because there is no assignment problem there it's just one box per cell I bumped it up to sixteen by sixteen by two the grid the yellow braid that is even better and this time I tried to think very hard about this assignment problem and and devised an algorithm that would kind of rationally do the pairings I think I went by the distance from the center so the first box is more susceptible to go with with airplanes which are closer to the center of the grid cell and the second one more on the periphery then it turned out that detailed mentation varying at random the hue and the orientation of the image that actually helps a lot and finally that lost waiting idea the multi part part lost figuring out that some parts should wait more it was actually a good idea and that gave me a little a little bit more accuracy yes as well and on here you can see in the reverse order the loss so the error function going down as we do this and and finally the the best one was with more layers so these were with twelve hours the last one is with seventeen layers and then we got something so you want to see this you know demo right let's go first model so this is what we had the best model obtained from the classifier now let's try our first sixteen sixteen time by by two so that the yellow grid you catch images in sixteen by sixteen grid and you generate two boxes per grid cell let's go analyze it's much cleaner but it's missing a lot of planes here this is this is something you can solve with with the data augmentation actually I did not have any data augmentation here but if by varying the the orientation of my cells and the hue of the cells randomly you got a big boost in inaccuracy now we're talking now it looks like a detector we still have some false positives there is one here I I saw a couple of other ones but it is not so bad actually and if we jump to a bigger model I think that from twelve to seventeen layers I think we are pretty good let's see if we are perfect we'll see almost perfect well it missed this this blue airplane here but that's because I need more data with blue airplanes but it's not so bad and I was able to piece this together with Lego bricks which appear in literature without being a super specialist in in machine learning and my message to you here is that okay it's kind of hard but it's not like fluid dynamics hard if you are a good developer if you are a good developer this is a learning curve through which you can go I went through this learning curve and and look my my my airplane detection model now works it will always have an accuracy of 99 point something percent you can never go to 100 that's something you have to build into your your use case your products but you all of you here are capable of building machine learning months alright then just to finish I want to show you the the tooling I was using so I'm using ml engine ml engine has basically two two great features one is my jobs I see all my jobs running and the other one is my models so with one click I can deploy a model to production and it's served behind a REST API with with with auto scaling and I don't have to manage anything I really love that I love my job tuning those models I don't not like the job of chasing VMs and which one is still running and why blah blah blah ml engine once you wrap your model in this estimator API it gives you a one-line config access to many different hardware architectures including TP use actually but let's start with something else so this is what I had initially training on just one GPU and this is the config file that you use for that skill here a basic GPU that's one machine with a GPU this model the 17 layer model the bigger one trains in 23 hours for a cost of 28 dollars we have faster GPUs you can bring this down to 10 hours I like that for the same price but with a very just a config change no change in your code you can actually deploy this to a full cluster of five machines with five gpu-z and it's a cluster so you can go to 100 if your model scales with this one I'm down to four and a half hours that's that means much more productive and with the better GPUs that actually below two hours I wish I had that in the very beginning but then I thought well let's try those TPU things first of all a little warning those TP used its new yorky texture there is a little bit of a porting effort to adapt your model to GPUs we are working on that but right now expect to spend some time tweaking the code it will still work on GPUs once you tweaked it but there are some things that need to be done for this new architecture it's a completely new chip oh sorry before that new intense reflow are those distribution strategies so very soon as soon as they shape it's in beta now you will be able to reserve one box with multiple GPUs in it and that's a different trade off so you don't have the network communication between the GPUs but on the other hand you can put 100 of them in one box so it's a different trade off and again it will be available with just a config change so now I want to TP use I this is available I just ported it so I don't want yet to show the numbers because it's not really like a really you know benchmark with a proper benchmarking benchmarking you know set up but those numbers are not secret cloud TPS are all available on a mail engine today and I published the code for this yesterday so all you have to do is run it and you will see the numbers and of course if they are if I am putting them here it's because the they are both both faster and cheaper than the second best option you have here on on this slide and very soon you will be able to access not just one of those TPU boards but sixty-four boards so a full rack of them connected with a high-speed interconnect and use all of that as one big supercomputer straight from ml engine with just a configure with just a conflict change if you want to know more about this there is a session exactly on TP use right after this session but you'll have to cross the street it's a very good session alright so that's it thank you for for your attention we have seen these the cloud ml Engine and cloud TPU as products my takeaway is not on the products but what I want you to remember is that if you are a good developer you can build a machine learning model there are best practices there are Lego blocks that exists you have to follow what is a state of the art but then you can piece those pieces together and if you want if you don't want you to build your own models where we have a set of pre train models and new we also have this cloud auto ml product that actually this morning it was announced that it was not doing just vision but also more things so I need to update my slide and with that you can just throw your data at it and let the system figure out the architecture of your model that's quite advanced I'm amazed that that we can build this and I find this really really really really marvelous and finally if you want to learn more about how to build models you can check out the other chapters of the tensor flow without a PhD series and all the code and code labs and all that is available on github you've got the URL over there thank you very much [Applause] [Music] 