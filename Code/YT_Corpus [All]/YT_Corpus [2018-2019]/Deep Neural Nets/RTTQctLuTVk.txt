 and we're given by me onion link we're going to be doing an introduction to some ideas in deep learning so the program for this tutorial is to have a look at general concept of supervised learning with neural Nets then we're going to switch to sent in the basic introduction to convolutional neural networks we're going to talk about recurrent Nets and we're gonna finish with an overview of some other interesting deep learning models so supervised learning I think most of you are familiar with the concept of supervised learning but since you haven't had the machine learning tutorial yet let's go over the basic idea first so that say that you are given a example input-output pairs x and y so in this case it would be you know say you have X is the position of dots on this square and Y is the color of the dots so say blue is 1 red is 0 what you want to learn is to predict the association between x and y so if i give you a new point y is this gonna be blue or red so the general idea is that you probably want to learn some sort of rule that allows you to map from the X sort of position to the color the Y such as this kind of coloring of the square as you might be familiar with there are many different approaches from statistical learning machine learning etc that deal with supervised learning some of the names in this space are various types of regression so like logistic regression support vector machines decision trees in your networks etc so today we're going to focus on the obviously the last element of this list so we will start with this simplest possible artificial neuron that can perform the type of problem that I just described here so the simple perceptron simple perception is something that is mathematically described by this expression here which is the rectifying linear linear fashion being unit which is something that was presented by already by McCulloch competes in the 40s as a first initial model of how a neuron might work composed by some you know some inputs represented that could correspond to the input synaptic inputs coming in other than writes linear integration and a non-linearity applied to it so in this case you would have at the neuron that performs a linear combination of the inputs at possibly a bias term and then perform some sort of non-linearity which represented here by G in this case for the problem that we're showing here the non-linearity would be like a step function so zero if the argument is negative or one is if the argument is positive this particular functional form can learn to perform this type of discrimination so in fact in this case you would have that if your vector W of your weights is this vector here the simple perception will learn to assign the value one to the dots in the blue area of the plot and the value 0 to the dots in the red area of the plot thus solving our initial problem Rosenblatt's already in the late 50s and 60s showed that you can actually if you have a problem like this you can train a simple perception to solve this problem by applying a learning rule that he expressed like this where eta is some sort of learning parameter and T is your target value and Y is your the output of the current output of the of the neuron right but this leads us to talking about linear separability because the example we just presented is kind of ideally suited to the very simple neuron that we were discussed because the different classes that we want to distinguish can be separated by some sort of linear boundary in fact it turns out that this this type of neurons can only learn to solve this particular class of problems as soon as you have a problem which is more complicated like the one that we present here where there is no single linear boundary that divides the blue from the red the perception fails miserably at distinguishing them in fact you can have one first you know if maybe you have one perception can learn you know this boundary here and then another perception can learn this other boundary here but really if you want to distinguish these four quadrants what you have to do is to kind of take the outputs of the two perceptions and put them together so you want to you know you want to see if this perception is happy and if this perception is also happy maybe you're in this area if they're both sad they're here and combinations thereof so this suggests the fact that we can solve more complex problems than linearly separable ones just by combining multiple units in in across multiple layers as we have schematized here this is precisely what is done in what we call a multi-layer perception which is just an arrangement of many simple perceptions in several layers arranged in a with a feed-forward architecture so a bit of terminology this we would call it the input layer composed by our input data this is the output layer and the things that we have in the middle we call them hidden layers that's why we call them H so this would be the first neuron of the second hidden layer and so on right so the way in which these multi-layer perceptions work is totally not surprising even the symbolical representation so each of these units just is just a simple perception and just computes its activation from its inputs so the units in the first layer just compute their activations from all the inputs from the input layer and the units in the second layer compute the activations of the first layer as their inputs and so on and so forth until we get to the end one might have like several of these output units here I have represented only one for simplicity what we call this procedure of kind of passing information forward from the input layer on through the hidden layers out and over to the output layer is called forward propagation because as you can see information kind of flows forwards from the input to the output right so one cool thing about MultiAir perceptions in general is that they are universal function approximator that means that given enough simple units and you can a probe you can use them to approximate arbitrarily well any function that is reasonably well behaved of course under certain assumptions as well for instance you need your kind of free to pick your non-linearity in a very broad class of nonlinearities but it needs to be a non-linearity for instance you can try to show it's simple exercise what happens if G is a linear operation and you will see that the essentially expressive power of of this network basically collapses down to one of two the expressive power of a simple perceptron right so since we have seen this very nice property that they can this multi-layer perceptions which say one or two hidden layers can represent any functions you throw at them then why do why are we talking about deep learning at all why aren't we just happy about fitting simple you know shallow networks with one hidden layer so there's two problems one is that this is just an existent proof it just tells us there is some network out there that can perform what we want but it doesn't tell us anything about the fact that the number of required units for implementing this function should be reasonably small or achievable small so a problem of expert CVT of your of your network and on the other hand we have no guarantee that actually we have a way of computing this network so we know that maybe it exists in a mathematical sense but we have no way of deriving it so this is something that leads us to in the direction of deep networks which in fact have two like interesting properties so there's like two arguments to be made in favor of structuring your network with multiple layers so the first is statistical deep networks you know if you imagine having a network with like many layers computing one from the output of the previous one our compositional in the sense that they compute features and then features of features and then features of features of features so this is naturally well-suited if you think about the fact that in our world perhaps many of the interesting data that you have and many of the interesting statistical structure that you might be looking at is has a similar compositional property so for instance in vision you have you know you have edges and you can compose edges to form simple shapes and compose simple shapes to form more complex state etc etc so in this sense the architecture of deep networks can reflect something that is going on our world and I believe perhaps max tegmark might say something about the heinous talk later during the school and the the other reason for which dip nets are useful is also computational under certain conditions you can show that deep architectures are more expressive so for a given number of hidden units that you can did you have for assembling your network you can basically learn more patterns if you arrange them in a hierarchical way right so we have seen why we care about networks with many layers and in the previous lecture I assume you have learned about how you optimize things right so and you have learned that for optimizing things is good you know gradient descent is a good approach so say we have a particular network and we can define some loss for our supervised learning problem it's it's an interesting problem to you know we want generally to compute the derivative of this loss with respect to the parameters of this network because we want to do gradient descent on it right so the classic way in which we do this computation is called back propagation the back propagation is really a mathematical trick it's an application of the chain rule and is this a way of computing derivatives in a simple and manageable way across an arbitrarily complex graph of computations defined by your network so the way yeah the two key intuitions that stay that that support back propagations are the following not intuitions and facts the first is that if you want to compute the derivative of the loss of your network with respect to say the weights that parameter is a specific hidden unit you know that the the the the loss will depends on these weights only through the activation of that unit the second key fact is that the derivative that essentially the derivative of that of the loss with respect to that activation will only depend on the activations of the units that are downstream from the first one but okay let's see how it works in practice so remember you want to compute all the derivatives like D of L over the W where the W's are inside these nodes here so you start by computing your first to compute the for the derivative of the loss with respect to the weights in the third layer here so the loss that the weights are inside here you can just do it very simply by one like applying once the chain rule is just derived the loss from with respect to D to the output value and then you write the output value with respect to the weights and you're done because that's just one step then the interesting thing is that when you keep applying the chain rule backwards and backwards to compute derivatives with respect to the the units in the previous layers you see that basically we can express the derivative of the loss with respect to the activations in the second layer as the product of the derivative of the loss with respect to the third layer by the derivative of the activation of the third layer with respect to the activation of the second layer and again exactly as above every time that we have the derivative of the loss with respect to the activation of in one layer then we can directly plug it in in an expression like that that immediately gives us what we care about so the derivative of the lost spec to the wait so the return again for a second layer derivative the last with respect to the weights is again just the derivative of the loss with respect to the activations in that layer multiplied by the derivative of the X activations with respect to the weights so you can keep doing this over and over again and the good thing is that as long as you keep these values here that we call the errors so the derivatives of the loss with respect to the activations so as you can hear with respect to Y and h2 and h1 as long as you kind of propagate them back through the network as you're taking these derivatives you can always compute these quantities in a fully local way you see here you only need this quantity that came from the layer above and this is an entirely local quantity something that only has to do with the relationship between the activation the second layer and the activation in the first layer as you can see here with these blue lines so yeah this is a essentially just a convenient way of a principled way of computing all of these derivatives that is what you care about for gradient descent through an arbitrarily complex graph of p4 feed-forward computations all right we can actually make a more concrete example just to give you a better idea say that we have a quadratic loss that is something that looks like that that's something that you define you pick say that your non-linearity is a hyperbolic tangent so this is just a kind of a sig Sigma translated sigmoid if you just plug this in into this machinery here you will see that you can compute all the numbers you care for so because of the definition of this of this way in which activations are coming from the previous layers you can compute directly the derivatives of say activation in layer L with respect to activation layer L minus 1 and this is just given by by this expression remember that the derivative of the hyperbolic tangent is just 1 minus square of the argument so you get like a 1 minus square of the activation multiplied by the derivative of what's inside the hyperbolic tangent that gives you w that comes out and the same thing symmetrically when you take the derivative of this with respect to the these weights WI K you get again this term 1 minus the square multiplied by the activation in the previous layer so you have this these expressions here you can just plug them in and you can see that when you take you know first step you take the derivative of the loss with respect to the output and it's just 1 y minus T and you can see why we're calling this thing the error that were propagating I mean this is really an error term and then you can just like go backwards and because now you know how much this is a number now right because say that for instance for a particular data point your target value was 1 for the classification and your output value was 1 this number here would be 0 if instead it was 1 and 0 this would be say minus 1 something like that so this is a number now and then you can plug this number into this other expression you can compute this because these are also numbers right because previously when you have forward propagating in forward propagated the information from the input to the output you have computed you know what is y and you know what is the activation of the previous layer so these are all numbers and you can compute this is a concrete number that you can plug it in to your gradient descent procedure and you can just do that over and over until you have everything that you need to take a step in your gradient descent yes sorry I'm gonna come closer and then I'm gonna repeat the question so because you're getting an error term at the output right adjust the weights yeah I ready for it because those weights need to adjust the weights before that also adjust to give you yeah yeah it's it's more yeah it's basically if you write down so if you want to write down the you know you want to write down you just computing derivatives here right I mean leave all under the idea of like adjusting weights but so you just want to compute the derivative of this with respect to that and then if you think about the chain rule the derivative the derivative of essentially in spirit the derivative of this with respect that would be the product of the derivative of this with respect to that and then you're going to have this respect to that etc etc so and that's why you get all these terms that that propagate back basically because you see that this this derivative here will appear this this term here will appear down here etc and then you will take this one and plug it here is it cool any other question yeah okay if you want to for fun you can derive it the gradient this gradient terms with respect to the bias terms B that I have ignored until now if you want to try to see how how this works okay so this is was the last light of our muscular perception stuff that I wanted to conclude it was just a couple of citations here just for historical perspective talking about cycles of hype so this was a next work from the New York Times in 1958 talking about the simple perception the Navy revealed the embryo of an electronic computer today that it expects will be able to walk talk see write reproduce itself and be conscious of its existence dr. Frank Rosenblatt a research psychologist at the Cornell aeronautical laboratory Buffalo said perceptions might be fired to the planets as mechanical space explorers so this is for the simple perception that does linear linearly separable problems so of course you know things for people got very excited and then 10 years later essentially I'm not gonna read through this but the community recognized that there were several limitations to these devices and that's basically when was if you've ever heard of the first or second AI winters this was the first AI winter that that happened when people realize that it was hard to Train multi-layer perceptions without knowing about back propagation and people were not really sure about whether they were ever gonna be useful for anything so now let's switch gears and say something about commnets so in general if we think about the problems in vision about say object detection and object recognition what has been traditionally being done in the computer vision community was the traditional approach was to essentially spend a lot of time and effort and from very smart people to devise in useful ways of essentially reducing the dimensionality of images and representing them with some features that would be useful for the purposes of say image recognition or say you know the problem that is illustrated in this picture is say Oh find the toy train that is hidden in this feature actually there's two of them one is here and one is there and there is a complicated algorithm to compute features from these from these pictures that allow you to make sure oh look this actually matches that even though it's distorted is rotated it's partially occluded and you can do all that if you spend a lot of time thinking carefully about how you're going to represent your images also part space model was something that were use so the idea was to really put a lot of thought into how to represent your input images but in deep learning we're lazy and we don't want to spend time doing that we want our machines to learn useful represent automatically learn useful representations of our input data in particularly envision the way we're doing this comes from an intuition comes from a from an analogy with neuroscience in particular the thing we're thinking about is the work by Hubel and Wiesel and of all their incredible and fundamental work the main ideas that we care about today are the idea that in visual cortex connections and activities are organized topographically that is to say that activities of neurons and visual cortex retain to some extent the spatial organization of the stimuli in pinching on the retina and on the other hand the that there is a hierarchical organization of different types of cells so that you could you could think of simple cells as being essentially a linear combination of whatever output comes out of appropriately arranged cells from the LGN and then that you have a complex cells that there are some more complicated combination of outputs of simple cells so these two ideas are what we care about today and already in the eighties in the early 80s work by Fukushima with the network called neo carbonate run which is schematize here was an awesome name by the way they this is he took directly inspiration from the work of Google and Wiesel and implemented this network where the idea of a hierarchical organization of different types of cells is represented by the fact that you have several layers on top of each other and each layer is connecting to the next layer and if it forward fashion and what the way in which he called these layers are simple layer complex layer simple complex simple complex which would correspond in modern terminology to convolutional pooling convolutional pooling etc we will see what that works what that means and the other idea about the topographic organization of the connection is that these connections across layers have some element of spatial locality so that there is this idea of some operation being performed like basically performing the same operation over and over again across multiple locations to go from one layer to the next right so but this is just to give the intuition from the connection to neuroscience but let's see what are the ingredients of a canonical modern convolutional network so this is the recipe that we're not gonna discuss in detail but basically the ingredients are this so you need to be able to perform compositions to take nonlinearities to do something called pooling and to have fully connected layers now fully connected layers is are something that we have it's basically what we have discussed this earlier with the multi-layer perception is the same thing and now we're going to go into a bit of detail concerning the other three ingredients to to see what see what they are basically and the way in which these ingredients are arranged is schematized by this figure here from the paper by Yann LeCun in 1998 as you can see here there is a convolution pooling pollution non-linearity pooling compositionally articling fully connected fully connected output so what is a convolution so to give them the idea of a convolution is basically intuitively the idea of having taking say some small filter and applying it over and over again at all possible positions of an image and taking the output of that as your next as the output of your operation so an example of this type of operation would be blurring an image by replacing each pixel in the image by an average of its neighbors so how will you do that this is the mathematical form of what I just explained you could do this if this is your image so this would represent numbers and these colors would represent grayscale grayscale values you could essentially you convolve your input with this filter here what that means is that you can imagine taking this filter which is just a bunch of ones well divided by nine so it's like 1/9 190 190 190 cetera kind of superimpose it to your input image and then basically take the dot product between all these elements and the elements in the picture so what this does is it just takes an ax because these elements are all the same it just takes an average across all these pixels so when you put it here on top of this area these are the values in the original image are all 0 so you're taking average of a bunch of values there are all zeros so the output is just a 0 I'm missing a there should be a zero in here anyway then when you move forward you have now you're applying now your convolution to the next position to the right and you have taking an average that is across eight pixels that have zero value and one pixel that has value of 90 so the result is 10 and so on so when you move again now the average here is 20 and in the same way you can see when you get to say computing the average for this point you can see that the average is not should be now 90 because all these elements are 90 so is it apply this operation sliding this winter all over the image you can see this is the your final output so this is your smooth version of your input image is it more or less clear okay so one tricky thing that you can notice about this is that the output image actually has this border here it doesn't have the so the real output image is just this part right it's not doesn't really have the same size as the input image and this is just purely due to geometry because there is no way of taking this filter here and applying it to every possible position of the of the input image in such a way that the filter doesn't kind of spill over the borders of the original image so because of just this geometrical constraint the output size will unless you're padding your input image with some values you're extending it somehow your output size will generally change according to this formula that if you spend a minute thinking about it you can convince yourself it's true the size of the output will be given essentially by 1 plus the linear size of your input minus the size of the of your kernel or your filter divided by the stride so I haven't said what this Friday is the stride is just the size of the step you take in the input for every step you take on the output so here the stride is 1 because every time we move by 1 in the input as our in the output we are also moving by 1 in the input if stride was 2 by 1 we moved by 1 here in the output we will be basically skipping a picture so you will be going say from here to like here so it's just a measure of like how far you jump on the input every time you move one step on the output so in our example anyway this is 1 so it doesn't really matter in this formula another thing about compositions is that we have seen this example that was done on just grayscale images but in general when you have an image you have if it's just most images you have we will have three channels because RGB it's a color image and in also if you're applying a convolution in the middle of a neural net you will have an arbitrary number of of channels in general so your filters actually they're not just two-dimensional things but there are three dimensionals and they have also a dimension a depth dimension and this depth dimension must match the depth dimension of your input layer and so for instance in this case where we have an image which is 32 by 32 and say three channels for RGB our our composition the operation what it does is it convulsed a full slice of this input with five by five say by three filters filter and it outputs just one single value right so it collapses the depth of the input onto just one value and so if you imagine sliding this thing all over the image you will get that okay according to the formula we just saw in the previous slide you will get something which is 28 by 28 by one because every pose every possible position of this filter gives you just one output value and then what you can do is you can have multiple filters so you can say if these are feature detectors you might imagine you know looking for different types of features and say you change the type of filter you recompute the whole thing you get another 28 by 28 by one bunch of numbers and you just stack it on top of the previous one and then you do it again and then you do it again so what happens in the end is that the depth of the output of your compositional layer will be equal to the number of filters you're applying so you have one element in this depth direction for each different filters and you call each of these colored things you call them feature Maps and these so each of them corresponds to a representation of your input scene through the particular filter that you are applying at that point so how this might look with a with a simple example is that say that you have one filter which is just kind of like horizontal stretching and the output would be something like that and then you might have say another filter which is just you know vertical kind of stretching operation and the output might be looking something like that and you might stack in these images and this way so you can have multiple filters and kind of stacking the outputs stacking the feature Maps so something that is very good about competitions is that as we have seen the dependencies so the output of your compositions preserve because the dependencies are all local they preserve somehow this the the the the special structure of your input you can kind of still see that you know pixels that are nearby here are related to pixels that are nearby there and at the same time the fact that we apply the same operation over and over again means that we have relatively fewer parameters to learn as opposed to say having a fully connected Network because we not only need to know what's in the filter and then the filter is the same for all they will positions so to make this a bit a little bit more concrete take for example an image say if you have an image which is 200 by 200 grayscale say and then you want to map this thing to a fully connected layer with say 40,000 hidden units that would amount to about 2 billion parameters whereas if you have the same image but you map it to a convolutional layer which has by appropriately choosing the parameters in a reasonable way also has 40,000 hidden units you would get that the output would be sorry you would get that the number of parameters would be only 4 million so it's a dramatic dramatic difference so this is one one one nice thing because we have way fewer parameters to reason about and and the other thing is that actually this way of reducing the number of parameters makes sense if especially if you think about a vision because if you think of these filters being again like some sort of feature detector to a very rough first approximation it kind of makes sense to think that the type of features that you're looking at say in the top left corner of your image might be kind of similar to the type of features you're looking at you're looking for in your bottom right corner of course I mean then you know to a higher order effect you can have you know you can think about why this might be different given the type of images you have but in general you can assume some sort of stationarity of your data in that sense so it it is a very clever thing to do to just share parameters across different positions right so that was for the compositional layers the next ingredient of ComNet or well really any any artificial neural network is you need to choose an appropriate non-linearity to put just in front of your convolutional networks and in this case what we use generally is is the so called raloo ryu unit which is a rectified linear unit which has this activation form so if you remember earlier in the multi-layer perception example I was I mentioned the hyperbolic tangent which would be something like that so like you know I can saturate in non-linearity the advantage of using a real ooh instead is that it doesn't saturate for very high values of the input which means that your your gradient never goes to zero when you have very strong but is of your input and in general you probably want to avoid having the zero values of the gradients in because if you go back and look at what we have written for back propagation if at some point you have a gradient you have you know you have a derivative which is zero it basically kills all the other derivatives that comes before that in the computational graph so and that basically means that you're not learning so you want we like gradients and this is the effect just to visualize what it does say this is a picture but our black points are negative and white are positive it only keeps the positive ones right so the final operation we need to implement components is pooling so pooling is a way is a Kherson operations what is you is a very popular thing to do is say max pooling is a popular way of performing pooling this is in this particular example this is done with filters that are two by two and strike two let's see what how it works so the fact that the filters are two by two it means that we're looking at one area of the input which is two by two and then we take the maximum of that and we discard all the other values so we take this red area we Loco the maximum is 6 and then in our output we write 6 then if you remember the definition of what a stride is to jump by 1 to the right in the in the output we jump by 2 to the right in the input so we go here where the Green Square is so there is an overlap in this case between the red and and and the green and we perform the same operation we discard the 7 or 24 we keep the 8 and we write it there and we do the same for the other two quadrants so this is a way of shrinking down your your inputs and it has two advantages so one is that it reduces the size of the representations in the layer that follows this makes your data more manageable it reduces the number of activations and parameters that you have later on and at the same time it introduces some invariance to a small translations because I mean if you're going to be moving this you know these values here I mean if you move this thing up I mean the maximum of this thing is always gonna be 8 I mean for a small adjustment of of this of these pixels so this is this introduces some invariants right so ok and just to wrap it up when we have finished assembling our convolutional neural network using the ingredients that we have just discussed about then we can just train it we define a loss and we just strain it by gradient descent and back propagation as we have seen for the multi-layer perceptron so just before I finish very quick historical nation so the key evolutionary steps that where that happened to bring us to the modern ideas of convolutional networks of course there are many more but these are just some that I picked so the first idea of taking inspiration from neuroscience to build the first and you know cognate Ron so the idea of this compositional structure of applying the same operation over and over again over the space by using local connectivity and alternating layers that effectively perform poorly then 20 years later Lynnette by a young lagoon that starts to show the structure of modern convolutional networks and very importantly uses the idea you know supervised learning backprop with backpropagation in gradient descent to learn the weights so that was a key advance and then really this is the prototypical fully modern convolutional network Alex net that has essentially the same structure as this as this other device that was published fourteen years later the main difference being scale so what happened between 1998 and 2012 essentially digital photography and the internet happened so that meant that people were able to download massive amounts of pictures from the internet and curate these huge training data sets using tools such as Amazon and Turk and that was the data I was needed to actually train much larger networks together with the dramatically increase the validity of compute and in particular the availability of general-purpose GPU computing so this were kind of like the key steps to arriving at the modern conception of a compact okay then just to finish we have talked about a little bit about how you do this they are you you know we were talking about this in the context of image specification but of course these tools are very useful as you all know we can use convolutional structures for many many things this is an example of image retrieval where you just take the internal representation of an image the computer is computed by the network and you just can use the you use it as a representation to compute lien distances between images to you know say something like oh give me pictures that look like this and this is what you get so other applications of course include update detection image segmentation captioning and so on and so forth so I think this is it for the comment part all right so we have been talking about all those information are like specially located so like they are represented a 2d X Y dimensions so when we're talking about a recurrent neural network before that we're talking about the sequential information processing why we're caring about this because like there are actually several applications they are not just like a two-dimensional or three-dimensional data we are talking about the time series like for example when we do Ling like natural language processing we're looking at a sequence of the sentence or paragraphs or speech the speech signals are like allocated over time and of course when you are watching videos those actions online they are like they are also doing like roll out over time and when we do captioning you describe the video so according to how the actions happens along the time of course you are doing like a biology there's a lot of examples in like a protein sequences or the molecular structures or activations so some classical view like I know some people probably from basic background of physics so one way like a lot of people dealing with the sequential information processing one view is from the classical dynamics we usually thinking about the world as a describe the world in in this state so for example the state of object as a positions velocity or accelerations and then we can apply some dynamics for example we know like the classical Newtonian dynamics and then apply to the state and we will see what new new position velocity and acceleration of the object so basically we just I do this over and over and see the new updates of new the new observations and of course is not just a closed system so you can also think about there are some external force applied to the system to dynamics so you can make more interesting changes and there's some another view we look at front a hatch in duration a point of view so yesterday we do the Provost of progress at a probability tutorial so you can think about when you have this sequence there are actually some hidden states like underlying that they have generation process and one common example people do in speech processing is they are like hidden state to describe how your mouths like a move to generate those phones and your listen so they are like transition probability to transition between this hidden state and also the emission probability to generate the data you are looking at so they are like organized along the time and also the emission to generate the observations so when we talk about recruiting your network I would like to take it as a pretty general form to describe and process this sequences and here it has a really a generic formula like this you take an input and then we apply the recurrent formula which could be the abscess or like functions to do the recurrent processing and when we apply for it it basically takes out the O state and give you out the new hidden states and then you can get output so very different from the view what we saw before is now the state is only consists of the hidden vector H so there's no explicit description of state so like the position or velocity or the discrete description of a category of the state so this are just a laboratory vector but we can think of it a summarization of what information accumulated till the time we are looking at so the general for is so just what I talked about you take a state and the input at T and you spit out the new state for the update information and of course when you take a find the observation you can apply another neural network to read out the value to figure out what could be the transformation from the state to the value you care about so we can take a more concrete example I hope this is a true I like this course and this is a sentence people do with like natural language processing and they usually wants you like take this this word for example the product speech or a sentiment Orla how to pronounce this word so for you and network we can actually do a person the texting batting to represent the world into a feature vector to describe of the word and then we have initially we have a hidden state but of course at the beginning we know nothing about the sequence or the word so we can just initialize the hidden state as all zero fact vectors and then to do the processing we give we fit into the input and hidden state you and your network and then give you out the updated the hidden state and then we can do another fully connected layer to do the prediction to whatever labels we are interesting at and then we can do it over and over again to unroll it over the input and time to generate all the prediction we care about and as I told about there are some labels like the part of speech we want to say okay this is for now this is a verb and this determinant and this is a announce so to do this we can have a evaluation function to come to compare this to to compute a loss and to do this to do this we are actually taking advantage of it's actually the same processing so we can combine the loss instead of just looking at one loss we actually accumulate all the loss together and then when do propagation web they propagate that through through all the units over the time so this is very general and very good property for learning so unlike we learned this each of a neural network as a separate function this allows us allow us to have a ability to process the sequences at different lens or a different different lens and different like ten durations so take an analogy between the convolution Network we just talked about when can come to illusion network we do talk about like filters so we can do the parameter sharing around the local attaches and similarly in recurrent Network we're also doing this parameter sharing but it is sharing the parameters along the time so the recurrent formula is like the filter we are doing in the 2d space so this is as I say is also margin a accumulator loss so there are more informations and also it can generalize better to different kind of sequences so but we talk about a lot of good thing you can do with recurrent neural network but one very big problem people are facing when training with this new recurrent net network is the very mannish vanishing gradients problem so take this a very simple three layer recurrent Nets for example so four inputs you apply the weight and the recurrent formula and over and over again for three times to get a loss so to do it we know how to do it for the very less layer you just take a loss and you want to update the weight as their layer and you're just using a chain rule to take a loss with respect to the recurring formula and the weights and this is easy but the problem happens when we're talking about if we are going back back in the time like 10 times step or twenty nine steps ago and we're actually unrolling the this divert derivative over and over so you can see like it expends really quickly and it could be like chin up to you like fifty and seventy times yeah so for for this you can imagine if this number is like a larger than one when you do several multiplication you just be kind of giant value and when you take gradient for the giant value you basically see the way it's just jumping over from here to there here there and then you basically basically learn nothing from from the gradients and another problem is if the value is very small so when the value is very small when you chain it up it becomes a even more smaller number and close to zero so basically when you updates the network the weights is basically just that and dozen moving anymore so you can see think like the information is not really flowing along the network so we doesn't really take taken advantage of the sequence and learning the dependency between the information at time 100 and time like 10 for example so this is has a problem in learning the long dependency but sometimes really one the long dependency for example when we talk about sentence talk about sentences or paragraph they have dependency from like the pronoun for the next two sentence is depend on the one I just talked about now so there are several approach people have been discovered to fix this problem so for gradient explosion people just very simple we just clip it to have a maximum value so it don't explode so I won't talk too much about that and for gradient vanishes so basically the information is not flowing along the time so people just people have been designing new new architecture in your network to keep the gradients in the cell or network itself so when when we do updates we can keep information from like a few time steps that go and then update the gradients instead of getting zero all the time so one very famous architecture people using you probably also hear about is a long short term memory so basically they are introducing memory into the neural network and there's other of variations and the proof have a very similar performance and easier to train so like Goethe recurrent net units and let's take a simple look at this long short-term memory so what they do is to introduce a case to decide if you want to let in the from the information flows through so in in addition to the hidden state I just talked about to summarize the state the information so far they introduce another state called self state so sale state is basically to decide ok if I help how much in from what's the information I want to keep and how much information I'll just release so they have a three different gates to control this sale state so the first one is forget so when when input get into of the cell the forget unit will compare the input and the sale state to decide ok what are the information or bits we want to forget because they are irrelevant to what I'm looking at right now and now they have the implicates to select from the input to decide what are the the input we want to update the current cell and then finally we can we decide from the output gate to decide what to which part of information we want to output and this has been nicely allow the neural network to trend even though the sequence is very long so it's being successfully are used at first in like a speech or a translation and then keep like having improvements and other people trying to simplify this a little bit just what I said the gated recurrent unit so when you are doing projects you probably will choose between different cells and see their performances so I would like to say the recurrent unit the recurrent network is one way to like flexibly allows you to assemble different architectures to dealing with your sequential data so what we talked about is a list vanilla recurrent net where we you you have input and met up to the output but actually you can be very creative to dealing with a sequence input and output so for example image captioning you just have an input of one image but you output a sequence of words to describe the object or action in the image and there's also like you can do it like inversely so for example you have a sequence of words or sentences and then you just want to output at the end have a one classification lab label to say this sentences is happy or sad and of course you're using like Google Translate they are also doing this kind of translation but since the translation though between different language is not exam mapping so they have like some many too many but they may skip some of the words or rearrange the words for the input and outputs and of course the example we just look at is two mapping between the word to its labels like part of speech and we can go on and on and to have a different examples especially when we talk about like actions and reinforcement learning people like to think about using this kind of sequence model to generate the actions or motor controls so this I have been we have been talking about recurrent net worth or convolution nets but what is all this training paradigm we are talking about now is we have a label and then we want to make some prediction to approximate this layer to to match this label to do the supervised learning but actually there are like some different kind of learning we want to look at and especially we want to do it on super in the in an unsupervised way where we don't have label but we still want to learn good representations or like good features about the input so once a one thing I want to talking about is an author encoder so this is in 1980s Yann LeCun have this idea so when we have this input if we don't know the label how can we know our network is learning something to describe your data so one trivial idea now not not that trivial one idea is to risk on earth is to have our network to learn to reconstruct the image and this also encoder is actually or do it in two step so first there's an encoder step to encode the input image until learned representation and urn technician one intended 10 dimensional vectors and then we can have a decoder network to decode what is encoded in the representation back into the original image and then for training we're actually trying to figure out how to update the weights to minimize this reconstruction loss so mathematically this is what we the four to generate the representation and use it to reconstruct the image and then compare against your input so by this way we're actually talking about learning the representation in the middle of the structure and and we are thinking if we learn a good representation it is actually can can keep all the information pretty well to regenerate whatever input we are given so there are some question about like oh there's some question about okay what this learn to representation are about so there are several views so first of some people may think about from like new linear algebra or like point of view they can think about this it's like multi many faults in the high dimensional manifolds or if you are familiar with like a PCA s there could be the reduced dimensions in the PCA space but there's actually another view to looking at this problem so some people in the generative model thinking about this as the let'em variable to generate the data so to make it more concrete we can think about there are some example from computer vision and graphics when you know this kind of variable like color shape and positions you can regenerate the shape in objects and in the scene so when we are to observe a data the goal of learning is actually to uncover the distribution of the data so we can like recent hole and regenerate it and one very desired poverty and why we want this generative model is once we have a model we can do a lot of interesting things using the model for example you can simple a new data point you never seen before from the data or when you get a new instance you want to really see if they are like front can describe using these variables or the similar distribution you can evaluate the likelihood of the data or more more important see when people are talking about representation you can shred the latent features from the network like using the letting variables you describe the model and data but there's a really big problem to do this also nicely when we talk about intuition but it is a problem to evaluate the district to come out the model and the distribution because it is really hard to computing the posterior exactly especially for some distribution they are really complex and it is hard because we need to you like simple through all possible let'em values and then do a marginalization over in the landscape which make it intractable so some idea people have been proposed this year one way is the variational auto-encoder so talking about the generative model we just described so you can think about there is a latent variable and there's a decoder name network to generate the observed data so but it is really hard to compute to compute the distribution so one idea is instead instead of instead of computing the likelihood of the latent variable we are we are doing this with a much simpler intractable distribution so it could be like a Gaussian or like some easier distribution which we are manageable to do it and then the goal is is from the data we learn we learn this approximate inference network and then use it to reconstruct the image so the learning objective now is becomes first steel we have a reconstruction error to figure out what's the difference between the reconstruction but there's another measure is how close this approximate distribution approximate network and how close they are to the real distribution of letting variables so this allows us to trend trend to generate the data so when you are only looking at this network we are doing the inference from the data to figure out what's the latent variable which are important and when you are using this part of network we are trying to using the learned generative model to generate the data and evaluate the observations of course there's also another formulation you may already hear about a lot of times from media so instead of formulating the previous probability distribution likelihood and posterior exactly so another idea is doing adversarial training so in several training it is actually also an implicit generative model so instead of talking about we want to approximate which distribution it model it as a minimax game so there's a instead of a generator they introduce the discriminator to to to to guide the distribution changes so for the discriminator they want to distinguish if the generator generate the data is simply it's the same or it's a generate data are they like a real or the fake samples and for a generator the goal is basically to generate the fake data which will close to the original distribution to fake to fold the discriminator so take this example they have in the paper they're like that is the original data distribution and we can initialize the discriminator and generator Ren Dooley at the beginning and learning starts the discriminator will figure classify what are the data points are the fake or real and then the generator take the the feedback take a gradient and to learn to approximate up to to the distribution to get closer to the original data distribution and then they do it over and over again till the conversions and the final learning goal is we is to maximize the loss of discriminator because by the time you basically cannot tell what is from the generative model or the original they'd have distribution so this is just a some idea about how people thinking about can we learn the data unsupervised Li or like from a different perspective like generative model and allow us to do more tasks and of course this is are just some quick and high-level introduction of different kind of network and we are only touching about the surfaces so feel free to like read the original papers or some link we included in this slice and this is a older tutorial we are talking about today and we will have a before we take questions so there we will have hands-on session tomorrow and we will do a PI torch tutorials all the topic or another generative model ones we're talking about today and if you are what if you want to run that use on your computer you can install a PI torch and Jupiter notebook but if you don't want to install it on your computer or it takes too much time to installing it so don't bother to do it we are going to Rhonda using Google collab so you can run the GOP don't notebook and installing everything from that we will have an instruction for that tomorrow so yeah so make sure if you don't want to install you have your Google account so you can do all the examples we gave you so let's so any questions all right if no we are [Applause] 