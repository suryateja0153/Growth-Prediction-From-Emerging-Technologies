 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu all right so that's xkcd comic kind of points out and Siesta can be very difficult to figure out when something is just really hard or something is virtually impossible and until a couple years ago people thought this idea of image classification would be something that was closer to the impossible side but with the advent of deep learning technology we've made significant strides in which classification and now the problem is actually quite practical so today we'll be going through how the process of image classification with deep learning works so well first we'll talk about what deep learning is and then we'll move into some of the image processing techniques that researchers use followed by the architecture of the convolutional neural networks which will be the the main focus in our presentation we'll also talk about the training process and then go through some results and limitations of cnn's and image classification so what is deep learning well the term is particularly vague and it's purposely so for a couple of reasons the first is mystery is always good for marketing but the second reason is that people earning actually refers to a pretty wide range of machine learning algorithms they do have some commonalities they all seek to solve problems of a complexity that previously people thought only people itself so these are more sophisticated classification problems than traditional conventional machine learning algorithms can do so how do they go about doing this well all of these deep learning programs tend to take all the processes that need to happen and split them up they've got different parts of their program working on different things all in performing calculations and then at the end it all comes together and we get a result of course this isn't unique to deep learning and in lots of just even system decentralized their calculations but the key thing about deep learning is that every part is performing these calculations the influences are not simple calculations they're not we'll do this one simple operation over and over again on a lot of data and then we'll get a result at the end each part is performing some particularly complicated complicated process on all the little parts before they come together so why is this architecture a good idea why did engineers come up with this sort of decentralized multi-layered complex process well we take the example of image specification it turns out that the human brain does a pretty similar process so here's the human visual system and it's very much a hierarchical process so you begin by moving from the retina into the first areas of the brain and as the information gets processed it moves from one region of the brain to the other and each spatial element of your brain is performing an entirely different calculation for example the v1 area over here is picking out edges and corners and then over here a couple steps later and before you're straighten group those fingers together and so the brain kind of operates in a way that is very similar to the way these you know let works operate so let's talk about how to classify face if I asked you guys how would you classify face what is the first thing you might do well as I mentioned before the first in our brain does is it finds these edges right the first thing to do is identify where the face is versus everything else now does anyone have any idea as to what we could do with the next step Julian immune idea maybe you could group these edges together right we could maybe identify some of these features that we're working with so these are things like noses and lips and eyes right and then what would we do after we have these you know individual features Steve well maybe we could poop some of those together exactly yeah we can organize them into what we know the pattern to be right we know that a face has to have two eyes above the nose and then above the mouth right so that is precisely what a neural network actually ends up doing we'll walk through the process of how it does this later on the top but as you can see the intuitive way that we would cost my face and the way our brains are wired to do it it is pretty similar to the way that we've gotten these neural networks to operate so like I said we're talking a lot about these convolutional neural networks there aren't other types of architectures involved like we mentioned before deep learning is a pretty wide variety of elements but we're going to focus on these cnn's so give you kind of a precursor idea of how good these CN NS are this is results from the imagenet competition so the immature competition is basically exactly what it sounds like a bunch of computer scientists get together and see how many images they can correctly classify and you know their error rate was pretty high you know almost almost a third error rate over here in 2010-2011 and then in 2011 these CNN's are introduced to the topic and the error rate plummeted as you can see over here 2015 we've got a significant improvement in these submission of competitions so clearly these CN NS have been very effective and it's definitely something that is exciting the field I'm happening right now all right so now we're gonna move into image processing okay so right give a sort of a nice overview of where we get this concept of neural networks but let's take a kind of a time travel and going to a quick history lesson so something was I had a chair and I wanted the computer to classify this chair right I have some a priori knowledge about what sort of things make up a chair so I might be interested in looking at arms in corners of the chair legs things like that so I would go ahead and to the feature engineer my my discovery scheme to be looking for specific things so I'm gonna talk about some techniques that are traditionally used for example chairs doors these things have corners so I might use an image processing we call the Harris corner detector where we basically look at large changes in intensity as groups of pixels move from an image to image that indicate the presence of corners and you can use use common corners to say that okay all of these images are chairs or doors whatever similarly I want to say I have a bunch of pictures of chairs of different sizes but they all must have you know so many corners or something so typically we use a search algorithm which is scale-invariant feature transform mislead says across different sizes I still should be able to extract information about the placement of corners say another common technique that's used in image processing is what we call hog histogram of greetings so basically for example in this image if I want to say I want to find all the images that have faces in them or consist of faces let's say I might come up with the template of a face that basically assigns gradients to groups of pixels that form that that an outline of what sort of looks like a face then scanning across my sample images and say ok a face is present in this image obviously varies from areas in the cap and a logo back here have become a face but this is sort of the traditional approach but here's the problem what if I don't actually necessarily know what features are the most critical depending on the data set that I get I want the system itself to figure out what techniques to apply without having any apparent a priori knowledge about the data set so this is exactly the idea of CNN's the convolutional neural networks we want the techniques to be learned automatically by the process by the system so if I'm trying to classify faces I want the system to figure out that eyes and ears and nose these are the most important things or if I'm trying to classify elephants their ears and trunks are the critical features without me having to say ok we're going to do coordinate detection so on and so forth so this is the idea so to be able to understand this process in greater detail I'm first going to go into a little bit of math and the idea is to present the most fundamental operation here which is the convolution so this is the formal definition of the true menschell convolution and since we're working with images we're only considered considering the two-dimensional case so in a more graphical presentation which is a little bit easier to understand than just seeing the formula the idea is that we have a kernel or a convolutional filter that we seek to apply on another image and that extracts some information about that image that we can use to help us classify the file so assume that this is our kernel or this is our subject and suppose this there's so suppose we're applying the the kernel from here to the image that's in green city is we want to slide this filter across the image and what we're basically doing is a succession of dot products so if each placement of the image we multiply the overlay numbers and the sum becomes a good image honorable vault upload so this is basically the way the process works you've probably noticed that there's a reduction in dimension and Henry will talk a little bit more about why this is when we get to it that your conversation so these are examples of what information we get by applying the convolution so you see at the image of a tiger at the top left when we apply a filter that's a low-pass filter basically it's it's a Gaussian then we get low spatial frequency information about this image so basically we've learned it and this tells us something that's specific that we might want to learn so the kernel actually looks like a two-dimensional Gaussian function that's been distributed across this three by three curl similarly we might be interested in high spatial frequency information so in this case we're looking at sharp sharp features so edges horizontal edges or vertical edges so a question for you is if I have this kernel which of these outputs when this kernel is applied to the original image which of these outputs do you think it produced yeah that's exactly right and it's probably pretty easy to see why that's the case given that the numbers are all horizontal bands here lastly we also might be interested in extracting information at a particular frequency so we can take the difference of you know say hi a high-pass filter and a low-pass filter and add if you're too low frequency you can extract information about that as well okay one last sort of helpful piece of information is that there's another way that you can think of the information that's learned at each stage because our convolution can also be thought of as Fourier transform in the frequency that maybe don't if you think of the image transformation that way so from an image perspective what Fourier transform is is basically a sum of a set of sinusoidal gratings that defer say in frequency and orientation and amplitude and phase so you can think about the zebra image here that's actually a composite of a difference of different gradients that might look like this and the coefficients the Fourier coefficients would be how much of each of these pieces come together to make that final image so just to get a sense of what kind of information this could convey we typically take a Fourier transform I break it apart into magnitude and phase representation so you see magnitude and you see phase so those are meters weren't particularly clear but this is a really good example for this so if we take the Fourier transform of all the horizontal text here you see how the magnitude reflects this you can go back to the maps to understand why it's reflected in a vertical working but and similarly if I'm going to take the same image and rotate it and then ask for the Fourier transform you see how that information is contained very clearly the magnitude spectrum so these might be things that if the network would learn at each stage to identify the size as a text or as body of text that's tilted one way or the other so on and so forth so with that we can now go into what the actual architecture a convolutional all right so as as was said earlier in order to classify or detect objects you actually need certain features you need to be able to identify these features and the way you can identify these features is using certain convolutions or certain filters in many cases we don't know what these features are and we and as a result of that we don't actually know what the filters are to extract these features and what convolutional neural networks allow us to do is actually determine determine what these features are and also the turn about the filters art in order to extract these features now the idea for you know convolution neural networks or the idea for replicating how the brain works started in about 1960s or 1950s after some experiments by uh by Hubble and Wiesel and what happened in these experiments as can be seen here is a cat was actually shown a light band at different angles and the neural activity of the cat was measured using an electrode and the outcome from these experiments show that based on the angle at which the light was shown the neural response of the cat was different as you can see here the number of neurons as well as the neurons that were firing were very different based on the angle so what you can see also here is really a plot of the response versus the orientation of the light and what this led that problem easel to is the idea that neurons in the brain are organized in a certain topographical order and that each neuron both acts a specific role and only fires when it's you know when a specific input is shown or when the angle is shown or the angle is specified now the first step to actually replicating how the brain works in code is really understanding how you know the building block the neuron works that's a quick reminder of 701 to here so the neuron is actually a cell with dendrites nucleus axon and E terminal and what the neuron actually does is aggregate the action potentials or the inputs it gets from all the neighboring neurons that are connected to it through the ten nodes and it sums these actions potentials and then compares them to certain threshold that it has internally and that would determine whether or not this neuron will fire an action potential and that very simple idea it can actually be replicated in code so an artificial Europe now the official neuron looks looks very much like it and like a natural one so what you would have is the set of inputs here we have three inputs there are some inside of a cell or inner on the sum here is not just a regular sound it's a weighted sum so the neuron specifies some ways which you can think of this how much it values the input coming from a specific specific neuron and then the Emperor is multiplied by its weight and then the total sum that that the neuron computes is then fed into an activation function that produces the output that the neuron then that then basically produces now what we just saw here is is really a simple neuron a single neuron you can't really do much with just one year on so what you would do is really combine these neuron in a certain topography or in that case we have a network with with seven neurons organized in three different layers and what you can think of of that as really as one big neuron with 12 inputs and one output so in for example in the case of the chair that was previously mentioned if you're trying to identify whether a specific image has a chair in it or not you can these 12 inputs here could be some sub images or some you know small areas of the initial image that you feed into the network and the output here could be a yes or a no whether the image has a chair or doesn't have a chair and that is really the concept behind convolutional neural networks which we'll go into into details in a bit so what each neuron would be doing in that case is really just performing a dot product which if you aggregate that with you know with the dot products computed by each of the other neurons you would be a you with obtaining volution so what we have here is three inputs if the input in that case is an image or a sub image then the inputs would be pixels the weights that you would be using here would be the filter weights which you know is the filter that you use in the convolution and then the sum here would be the dot product of the weights and the inverse and that sum would be computed by a specific neuron in your network then that would be the convolution step and then that convolution step would happen at the first layer in the networks you'd be applying this to the input but you also would apply this and the second layer and third layer that case we're only showing what happens in the first layer the next step after the convolution would be the activation step so the dot product computed here would would be there's a function that would be applied to that to the sum and then predict that function to produce the output of the neuron and this is where the activation layer is you also have another activation layer here and the final one here what we just what we just went to now or you know convolutions and activations but this is not the only thing that actually happens in a neural net well your we also have is a step called sub sampling which we will we will be talking about next for now we will dig deeper into the activation and specifically you know what activation functions to use in that case we can see that that's that's in your on and what the neuron is doing here is the way that some that we talked about or the dot product and then the output from this should be fed into a certain activation function common activation functions are sigmoid 10 H or rectified linear unit and we will go through each one independently so here we can see the sigmoid activation function so what this function essentially does is map any input to an output in the range of 0 to 1 and it's defined as 1 divided by 1 plus e to the minus X the other common activation function is tan H and maps any input to an output between minus 1 and 1 and then finally would be the rectified linear unit which Maps an input to itself if it's positive or to zero if it's negative now in theory you could have you could use any function as an activation function you know in your network but that's not what you want to do in practice you want your activation functions to be nonlinear for one main reason that the goal of the activation function is actually to introduce non-linearity in your system and if all your activation functions are linear then you would essentially be having a linear system which prevents you from achieving the level of complexity that you would you would ideally want to achieve for the neural network and there's a formal proof for it as to why you need nonlinear activation functions they don't all need to be nonlinear but you need to have at least a few nonlinear activation functions in your network and the proof is available in the appendix or the link to the paper that has the proof so after after we've discussed what happens at the activation layer now we want to talk about convolution so as I was as I said earlier an image is is obviously a two-dimensional image but we're using RGB images so we actually need three channels so what this means is that an image is actually three dimensional and each 2d matrix represents one channel one of them corresponding to all of them corresponding to G one of them corresponding to B so a 32 by 32 image would essentially be represented by 32 30 by 30 2 by 3 matrix as can be seen here so what happens at the convolution there so here we have a nice animation that shows what is happening at each you know and each convolutional layer and so assume we have a 5 by 5 by 3 filter so what this essentially would be doing is covering a certain patch in the original image which is 32 by 32 by 3 so what we can see here is that for that 5 by 5 by 3 patch in the original image we have a neuron that is actually performing the dot product on all the pixels in that specific patch so what is happening here is the pixel values which in that case we have 5 by 5 5 by 3 pixels are being multiplied by the filter values and this operation is being performed here then after that dot product is performed on it's fed into an activation function as can be seen here and this produces the output of this neuron now this is what this single neuron is actually doing it's just covering that area of the original image what you would have in a neural net is many neurons each covering a certain area of the original image and if you aggregate the output of all of these neurons what you would be doing or performing is essentially a convolution on the original image and to formalize what happens here or what what's the output that's being produced from that operation we can look at that from a more mathematical perspective so if you have an input of size H 1 w1 d1 and you're performing a convolution where the filter then the output w2 will be related to w1 with the following formula so w2 is w1 - fortuity plus 1 and the same formula applies for for the height and the depth will actually be the same because in that case we're using a filter that has the same depth or 3 as the original image so what this what this would produce in aggregate is if you have 28 by 28 by 1 neurons each one performing a dot product on some pixels in the original image the output would be an activation of size 28 by 28 by 1 and the actor of each neuron would be one pixel in the activation map now if we go back to the points that we made earlier one thing we said was that you the reason you using your network is because you don't know exactly what features you want to extract and you don't actually have specific filters that you want to apply to the image so ideally what you want to do is have multiple filters being applied to the first image and perform multiple con and this is what you can do with multiple neuron layers so what we described before was just for one year and there in that case we can assume we have five different neuron layers each one performing a different convolution on the original image so in that case we would have 28 by 28 by one neuron per layer and then if we aggregate all these neurons together we all need to multiply it by five and that would be the total number of neurons we have in that specific and that specific network so this this actually leaves us with a pretty complicated system we would have many parameters the neurons have weights the number of neurons is also a parameter so how do we actually formalize that if we have an input volume of 32 by 32 by 3 which is our original image and the filter size of 5 by 5 by 3 then the size of the activation map that would be produced would be 28 by 28 by 1 then in that case we also said we have 5 different neuron layers that perform five different convolutions then the total number of neurons would be 28 by 28 by 5 and then the weights per neuron or 5 by 5 by 3 which is 75 in that case we're assuming that the neurons in depends the e-track of their own weights this could be simplified to each layer having their own weights which would tremendously reduce the number of parameters but in that case just to get an upper bound this leaves us with a total number of parameters of 294,000 and this is just using a 32 by 32 by 3 image you can think of you know this is a pretty small image so if you have a bigger image you can you will have many more parameters great so what we just saw now and described were convolutions activations and this these these steps happen sequentially in your network near the convolution here and I work specifically as can be seen here now one step that also happens at occasionally is sub sampling and we'll discuss that step in details here so there are two main reasons why you would actually sub sample your input one is dogs to reduce the size of your in your input and you know your feature space but also because you want to keep track of the most important information get rid of everything else that you don't you don't think it's going to be relevant to your classification and the common methods used in subsampling are either max blue or average bullying we will describe max pooling here so what happens in max bullying is essentially you're dividing the image into different sub images non-overlapping sub images and you would perform an IMAX operation so in that case if we consider two by two filters we would split the image which in that case is four by four it's better to four sub images and for each two by two square would take the maximum in that case you know for the first square it would be six then 8 then three and four and the reason that actually works is because what you want to do is really keep track of the response of the neurons depth or did the highest response produced by your neurons and that case for example the first highest response in the first squared is 6 and that means that if you get that high of a response he means that something's been detected in the image or it's been detected and this was this is something you want to keep track of as you go forward in your network and although this you know moves around the location of pixels because you can think of that as you know sub something it image it does keep track of the information you care about because you only care about the fact that something it has been detecting in the image at this point you don't really care about where it's located in the image and you want to keep track of you know all the features that your neurons have detected in order to be able to eventually classify classify the input correctly so if you have multiple feature Maps so in that case if you have to 1024 by 1024 by 64 what your sub sampling operation will be doing is reducing reducing the height and the width so the depth would remain unchanged so in that case you would go from 224 by 224 by 64 to 112 by 112 by 64 and that would be reducing your output size by by a factor of four and formally what this would look like is if you have input of size each one w1 d1 your I put W the size of your output do edit your input in the following way so w2 would be W 1 minus the pool with plus 1 the same applies for H 2 and the depth would remain unchanged right so these are these are essentially these are the steps that happen in a convolution your network then what you could be doing is repeating these steps on a certain a certain a number of times in your in your network but eventually you would have to make a classification and decide you know and that can our case whether our image has a chair or doesn't have a chair so how does that happen so after you perform all these steps there's a step that happens here that would allow you to make that prediction and that step is usually called a fully connected layer or a multi-layer perceptron and what this essentially is is layers that are very similar or exactly the same as what you had before except that every neuron in a layer is connected to all the previous neurons so what it's allows you to do is really consider everything you currently have about your input or everything that's left about your input to to and and compute a dot product and that rather than specify a time focusing on a subset subsample of your input like like previous layers do in that case if you're actually trying to classify your input into four classes you that you only have four different neurons in your Apple layer each one corresponding to one of the classes that you have and then you would perform the same operation as you would in a previous layer compute the dot product and then once you obtain values your the values that every year on you would perform an organization operation and all the output the summarization operation is called softmax or normalized exponential and what it does is really puts more weight on the highest value and by computing the softmax at the output you're able to compute you know the posterior probabilities and this will allows you make them more informed or basically make a classification decision on on your input great so that's everything and now the next step would be talking about back propagation okay so now that Henry's given us an overview of the entire architecture to see and then I'm gonna quickly spend some time and talk about standard pre-processing tricks and tips that people might use on the image data set before they actually feed it through a neural net to classify the images um so let's imagine suppose we have a table a dataset X and there are n number of data points in the dataset and each point has a dimensional value D so they have D features per point so in this example we use these graphs as an example basically our original data here has just two dimensions and it spans this range of values so for example if we want to Center this data what we would do is the mean subtraction so you basically subtract the mean across all the features of all the points and we basically Center it and you can see that transformation here and then we might again go for normalizing the dimension so that you have it the data points and the same range of values in both dimensions so you can see that transformation and how it's taking place here and we just divide by the standard deviation to do this something that's very commonly done is called PCA or principal component analysis and the idea here is sometimes we have a data set that has a very very high dimensionality and we would like to reduce that dimensionality so basically what our goal is is to project a higher dimensionality space onto a lower dimensionality space by taking a soft side with those features and if you've seen a little bit of 1806 from linear algebra the way we do this is by generating a covariance matrix and doing the single IRB composition and also cluster the math now but that's that's the idea and you can kind of see here where the original data spanned two dimensions I'm D correlated so that it spans a single dimension and either with this data you might want and sure that it's whitened which is the single you want the values to spend the same range in both dimensions so then you would just divide by your eigenvalues to get the wedding data this last bit is something that's very commonly done as a pre-processing trick though people aren't entirely sure why it works very well very well or that it that it really does help but it's something that people do and it's called data augmentation so basically if I have a data set that contains you know a bunch of a bunch of interests a chair as much of images of tables and then a bunch of images of say trees I might want to intentionally augment that data set further by introducing a few variations on these on these state manages so I might take the chair image rotate some reflect a few more scale crop the amount the color space or just kind of have a process that does this randomly to create more variation on the same data set and this is what kind of a good illustration of why you know why is makes a difference I can give an image here of what looks like a waterfall or some spotted nature and simply just inverted the colors and what I see if I were to just see this limiter luma thinking looks like a curtain north of a texture or something yeah yeah into a human perception these they would just have effect two very different meanings and so it's interesting to see what effect they would have on a neural network and if that will go over to image classification results so so far we've seen how convolutional neural networks are built and certain image processing techniques we can use on the input images to get them into formats that are there for the classification process but so far it seems a bit abstract it's good to know how CNN's word why CNN's work but why don't we take a look at some of the practical results from CNN's and what they're used for so so that when you're doing watching this lecture you can go home and try stuff try classifying images on your own time with that let's first revisit the original competition I hope you remember the graph at the bottom from the beginning of lecture where we sort you system the use of CNN's CMS came onto the picture in 2012 but the winning CNN for 2012 was used on the 2010 imagenet competition as well and it managed to bring down the top-5 Airy 2017 which is pretty much on the same level as how performed in the 2012 competition when it was first used which was at point 60 so this just goes to show you that accomplished in here and that works are the state of the art when it comes to energy conservation and that's why we're currently focusing on that but you might be wondering what the image net competition exactly looks like what David just look like so let me take a look at that as you can see here these are certain images from the image net competition underneath each image is a bold caption which is considered to be the ground truth or what the competition believes to be the correct classification of damage underneath that ground truth you see a list of five different labels now these five labels are produced by a convolutional neural network and the different bars the different length bars some pink and others blue represent how confident the CNN is the waited season that image is not specifically well as you can see in certain examples the CNN is pretty confident in the house the correct answer for example when you look at the container ship it's pretty confident that what's in that image is exactly a container ship there are certain cases when it doesn't get the correct label on this first try but does happen spot five labels for example you can see in grill and mushroom now the funny thing about the mushroom image is that what it thinks the image should be classified as bizarre Garrett if you don't know a Garak is actually a type of mushroom and in fact it's a mushroom in that image and it makes sense that their confidence levels are pretty much the same I get it just slightly it's more slightly more confident that what it sees an image is a Garrett one but there are certain cases one the CNN fails to classify the correctly its top five labels this would be registered as a top five air as you just saw in the previous slide about the top five very what an example here on this slide is cherry now the images are composition believe that this should be classified frankly as cherry even though there's also a domination in the background the CM on the other hand is pretty confident that would sees in this image is the Dalmatian but if you look at some of the other results within the top although it doesn't just share you at all it does get served fruits that you may think looks sort of like cherries like great or elderberry so the CN n those actually picked up on two different distinct objects within the image but as a result of how it's built or its training side and so classifying it as a combination but opposed to show you that CNS could also be used not just as it was supposed to be she was also as object detection which we did not touch upon in this lecture so I'm not going to go further into that now this is all fun and all but what about some real world applications so this is a study that they did at Google with Google Street View house numbers where they use the CMM to classify photographic images of house numbers as you can see here of certain examples of these customers what they look like now so what the CMM was task of tasked with doing was that was supposed to recognize the individual digits within the image and then understand that's not just one image one digit that's looking at but it's actually a string of digits connected and successfully classified as the current house number this can be quite challenging even for human sometimes when the image is quite blurry you might not exactly know what the house number exactly is but they managed to get the convolutional neural network to operate around human operator levels so that corresponds to around 96 and 97 percent accuracy and what that enables Google to do is that they can deploy the CNN such that the CNN automatically extracts the house numbers from the edge from the images and uses that to geocode these addresses and it's gone to a point where the CNN is successfully able to do this process in less than an hour for all those Street View house numbers in France and all the friends now you ask me where did this could be useful for if you don't have a lot of if you don't have access to a lot of resources to actually do this geocoding process pretty much latitude and longitude to street street addresses then you might want then your only resource might be actually these photographic images so you actually need something hopefully not human but some sort of software that can do this successfully and so this is for example a place in South Africa bird's eye view not sure if you can exactly see but there are these small numbers on top of each of the houses all of these numbers were extracted and correctly classified using this previously seen CNN another example is from robotics is recognizing hand gestures so obviously robots come equipped with a lot of different hardware they can send sounds they can also capture images of their surroundings and if you're able to classify what you see if they're robots able to classify what it sees that it can actually act upon and take certain actions that's why it could becomes really helpful to successfully classify the images so this is what they did using hand gestures where there were five different classes each class corresponds to the number of extended fingers so a b c d the top for a corresponds to the same possibly all have two fingers sticking out the barn row has three fingers sticking out so that's another class and they've actually dipped down to get the error rate down all the way to 3% so 97% of the time the convolutional neural net correctly classified the hand gesture and you can use these hand gestures then to give certain commands to robot and you can train if you train to CNN to act up on something else besides hand gestures for example if it's in some sort of terrain you train there are certain images that you might find in nature then it can take those classifications and act upon it once it sees for example a tree or some sort of body of water it's all things to image classification now obviously gestures are not necessarily static you could be waving your hand and so that would require your a temporal coppola component so it's not just an image you are looking at the video and so it follows that it follows that we can probably extend image classification into video classification after all videos are just images with an attic components specifically time obviously this the added temporal component comes with a lot of additional complexity so we're not going to dive into any of that but in the end it comes out to the same thing you extract features from the videos and you attempt to classify them using convolutional neural nets so why don't we look at a study done again at Google where they extracted a 1 million videos from YouTube sports videos with or is somewhere between 400 to 500 different classes and they use TMS to attempt to classify these videos now they use different approaches they use different approaches different tests different types of CMS but I'm not going to go into but as you can see here these are service spills from these videos where the caption highlighted in blue is the what the correct answer should be and underneath it the top five labels that the convolutional neural network producers the one highlighted in green is the correct that's supposed to be a correct answer so you can see on all of these it gets it within the top five and used for the most part within the top two and it's pretty confident when it does get it correctly now when I said that they use different types of classifiers some of them work more a stack possible were they just were they were just trained on stills within these images while others were what they call future ones where they sort of add the temporal component by fusing in different stills from these footages together now the current accuracy rate the best one they've achieved so far has been around 80% accuracy within the top five label now 80% accuracy is nowhere near what we saw with the image that classification we're in 2015 they had managed to get up to 98 or 99 percent accuracy but obviously there's way more complexity involved in this so it makes sense that it's not quite there yet but it does provide a good benchmark and something to improve upon in the future as well now that being said every convolutional neural networks do come with certain limitations they're not perfect and so Julian will now talk about the limitations thanks Sully so only talked about the imagenet competition and talked about how the recent winners have been convolutional neural nets so before the best was about 26% top-five arrey but now they've actually gotten it down to a four point nine percent top-5 area and that was the winner of that competition the 2015 level is actually Microsoft they've got the current state of the art implementation and so because it's the imagenet competition that means they can identify exactly 1,000 different categories of images so there are a few problems actually with the implementation and we're just in general with convolutional neural nets so one of them is that thousand categories while it may seem like a lot of English notice action one of the largest competitions that's not actually that many categories so it doesn't contain things like a hot-air balloons for instance so so these these things that children would be able to classify the neural nets actually aren't able to even in the biggest competition and each of these categories also requires thousands of training images whereas you could show a child a couple examples of you know a dog or a cat and they'd be able to generally get a feel for what a dog account looks like it takes thousands of images per category for the neural nets to learn them which means the total number of images you need to train for the internet competition is over a million and so this leads very long training times even with all the heavy optimizations that Ishwara it was telling us about how efficient competition is this still takes weeks to train an ottoman multiple parallel GPUs is working another to train in that there's actually a more fundamental problem with neural nets as well so so here on the Left we have school bus some kind of bird and an Indian temple and all of these images on the left side are actually correctly identified by called convolutional neural nets but when we add the small distortion here in the middle that doesn't change any of the images perceptively to the human this actually causes the neural network to miss classify these images and now all three of them are ostriches so that's a little bigger how does this work how did we find those distortions so here in the left side we see how a neural network typically works you start with some images you put it through the different layers than your own network and then it tells you a certain probability that is a guitar or a penguin so it classifies it and so we can use I guess a modification of that method by applying an evolutionary algorithm or a hill climbing or gradient descent algorithm we take a couple images when you put them through it classifies it and we see what the pasta fication is and then we can do some crossover between the images so we take the ones that look a lot like what we're training for the guitars or penguins in this case and we we take the features of those that identify very strongly as a guitar and we combine those together in a crossover face this is for the evolutionary algorithm and then we mutate the images which is making small changes to each one and then we reevaluate by plugging it back in through the neural network and only the best images the ones that looks the most like a guitar or penguin are then selected for the next iteration and this continues until you get to a very high identification rates even higher than actual images of the objects so using gradient ascent these are some of the images that you could produce if you start with just a flat gray image and then you run it through this algorithm so if you're here on the side we have the backpack and you can actually see the outline of what looks like a backpack in there and over here we have what looks sort of like a Windsor tie right here but but all of these objects perhaps there are things in these other images but they seem to be lost in the LSD trip of colors here so that's kind of strange that's definitely not how humans do it so let's try a different method what if instead of directly encoding which is where we change individual pixels what if we change patterns and images like different shapes then this is the kind of output that we get so never left we have a starfish so you can see that it has the orange and blue of the orange of the starfish and also the blue of the ocean of the environment the typical images of star fishes are taken and you can also see that it has the points you know the jagged lines the triangles that we associate with the arms of a starfish but the strange thing here is that they're not arranged in a circular pattern they're not pointing outwards like this like we would expect of a national starfish so clearly it's not watching on to the same large scale features that humans do it's actually just the lowdown features even though it's a deep neural network it doesn't it doesn't grab on to these more abstract concepts like the human would so the reason for this problem or at least why we think neural networks aren't as good as humans things like this is the type of model so a human would have more of what's called the generative model which means if we have examples here these dark blue dots say of lions a few examples of like images of lions then we could construct a probability distribution and say that images that fall somewhere in this region are lions and over here we have a few examples of giraffes say that's so anything that falls in this region would be a giraffe and so if you had a red triangle in here that would be aligned but if the red triangle is instead over here it actually wouldn't classify at all we wouldn't know what that didn't mean say that that's something other than a lot better giraffe but neural networks don't work the same way they just draw a decision boundary they just draw lines between the different categories so they don't say that something really far away from the lion class is necessarily not a line it just depends how far away it is from a decision boundary so if we have the red triangle way over there it's very far away from giraffes and it's just generally closer to lions even though it isn't explicitly very close to it at all and that will still be identified as a lion so that's why we think I were able to fool these neural networks in such a simplistic way or in such a really abstract way so the main takeaways from our presentation that the salient points are that deep learning it is a very powerful tool for image classification and it relies on multiple layers of a network so multiple processing layers also a CNS outperform basically every other method for classifying images that's their primary use right now we're currently exploring other other uses but that's generally where it's at and this is because of convolutional filters are just so incredibly powerful there's very fast and very efficient also back propagation is the way that we train neural networks normally it if you were to train a neural network that has a lot of layers that actually an exponential growth in the time it takes to train because of the branching when you go backwards because each neuron is connected to a large number of neurons in the previous layer you get this exponential growth in the number of dependencies from a given neuron we using back propagation it actually reduces it to linear time to train the networks this allows for efficient training and even with back propagation and convolution being so efficient it still takes a very large number of images at a long time a lot of processing power to train a neural networks also if you'd like to get started working with networks there are a couple really nice open-source programming platforms for neural networks so one of them that we used for our pset was actually tensorflow which is Google's open source a neural network platform and another one would be cafe which is Berkeley's network platform and actually have an online demo where you can plug in images and immediately get identifications so you can get started very quickly quickly with that one thank you you 