 [Music] well thank you everyone for coming appreciate you coming out and joining this session I am Kyle's story I'm a computer vision engineer at Descartes labs and today I want to tell you about how computers see earth machine learning approach to understanding satellite imagery at scale and so there's a few things that I'm hoping depending where you're coming to this talk from that you might take away first I'll I'm hoping that everybody will understand how we've been able to build a platform within the Google cloud how that's allowed us to provide an efficient development environment for building machine learning models and then scale those models across large data sets to be able to understand the entire globe second I'm hoping that you will take away the types of problems that we've found compelling and that we've been able to address using satellite imagery and machine learning techniques and then third hopefully gets you thinking about the types of problems that you might be able to address with these data with this type of infrastructure and how you might be able to address some of your business problems whether that's you know interest monitoring infrastructure supply chains agriculture even understanding the natural ecosystem so the world is teeming with activity whether that's man-made infrastructure like here in the zoom-in sports stadium inside of a city or that's agriculture shown here with crop circles in south america or the natural ecosystem there are things happening all over the entire globe all the time and satellite imagery is a particularly effective way of being able to zoom in and understand what is going on all across the globe so it's a there's a big dynamic range and what we want to be able to understand we want to zoom all the way in to the human scale of a few meters or a meter or less and then we simultaneously want to be able to extend these analyses across countries and continents so it presents a big challenge within satellite imagery we are at the beginning what we think is of as a regime change in just in simply just the data set so previously satellites were big and expensive you know costing of order a hundred million dollars weighing a ton companies and governments could only send up a few of these at a time and so while these satellites provide excellent and incredible information you can't monitor the entire globe with any sort of regular cadence with the advent of cube SATs and smaller satellites that's really changing so now companies are able to build satellites for 60 70 thousand dollars a piece these small things and then they can launch entire fleets up into the sky to be able to effectively monitor the entire globe almost in real time and so what we're looking at for the near future is where as the data that underlies Google Earth takes approximately two years to complete a new picture of the entire world we expect that within around five years private satellites will be able to cover the entire globe every 20 minutes so this is a completely different class of the types of problems that you can address and then also a completely different regime for the big data that you need to deal with in order to make sense of all of these these information so Descartes in order to make sense of these data we're building a living digital atlas of the globe and so during this talk I want to tell you a bit about that and then how we're using machine learning to understand the value and the information in these pictures so we can think about this as a classic Big Data challenge with the three V's volume variety and velocity and so in this context volume is what it usually means the massive amount of data that we need to be able to pull in and work with variety in this case means the different types of data that we want to synthesize into analyses and I'll talk about all of these in more detail and velocity here I really mean two things I mean first being able to access all of that data efficiently but then also being able to analyze and scale your analysis across the globe rather than having someone look at individual pictures so to kind of work through those in with some numbers in a bit of detail global goal satellite data sets are very big this composite that we made is comprised of three point one trillion pixels per band and the Landsat satellite that took this has collected over seven seventy trillion pixels between 2013 and 2017 so that's comprises three hundred 20 terabytes on disk and that's just one saddle mele among many so we have we've processed around 10 to 12 terabytes of data sorry petabytes of data on to our platform at this point and this is just going to expand exponentially but just to give you you know a bit of an image in your head of what that looks like let's take one satellite the Sentinel satellite and take all of the picture that it's taken and print each individual band out on a one millimeter thick sheet of paper stack that up and that would form a column that's 410 kilometers tall so from the surface of the earth up into the thermosphere so it's just it's a lot of data the data variety that we're working with comes from different types of sensors that we want to be able to work with and synthesize together so here on the left there are passive sensors like normal pictures that you would take on in the middle there are active sensors which actually send down rate radiation or radar and then receive the signal and interfere it to get 3d information about the surface of the earth that's called synthetic aperture radar and then there's near-infrared data that actually measures the thermal emission from sources on the ground and then there's all kinds of other geo relevant data like whether cellphones derived products dry mass customer data so we want to be able to put all this into the cloud in a way that customers can access in an efficient way to be able to build these types of analysis algorithms and then velocity so thinking about the how long it takes to to work with these types of data let's consider again the sentinel satellite so if we take one year of data seminal satellite covers the entire earth approximately once every five days let's now chop up the entire earth into 512 by 512 tiles at 10 meter resolution and then give a human analyst 10 seconds to look at each picture that would take them a hundred and 30 years to go through one year of satellite data for one satellite so obviously that's silly we're not going to do that but that kind of illustrates the problem of the the old way of thinking about satellite imagery analysis and why we need new approaches and then also the four-velocity thinking about the computation that's required to handle these types of data so again coming back to this composite that we created this was made from processing over a petabyte of NASA imagery and we did all this processing within the cloud using preemptable VMs and by spinning up 30,000 cores we're able to process and create this image in under 16 hours and this also Dementieva sets but it demonstrates the capability that google cloud will be able to scale as the data that I'm talking about continues to grow in the way that we expect so to handle all of these data as I mentioned at the beginning we're building a data refinery or a platform to better understand our planet and so what we want to provide is ready access to petabytes of geospatial data and then scalable compute to back that up and be able to create automated and scalable analyses these are just a few of the satellite sources that we have on platform right now and this is going to just continue to grow so I've talked a lot about all these you know all this raw data but what's really important is that the value lies in the information in those pictures and the decisions that you can make from that information not in the raw pixels so it's important for us to be able to sift through all of that data and arrive at decision points all right you know being able to monitor something in supply chain or make a decision about whether we're gonna go here go there so being able to pull together all different types of data so here is SAR over Tokyo what you see the ships coming into port being able to understand the changes over time this middle image are segmented crop fields in Nebraska that were created by looking at detecting edges and images and then looking at which edges persist over time to be able to tell what fields or what and then using machine learning which I'll spend the second half of my talk talking about to be able to analyze all these data at scale and then what this gives you is really a geospatial refinery for business intelligence so again taking all this data processing it and then turning into value and decisions that you can use so what the platform actually looks like then is effectively a Python API for Earth that's what we like to call it so let's say for example you want to take we want to gather Sentinel imagery over talus in northern New Mexico if you were to start from scratch trying to go directly to the satellite source this would be quite challenging but for us it only takes three four lines here importing the package specifying the location doing a search over all of that giant catalog of metadata to find the Sentinel imagery over this place in 2017 between March 12th and March 20th and then taking all of those numbers and burning them into an image or a picture that you can actually see and work with so under the hood what we've provided is fast access to over a petabyte of data on this on the platform the search function for searching through the metadata is done with elastic search and it provides millisecond response times over all of those data and then an automatic tiling system so you don't have to worry about pulling together different images and making sure you register them correctly all this if you want more detail about the underlying platform you can go back and watch tim's talk from yesterday io 2 6 3 so the second half of the talk I want to talk about the machine learning approach that we've taken to trying to understand these data and derive value from it so I think another important change that at least the geospatial data community is going through right now is where as previously images were thought of as something that a human analyst could pull up on their screen and then look through and study and annotate with with this massive amount of data as I mentioned that's no longer sufficient and so we should start thinking of these images and all of these data as input to computers input to algorithms what computers are seeing out of this rather than what humans are seeing out of this and it really changes how you think about the type of analysis and the type of questions you can answer and so that's what's going to be required to automate data analysis that can you can scale across the globe and then this also encourages us to simply think differently about the types of questions that one can answer so hopefully I can get you all thinking through this talk so a little bit about what the input data looks like from so now me as a computer vision engineer I come in and I say how am I going to create a new useful tool so let's start with the input data as I mentioned previously there are several different types of images the most common are passive sensors where the Sun is the light source shines reflects off the surface of the earth and you take a picture of that reflected light so that's like normal CCD cameras then there are active sensors which are their own light source so like for example synthetic aperture radar which sends a radar signal down reflects it off the surface of the earth and then receives that full wave front which it can measure and interfere and then use that to reconstruct 3d models of the ground so we collect these data and what many of the algorithms that I'm be working with are supervised training supervisor algorithms and so often times it'll look like image target pairs so here for example on the Left I have a picture of a city of high-resolution satellite imagery and then on the right I have a labeled image for where all the buildings are and so this is the type of input data that would go into training and algorithm these data are because the cloud the platform is in Google Cloud it's trivial to be able to process all of these data and create these pairs directly in the cloud and then save them to cloud storage and then be able to access them seamlessly without having to use any any local resources for those of you who think about analyzing imagery I just wanted to highlight a few of the similarities and differences between processing normal imagery so many things are similar but there are some key conceptual differences so in particular satellite imagery usually has an own size and scale with units pixels usually correspond to a specific size on the ground in terms of data augmentation these images are left right and rotationally invariant so you can flip them around to augment your datasets easily one of the key differences is that often times were interested in finding things right at the limit of the resolution of the imagery so small objects within images whereas in normal imagery whether it's the cheese example that we saw from Otto and Mill that piece of cheese takes up almost the entire image here we're looking for you know you saw the buildings we're looking for those little specks so the scale in terms of a transfer thinking about transfer learning the scales can be different there are almost always confuse errs in the data like clouds haze and shadows so that requires a lot of pre-processing and I'll show a little bit later dealing with that and then in normal imagery there's usually three bands whereas in satellite imagery we can have an arbitrary number of bands for example the Sentinel satellite from Europe that I've referenced a few times has 12 bands from the visible all the way up into thermal bands and then a you know similarity that works well with being working within the cloud is that these problems are generally embarrassingly parallel you can take the full stack of images and normal image processing or you can tile up the earth into a stack of images and then send them off to an arbitrary number of processors to be able to run that run those jobs in parallel okay so now we've thought a little bit about the input data how do we want to analyze these data well there's three sort of dimensions that I like to think about when thinking about saddle spatial spectral and temporal so spatial is just where you are on the ground so the 2d context of where you are on the earth or within an image spectral are the different wavelengths of light that we record when we take imagery in different bands and then temporal is for a given location obviously as you take pictures over time you can see what changes and so by combining these these different aspects in different ways we can access different types of problems so you know for example computer vision will usually be combining spatial and spectral information where you're trying to understand the spatial context within an image and then look for spectral signatures of different types of objects or land-cover that you want to be able to find and map and spectral and temporal can often be used to look for and monitor change so for example looking at crop fields over a growing season you can see how the how the plants are growing and changing over time and then obviously many of the final solutions will be hybrid approaches that will combine everything and I'll talk about one of those at the end so because we have chosen to work within Google Cloud building these mo models has is a really efficient process so we work with in virtual machines on Google cloud platform that allows us to easily scale the resources that we need to process images train algorithms as needed and then we're able to maintain environmental consistency from that development and training process all the way through to deployment by using docker and kubernetes because our data live in Google Cloud it provides a super fast access to that giant body of data and then for training we can download smaller sets to the m's or work with it directly from the cloud and then we can deploy at scale so as I mentioned most of these are embarrassed at the deployment stage most of these models are in an embarrassingly parallel problem and so we can simply spawn up an arbitrary number of processors if I want to run a model over you know the world or over the United States I can just spawn up that number of processors and get the results that much faster and then results are saved directly to the cloud so it makes it from it from your research to building models to deploying those models working within the cloud provides us an incredibly efficient environment for doing this work okay so now I would like to tell you about three different more specific machine learning approaches that we've taken and the types of problems that we've been able to access with those so I'll dig in a little bit deeper into the the machine learning side of the models for building so first its object detection and for those of you who are not computer vision engineers the basic idea of object detection is exactly what those words sound like you want to find objects and so for example here we're looking for electric substations so you want to find that object draw a box around it and say there it is the particular implementation that we've been working with so far is called a single shot multi box detector SSD single shot because it takes a single forward pass through the architecture multi box because it uses a set of predefined boxes rather than dynamically finding those boxes and then detector because it no detects things how it works in one slide is the this algorithm as I said predefined a set of boxes and then for each of those boxes it uses a convolutional neural net to generate a set of features and then finally predict the confidence of whether or not that box contains the object of interest so basically it boils this search problem down into a classification problem so here this is from the paper that defined this architecture you know the blue box has the highest probability of finding of being a cat and the red box has the highest probability of being a dog so we've taken that and modified it to work with satellite imagery and here's an example of the results of that process so using Open Street Map for the known locations of electric substations we've been able to train this using NAIP imagery so that's the national agriculture imagery program that covers the United States this particular data set has imagery at 60 centimeters per pixel and for bands and we've trained this algorithm to be able to find all the rest of the substations in the United States and obvious so that was one example and once we have this set up with all the imagery easily accessible and the modeling framework in place it's easy to train a wide variety of models across the industrial supply chain to be able to find these objects so here are just two examples of being able to find storage tanks be able to find boats but you can just let your imagination run wild for what you would be able to do with this the second class of models that we worked with a lot is called semantic segmentation the so the basic idea with this as opposed to bit as opposed to object detection where you're trying to find objects here you're trying to say for every pixel in an image is that pixel part of a class or not so in the example on the left hand side is the high-resolution imagery again from nape and the right hand side is the probability of whether or not each pixel is inside a building so similarly how it works here we define a deep convolutional neural net and convolutional neural nets are a particularly effective architecture for these types of analysis because they take into account the two-dimensional context of an image so they so when you're if you're if you've ever zoomed in on Google Maps and then zoom the way out and to try when once you zoom all the way into pixel scales it can be really hard to tell what you're looking at and you actually need to zoom out and be able to see that spatial context that's kind of conceptually what convolutional neural nets are able to capture so we define a deep neural net architecture in this case a unit and then these create a bunch of these image target pairs that I mentioned at the beginning so in this case about 10 to 20,000 of these and then train the model within the within the cloud platform and this is what the output looks like so these are detection from my model over LA County so you can see the high-resolution imagery on the far right hand side this is the raw probability map output and then in the middle this these have now been segmented into individual buildings and so what's especially powerful here is once we have this model as I mentioned earlier being able to run this over large areas so I was able to map all the buildings in California bite hitting run going home sleep and then by the time I got back in the morning we had a map of all the buildings in California and in principle we can do this over the entire world another type of siming segmentation that we've been able to have success with is using neural nets to mask out clouds as you remember at the beginning I mentioned this is one of the pernicious problems with satellite imagery as clouds exist I watched two hour bane bane of our existence I would say rather and so by training a neural net to find clouds and cloud shadows we can now automatically mask out these out of images and then be able to remove those parts from the analysis and this is you know this can always get better this will be an ongoing challenge but this has been a particularly successful approach for being able to deal with these types of non ideologies in the data another approach is what I've called here course segmentation so this is actually basically the transfer learning approach where we start from a ResNet 50 a pre trained algorithm and then we tap into one of the lower resolution feature layers and then simply add a final classification step to say whether or not each each of those coarser pixels is the type of thing in this case windmills that we're looking for and so we've taken advantage of the fact that residents been able to train on many many many many images and then with a relatively small number of examples I think of order a thousand a few hundred to a thousand examples here we've been able to train an algorithm that can find and map windmills so this has been done across the United States and across India and then finally it's still semantic segmentation but a bit of a simpler model is simply pixel classification so taking that remember I mentioned the spectral information within each pixel what what you see in different bands and then feeding that into a random forest classifier and just training it with a bunch of data to say whether or not each pixel is land or water so this is in the Mekong Delta and you can see it clearly gets the oceans and the rivers and then there are a lot of rice paddies in these areas so that's kind of what the speckling pattern that you see throughout the Mekong Delta is here a third type and this is fairly different is what we've called visual search so I'll demo this in just a second but the idea is that this is a website that you can go to you can click anywhere either in the United States or on the earth and it will simply find other places that are visually similar so let me go into a little bit of detail and what's going on under the hood and then we can play around with this a little bit so what's going on is that we've taken the entire datasets the entire globe we've chopped it up into lots of little pieces and then we feed each of those pieces through a convolutional neural net to produce a set of features and then we simply search for nearest neighbors of features so in you know even more detail than that we started from a customized ResNet 50 so started from ResNet 50 using all of that the training that had gone into producing that model and then remember I mentioned that often times you're looking for small things and images that are different from normal image processing so we fine-tuned trained that with the OpenStreetMap data set and then also some unsupervised autocoding Auto encoding to be able to produce a useful set of features we then encourage those features to either be 0 or 1 so binarized that set of features and the result of that process is that for each tile we get 512 binarized their bits near 0 1 then we can simply search for they search for which sets of features are close to each other and using a Hamming distance in so we defined that distance using a Hamming distance and then we search for it using an approximate hash based nearest neighbor search and so the result if I can switch over to the other laptop oh yeah okay so this is now the website running live and so on any of you can go to this at searched at Descartes Labs calm so first let's play around with the United States so this is this national agriculture imagery program data set that I've mentioned a few times and so highlighted a few popular searches but let's click on wind turbines so what what this has done is I've grabbed this tile and then like I said it is we've already indexed the entire United States into these 512 bit long feature vectors and it simply searches for other ones that are similar so it's pulled up a whole suite of different wind turbines all over the country so you know some here in the Midwest some in the Northeast and this is not you know this is not a specifically trained model I didn't have two input and I didn't have to know anything about wind turbines to be able to create this this is just simply searching for visual similarity another one that I thought was kind of neat was the solar farms so let's looks like this is a solar farm maybe in Arizona or New Mexico and so yeah so it click on that and solar farms look very similar so this does quite well yeah at picking up solar farm installations so let's then also look at the entire Earth so we've done this with the Landsat data set as well and so you know same idea being able to index the entire globe so pivot irrigation are a very unique signature so I can click on that and all of a sudden I can see pivot irrigation in Australia all over Africa you know the sub-saharan in the United States down in South America all right so it's a it's a it's a very powerful tool to be able to effectively search the entire globe for things so you can go to this today and say huh I want to you know I want to look for railway stations that look similar to the one near my house let's click on that and see see where else they are around the world and so this is this incredibly fast search and so you can you can see how how quickly we're able to Zoomer this is all enabled by having the platform built within the cloud so if I can go back to my talk slides perfect so with the with those two approaches we there we basically have a two-stage approach to performing searches when when we say we want to try to find some type of thing because we have because of the capabilities of the geo visual search we can do a low capacity a quick search for things like like the wind turbines just click on that or build out a small model and say you know let's just try to find roughly where some wood turbines are and differentiate them from corn fields or forests or whatever and then if if that's enough for your particular solution super cheap super fast and easy you're done no work then obviously many solutions will require much higher precision like mapping out buildings for example wouldn't work with that and so then we can develop a higher capacity model that takes you know more time and effort and so that you know that's what the examples I showed with buildings or wind turbines or power substations and then of course the last type of models I want to talk a little bit about our hybrid models in an example of something we've done that sort of pulls all these pieces together and so what this is is one of the original models that Descartes labs produced which is the corn forecasting model so the goal is to be able to forecast corn production for the United States so first as I mentioned earlier using the temporal information we can segment out which where different fields are and what those field types are so how this actually works underneath is because we have time series imagery over the United States we can just use normal image processing to find the edges there's an edge detection within those images and then we can look at which edges persist over time and use that as a way to segment out field boundaries then once we have those feelin down ders it becomes a classification problem so taking into account the the spectral and the spatial context information we can classify those fields here these are either corn or soy corn yellow and soy and green and what you're actually seeing here is the fields like field classification released by the US Department of Agriculture with a lot of measurements on the ground and what you see here is what Descartes released six months ahead of time so the well you should say is hey look those look pretty much the same and they should because the US Department very culture accuracy is quite good the key is that we are able to do this in effectively real time rather than having to wait six months and then the final step of that model is a regression problem so saying we have historical data we know how much production corn production corn was produced in previous years so let's run this model and fit to a model to predict how much corn can be produced and so you pull all those pieces together and what the graph on the right is showing is the y-axis is the uncertainty on how well you have predicted corn so small uncertainty is good so down is good you can see and then as a function of time and so you can see on as you go to the far edge of the graph and so you extend a time they they approach the same accuracy but the blue decart labs points got there way earlier and so this gives this gives businesses and customers a real actionable insight that that they can turn into into real value and money so that's wrapping that up if you any of you are interested in building models or have this brings up ideas feel free to come up and talk to me there's information on our website and then you can email either of these email addresses and I'd love to talk to you more about what types of problems you might be able to approach with these types of data or more questions about how we've been able to build out all this infrastructure and our platform using Google so with that thank you all for coming you 