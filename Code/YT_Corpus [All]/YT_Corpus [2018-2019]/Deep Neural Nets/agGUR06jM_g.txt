 [Music] we go down the next module which is adding noise to the inputs okay so we have some kind of a noise process and now can you relate that how that was related to regularization that was exactly the motivation in that case that we could have an over complete auto encoder which is a very complex model because it has a large number of parameters and to avoid that we were adding this noise to the inputs so that even if it tries to minimize the training error it's not actually minimizing the true training error right because you have fed some noise to it everyone gets this right okay now actually we can show that for a simple input/output neural network right that means you don't have any hidden layer you just have a set of inputs and you have the output layer then adding noise to the input or rather adding Gaussian noise to the input is equivalent to weight decay okay so this can also be viewed so we will do this part right so we will just quickly do a small derivation where we show that adding Gaussian noise to the inputs is the same as doing an l2 regularization that's a very neat idea so this can also be viewed as data augmentation right exactly what I shown on the previous slide if you had that - you just corrupted some inputs of it that's the same as adding noise to the data so essentially augmenting the data right you have some training data and just augmenting it so to get more training data is that fine okay now about this smallest derivation this is again just a set of steps I'll go over it reasonably fast I'll give you the set up and then we quickly work through the derivation right so what I was trying to say is that if you have a simple input-output neural network that means you just have inputs and the output you don't have a hidden layer then adding a Gaussian noise to the input units where the noise comes from this distribution it's a Gaussian distribution zero mean I want to show that doing this is effectiveness the same as doing l2 regularization okay now again see this is the same thing squared egg in a vacuum because this is not the kind of networks that we deal with it's good to see what happens at least in these neat conditions because we will never have a simple input/output network at least not in this course we'll have a deep neural network always so but at EC what happens in the simple case right so what we are doing is from the exercise we are creating a noisy X I by just adding some epsilon noise to that and what is our model going to be it's just an aggregation of all the inputs ok so this is what our original model would have been without the noise fine I would have just aggregated all the inputs I am assuming there is no non-linearity at the output and I am just taking via is equal to summation of all my inputs AVN fine with this side or this is too simple for you guys to understand because we've been doing a lot of deep neural networks I had only one layer Network I don't know what it is Evelyn gets it right ok and instead of Y hat now I have Y tilde because instead of X I I have X I tell de ok but what is X I tilde X I plus epsilon I right so I can write it as this this is fine so actually Y tilde is nothing but Y hat plus some quantity ok what are we interested in always this quantity the expected mean square error expected squared error and why not Y hat so that we have added noise to the input so now Y tilde are the outputs that wave ok so let's see what that quantity is and again just assuming to be some simple stuff so I replaced Y tilde by this that we just derived on the right hand side on the left hand side ok so I'm going to take these two terms together so I can write it as this Plus this the whole square fine ok and I'm going to keep this as it is what is this quantity the original squared error expected squared error right when I was not adding noise to the inputs ok and you see how we got these two quantities this is just a plus B the whole square is equal to whatever it is equal to right now let's look at the last term this is a square of a sum right so what kind of terms would you have inside you will have some terms which are epsilon I squares and you'd have some terms which was epsilon I epsilon J right okay so we'll have some expectations which are going to be something into epsilon I square and some expectations which are going to be epsilon i epsilon J avian gets this some terms there now which of these terms would disappear these terms right why because the noises are independent okay I'm not if I have drawn a noise for one instance it does not have any influence on the noise that I'm going to add to the next instance if I've taken one X I corrupted it with some noise there is no bearing on the noise that I'm going to use for the next epsilon I write all these features are the noise added to the features are independent right is it okay fine so now from these terms only the square terms are going to remain is that fine and similarly this quantity what can you say about this we just did something similar why am I saying that this is going to 0 again I can show that this is the covariance between this random variable and this random variable okay and now are these two random variables dependent what is epsilon I the noise that I am adding to the input does it have any effect on Y hat no right because Y hat does not depend on the noise what is y true output does it have anything to do with the noise no right so that's why these two random variables are independent so I can again write there the expectation of their product as a product of expectations and then the expectation of this is going to be 0 because epsilon I was drawn from a zero-mean distribution is that fine everyone gets that the same trickery that we did earlier so this is the quantity that we are left with you see how I got from here to here this is an expectation of a sum which is equal to a sum of expectations WI has nothing to do with it it's not a random variable so it's just the expectation of Sigma I squared which is nothing but the the variance right so I get this what does this look like I already told you the answer before starting right this looks like l2 regularization this is the true error I mean this is the empirical estimate from the training error and this is the weight DK term even guess this how you see that this is an equivalent thing so at least in this neat setup you get the intuition that adding noise to the inputs is the same as adding an l2 regularization term everyone is fine with this okay [Music] you 