 this is a talk hosted by the center for brains minds and machines and I'm Tommaso Paju and very glad to welcome Mischa Belkin here Mischa was a student of a student of mine Parton EOG was part I was one of the purest intellects that I met in my life and Mischa worked with parta on laplacian icon maps and semi-supervised learning something that has still milestones in modern machine learning and which has always been extramural eyeball something someone you can believe in all the formal details of proving something rightly and I think today for the first time you will speak about something which is a bit more crazy and I'm not sure I'd believe it but we see I'll moderate the craziness level maybe okay so what I would like to talk about the title of the talk it fit without fear and this is actually kind of a technical description of what we do in machine learning now I'll explain this in a minute but meanwhile this is this is based on joint work with collaborators my students c1 Massa McMahon don't leak away see one in particular did an amazing job on this and also my colleagues Reb isolator hosted Daniel Stewart Colombian party me train Spring Harbor labs okay so well we all know that machine learning AI is becoming really a backbone of Commerce in society and this is changing the way we do a lot of things both in commercial applications and science and all kinds of domains then what is behind it well if you look at a typical architecture for say a vision problem you have something like that this is Google net from 2014 this is the network with about 5 million parameters and as you can see from the picture this is a very complex object so at this point we have this object which are kind of running our inferential tasks but we don't necessarily understand what this project do as well as we would like to and there is a little bit of well fog of war if you wish if that there is a lot of excitement about it a lot of success but we need to actually from a scientific point of view we'll have to gain better understanding of what I do what are the key aspects we have to isolate and analyze components of how this methods or other methods work let me very briefly to state the problem of supervised machine learning just to make sure we're on the same page and show everybody you know so basically you have data X I Y I X eyes point in some D dimensional spaces have features say pixel values of images and why I said for simplicity I'm just taking them to be binary minus one and one values they can be multi class and most practical fact they're multi class the goal now of machine learning is to construct a function given this data to construct a function from the feature space to R two minus one one more specifically the generalizes to unseen data well it's kind of an interesting question what does it mean to generalize but in many sort of theoretical at least analysis what we mean by that is that we have some probability distribution for the data and what we observe on the data which we haven't seen should somehow be reasonable be reasonably good it should basically perform well now how is it done well the typical algorithm for this is empirical risk minimization well strictly speaking this is not an algorithm algorithms are based on that you take a class of functions F and this F may be neural networks for example an L talk particular about one class of kernel architectures and you minimize the loss function over that class of functions and F star in this case is simply the function from this class which minimizes the loss and by the last year it may be a classification loss or a regression loss like the square loss now there are a lot of theoretical analysis based on the complexity of H in particular some known common analysis are based on VC dimension which if you VC theory things like Rademacher complexity and so on and many of these analysis result in generalization bounds of this form you basically look at the expected value of f star if this is the prediction on the future data and you say that the prediction on the future data this is generalization is not much worse than prediction on the test on the train data plus some term sorry which is roughly one of a square root of it so when n is large this two should be close that's basically the upshot there are the analysis - I'll maybe say something about this ok so that's now I will talk quite a bit about the interpolation and overfitting in this talk so what is interpolation interpolation is a following so this is just a cartoon of my data the my feature here is one dimensional and the label is one a minus one interpolation is simply classical interpolation I say that a function interpolate my data if f of X is equal to Y i overfitting is very similar except I'm kind of interpolating the sine so over fitting to me if f of X is sine is equal to Y I you can view it as zero loss fitting if you don't like the term overfitting you overfitting has some sort of negative connotation I in this case I'm just using it as a purely technical term there is nothing negative negative or positive about that for me in the context of this talk okay so that's zero loss fitting overfitting interpolation that's what they are now what what about modern machine learning well one very important feature of modern machine learning if massive of a parameterization and I would argue that it's actually in innovation why why is this an innovation well it's not strictly speaking new but it's kind of new so let me be first tell you what is our parameter ization of a parameter ization is simply when the number of parameters is large or equal actually but in practice it's often much larger than the number of training data and if you look at this nice diagram of various architectures from a congenital you see that the size the circles here correspond to different neural net architectures the size of the circle is how many parameters it has so Google net which I show you and the first one has about 5 million parameters the big ones have about 150 million parameters now you have hundred fifty two million parameters your training this on you know maybe ten hundred or even a million data points you have many many more parameters than data points so that's of a parameterization now what upshot wise of a parameterization important well let's look at this table and this table is from understanding deep learning requires rethinking generalization from Django doll 2017 they give an example they train using this is Ising train on a data set CFR which is I think it has about 50,000 data point they use this in sumption model which is a type of neural network it has 1.5 million parameter so many more parameters are data points and they get trained accuracy of 100 percent now 100 percent may be this this could be kind of viewed as surprising well how do you get trained accuracy of 100 percent or they all refute the data using you know the notation of this talk they have zero loss perhaps this is surprising right because we know that neural network is some sort of non convex complex architecture why can optimization actually get us to zero loss on this non convex problem in general dota card to optimize and there have been quite a lot of work showing that all local minimum such networks are global but perhaps once you think from the over parameterization point of view this is not too surprising your parameters variables in the data pointer constraints if the number of variables is much larger than the number of constraints you usually can solve the system of equation I mean this of course is not always true but generically there is some sort of mathematical codes and code theorem saying that those things are solvable so it's not surprising that we can actually get to zero laws if we have so many parameters so of course we cannot get below zero laws because the way we construct the loss function it's always positive so that's a constraint so perhaps it's not too surprising that all local minima are global of course you have to still prove us in particular cases the kind of more important thing is that over parameterization leads to overfitting pretty easy relatively speaking overfitting and interpolation your optimization just goes down to zero loss now perhaps a more surprising aspect of this table is the following you train to have zero percent loss 100 percent accuracy yet your test performance is still very good it's about 90 just under 90 percent so how come that you are not overfitting in the bad sound even though you have this overfitting you have zero loss but you are still performing very well why is it that you're getting good results and in fact there is a nice quote from Ruslan salakhutdinov simon's tutorial he gave it to his well-known expert on neural networks he gave it to toriel at the Simon's Institute last year in Berkeley he said the following this is actually a quote basically you want to make sure you hit the zero training error because if you don't you somehow waste the capacity of the model so I was actually in the audience and it really struck me at something very surprising because you know we tell our students not to do this when we teach machine learning this is just saying that there is no penalty effectively for fitting the data exactly and this is somehow when you train these deep networks this is somehow the best practice according to Roslin so let me let me give an overview of this talk so first the first point I would like to make is that this very strong over fitted performance is not really special to deep network and what I'm going to show is that classical kernels have something very very similar to this with classical kernel machines and in fact we can basically absorb it in exactly the same form that's one second that overfitting is actually hard in the following sense that two able to fit with zero error is a non-trivial optimization problem for large data and in some sense once you make this happen once you have some sort of good method for dealing with large data the performance of kernels becomes competitive with neuron that at least four problems which are not conventional in the structure like for images you need some sort of convolutional features but for our many other things it seems have become very competitive or maybe better so that's the role of depth of overfitting now the power of overfitting it can be shown mathematically that if you are in the over fitted or more precisely in the interpolated regime the Casti gradient descent becomes extremely efficient computation and in fact you get a very large benefit from being able to run it and in particular it's a very good fit to GPUs on this contract so that's why optimization works as well or at least that's kind of an explanation for we believe that why neural networks are optimized so efficiently and finally I'll discuss a challenge of overfitting and really there is not so much help from current theory with some exceptions but very little so I would moving forward I would present some sort of analysis of an algorithm which is nearly based optimal and can interpolate the data so at the very least we can show that this is not so crazy maybe to overfit a sort of traditionally thought okay now let me very briefly describe thermal motions so colonel machines is one slide introduction to kernel mushrooms kernel machines basically you do empirical risk minimization over a space of functions which are linear combinations of kernels of this form of this there are other ones as well but this is a Gaussian which is by far the most common in practice what you have you can show that if you optimize us your solution has a following form it's just the sum of your kernel on your data point so basically your classifier is a sum of gaussians on your data point it's a Gaussian mixture if you wish for the Gaussian kernel we'll use another kernel as well and typically this is done using a regularization term and this is called Tikhonov regularization which of the form lab the norm f of h squared if you put lambda to be 0 that forces the output of this algorithm to interpolate the data so it will fit the data exactly and this is so called minimum norm interpolation and the solution to this is simply alpha is equal to K to the minus 1 why well wire labels an alpha is a coefficient in this expansion so you get an explicit solution for this which is nice because you can analyze off mathematically so there's a kernels and for all generalization analysis pretty much you have to assume that lambda is greater than 0 but what we do we basically put lambda to be 0 you cannot quite put lambda to be 0 but you can take lambda to be infinitesimally small which results in the solution otherwise your solution is not well-defined now well there is a lot of beautiful classical mathematical and statistical theory of kernels reproducing kernel Hilbert spaces were introduced by mathematicians our Ranjan and there are splines and more recently kernel machines well still 20 years ago and why is this an attractive setting well it's actually it's convex it is analytically tractable and also there you can view this and that would require a little bit of an explanation but you can view it as a two-layer neural map so it's a special case of new robots now let's discuss interpolation so this is an actual data set this is actually a mist in what we are doing here we are running this kernels so I'm basically solving kernels the same way as I would solve a neural network and how do people solve neural network they run stochastic gradient descent for some number of epochs so it's exactly the same way you are running this carrillo we're running this kernel so this is the number of epochs and this is the result so this line is there are two different types of kernels you can just concentrate on the blue one it doesn't really matter so much what you see is that as you're running the result improves of the one epoch is about 1.9 and then it goes to about 1.1 here is interesting thing the interpolation solution is actually this line and as you can see the so the line really interpolate the data meaning that f of X is equal to Y I exactly well up to numerical precision there is something like 10 to the minus 26 so this fits this dashed line fits my training data exactly I don't train anything here where I train but I just invert this matrix that's all I do for smaller data set you can just invert it for lunch of course you cannot so I'm getting this solution and what I see is that my is the number of a box increases I basically converge to the solution my performance on the test this is not training this is test right so it never goes up so early stopping would be like regularization so if I wanted to regularize I would stop here but if I regularize I just get results which are Wars this is not completely 100% true you can see here it dips just a tiny little bit under the dashed line but the difference is very very small so if there is an effect of regularization it must be very small okay so this is Chi Sigma is chosen by some sort of validation set but we can we can run the same thing with Sigma being twice at one half that and you see the same result so the baseline decreases so you get instead of a one point five percent you get maybe one point eight if Sigma is very small you get one nearest neighbor classifier but the performance there will be much worse at performance this is far from one nearest neighbor yeah this is quite far from one nearest neighbor one nearest neighbor would give you about 3% so this is not it so yeah it isn't clearly not one nearest neighbor no no Sigma is not like lambda know lambda prevents you though Sigma doesn't prevent you from fitting the data exactly lambda Sigma doesn't the error he's ten to the minus 26 so we're actually fitting the data up to numerical precision by the way notice that loss function is actually irrelevant here for the interpolation right because interpolation has no loss function you're just fitting your data exactly so interpolation fits the data exactly but if you do this kind of iterative update the lost function will change you change your path to the convergence but it wouldn't change the actual result okay now I'm not going to look at it to go over this in any detail but basically we did it on a bunch of data sets and it always looks the same the shape of this curve is the same in every case every case regularization either give you anything will give you very small very small improvement those are like six different real data sets yeah so I'd fold it in two ways one is inversion direct inversion that is an iterative solution so inversion of course there are no epochs but there there are two different ways of solving this this is classification and this is actually regression No yeah over in the laws I really truly have 0x0 bosses regression and classification but inclusive but in classification on the test it's fine by overfitting I mean my overfitting which is just means zero loss turns it yes that we don't observe we never observe us and not to any significant effect there is a very small effect here that may be special to deep networks but I would but we'll get to that later because I'll I think deep networks they combine a lot of different things so it's very hard to separate those issues so that may very well I I am totally believe value okay so let me so okay what's going on fitted and interpolation generalization is not unique to deep architectures that's very clear that's what people have observed with deep networks we are observing exactly the same four kernels can now be examined in a convex analytical setting that's nice because that's actually where mathematical tools work to lay in your well if we don't even understand setting for tool a neural net we cannot have a full theory while I would argue we cannot have a full theory for general neural nets which are much more complex okay so let's continue so now let me tell you why this is hard and why it takes actually some effort to scale no methods to large data what is hard about it so kernel methods it's known it's known well known that they work quite well on small data not so well on large data what's going on is that something intrinsic to kernel methods or is it because we don't do something correctly and basically that we need to undress computation here the good thing about interpolation it makes a goal really really simple we don't have regularization parameters we have nothing we just have this lean system of equation we have no choice of loss function we have no choice of regularization parameter so we just concentrate on solving this linear system of equation as efficiently as we can and it turns out basically we get very very strong performance once we can solve the system well for large data so why let me first point out of course F sty I found algorithmically for large data we basically have to map it to GPU if graph if we can if we need to get good performance why because GPU is basically a parallel machine which does matrix matrix computations very efficiently it's parallelizable matrix matrix computation so that limits the algorithms which a that basically any algorithm which sort of has hope of scaling to large data has to combine some number of matrix matrix product and some limited amount of other computation if you don't have this you have to use CPU and CPU is maybe 10 or 50 or some large factor time times slower so very difficult to get good performance so now matrix inversion well we can of course do direct inversion the cost is n cubed that's basically impossible for like if you have 10 million data points you have 10 to 2114 point operations that's impossible and that doesn't map onto GPU as well well is impossible it may be possible in some circumstances but it's essentially we cannot do it on how much maybe on the supercomputer very close to impossible so now what's the alternative the alternative is doing some sort of iterative update that there are the after interphase well by this one is you know particularly inviting in a sense and also parallel to what's happening in neural networks if that this is really gradient descent for solving the system and this goes under several names this is gradient descent this is Richardson iteration land web iteration there is of course quite a bit of quite a bit of literature on this this is no this is good because this is will take only N squared perturbation and this is easily GPU compatible that's matrix vector multiplication right I am updating my my matrix my vector of weight iteratively by multiplying it by the kernel matrix K but how many iterations do I need right there is a number of iteration this is not so useful well how many iterations well let's look at this function this curve a sidestep function just one dimension you have one here and zero here right that's one of the simplest function you can consider after hundreds of iteration with a Gaussian kernel you get something like this okay that's maybe not too terrible and you have like some loss like six times ten to the minus two in the square loss if you do 1 million of iterations however you see that nothing changes literally almost nothing you did not even get one bit of precision here what is going on why is you know from going from 100 to 1 million hopefully you would see something good happen but it doesn't turns out if that well let me give you before I sort of have an explanation let me give you a sort of real example of this the same thing we can observe on some data set this is some 10,000 data points up set so under data sets basically you can see that to get optimal performance you need more than 10,000 iterations of gradient descent that's bad because you're worse than cubic now you are at cubic or worse what is going on well the point is in that convergence is controlled by eigenvalues of matrix and you can prove that eigenvalues of the kernel matrix decay nil exponentially and that's very bad because well if your eigen value is canoe how many iterations is exponential then the fact that you know each iteration doesn't cost very much is not really help and you can prove that the dimension of the functions reachable by those iterations is kind of pulling algorithmic so basically this is kind of bad news if you want to use that now at least with this is for Gaussian kernels I should say so it doesn't necessarily apply to other kernels but that's a popular type of kernel in fact you can prove that only very smooth functions can be absolutely approximated by a smooth kernel in some sort of polynomial number of iterations and the problem is that we don't really expect classification functions to be smooth now what's going on well actions that contradict classical rate for gradient descent there is some classical rate if you've seen it which are of the form one of epsilon squared I am saying this is like some logarithmic thing why is that well it has to do with infinite dimensionality I can sort of explain it offline if you would like but that's basically an issue well what do you do how do you solve this well there are there is a clear problem right the problem is that the eigenvalues decay quickly and basically what your convergence depend on it depends on the ratio of eigenvalues so a way to address this is to actually turn the spectrum of the matrix and you can do this by constructing this kind of new kernel if you wish it's basically literally maybe this is the best way to sort of explain this you just look at the spectrum eigenvalues as a function of the index right and you have some sort of decay like this and what you do you literally flatten the spectrum up to some point so that's a modified kernel how much performance gain do you get you feed first eewan aka in one iteration so first K eigen directions you fit in one iteration and then you get lambda 1 divided by lambda k plus an acceleration for each next one and that is exponential well hopefully exponential so you potentially getting exponential improved in accuracy so that's kind of what it looks like gradient descent somewhere here and this method that we have is you know it reaches much large space of functions using the same amount of computation so you can see that actually the Fed shattering dimension of this is both a poly logarithmic but this has some sort of exponential constant in front of it okay well how do you actually do this algorithmically you can there is I don't want to spend a lot of time on it but basically there is a way to do it you take a small sub sample and you can construct the pre convey way to do it is to construct a pre-conditioner and you can construct eigenvectors surface from a small sub sample and then you do nice from extension and you use that to precondition that turns out to be quite efficient and accurately enough to get a very large increase in speed so of course it's an approximation but it's a good approximation and the nice thing is that it converges to the correct solution so even if your approximation is not optimal you still converge to the correct solution so in accuracy in constructing this only is problematic to the degree that you will not get full acceleration notice that we don't use any regularization in so basically the kind of high-level idea of again Pro and so what we get this is some practical result and I think the interesting part about it is that if you just look this is some standard data set of reasonable size if you look at the time which our method requires for for example for this M&E this is some sort of large version of feminists we took 1 million data points it takes 0.35 hours which is about 20 minutes well little yeah about 20 minutes and you know if you for example look at the literature comparable results slightly worse requires one hour on three scene 103 CPUs on AWS so it's really a very different level of computational expenditure we are running it on a single GPU so it's maybe thousand times faster not quite thousand times but a lot faster the same here we need 25 our process 1 million data points on timid and we need point 8 minutes here well that's particularly easy data sorry so yeah you compare the amount of time it takes and this is drastically faster than everything available there is actually another fast kernel method from Lorenza Rosasco Falcon which is also quite fast but this is even many times faster than that from last year now if you look at the deep neural networks there is a actually you get with deep neural networks on timet which is a speech data set it 32.4% and here we get 32 percent and that you know takes 20 minutes to get so really the scaling is kind of almost solved it's not completely solved because we still cannot do it on 500 million data points for that we need infrastructure but it is for degree that we can run it in 20 minutes on a million data points it's really not a problem anymore so better performance with far less computational budget far far less now this is some very recent work but basically we can easily match results from deep neural network concert to improving speech eligibility tasks and I am NOT going to play those but you can see this is kind of clean speech this is noisy speech and you can reconstruct that clean speech it's can be posed as a classification or regression problem you can do it and it works really well and it's kind of the results we get out-of-the-box Mallis it's not even particularly difficult to tune the neural network which we are matching against well it's not the newest result but it's it's a rather complex architecture and we're using a single parameter so if you basically can scale if you can address the issue of computation you do get a good result with shallow networks this kernel methods kernel machines now let me talk about the power of overfitting why is overfitting so good and by good I mean from a purely optimization point of view I'm not addressing generalization here just optimization well let me just very briefly remind you about stochastic gradient descent stochastic gradient descent does something like this yo you are minimizing a sum of this loss function and the loss function is computed at each data point so it's a sum of the things and the kind of natural idea here is for example it can be the square loss is to optimize this one at the top okay so that's basically what's the catch the gradient descent this you take a driven straight of taking the derivative for full gradient descent you take the derivative of the firm but we know that the derivative of the sum is the sum of a choice right so why not just take one at a time unfortunately here is a big issue each of these guys is only weakly related to the total sum because they somehow had very different and what is happening is that after T steps the kind of bound that you get from commands that you get for stochastic gradient descent is 1 over T for gradient descent you get e to the minus T right gradient descent converges exponentially or linearly in the optimization literature stochastic gradient descent converges at this rate 1 over G or even worse what is going on so why would anybody use this when you can use that well you can say okay gradient descent is more expensive sure but it's not actually any more difficult to compute it's just more expensive to iteration but the if this is exponential as this is not exponential what's the rationale for using that well however all the neural network architecture do use the cost agrarian dessert well so here is a key observation the cape generation that if you end this overfeed well more precisely interpolated regimes so if F of W star W star is optimal solution fits the data exactly then what you have is that all objective functions alight and the variance of your stochastic gradient descent actually decreases was there so what you get you get exponential convergence of the caste gradient descent and that has been observed in the literature well not in connection with interpolation in particular and in particular strowman and Virginian analyzed the original message goes to customer but remember seen in actually showed exponential convergence of that and once you even realize this you totally get fast stochastic gradient descent for free effectively so what you have is that if you compare this to some other methods which are available to accelerate stochastic gradient descent which are not really widely used in practice you get the stochastic gradient isn't this faster it's just faster under this condition and the question of course well so that kind of explains why stochastic gradient descent is good it's exponential but it doesn't explain why this is better than gradient descent itself so why is this better than gradient descent or why what's the connection to gradient descent it just shows that it's exponential right but maybe gradient descent would still be better exponential so what we control is a following this is our result we see on my right bacilli if that in fact mini-batch of size one has optimal per computation improvement for a stochastic gradient descent so if you have a mush if you're actually in the sort of classical computational modeling you're just counting the number of operations then it's the cost gradient descent we this mini batch size one is that of course doesn't sort of you may say well sure but nobody uses us right nobody uses the caste grading decisions mini batch there is one right why not well the thing is it's inefficient why is it inefficient because we're running the things on GPUs GPUs are not sequential machines right GPUs are parallel machines what is happening on GPUs is that you basically want to do some you can kind of get a small mini batch for the same price as the Spinney batch of cells what it's quite a surprise but it's clock what is happening here is that we need to analyze dependence of mini batch size on dependence on performance on the mini batch size and we can prove the following here in that one step of mini batch there is some sort of optimal critical point here such that up to here if you have the mini batch of size 10 one step of mini batch of size 10 is roughly equivalent to 10 steps of mini batch of size 1 so we call this linear scaling so it really doesn't matter so if you have a fully parallel machine then this will be 10 times cheaper than that on a sequential machine it will actually have the same cost now beyond this point beyond the same star you have saturation and and what the situation does it basically you don't get any more this is some sort of law of diminishing returns beyond this point you flatten out and Adam star you get at most one four so one step with M star CD is only one force is a full gradient so if um M star is much much smaller than the full data size you gain nothing by going to full gradient descent interestingly enough this M star is actually almost independent on the data size itself it's basically just tagging values of discussion matrix and you can get a direct formula for that it's simply trace H divided by lambda 1 of H so if you take the hash and its optimal that trace divided by lambda 1 is M star and as you can see at least for things like kernel method this is in roughly independent of the data size that means that potentially you can get something like all fan improvement or SGD over the full gradient descent and that actually is consistent with this behave is consistent with linear scaling or something which was observed empirical in Europe let me give you one example of this this is a real data this is a meanest and we are using it for the kernel here not for neural net because for kernel much easier to analyze the eigenvalues of this matrix so it turns out critical size is M star so this is 10,000 data point so you see that acceleration factor is about 1,000 or maybe even like 10,000 for feminists and what you see that indeed M equals 1 is optimal this has kind of computation to our POC so notice that Airpark is a fixed number of computation in the sequential model so what you have is m equal star is optimal and M equals 8 which is a critical value is very close to optimal and then when you increase the mini batch size you get much worse performance so it's pretty consistent weather theory but the interesting thing is well if ty is 8 and you have 60,000 data points right you have about 10 to the 4 acceleration and that's kind of almost like Moore's law right if you have a really big data like 10 to the six data or ten to the seven data points you can get acceleration which is basically 10 to the 6 or potentially gigantic acceleration so we know this is happening in Colonel methods we don't know for sure this is happening in neural networks but it's sort of seems that efficiency of HDD indicated in the fact that it actually is converging to this interpolated solutions indicate that probably something like that is happening now we can in fact learn kernels for parallel computation maybe I'll skip this part but we can modify the spectrum specifically to address this parallel computation and to optimize it for GPUs ok so there was kind of the power that that was explaining why as GD was so efficient but now what about generalization right where does generalization stand on it so first I showed you that well this over fitted or interpolated classifier work very well so that if you are in this regime your s GD will work very well but why actually why does overfitting work and here we actually kind of have very little help from theory and here is what most theory is something like this if you look you know if you take a machine learning class or if you teach a machine learning class you probably see something like this you have training error which goes to zero and you have a validation which goes down and then up and this is kind of bias-variance tradeoff and you are supposed to look for this point so that the theory that's what more theory suggests now what is practice practice is like this there is low variance and there is no bias biases zero in our examples of numerically zero what is going on well there are a couple of explanations which you can suggest one explanation is kind of a classical margin explanation is the following well suppose your data is linearly then the fact that they have a lot of data is sort of okay because the space of classifiers of this line separates the data perfectly but yes it it's a low VC dimension sort of thing you can analyze it using classical results if this is the case then most of the theory already work an alternative is something like this one did they tell you you know this data points are kind of crazy and your interpolated clash if I look something like that and we don't have much theory for this what is going on here which one is real data you can test it you can test it in two ways first you can do synthetic data second you can do real data and label noise here is how we would test it right you take data and suppose this is linearly separable well can I modify the data to make it not linearly separable very easily I just flip some of those points at random wait this is my random and I just toss a coin and flip some of those levels if the first was a true explenation then this would really break my classifiers well let's see what happened they said two data sets this is - I'm sorry two gaussians in 50 dimensions and linearly separable ball is linearly separable so I run this kernel method and I get zero error so zero error of course that's what you would expect now I flip some of the label one percent of the labels you see that classifiers are now still at exactly the Bayes optimal so they still work perfectly maybe one percent is not Sigma is fixed yeah Sigma here I think is just freaked throughout those experiments for on there I think it was on the original date I don't need to check but I think it's a very regional data set without noise but really makes very little difference but it's not very small I know it's not very small what one nearest neighbor for any other Sigma I I mean we don't really have theory so it doesn't really help but I know it's not one nearest neighbor classifier it's very far from one nearest neighbor no it's less yeah and yeah I can though when I had 10% of the noise it's gets slightly worse but it's not much worse it's still in reasonable result so clearly I don't have some sort of crazy explosion of the laws here I can do the same thing with you can look at the norms and see that the norms increases dramatically of those classifiers but it sort of doesn't seem to affect anything a lot of generalization bars are based on the norm so you see that there is no performance model complexity we can do it with the gaussians which are overlapped so there is some sort of natural noise because there are basically the same thing I don't want to go over it here is a really interesting example we're eating large amounts of noise here so weighting 20% 40% 60% 80% of label noise of noise even at like 60% of label noise the green is based optimal so that the best the kernel does slightly worse than the optimal because this is a synthetic data set so I actually know what the pastures the kernel does slightly up slightly worse but only a few percent worse there is no it doesn't you know it doesn't produce nonsense it's still very good so clearly even for very very noisy data this interpolated classifier work remarkably well in fact we can prove some theoretical bounds maybe I'll skip that but here are some explanations one is we see dimension and rather make a complexity type of bound I don't think they can work because they cannot deal with interpolated classifiers when Bayes risk is nonzero for zero risk they work fine but for non zero risk they cannot bound the gap between the empirical risk and the expected loss regularization type analysis they all of them diverge as lambda goes to zero the regularization parameter algorithmic stability has a similar issue there is some really interesting work on classical smoothing methods like neither I Watson most of them don't support interpolation but there is one there are really two interesting examples one is just one nearest neighbor classifier and one is something called Hilbert regression scheme let me skip this one nearest neighbor classifiers particularly interesting it's really it gives have zero loss right one nearest neighbor you just take the nearest neighbor but it has no performance guarantees is it worth twice a base risk and this is a sharp bound but due to cover you know in the 60s and interestingly the analysis does not based on margin assumptions not based on uniform bouts and it directly estimates generalization doesn't try to bound the gap so that part of when with we're trying to look for a way forward that type of analysis and let me give you an algorithm which actually is has some of these guarantees it interpolate the data but it can be shown to be almost tamaak and this is joint work with Daniels or partner Mitra basically what you do you take your data you triangulate it's not a practical algorithm you wouldn't want to triangulate data that's a very expensive process but let's not worry about it Lu triangulate data and then you just linearly interpolate the values okay look something like this if this four points are red and this is blue I get this this you know this values phase are 0 and this is 1 so I have 0.5 around this point and high around this point and then I just threshold so this will be now blue and the rest will be red very simple classification out it turns out that you can show that this is nearly based optimal and high dimension and in fact well this is under some margin come this is massage margin condition but you can prove it without margin condition you just won't get an exponential here what you have is that basically the high dimension this is very close to the true base risk the risk of that class if I give insufficient amount of data so it's like one nearest neighbor buttock step of a factor of two here I have something like 1 plus 1 over 2 to the D so this is very tact at least one dimension is high enough when you have enough data and interests have a blessing of dimensionality that this algorithm actually becomes better as you have more dimensions so that's a unusual property of this algorithm so that may be something which gives away forward on this algorithm so how do you view interpolation maybe something like that is happening well let me now kind of summarize a couple of slides first I think clearly this is kind of the take-home message overfitting allows for very efficient optimization by stochastic gradient descent generalization which we don't fully understand why that happens and it's not a trivial issue to actually overfit the data when you have a lot of data solving the systems for doing this optimization really requires thinking about what kind of algorithms are involved and how to do and the question from a practical point of view can we actually scale those kernels because kernels we understand much better than we understand deep networks now kind of you can maybe think a part of deep learning you can maybe understand it in this form so first you have this over parameterization which naturally leads to interpolation and surprisingly that generalizes well but once you believe that this general is just well this maps on top fast results in fastest GT in faster of naturally two GPUs because that's really like almost like designed SGD really is designed for cheaper um sometimes so GPUs designed for his Lydia I don't know I mean it was developed independently but it's an extremely good match so I think there is a lot of serendipity in that how the things worked together and a lot of sort of surprising aspects of that there is one aspect learning which I didn't talk at all about this convolutional structure I think that's a really important aspect which are kind of our thorgan all to work to those optimization and translation aspects of that and so that's one and finally I think I think we I think it's really a fundamental question of high dimensional inference why do those classifiers generalize it has been observed for deep neural networks we are now observing it for kernel machines random forest was observed maybe you know 15 or 20 years ago for adaboost it goes back to 95 people said that are the boughs doesn't all of it and clearly the margin explanations are not the complete story there and I think it's actually time to revisit high dimensional statistics maybe get some new understanding of these problems thank you stop here [Applause] 