 Hello, everyone. My name is Junhee Seok. I am a professor of Electrical Engineering, Korea University, in Korea. This course is about machine learning, especially for data science. Machine learning itself was originally developed in the field of computer science. However, nowadays, it comes up with many other fields of engineering and science. Especially, with accumulated big data, machine learning has been considered as an essential tool analyze and utilize these big data. The original machine learning includes many subcategories. However, in this course, we will not cover all subcategories. Instead, here we will focus on the common area between machine learning and data science. Okay, so this is the very first video of the first week. In this week, we will go over about machine learning and supervised learning. In this week, I will introduce the basic concepts and various types of machine learning. And also, I will introduce some concepts of prediction and inference problems. and here, importantly, I will introduce some of the tradeoff, between interpretability and flexibility as well as tradeoff between bias and variance. Let’s see one by one. First, in Machine Learning, when we say a machine, it usually means a computer. Some people might think a machine is something like a robot. Usually, a robot has sensors to collect data about the environment, and also it has computers to make a decision, and it has actuators to actually to perform something physically based on the decision. However, throughout this course, we will use a machine to refer a computer that makes a decision based on the data. An ordinary machine or computer works by pre-determined and specific rules in a program. However, there are many cases that pre-defining specific rules is not possible or, even though it is possible, it is very inefficient. It is because the environment around a machine can be very dynamicaly and unpredictably. In such cases, we need a machine that adaptively works as the change of environments by learning new rules that are not specified in a program. In other words, instead of program with a lot of rules, we want to provide a program or algorithm that finds new rules from the data. It is the key of machine learning. In this diagram, such a machine can be resented as a block that has environments as an input and action as an output. This is a black box This black box is between environment and action, or we can say, this is between input and output. The environment is sensed by sensors, and only the sensed data is transferred to the machine. And the action from the machine is the result of decision, and it is transferred to actuator and make real physical actions. Anyway, mathematically, a machine can be expressed as a function between input variable X and output variable Y. However, there are always errors and noise. So, by equation, a machine can be expressed by Y, the output Y, is equal to some function f(X) + noise epsilon, or e. Y is an output variable that can be used for actual performance. It can be target of prediction, assigned cluster, or rewards that the machine may have. X is input variables that generally describe the environments around the machine. It can be predictors, observed data, or current states and actions. If this f is already known, it is a program with pre-defined specific rules. The key point of machine learning is that this function is not known. So we need to findthis function f from the observed and collected data. The collected data can be both of X and Y, or only include X. Depending on the provided data and types of problem, machine learning can be further categorized into supervised learning, unsupervised learning, reinforcement learning. But commonly, we need to find out this function f(). Even though we don’t know function f(), we can model it. The function f() can be modeled with a simple linear function or it can be modeled as a complicated non-linear form. For example, let’s see the observed data, bewteen X and Y. If you want to find a function f() between X and Y, you may fit this data with little bit complicated this S shape of curve. This curve looks good, however, for more simplicity, you may want to use simple linear line. You many want to use this linear line because it is easy to be expressed analitically. Anyway, we want to know or estimate f() for many purposes, but mainly for two problems, predictions and inference. First, prediction problem. In the prediction problem, we want to predict unobserved value Y through the fucntion f(). In order to do this, we first estimate this f() function. Here, we use a hat, this hat notation, to indicate estimated or predicted values. So, this Y is the real observed value, and Y hat is the predicted value. Similarly, this f() is the true f(). However, f() hat is estimated or predcited f() for this true one. So the relation can be expressed like Y hat is equal to f() hat of X. Here, Y hat is estimated value of real Y. This real Y is actually given as real f(x) plus noise. In this prediction problem, the fundamental goal of prediction problem is to minimize the error between the observed Y and the predicted Y hat. Mathematically, it can be written as the expectation of Y minus Y hat square, or, equivalently, beacuse this Y hat is f hat of X, so we can write it Y minus f hat of X. Before, we move further, let's considerr why we want to minimize the square of errors, not the absolute value of errors like this. Believe it or not, it is because the square function is easy to analyze and calculate. In many cases, polynomial functions are easier to handle than absolute functions, because they are differentiable at all points. In old days without computers, it is a very important merit. Anyway, if we accept this square error, it is easy to show that this expectation of error, Y minus Y hat square, it is equal to, because this Y is f(x) plus epsilon, then this Y hat is f hat square, so if you expand this square eqaution, then we say this noise is independent to this f(X) and f hat of X. So we can actually rewrite this equation like this form. Expectation of f(X) minus f hat of X square plus variation of epsilon, or error. So this error can be seperated into two terms. So, the first part of error, this one, is the error between the true f and the estimated f. So, if we have a really superpower, like a god, we might be able to obtain f hat very close to the true f. So, the first term is an reducible error. Extremely, if we can find the exact answer, we can make this error to 0. However, the second part is the variance of errors, which has nothing to do with estimation of f(). So, this is an irreducible error, which means regardless of our power and efforts, we cannot reduce this error. So that actually means the actual error can be seperated into reducible error and irreducible error. So some problems are difficult because it has a big reducible error. In this case, we have hope that we can do better. However, a problem is difficult because of irreducible error, and there is actaully no hope. There is no way to improve it. Given a problem, it is important to realize the portions of reducible and irreducible errors. For example, speech recognition has been considered as a difficult problem for a machine. However, actually human beings easily recognize speech, which means the difficulty mainly comes from the reducible errors, not irreducible errors. So, we have hope to be better. Then next is about two kinds of prediction problems. There are two types of prediction problems. One is regression problem, the other is classification problem. The regression problem is the case prediction for the continous outcomes. That means when output variable Y is continuous variable, like height, salary, lifespan, and so on. In this case, for a given data set, the error is measured by mean square error, which is the sample mean of yi minus, yi is an observed value, and minus the estimated or predicted value by this estimated function f(). The other type of problem is a classification problem. In the classification problem, Y is a discrete outcome, such as pass or fail, subtypes of breast cancer, preference to TV channels. But importantly, this discrete outcomes have no order. That's an important point For example, three classes like low, middle, high, these classes might look discrete outcomes however, the problem is that they have an order. But, here, we're interested in discrete outcomes without such an order like men and women, or colors - blue, red and black- something like this. So in this prediction problem, our goal is to find out that minimizes this MSE. But this MSE is actually measured by, this I is the indication function, which is, let's say this I(x), this one is 1 if x is true. If x is false, this value is 0. So that means MSE can be expressed in this way, and it actually means error rate. So MSE is measured in error rate, and that means, in other words, it is measured as an accuracy, in the classification case. Inference is another important problem. Inference is to find significant or important X for Y. In the actual data, there are a lot of input variables. Some of them are really important to explain or predict output variable Y, However, some of them, they are not that important. For example, between height and years of education, which will affect salary more? Definitely, years of education do. Then, the question is how can we quantify the importance? There are two ways. One is measuring the effect size. The effect size is how much Y increases when we increase X. Definitely, large effect size means high importance. Another measure is statistical significance, which checks whether X really has effects on Y or not. We will see later how we can measure through statistical tests. Ok, anyway, we want to estimate this function f(). In a big picture, there are two approaches. One is parametric, and the other is nonparametric. Parametric approach assumes a certain type of relation between X and Y. The most typical assumption is linearity. For example, we all know that test or exam score is affected by difficulty of the test and hours of study of students. But the exact relation is not easy to know. So, we may want to simply assume linearity, and model test score, with b0 + b1 * difficulty + b2 * study hours. We all know it is not right, but many people use this approach because it is simple and easy to analyze the result, more importantly because there is no better way. The other approach is nonparametric approach which assumes no prior relation. In this approach, we need to find f that predict Y, or fit Y not much roughly and not much wiggly. Actually, it sounds non-sense, but it is true. It is why nonparametric approach hasn’t been used that much. Even though with more data and computational power, Okay so in these days, with more data and more computational power, it is getting easier and easier to apply non-parametric approaches to the data analysis in these days. Here, I will introduce an important concept. The model flexibility or complexity. A model is considered to be more flexible or complex if it can represent a wider range of functions. For example, when you compare 2 linear combinations, y = b0+b1X with y = b0 + b1X + b2X^2, This second model has more flexibility or more complex Because the second model can include any model of the first model. So this is the concept of model felxibility. Generally, neural network is more flexible than linear models. And then there are many such cases. Here is a trade-off between flexibility and interpretability. In this toy example, you may want to use s-shape of curve to fit the data or simply you man want to use this linear line. Definitely, this s shape of curve is more flexible than this linear line. However, in terms of interpretability, for example, if we want to answer about the effect size, which means when X increases by 10%, how much Y increases? For such a question, we can answer simply with this linear model because the increase rate of Y increases is all same over all range of X in this simple linear model. However, in this complicated model, that rate actually changes. So it is not that simple to answer. So this model has less interpretability. Generally, there is a trade-off between flexibility and interpretability. If we increase flexibility, we have to scarify interpretability. Simple models have high interpretability but cannot model the complex relation. Usually, linear models has low flexibility and high interpretability. However, neural networks and support vetor machines they have low interpretability and high flexibility. Neural nets something like deep learning, they have really flexibility but they have low interpretability. Okay so this is end of the first session. I will stop here, and I will discuss more about this machine learning stuff in the next session. Thank you very much. 