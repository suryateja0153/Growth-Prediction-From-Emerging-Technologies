 you [Music] so welcome today we would be starting up with our next version which is on multi-layer perceptrons - deep neural networks and while in the earlier lectures we have all studied about what neural networks are and we have done a few lab sessions as well one and that was about just understanding neural networks from a classification point of view and now where the extension comes down in perspective of visual computing is that here in contrary to what we were doing in the earlier classes was that in the earlier classes while we were using feature extractors and feature descriptors which are hard-coded functions over there in order to describe an image and then we extended all the summary or the synopsis coming out of each of these feature descriptors together into our classification framework using a neural network on the contrary today when we are going to do it so here is when a neutral network itself has to come down to be an end-to-end learning framework which means that input to the neural network itself is an image while the output from it is still a classification output so it can be a classification output it can be a regression output any of these things which come out so where we would start down very specifically is that here we are going to look into a multi-layer perceptron and that's our starting point and from there eventually we will enter into what is known as the deep neural networks and then what are their existential crichton ears and how they work so effectively we would be doing a basic review of the perceptron model and the perceptron learning rule once again and then entering into the multi-layer perceptron from there we entered into something called as the signal flow graph representation and this model of a signal flow graph is how is my input and my output related and what happens during the learning phase and this is a quite critical part over here since in the last lecture and the lab which we had done so you were introduced to the concept of error back propagation and from there we had a gradient descent based learning rule now what exactly happens in telling Aaron backpropagation and why it happens the way it has it has been named and what you have seen down in different snippets of code is what we are going to explain you through this signal flow graph representation following that is a very important aspect about gradient calculation and that's to show down what happens within these functions and whether the gradient is just for the classification cost function or then or does it need to exist throughout the network and that's where we will enter into something called as an existential criteria for the network to exist and all other transformations and cost functions also to exist and then eventually go down to the learning rule and from there we more or less come down to an end of what happens we're done with these kind of deep neural networks so as with a simple urine model so just to do a brief recap of what it was so say that there were three inputs over here in the earlier case last week when we were doing it so these were given down as features say three different features but here now this these are no more three different features but these three can be three pixels so you can consider just three pixels in an image and give them as an input to the so the pixel in its own way or it can be even say for a given pixel in colored space if you have one particular image in RGB color space so each component itself is represented as one independent scalar value so your x1 can be the red value of a pixel x2 can be the green value of a pixel x3 can be the blue value of a pixel and accordingly so poor pixel basis you can make some sort of a decision coming out as well so let the decision associated with a particular pixel over here be P hat and now with the simple neuron model what would happen is that we will have a weighted combination of these inputs going down to a viewer and from there add down a bias take a summation out over them and this summation is what is has what has this form so it's W naught plus W 1 X 1 plus W 2 X 2 plus W 3 X 3 where each of these weights W 1 W 2 and W 3 are three weights associated with each of the three values X 1 X 2 and X 3 and W naught is is called as the bias or the one double unit can also be written down as 1 into B where say B is the weight over there and the constant input to this particular edge over there is what is 1 okay so in its linear algebra form which is in its matrix representation this is a form which you would be getting now so you get Y is equal to the inner product or the dot product of two matrices one of them is the weight matrix where you have the weights and the bias taken together and the other matrix is a column matrix over there so that's why it's X comma 1 transpose where X is this circular arrangement so capital X is basically a matrix arrangement of these three scalars which you get down over here now having taken all of them together the next part is to apply some sort of a non linearity and that's the FNL non-linear function which you get down over here and these nonlinearities can have multiple different forms and we consider these two forms over here the first form is called as the Sigma the second one is called as the tan hyperbolic non-linearity and to do a very basic recap so you remember that in Sigma what happens is that as the value of y tends towards plus infinity this value tends towards plus 1 as the value of Y tends towards minus infinity this value tends towards 0 and on the contrary with the tan hyperbolic what happens is as the value of Y tends towards plus infinity you get a value which is saturating at plus 1 as the value of y tends towards minus infinity you get a value of this non-linearity which is at minus 1 so taking these two together is I either one of them you can be using now and based on whichever you are using your peahats value will appropriately be decided so if your P hat has the non-linearity associated as a tan hyperbolic its value will be in the range of minus 1 to +1 if it has sigmoid as non-linearity then its value will be in the range of 0 to 1 and this was the simple perceptron model which were now from taking down a perceptron to getting into a neural network formulation which is that given I can have multiple kinds of inputs over there it can be different kinds of scalars over there and I can map it down to again a different group of scalars so maybe my first prediction over there is what is called as p1 hat and this will be the form of representing everything - might be one hat now note over here that as we had also discussed in the earlier class is that these weights are no more with just one subscript but there are two subscripts which you see with these weights 1 comma 2 1 comma 1 1 comma 3 now the reasoning behind these weights is that the first subscript is to the target where it's mapping so the output over here say I have x2 which goes to y1 and that eventually maps down to my target which is called as p1 so my first subscript is going to be the subscript of the target my second subscript on the weight is the subscript of the source from where it is connecting and that's the nomenclature which we are following now if I arrange all of these weights W 1 1 W 1 2 and W 1 three in a row matrix form then that is what is written down as this bold W 1 which is the matrix given down in the equation ok along with that I have my scalar value which is my bias W 1 0 or B 1 ok and accordingly X is my X X comma 1 transpose is my column vector which comes down and this gets my in a dot product and then my non-linearity applied similarly if I take down my second neuron on the output side of it and feed it appropriately so I would be getting down this second part of the partial network coming down and my group of equations which represent that now projecting onto this and going out similarly so I can have my XJ 8th neuron connected down to my YK eighth neuron with a weight which is called as W K comma G and then put a impose on only near 80 on top of it and then taking all of them together this is a particular form where I get where W subscript K is a rotate row vector which which has a size of so the number of connections over there will be basically from 1 to G which is because there is a total number of X's which you have over here so for this combination this is a particular kind of an output relationship which we see now if you look into this matrix of weights and biases which are come together then you can see down that all of these are outputs which I see Y 1 y 2 up to YK so if I take all of this together and just concatenate them so I arranged them in a column form in a call in a column major format which is that it is it just has K number of rows and just one column over there accordingly my bias B that can also be arranged into a column matrix over there okay so these are the two matrices which we see over here and then my W each of these W 1 W 2 up to W K they can also be stacked one on top of the other because each is independent of the other one and now that would give me some sort of a rectangular matrix now if I clearly look into it then my total transformation equation over here can be written down in terms of just a matrix multiplication so this will be a matrix multiplication of my weights W and B bias these two kinds of matrix with the input over there which is erased as a matrix and then that gives me an output matrix over there and this output matrix is a column matrix my input matrix is also a column matrix so that's my Y and then I have a non-linearity applied on a matrix which means that each element of the matrix is appropriately subjected to the non-linearity over there and then taking all of them together is what I get down as my target output so this was my very basic understanding of how a neural network works down as such and then this was what we had done with multi-layer perceptron in the last class itself now again going down a bit more into the division right over there so my error in prediction how it was different was that if I have one of these predictors p1 then I get down one value of a scalar for another predicted each p2 I get down another's error which is e2 now if I have an array of these predicted variables over there then I cannot keep on calculating each and every error singly because in that case I don't get a consolidated knowledge about the total network as such so in order to do that what we do is we find out what is the Euclidean error over there so a Ukrainian error or the total error of the network is basically a scalar value which is the Euclidean norm or the l2 norm of mic of all my predictors so whatever is my actual ground truth which is P and my predicted value P had these two matrices are subtracted and then you take the amplitude of that or the l2 norm of these two subtractions so and given that the next part is that you will be doing a back propagation in order to learn down your algorithm so the idea is that you have these successive bunch of observations and predictions and what is the ground truth so if x1 is a matrix is a of all of these scalar XS over there and one of these samples is X subscript 1 then the ground truth corresponding to that is what is P subscript 1 and at any given point of time when you feed the whole data x subscript 1 through your network you would be getting down a predicted value which is P hat subscript 1 okay so similarly I take my input sample as X subscript 2 and I feed it forward through my network I get a P hat subscript 2 okay while the actual ground truth over there is p2 so I keep on doing this together and then for my NH sample which is my last sample in my training data over there as I feed my last sample through my network over there my output is PN hat and that my actual ground with corresponding to that is P n so together if I have all of this so what you would see is that there comes an error which is there for each sample so for my first sample second sample third sample in my net sample I will be getting a different value of error but can we give some sort of a consolidated error for the network in terms of its performance across all of these training examples which we are taking now and for that reason we devised another metric which is called as the cost function of the whole network in terms of its weights now that's what is defined as J W over here and that's a summation of the Euclidean distance between these predicted P and the actual observed and the actual ground truth of these peas we just supposed to be there now if you look into this cross function over there what we said is that GW is your cost function where these varies in terms of your weights which are W now what comes down definitely in somebody's mind is that why is it varying with respect to something called as a W and the reason is that these WS are weights which are the only thing which now would be guiding down and accordingly manipulating what happens to your predictions over there so because there there isn't anything else on which it can change see my input X is constant so that that will be different number of samples and across samples and across so between two samples it will be a different value that's always known but when I'm training across an epoch learning which we had done studies in the last classes as well so when you are training across epochs what happens is every epoch you are going to send the same sample over there the only reason why the prediction value of putting down the same sample so say at my first epoch which is my epoch number zero over there I put down X 1 as my sample and I get down a predicted value P 1 hat I do all my updates and everything and then it comes to my second epoch which is epoch 1 in my epoch 1 I put down X 1 it would be getting known a different value which is P 1 hat but P 1 hat at the epoch 1 is very different from P 1 hat at epoch 0 and the only reason why this was changing is within the network the only variable component is week which changes so that's the reason why we would write down this cost function in terms of our weights itself now that I have my cost function written down in terms of my weights my final point is that we need to come down to a point where to a point in the weight space so I said my argument of these cost functions is minimum or as I keep on it says the the point is that if we keep on changing the weights there will be one particular combination of these weights such that my error is minimum and that's the exact one which I would like to achieve now and how that achieved is through something called as the gradient descent learning rule so in this gradient descent learning rule what we do is basically that it's an iterative process in which what you do is you start with some randomized assumption of weights in the first epoch and then see that is W key okay and then you computer whatever is your gradient of the weight space over there in terms of your cost function and that is del Del W of JW and then you weigh it by a factor an empirical factor which is called as ETA also known as a learning rate so what this controls is that your gradient of this error function over the Delta W of JW that can have any range of a value now if the range of say this W weights are in a range of 0 to 1 and then say my Delta W of JW is in a range of 10 power of minus 9 so the rate at which it would be updating or impacting the value of W is going to be very less and in that case this ETA factor over here comes to your rescue because what you can do is you can set an ETA factor say 10 power of 6 so if I multiply your value into a power of minus 9/2 of 10 power of 6 that will put me a give me a value which is in the range of 10 power minus 3 and that value is something which will actually be impacting significantly how the value of W is changing over there so this learning rate basically is a fact way of mathematically modulating the gradient over there so it's that we have a value of this error and the gradient coming down which will be in some ways significantly impacting the change in W and that's how my W of K plus 1 will be revising at a much better rate then W of K would have if we did not have this learning it called as ETA now given that we have all of this done so what typically happens within a gradient based learning rule is something like this that you would be starting down on your first epoch and then as you change these case so you will be getting your first value of JW based on your JW you will be calculating your gradient multiplying that with your empirical constant ETA or the learning rate and then you update your W to W of k plus 1 as you update it to W of k plus 1 you would be getting a different factor of GW which is the W of K plus-1 and accordingly these keeps on changing and so on and so forth till you are at the final conclusive step over there okay now this was one way of trying to visualize our learning in terms of its cost function versus epochs and this can be a typical graph so you would often be seeing that you start with a particular error and your error increases and then keeps on decreasing or it may so happen that it increases then keeps on decreasing and suddenly again it's a local Maxima it increases slightly then goes down it may keep on jittering and these are all aspects about what's happening within the learning itself so if your value of ETA you can keep your value of ETA very very less in that case what will happen is it will come down very slowly but it will be a very smooth transition which it will be getting if your value of ETA is very high then it can start oscillating and jittering over there and that means that it's basically overshooting the local minimum point at every time when it's coming down somewhere closer to the local mania and these are different issues which we would be tackling down through experimental processes and some more learning experiences subsequently now looking at the same part of gradient descent again in the weight space so what we had learnt was that say I start with some random value over there and this is my point and if I so here what I'm doing is typically I'm looking into two different plots so one is my plot of epoch versus cost function the other is my plot of weight space versus cost function and these are two different aspects as such so the second epoch when I update my weight so it shifts to a different weight vector coordinate over there and subsequently I have a different cost function value also calculated through it and subsequently it goes to the next one then to the next one and and finally to my convergence now if you look into this part of the plot what you would see is that for any kind of a perceptron model you would be seeing that the error function over there form some sort of a very structure which quite mimics a a cascade like design where you have multiple number of crests and troughs present over there so as these points of my weights they keep on moving down they would always be encircling and coming down to my local minimum point as soon as possible and the way it comes down to this local minima is what is my learning which is happening and also from the last lecture about introduction to deep learning and what happens within this multi-layer perceptron line challenges you did understand that one one of the major points is that we can actually initialize a network at any random point of order and based on that it can start converging and oscillating around any of these trucks and that definitely means that where it's going to converge is now some sort of dependent on where I started if I started down in the neighboring one then it will be in the rough of the neighboring if say there is no global minima but everything is equivocal point over that there is no unique global minima in that case now if we have a unique global minima then the challenge is obviously that you don't lock into any of these non global minima positions but rather somehow escape into this from these small rough like regions and exactly converge onto your global minima position now having said all of this what comes down to our mind is something interesting so you have seen that there is for any kind of a given Network if I have three different scalar values over there then I can take in these scalar values and I can predict out one of these predicted outputs over there okay and for a simple perceptron how it goes to is something of this sort that I have my weights w1 w2 and w3 I take all of these weights together and then I sum them up and accordingly I get now my bias also coming into play and then I am wrapping down to my output over there okay the question is that now that I need to find out my del Del W of JW then what would I be doing so in order to solve it out the best possible way is basically trying to look into something which is called as the chain rule of differentiation which you have done in your high school mathematics itself so what goes down by the chain rule is that we would try to break down all of these into its constituent components so the simplest way of doing this is that let's bring down this derivative product which is partial derivative of J W with respect to W because we cannot directly compute so what we will be doing is so you know that the output of this JW so JW was a cost function and that was for our case a Euclidean distance of the predicted output with respect to the ground truth okay so I don't have any component of exercise directly visible neither my weights directly visible over there but what I have is definitely my P hat or P over here which is my predicted output State so I can take a derivative of the cost function with respect to this output next is my output is dependent on Y through our non-linear function okay so that means that I can't take this partial derivative of P with respect to Y now if I look till the first two partial fractions on my right hand side over there so you can see that del Del W of JW can now be represented as a product of del Del T of JW and del Del Y of P so together these two first two parts of other will give me del Del Y of JW coming down now the next part is now that I have a del Del Y of JW I should be getting on another part of the partial fraction which is partially a partial fraction of these derivatives which is my del Del Y of del Del W of Y which is my output of this summation block in my neurons so together this is what will help me in getting down my total gradient computation now the first part of this gradient which is the grid which is some sort of a derivative of the cost function if you look into the second part of the gradient then you see that it's a derivative or the non-linear transfer function and if you look into the third part of the derivative that's a derivative of the linear network itself and these three things together are what will be helping me in finding out the gradient part for my whole network in order to learn so with this I will end up our lecture for today over here and in the subsequent lecture we would be starting up with this point on gradient computation and subsequently going down to how this can be extended for a multi-layer perceptron and then enter into eventually the deep learning and how to Train down these deep neural networks and then what will be the exist engine criteria so with that thank you and stay tuned for the next lecture 