 It's my pleasure to welcome Bhaskar Mitra today. Bhaskar is actually stationed in our London office currently. He's been at Microsoft now about 11 years, is that right? Yes. About 11 years as a Scientist in Bing. And somewhere during that time, he switched to being a student, about a year or two ago, still while working at Bing. So, he's working full-time and completing his Ph.D. at UCL, under Emine Yilmaz, who many of us know and have collaborated with as well. He's become one of the leaders in applying neural models specifically to Information Retrieval problems. He and Nick Craswell have a nice paper if you would like to read it, that's available on the Web right now currently under submission to foundations and trends and information retrieval that compare traditional ways of approaching information retrieval models and where their parallels are in the Neural world. And he's going to talk to us about some of his recent work. So with that, I'll have Bhaskar take it away.  Thanks for the nice introduction. So, today's talk, I'm going to be focusing on some of my research that I've been doing for the last couple of years both in the context of my work in Bing as well as, as Paul mentioned, I've recently become a year and a half back a research student at UCL. So, the research in that context as well. This is not going to be a general overview of neural IR, but as Paul already also plugged in, Nick and I have been working on a manuscript on an overview of the new neural IR field in recent years. So this is currently under review for Foundations and Trends in Information Retrieval, but we put up a early pre-print online. Nick and I, and also some folks from University of Amsterdam, we've been doing a few tutorials recently. This year early specifically WISDM and SIGIR. So, if you are interested again on a broad overview of this area, there's also all the slides and materials from those tutorials that are available online. So, having said all that, now coming back to the stuff that we are actually going to talk about today, Neural networks. We've seen a big penetration of neural networks in many different application areas. The same is true for IR. If you've been keeping track, for example at SIGIR papers, we have seen a pretty big spike in terms of the papers that focus on neural models in last couple of years. And this also happens to be sort of my personal main focus for my Ph.D. as well as work for Bing. The part that I'm really interested in when I talk about Neural Networks is that if you look at how neural networks has entered the different fields, what's also has happened is if you look at Speech or Vision, and all of these other application areas, each of these application areas has different requirements and challenges. And they have led to different kind of understanding or innovations in the field of machine learning. So, that's the area that I really want to get into where we're thinking about doing machine learning differently based on being motivated by the specific challenges in IR. So, what do I mean by IR? What would a typical IR task look like? So, the most popular IR task that we probably all familiar with is a web search, right? So, it involves you have a short text query, you have an index of billions of long text documents, and then the search engine fetches you top key documents, and presents it to the user. But when I'm talking about information retrieval, today, it also includes other IR tasks. So, for example, I would refer to Query Auto-Completion where given a prefix, you are trying to help the user formulate the query by completing the query or recommending possible suffixes for the prefix. I'll also give some examples of IR task where IR task could be like predicting the next query in a search session. So, I'll take these three tasks as example for some of the discussions that we'll have during this talk. One of the ways I can fit any of this IR task is in this kind of a crude framework. So, this is very intentionally a crude frameworks, so bear with me when I describe it this way. But basically, you can think of all IR task as you have some input text which could be a query, it could be a query prefix or something else. And then, you'll have a set of different candidates that you want to retrieve from or rank within. So, for every candidate and an input pair, and you can have some representation of the input and the candidate or potentially a joint representation, and then from that representation, you estimate relevance. The reason I bring this framework up is because while we were writing that paper for FnTIR, this helped us classify how different neural approaches have tried either influencing learning query representations, or learning representations of documents or even just applying neural networks just for estimating the relevance. So, for example, the very popular learning to rank framework the way that would fit into this visualization is that you have an input query text, you have a document text, and then you're generating manually designed features from these two to give a joint representation, and then you can have a neural network or any other machine learning model of your choice for estimating relevance. The part that I am primarily interested in is actually the representation learning part. And that's what I'm going to be focusing on during this talk. So, but before I talk about specifically Neural Models for IR, I wanted to make a slight segway to talk about something that personally is a favorite topic of mine, to just talk about the basics of different kind of vector representations, and specifically, this topic of the notions of similarity in different representations. So this slide is really, really kind of the fundamental. So, everybody here knows it, so, just bear with me as I quickly go through them. So just defining a couple of things here, local representations and distributed representations. So, what do I mean by them? So these are terms that are pretty popular especially in the neural network literature. So, a local representation or one hand representation is when you have, let's say you want to present k different items, then you represent each of these items by a vector of length k, where all the positions are zero except for a particular position that corresponds to that item. So, let's say in this particular example, if I had a set of different items of which three items were banana, mango, and dog, then in my one hand representation or local representation, for banana, you have everything as zero, and then one position that has a one or a non-zero value. On the other side, if you talk about distributed representation, this is the idea of distributed representation is value, represent items by a vector. While a particular item is represented by having more than one non-zero items in your vector. A very simple example of this would be if you think of any kind of feature space. So imagine I had a feature space where I had features like does this item bark, is it a fruit, is it of a particular shape, does it have a tail, and so on so forth. So if you imagine some kind of a feature space like that, then you could say that a dog could be represented by something that barks and has a tail. But as a banana, it could be represented by something that's a fruit and has a particular shape. The interesting thing about distributed representation as opposed to local representation is that the moment you have this kind of a feature representation, you can actually reason about which items are similar to each other, based on the features. So, for example, in this space, you can see that banana and fruit are similar because they're both fruits, banana and mango. And they are different from dog because they don't have any overlapping features. But as if you're talking about the local representation, all of these items are distinct because no vector is similar to another vector in that space. The important corollary here to point out is that the moment you define a feature space, you are in fact implicitly or explicitly making a choice of what items are similar to each other. For example, you could easily come up with the feature space, where let's say a banana and a dog might actually be more similar to each other than a banana and a mango. So maybe your feature space is about, I don't know, some kind of shape like is it long versus is it round, or something on those lines, or some something based on color, by which you could actually say that in that feature space, banana and dog is more similar than let's say mango. So, my last definition on these slides is about embeddings. So, you'll hear a lot of, like especially in the Neural Network, all of the people say embedding is a very popular topic. One of the things that I just want to highlight is that an embedding is a new latent space that retains the properties and relationships between items from an original feature space. So, a lot of the work where we talk about, let's say [inaudible] or other kind of feature, other kind of embeddings that we talked about. One thing to keep in mind is that all of them are derived from an original potentially sparse feature space, and the embeddings basically have the same relationship as in the original feature space. However, your embeddings might be, because of the nature of them being more smaller dimensional and dense, they might be generalizing more or other factors. But the important thing is that a lot of the relationships that are interesting properties that you're seeing in your embedding space are actually existed in your original sparse feature space as well. Why is this important? This is important because I personally have found that it's quite difficult to reason about embedding spaces, because of the nature of these dense vectors. But if you think about the same items and their original sparse feature space, sometimes you can build a lot of other kind of intuitions that can be used. Well, I'll give some examples of the same thing. Right? So, before I go into the example, this is the kind of the question that I wanted to put forward. And this is one of my favorite topic, so, bear with me if I talk about this a bit too long. So this is the idea of thinking about, explicitly thinking about what kind of relationships are you capturing in a certain vector representation? So, let's say you've learned an embedding of words or concepts, and then you have Seattle, and the question might arise. Is Seattle similar to Sydney because they're both cities? Or should Seattle be similar to a Seahawks because of Seattle Seahawks? So, I want to take some example and actually answer this question about like how some of the choices you make in designing, or how you learn your embeddings would actually influence which of these relationships you would actually modeling and your embedding space actually captures. So what I'm going to do here, is I'm going to take a really small toy corpus of 16 short sentences. And from those 16 short sentences, I'm going to try and derive different kind of representation for the four domes, Seattle, Denver, Broncos, and Seahawks. And basically, my goal here is to show that depending on how you featurize these words based on this corpus, you actually end up capturing different kind of relationships between these four domes. So, the first thing I'm going to do is I'm going to featurize these words based on which documents they appeared in. So we see that in our document, for example, Seattle appears in document one and two, Seattle Seahawks appears in document three. They both appear in document five, and so on so forth. Just a quick note on the colors here. So the white here means a zero, the grey means a non-zero value, and the only reason I have a light and a dark gray is to indicate those columns where you have more than one non-zero element. So the diagram basically shows there is some overlap between at least two of the vectors in here. So, basically, if you're going to squint at this particular diagram hard enough, you'd start seeing that the vector for Seattle based on this very crude rudimentary example, actually starts looking very similar to the vector for Seahawks. And similarly, if you look at Denver and Broncos, these start looking similar to each other. For people in IR , this immediately would remind you of the dome document metrics that you would typically factorize for LDA and other representations, right? So now, let's do a jump and do a very different kind of featurization based on neighboring words. But with one small additional detail that I'm actually going to consider the distance between the two words. So what do I mean by that? What I mean by that is that I have a feature Seahawks plus one, which means that this is a feature that says that on one position to the right, the word Seahawks appears. So for Seattle Seahawks, you have a non-zero value, because somewhere in here, you have a bunch of Seattle Seahawks that you see in your corpus. So, then, you also have a, let's take the example Seattle Seahawks Wilson. So the word Wilson actually ends up giving you two different features, Wilson plus one and plus two. So Wilson plus one means it occurs one position to the right of this word, which is true for Seahawks. So you will see that Wilson plus one actually is a feature for Seahawks, but not for Seattle. But Wilson plus two, which means it's two position to the right, is a feature that's non-zero for Seattle, but again, it's zero for Seahawks. So you take this feature space, and now, again, do the same exercise of squinting really harder this diagram, and suddenly you start seeing that Seattle and Denver starts looking similar, and Seahawks and Broncos starts looking similar. Right? So this starts giving you more of a notion of a tie based similarity in this diagram. By the way, for people in NOP, people refer to this for a pretty long time as kind of syntagmatic and paradigmatic relationships on using a different dome, but I mean something very similar. The third version is similar to the previous one but we do want change where we no longer consider the distance between the words. So now, Wilson is a single feature that's true for both Seahawks and Seattle. And again, if you do the squinting thing again, what would happen here is now we actually see that pretty much all four of them has some kind of an overlap. And like Seattle is similar to both Seahawks and to Denver but because of different sets of features. If you think about it, this is exactly the kind of feature space that something like Word2Vec or GloVe operates on. So the point that I'm trying to drive towards is that, if you think about this feature space, you realize that it's the property of which words are similar to each other, or the fact that something can do a vector algebra in the embedding space is actually coming from this original Features Space. So this, for example, Levien Goldberg did this work where they showed that the vector algebra like in minus man plus woman, you could actually do this on the sparse feature space without even and learning the embedding stuff. And similarly, it doesn't really matter if you're using metrics factorization on neural nets or some other approach. The relationships you're modeling is actually much more influenced by the basically what original feature space you're trying to compress. This by the way, one of the things about Word2Vec is the window size that you consider when training Word2Vec. Actually, has some strong influence on the balance between the type and topic-based similarity. So, basically if you think of this, this is the reason why in the Word2Vec embeddings, you would see Seattle being close to both Seahawks and Denver. So, it has a mix of this type and topic similarity. And how much of this type versus topic might depend on the hyperparameters, for example like the window size and stuff, and which I'm not going to go into details but we can talk about why that it is after the talk. I wanted to jump into this example because this often is pretty intuitive when you look at this visually like this, but then obviously, the questions come up, why is this important? So why are we talking about this in the context of neural IR? This is important especially because, so if you think about the IR problem, to be honest this is true for any variable you use embeddings, not just in IR, but taking the example of an IR task, what you would typically do is given, let's say an input text like a query or a prefix, you could come up with some projection into an embedding space. And then your assumption is that the neighbors in that embedding space are the good results or candidates that you want to rank high and show to the user. So, what that also means is that for a particular input, if these are all my right candidates, then it means that these right candidates should also be similar to each other. Now, if you're talking about different IR tasks, so we talked about in the beginning like document ranking, Query Auto-Completion or related search, the argument that I'm really trying to make is that each of these different IR tasks expects different notions of similarity between the different items that they're trying to rank. And it's important to understand what relationship you've modelled in your feature space and whether or not that's appropriate for the task that you are trying to apply to. So, going back to the same three examples. So, let's achieve my document ranking example as that the query is cheap flights to London. And let's assume these are the titles of the documents that I'm ranking. So cheap flights to London obviously, should be relevant to the document cheap flights to London. But then obviously, you don't want to see documents about cheap flights to Sydney or hotels in London, right? However, if you take the Query Auto-Completion task, imagine this is also modelled in a similar way via given a prefix, I have some prefix embedding that projects into a space and then I'm doing nearest neighbour search based on suffixes. So, I'm just ranking suffix. Let's assume that's how we do the Query Auto-Completion task here. So here, now what happens is if you have the prefix, cheap flights to, you actually would, as you expect London and Sydney to be there. They are potentially right answers here. So, what that means is that in this space, if you've learned the representation, London and Sydney should actually be close to each other. Whereas, they definitely should not be close to each other here. On the other hand, London and Big Ben should not be close to each other here. Whereas here, you could potentially rank a document high that also mentioned Big Ben. So, this very kind of a topical similarity versus type similarity. And then we'll talk of next query suggestion. There's a whole lot of other things that opens up. So for example here, the same topic is obviously relevant but you're also trying to predict the next step in the task. So here, there's almost like a directionality factor as well. Like you can think of certain pairs of queries where query B will always follow query A and not the other way around. So for example, if you see two queries like the big clock tower in London and Big Ben, you would probably expect that the big clock tower in London query came before Big Ben because it indicates that the user did not know what it's called. So there's a potentially different kind of relationship that you would need for doing this particular task. I'm going to stay on this topic a little longer by showing another single model trained on three different types of data that demonstrates these three different types of relationships. So this is the very popular, the DSSM model. So again, people here should be pretty familiar with this model. The DSSM model is a Siamese network. It was originally proposed for the document ranking task. What this model basically does is it takes a query, it takes a document title, it projects the query into some embedding space of 120 dimensional embedding space, it projects the document title in the same space, and if the query and document are relevant to each other, then you would expect the distance to be small, the cosine similarity to be high. And the way you train this model is you take a query, a positive or a relevant document for that query and you take a bunch of negative documents which are not relevant to the same query and you train to optimize cross entropy at the top to predict the right document from the negative documents. So now, we take the same exact model. The model doesn't really specify what text, like you could present any pairs of short text here and train this model. So, let's take the same model and actually train it on three different types of data. So we're going to train this on query-document titles, which was originally proposed. We are also going to train it on prefix-suffix pairs. So, we'll take a query, split the query randomly at a word boundary, and then train the model on, given a prefix, predict the suffix. And then the third one is to actually take pairs of query from such sessions and then given a query, you try to predict the next query in the same session. And basically what you end up seeing is that if you've trained the query, let's take a particular example, let's take the two models trained on query-document pair and prefix-suffix pairs. Once you've finished training the model, you take the query model from the first set. You project a piece of text in the space and then you look at it's nearest neighbor. And on the other side, you take the query that was trained on prefixes and you do the exact same thing. You're given a piece of text, you project it using this model and then you look at its nearest neighbors. You almost immediately see this pattern especially by the way the models are trained on short text. Sorry, this was not for short text. Ignore the last comment. So what you immediately start seeing is that the model that's trained on query-document pair, the nearest neighbor to Seattle is weather Seattle, Seattle weather, Ikea Seattle, everything about Seattle. Whereas, if you look at the model that's trained on prefix-suffix pairs, it's nearest neighbors are all these other cities Chicago, San Antonio, and Denver, and so on so forth. Similarly, if you project query on the piece of text Taylor Swift, the model that's trained on query-document pairs gives you everything about Taylor Swift. Whereas the model that's trained on prefix-suffix pairs, it actually gives you everything that's of the same type as Taylor Swift, like celebrities. Kind of the very same idea as we were looking at before.  [inaudible]  Sure.  How do you train the prefix-suffix pairs?  So, you basically take query compass. For every query, you randomly split at a word boundary and then you would pass the prefix here and the suffix here and some negative suffixes here. So the actual paper that this one cites, this is where we were looking at suffix prediction given prefix for dealing with Auto-Completion for rare prefixes where you actually haven't seen any query with this prefix. If you still had a candidate of, let's say, 100,000 most popular suffixes, then you might still be able to predict with suffix goes with the prefix.  Do you have two different endpoints on the left and right?  Yes. So, the Siamese network, what's happening is it's actually, you have two different models. So, the weights are not shared between this and this but this is the same model here. Right?  Okay. That makes sense.  One question.  Yeah.  If you go back to the previous slide, it seems like there are two things going on from what you said earlier. One is, the supervision signal of what's related. And then, the other question is the co-occurrence of different approaches that people often apply. If you take the embedding, so take some general embedding, embed these things, and now use this revision signal to actually learn, for each of these domains so that you don't complete both of them. Go back to the earlier example, if the relationships were there in embedding, then one thing you can do is just project to the right relationship. And there is a separate question of, are the relationships even there in the first place? Now, you're getting? How much did you get from each of those?  The other thing is also that you might have learned an embedding with certain relationships that's almost so orthogonal to the task that you are trying to learn. So, one example is in Bing, when we were working on Autosuggest, somebody took a particular embeddings that was trained, basically to capture topical similarity, sort of trained on like basically query and documents. And then, what they wanted to do is they wanted to actually use that for Auto-Completion. So, what they did was, they initialized the embeddings based on what was pre-trained, and then, they did continue training. And they actually realized that it was worse than randomly initializing the embeddings. Because what happened is, if you think about it, if Seattle is close to Seahawks and far away from Sydney, but in the Query Auto-Completion where somebody says, map off where Seattle and Sydney should be close together, this embedding had to actually push these two things apart, and pull those two things that were far away close together. Now, if that's your only representation of these items, and you're just trying to learn a function on top, then it depends on whether the original embedding space has captured the useful information in addition to capturing other stuff. But it could also be that the original embedding space is completely the wrong space. That there's no other function you can apply on top to recover the actual useful information that you're looking for. So, it depends. And basically, the argument I'm trying to make, I was going to have that slide after this is that, all of this really matters if you're going to learn representation separately and then, use it for a different task, which is actually pretty practical because a lot of time you don't have enough data for the supervision signal to do the learn representations. Representation learning takes a lot of data. So, that's what people for example, will train what to work on, the document corpus alone, and then, you might plug in those embeddings, and doing ranking or something else. So, that's when it's becomes important to think about, is this even the right thing? Or if the thing doesn't work, then when you're debugging then it comes to fact like, maybe there's a disconnect between the embeddings I've learned and the tasks that I'm trying to apply it on. Or it gives you ideas about how to write, learn the right relationships so that it might be more useful for the task. So, it doesn't matter if you're learning everything and doing, then, this question goes away. The third example I wanted to show was the case where the model was the same DSSM model is now trained on pairs of queries from surf sessions. And this again shows very different kind of regularity. So, here, you can actually see a similar kind of regularities that you might have noticed. You can probably make a connection with like the Word2vec at a term level. So, this basically allows you to do similar kind of vector algebra on short text, like you could do with Word2vec on terms on. Because you see that Denver to Denver Broncos is parallel to San Francisco, whoever the team is, sorry. There you go. Seattle to Seahawks, I should always stick to Seattle because I know the answer. And yeah, so this basically, if you train it on session query pairs, you can basically then use the same model to do things like University of Washington minus Seattle plus Chicago gives you Chicago State University. Obviously, this like any other vector algebra based solution, obviously is not always going to be correct. So, you also have like wrong answers sometime here. But it just kind of shows you that it has a similar property. And if you think about it, these models of training on pairs of query in a session or training on prefix, suffix that actually similar to Word2vec, but it's training on neighbors. Whereas the original model, if you think about training from query to document is more like training like an LDA model where it's termed to document ID. So, it's kind of has those, the same intuitions that pops up. So, yeah, so I skipped, I set prematurely talked about the site, but this is the main point that you really care about all of this, if you are going to use pre-trained embeddings. If your model just learns here embeddings in C2 for the task that you're doing, then you can be completely ignorant, and let the model learn whatever is the right representation. So, I want to give an example quickly of some, but this could be useful. This was actually a joint work with Eric Nalisnick. He was an intern here, and Rich Caruana, and Nick Craswell. So, this basically led us to thinking about using word embeddings for document ranking. So, in document ranking, one of the things that you could use these embedding for is sort of soft matching. So, for example, let say we have these two passages, and we want to know which of these two passage is more relevant to the query Albuquerque. So, there's a lot of hints. If you look at the first passage from the fact that it has words like metropolitan, population, area, and so on and so forth. So, if you knew at some way you could determine the other words you expect in a passage or a document that correlates with your query terms, and that can be a useful signal for relevance. Unlike typical IR models where you only count the exact occurrence query terms in the document. And you can say that the second passage is not relevant because this passage is actually taken from Wikipedia, described from the Wikipedia page on Microsoft that just happens to mention Albuquerque in there. So, you take this idea, so what you can then do is you could, given a query in a document, you could represent all the query terms using the word embeddings that you have. You can represent all the document terms with the embedding you have. Let's assume you just do a simple centroid of the query term, centroid of the document terms, and you have a simple cosine similarity or a dot product to say if the query and document is relevant. Let say that's the simplest model you can do. Now, the interesting, and this is where thinking about these relationship explicitly might help you is, so when we started thinking about this, one of the things we suddenly realized is that, if you look at the Word2vec model, there's actually two different embeddings that are being learned. So, the Work2vec model, what happens is you have an input word represented as a one hand vector, multiplied by a metrics, multiplied by another metrics. This is the bottleneck clear. And then, you're trying to predict the correct neighboring words. What happens here is that, you actually have two different weight matrices. And I'm going for the ease of reference, I'm just going to call them the in metrics and the out metrics. So, the IN embeddings and the OUT embeddings. Typically, once a Word2vec model is trained, people completely discard one of these matrices and just use the other one as the embedding. Now, if you train your Word2vec model on short text, for example, like queries, turns out looking a similarity between two different terms based on both of their IN embeddings or based on both of their OUT embeddings, actually gives you a notion of similarity that is closer to a Type A similarity. So, Yale becomes close to Harvard, NYU, Cornell, and so on and so forth. However, if you were to represent one of the terms with IN, the other terms with the OUT embedding, and then do the dot product or cosine similarity, you end up, the neighbors of Yale starts becoming faculty, alumni, that is misspelled, anyways, orientation, and graduate. It took me two years to realize that is misspelled, that's something. Anyways, for document ranking what you're really looking for is, if you have the query Yale, you're looking for a document that not only has the word Yale in it, but likely also has other words like faculty, alumni, and orientation and so forth. At least, you expect that more than occurrences of Harvard and NYU. Although Harvard and NYU's presence might also indicate some kind of relevance. So, basically what we did in this case is we plugged in this intuition into the word embedding model we showed before by basically doing something very simple. We would use the IN embeddings for the query terms, and the OUT embeddings for the document terms, so that when you do the cosine similarity or the dot product between the query and the document, you're basically computing an IN-OUT similarity for every query term with every document term. And that gives you an improvement in relevance in this context.  A word, and the output is the next word?  Yes. So the Word2vec is typically trained, given a word, predict another word within a given context window.  Including itself or not including itself?  Not including itself. It's basically, let's say, if the window size is 10 words on each side, it will try to predict each of the 10 words on either side of this particular word.  How does that compare to using the DSSM sector?  You mean in performance wise?  Yeah.  Well, the DESM would perform better just because it's trained specifically for that task.  I see.  Right? And the DESM model is learning a topical notion of similarity.  Because DESM flips like you're trying to map that query to the document pair. So, you are extending the query in that sense.  Yes. I should clarify. I mean, the DESM model trained on query document pairs. If you look at the DESM model trained on prefix-suffix pairs, it's basically learning a similar relationship as Word2vec training on word and neighboring words. Again, you can use even that model. We haven't really worked on this, but there is no reason why you can't actually train, if you had for example, small amount of query document pair of data, you can actually train the DESM model on prefix-suffix pairs, and use it in the same IN-OUT way as the DESM thing proposes for document ranking as well.  And in that case, you're training on query?  Yes.  Okay.  This model was trained on queries.  I see.  Basically, the two secret things here is that, if you train this model on long text and have a window, the window of text long enough, then even the IN-IN and OUT-OUT starts becoming more and more topical, the balance moves towards it. So basically, the reason this becomes important is, you actually get a big benefit by training on query terms as opposed to document text, and for short text, the IN-IN and IN-OUT similarity, actually, the difference becomes bigger. So, you combine the two things and then you see a big improvement. But if you were to train this model on, as I said long text with long context window, then you might see a less difference on this.  So, if you built this amounts that give you score of the next word and the previous word, what happens if you create like ten slots in the sentence, and then run proliferation to fill in the transport. Can you generate sentences that way?  The Word2vec was originally proposed as a simplification for RNS, if your goal is only to learn on embeddings. But this basically goes back to the same idea of sequence completion. Right? And depending on a few different things, so for example, you could train Word Embeddings, where the word appearing next to you is different from the same word appearing two position away from you. So, you could actually take that into effect, and again, that would change the relationships you're learning. But in terms of sequence completion, yeah, this basically goes back to the RNN and all of those kind of word directly.  [inaudible] RNS, but just using this thing, to generate sentences by simply running a little bit variation through the slots.  You could. You just have to, in that case, also take into context then the position and the distance but there is...  Yeah.  You could come up with a, I mean, again, if you start doing that slowly, it will start evolving into something that starts looking like RNN. Basically, the point with RNN is that I want context of not just the previous or the last three or four words, I want to have longer and longer context, the whole RNN analyst theme is about how can you have some notion of the things you've already seen in the sequence with the fixed memory size.  Okay. [inaudible] to understand what RNN is. I'm just curious, what will happen just in any case.  As opposed to RNN, like you can always have a model that has, if you knew your sequences are always short, you could come up with a model that always knows that there are at max key items before, and you have an exact memory of what you've seen in the past, and then you're not doing reckless, but you actually have different way matrices based on positions for the lobby. I've never worked in that space, so, I wouldn't comment on what works or what doesn't but I'm just saying the answer. Sorry?  And you don't know if anybody has tried that, just in common?  I'm pretty sure the NLB space, people have. But again, I would probably defer the question to somebody who operate in that area. So, moving on further and a bit faster.  Think about it, what was the data that's available?  We actually put OUT the IN-OUT embeddings data for two point seven million words trained on Bing queries, just publicly we had released it.  Is anyone using that training?  I got questions from different people, so, I assume some people are, but yeah. This was another work joined well with Fernando and Nick. So, basically, in the same spirit of looking at word embeddings, the other question that came up is that, obviously terms are inherently ambiguous. This is one of Fernando's favorite topic, and he's really convinced me that this is a very important thing to think about. He's got to think about this idea of local and global analysis, right? So, the point is basically that, let's say you learn a representation globally based on your whole corpus, so then you're basically learning some sort of a crude representation for every term, especially when the term is ambiguous and can take different meanings. Right? And this is not just like the crude Apple computer versus Apple the fruit, it's also very subtle. For example, the word "CUT" could mean different things, but then, if I know that I'm using the word in the context of gasoline tax, then this "CUT" means like cutting tax or reducing deficit or whatever. Basically, the idea was like, "How can you get really extreme in thinking about a local sense of a term, or a representation of a term?" What we did is, we tried this experiment in the area of query expansion for retrieval. What we would do is, given a query, we would send that query to the index, and get a bunch of results, and then we would at run time train a work to work model on that corpus of top thousand documents, and then use that embedding to expand the original query, send it back to the corpus and get results. And basically, the point comes down to is this. What this diagram shows is, the blue dot in the center is the query centroid, and the way you are expanding the query is by finding other terms that are similar to the query center. So basically, you're doing a nearest neighbor around the space. These are just contour lines, but every other circle here are the terms candidate terms that you can use to expand the query. And the red terms are the actual good terms, that we know that if we add this to the query, you would see an increase in NDCG. And that's what it looks like for a global word vector space, and this is what it looks like with a local vector space. But the idea here mainly is that, I think this actually relates to a slightly broader idea that, for any kind of modeling, for example, when you're learning a query document ranking model, all these Neural net models are basically starting to encode real world information about correlations, right? So, if somebody says, let's say Albuquerque. Right? So you expect that for the word Albuquerque, you would expect these things like altitude or population or area, as to be the correlated words that you would expect in the document. But these models are always going to be inferior to a very, very narrow topic, like something specifically in microbiology. It's very hard that this model can memorize those correlations. And the idea was that, we can think of actually training models potentially at run time, or doing things differently, such that these models can still go in and effectively research a particular topic, learn a new representation and then apply it to a problem. The metaphor that I use with Nick, is thinking about whether your model is a librarian or a library. Can your model practically learn everything about everything in the whole universe and be a library? Or is it like a smart librarian who might not know enough, but knows, given a question on microbiology, that that's the section in microbiology, let's go there, let's read up all the books, learn a new representation. Now, I know which book to look at, and so on so forth. So, that was kind of the idea, but this was a crude way of doing this.  The local embedding is trained on different area.  The local embedding is, given a query, it actually retrieves a top thousand documents. And the embedding is trained on those thousand documents.  The local one has not been trained from that data.  It has been trained on a random sample of documents from the same compass, but not for the query specific.  [inaudible] seem the same there.  Yeah.  So, this is like, because usually when they talk about this bearing they're saying well, all the information is there, you just have to just find the right projection.  Yes, but the whole...  In this case it's just, it's actually different data, so, you may not even be able to find the projection.  So the point is, even if your corpus is fixed, it's almost impossible for you to learn on everything in the corpus within. So, this also goes back to thinking about machine learn model having certain amount of capacity. And it's going to prioritize learning things well that it sees more often, and not learn those things that it sees rarely. Whereas if you knew all you wanted to do was to answer this question about microbiology, you could go find only those thousand documents on microbiology. So, there's enough data, but the model basically doesn't prioritize if it has to learn the whole universe. But then, it can go and learn that small universe much more efficiently, and be more effective.  This is hard to tell, because it's a very high dimensional embedding. So, you have space for everything.  Technically, yes.  The difference may be in a very [inaudible].  It could be. So there might be some thing that you could do to force this. There maybe others solution to this, but yeah.  So, let me come back to the question in different data. I mean, the local is just a sub-sample of the global, right? So, with that different data it's a subsample.  No, it's different. It's actually.  It's a different sample. It's not different data, it's not different documents, it's a subsample of a subsample of.  But they are different samples.  There different samples, they are not different data, right?  Well.  If everything was learnable from the global, then you would still have the relationships that global has access to everything local.  If the global is big enough to get the samples. If in general all the data.  Local is a subset of global.  And what if you sample? If you sample you will not get anything that's in the datas.  You can go global over everything.  It trends everything but it is sub-sampling from the overall distribution because...  So initialize the seeds and you can do a sub-sample relationship. Same thing is done on the learning curve.  Basically the idea is that global might have a lot of noise. So some of the sample relationship may be drowned out by noise, so and then kind of buy like two level ranking where you use some sort of naive textural benching type of stuff to discover lightly relevant subset and then you hope to discover locally stronger relationship from that subset. Is that, kind of, reasonable way to look at it?  Right. So basically your global, the model, it's any model that you train on somebody that's going to prioritize learning things that it sees often. But then you can use for example the global model to get some ranking of basically narrow down which domain you really, this belongs to. And then you can have a local model that's trained on that domain specific data. And then it would likely perform better on that specific topic.  So if I was to put, you said previously, for example the global corporates you will have many fewer documents on like leverage. Then you'll sample these slots on purpose. So you don't see the same data in the two cases.  Yes. It's true. Having said that I would still imagine that if you did train on the exact same samples but like subsample, I think this would still hold. But what you described is what we write instead.  If you just take the global data and then edit the local sample to it. Now the question is, those are two different questions right? Would you then have a problem with learning and still ignore the data just because, but in this case you are not even seeing it, so then...  Potentially yes.  Yeah.  Okay. So I'm going to go into the last part of this talk where this is more recent work where we actually started looking at deep neural nets trained end to end so no longer thinking explicitly of these representations and all that stuff. But having deep neural nets and kind of a very clean set up and training them for document ranking. And obviously this is all based on the fact that we have a lot of data in Bing so we can actually train these models end to end on our ranking data. So what we started looking at was the kind of a classical document retrieval task where you have a short-text qwerty in a long text documents. A lot of the models in IR today actually looked at short-text verses short-text the book. And one of the things to kind of realize is the challenges of doing short-text to long-text are different, both good and bad. So, short-text to short-text, this embedding based models actually benefit a lot from the fact that short-text to short-text the vocab mismatch problem is much more severe. So if your model understands synonymy or related things it's most likely to show you improvement over exact matching based similarities. On the other hand for long- text the opportunities lies from the fact that long text has mixture of many topics and the matches could be actually in different parts of the document and based on which part of the document matters, it might indicate more or less relevance and also things like term proximity and all of those other things becomes important. So, there's a lot of possibility of modeling other stuff when you're dealing with long-text not just the vocab mismatch. An interesting way of thinking about this neural nets for IR, is to take these two particular queries. So these are my favorite poster child queries, Pekarovic land company and what channel are the Seahawks on today. So the point that I want to make with this query is, Pekarovic land company if this is the term Pekarovic, you've never seen in your training data, then I don't care how you, have your neural model input representation. So, for example it could be 100 presentations of terms or character trigrams and whatnot. Your model will never going to have a good representation for the term Pekarovic. So even for DSSN that takes character trigram based inputs you would expect that for a term it has never seen before, it would probably have some sort of a random projection or not so very informative protection let's put it that way. So the point is that for this query, for an embedding based model it's actually, it really struggles with a query like this right? On the other hand we know that typical, a classic IR models actually really do well with this because if there are like 50 documents in your a billion index that has this term, then fetch those 50 documents, rank them based on where this term appears and how they appear next to each other and so forth. But you could learn all of the same functions within neural net. On the other hand, you have the query what channel are the Seahawks on today, here lexical matching model would actually suffer because the right document potentially doesn't even have the word channel in it. It probably has actually terms like ESPN or Sky Sports right? So it's almost like a translation model kind of thing happening here. If you see these words in the query you expect these other words in the document. So this is where an embedding this model is likely to really shine. And one of the first work we did in the space was this thing we call the Duet Architecture which was basically just the simple idea that, hey why not, if these are the two different aspects that might be important in IR why not use a neural net to learn both of them and learn them jointly. So, that basically led us to this idea of the Duet model. Now this is an architecture we proposed but then the idea of the Duet goes beyond just this architecture. You can come up with other architectures that has the same Duet property that you explicitly are mapping, learning the lexical matching as well as the semantic matching signal for retrieval. And the final model in our case was basically a simple linear combination, and you're learning this whole thing as a single model jointly on your supervised data. Well, basically it has query document and labels. So, training like typical learning to rank models are based on pairwise loss. So, how do these things differ? Right? So the local model it's trying to learn how to do lexical matching, exact matching. It has no interest in learning any kinds of representation. So the input representation is such that it can not even learn any kind of embeddings in this case. So what's the input representation look like. So these are really simple matrix representation. So think about a query that has four words and a document that has thousand words. So the input representation for this sub model is a query words by document words matrix which is a binary matrix and it's non-zero every time the word matches the document. So this means that for example, if you look at this line, slightly shifting line it means that you actually saw the occurrence of big deal derby as a phrase in that document. And if you look at these documents here, these examples, it's almost visually suddenly becomes clear which documents are relevant and which are not. So the relevant documents has more matches. It has more matches towards the beginning of the document. It has more phrasal matches. It definitely has matches for all the words in the query unlike this guy and so on so forth. So you can see these visual patterns from these matrix that then you can imagine that you can train a neural net on top to basically learn what are the good patterns of matches.  So you are like ignoring total frequency completely in this representation?  Term frequency, no but inverse document frequency, yes. Term frequency you can get by actually just counting along this line. Right? So if the neural model can just count the number of lines in here and it knows the term frequency for each of these terms. So what we did is, again, you can have different kind of architectures but the simplest thing we did is we had a convolutional model on top that one of the important thing was to have the window of the same length as the document. Basically [inaudible] important thing is for it to map how early in the document you see the occurrence, that was an important thing. But other than that there we tried different things that didn't make too much difference but I'm sure there are things here that you can explore but basically you're doing a convolution model that runs over that same matrix that I showed and then it has some more fully connected layers and so on and so forth, then it gives you a single score.  In the line of the document, then there is no convolution?  It moves along the query direction. It's moving along the query direction. So, on the other side.  About the query and the action?  Yes.  There's something that worked on proximity just because of time. I wanted too may be go faster and then we can come back to the questions but I'm skipping some things about proximity and the query, sorry. So now on the distributors model, what it does? So this again, if you're familiar with the DSSM work. This model is similar to the DSSM with some changes just to specifically deal with long text. So what this model does, is it takes the query and then projects the query into an embedding and then it takes a document. But, instead of projecting the whole document in the embedding space it actually takes a window over the document and predicts each window to an embedding space. Compares the query embedding with the window embeddings and then it aggregates all the information on the top and gives you a single score, right? So, this is a very simple way in describing this model. For more details you can always look at the paper. So yes. So basically, we then trained it in a very similar way as the DSSM model. You would have a query of positive document and a bunch of negative documents. You train it to maximize cross entropy loss. We have a bunch of different findings but one of the main things we found is that, and this was kind of expected is that the part that really takes a lot of training data to learn is that it, text representation. So the local model or the lexical side of the model actually doesn't benefit too much from being thrown a lot of training data at it. Whereas the part that focuses on learning embeddings is the part that actually. So, for our first Duet paper, we didn't even converge. We basically trained it for like 24 hours or 36 hours and kind of stopped at that point. But basically, if you look at the training curve it looks like it was still going up. So, we could have had a lot more better MBCG's by training it longer. And the only reason we didn't is because of time. We later ran the same model on TREC Complex Answer Retrieval Task but the data was smaller and my training time was faster. And there, we actually got to a point where it kind of flattened off at something around like 32 million samples and so on. And this is one of the important points, why if you look at a lot of papers in academia today. It's kind of unfortunate but a lot of focus is on Lexicon Matching Model just because of the lack of big datasets for training these Supervised Models. I'm going to skip this slide. This is also out there. The implementation is out their publicly on GitHub. But this let us then this is the kind of the last recent work I wanted to make a mention of. So, we had another intern Ahmed who came in from UMass earlier this year. And he looked at again, Deep Neural Models given we had a lot of data. It was something he was interested in to again look at Deep Neural Models for IR. And the problem we looked at is the fact that it's not just documents and web searches, not just body text but it's also title, and URL and Anchor Text and Clickstreams and all the other kind of data sets that we have, right? So, this work was basically kind of the same kind of you can compare it with like going from BM25 to BM25F. So basically, how can you use a Neural Model that when you think of document as a single piece of text but my normal document is composed of multiple fields. So, how do you have a Neural Model that actually can make use of multiple fields? So this is a wisdom paper 2018. So basically, the idea is this, you have a document that has Title Text, URL, body, Incoming Anchor Text, and Incoming Query Text. And what you're doing here. I'm going to just give you a quick overview. So basically, I'm going to touching up on the main design decisions that makes a big difference. So, you learn an embedding for each of the Metastreams separately because each of these fields of Metastreams might have different properties. So you want to learn different embedding spaces for them. For fields that have multiple instances. So for example, for Anchor Text and Query Text you have multiple clicked queries associated with this document and multiple Anchored Text associated with this document. You are effectively doing an average pulling but making sure you take care of how many instances are there and do the right averaging. So, by doing all of that you end up with these field embeddings and then you learn different query embeddings to match with each of these different fields. And this comes from the fact that for example, the query embedding you want to learn to match with title might be different from the query embedding that you want to learn to match with URL because they captured different properties. Once you've done this hadamard or element wise. Product between these two embeddings you end up with a match vector corresponding to each of the fields. So this comes from the intuition from like same similar intuition as BM25F, where you don't want to combine the different fields based on individual scores. So this is the same idea that you could end of let's say the query was Barack Obama Facebook. By having a vector and not a single score it allows the model to realize that maybe Barack Obama matched on every field, but Facebook never matched on any of the field, versus Barack Obama matched on this field and Facebook matched on the URL field. So, by having a vector it can encode that information but it wouldn't if you had a single score there. And then you have some fully connected layers on top to combine those. And one of the things that's important in this context is and this happens even for typical learning to rank model, is that if you have a very strong precise signal such as Clickstreams, it can actually hamper while you learn the other fields. So, one of the things that we found really useful especially in the presence of a field like click data is to have a field level drop out in this model. So yes. So that's basically it. So basically, one of the things on a concluding note wanted to say was, these Deep Neural Models obviously there's a lot of excitement in the field. And people have been thinking about like how this is going to influence this IR field in general. My personal observation is that all of the stuff that we typically talked of IR initially went into designing of features during the learning to rank frame days. And now, when we're starting to talk about these Deep Neural Nets. The same intuitions, scored intuitions from IR is actually fueling the design of these different architectures. Just this one I'm not going to talk about what we've also learned recently some work on proactive recommendations such as pro actively, recommending attachments for email. And yes, these are all the papers that I covered. And thank you. Sorry for going a bit over time.  So we have time for questions for Professor. [inaudible]  Those commercial network filters, what they looked like, I didn't quite understand how you built that up. But it seems like it would be very interesting to see what these filters are-  For the local model, the disclaimer is we didn't spend a lot of time doing too many experiments. But one of the things we did find was to have the length same as the documents. So basically, the way we had this was, we would assume we had 10-  There's also different lengths.  Yes. We made some simplistic assumptions of saying, there are 10 query words, 1,000 documents, zero padding if there is less, truncation if there is more, and then a window one by thousand and moved it for us.  And the window is one by 1,000?  One by 1,000. So, that's query was being 10 and 1,000 being the document words. So, it's the length of the document and moving in the query direction.  So then, you're not capturing the thing that you're talking about. Where do you get these lines?  Yes. So, what happens here, that's a good question, is it actually does get captured to some extent. But again, this is just having a single convolution. You can do different kinds of convolution and combine these things. But even in this case, it does get captured crudely where what happens is, by having a vector here, it can still, if you can just hypothetically, this vector could capture that this term matches a lot in the beginning versus later in the document. So, this vector could encode such an information. It wouldn't encode exactly if they are next to each other potentially, but yes.  If you do directly, like three by ND.  You could do other convolutional configurations in addition to having something that captures the whole document length. We just didn't go into that space. But I'm pretty sure you can do a lot of different architectural stuff.  And you said that it was important to make it as long as the documents, so that you don't have any shifts this way.  Yes. So, that's basically because we noticed that it actually learns that if you have the query terms up here in the document, it's much more important than if it appears later in the document. And that's important enough-  -in the next layer.  It could. Well, we actually had run a few different experiments where this showed up. This is obviously, the explanation is a hypothesis that that's maybe what is happening. But we did see that it made a big difference in having this contingency.  Have you thought about filling this up not just with co-occurrence. I mean, matching up the words but some sort of a measure similarity in some-  Yes. There is other work. If I'm not mistaken, like the match 10 serve paper from Facebook and others. They've tried other architectures where you basically have the lexical and semantic and maybe you have the same kind of matrix representation that captures both, whether the terms are exactly same or-  You're talking about three or four different things-  Let me go ahead and see if we can do a few more of these. That stuff is available all week and I think maybe a longer conversation with Nabosa might be in order.  Yes. I thought so. Yeah.  Are there few other questions?  I have a question about local verses global model you were talking about. The way that I understand it is that it's a neural version of pseudo feedback, basically. So that you compare it to traditional method of trying to do pseudo feedback.  Yes. Actually Ahmed and, I think Ahmed and Bruce actually wrote a paper where they, basically, this wouldn't improve over pseudo relevance. This would not perform better than pseudo relevance feedback. But they combined it with pseudo relevance feedback and showed that it improves on top. But that's exactly correct. So, this paper was written in the context of query expansion. But the core idea that Fernando and I, especially Fernando, was really trying to put forward is this whole point of learning a model specific to the situation you are in. And so basically learning at runtime and going from there. But it was tested in that context.  So, I have a follow up question related that I know I would talk to you later. When pseudo relevance feedback fails, it can fail badly. If you have a bad starting value, you're going to do poorly. But it seems like if we use this in two places, which is why bother use the pseudo relevance feedback? We have query logs that have click information, which give you weights on which one should be the focus for any query we've seen before, how to expand for that query, or even in a session based case, using the previous click documents in the session to expand for the next query, where we know the thing stays topically constrained. Have you looked at or thought about either of these?  Maybe I didn't get your question.  The first one is, for any query we've seen before, instead of looking at top 1,000, you can do a click based weighting, click base if you like, to look at those documents. That's going to get me a much more targeted at relevance and not have the failures of pseudo relevance feedback so we can do expansion for any query that we've seen before. The second case is you can do same thing at a session based example, which is the documents that somebody has clicked before current query, is there a case of what is relevant to this topic, right? And we know that people think topicly relevant within a session. Again, you are in a much better case by not doing pseudo relevance feedback.  So, two different answers. One is, in terms of the first part of that, I think this whole point of local global analysis, if you think of the spectrum, the local analysis becomes really interesting when you're dealing with a query that has effectively no presence in your click data. So for example, the Pecarovich Land Company. You would expect that if there are even 50 documents in your index, it might fetch those 50 documents and those 50 documents might be enough for you to learn a local representation and do something interesting. If you have enough presence of that in click streams, you could imagine that the global representation might already capture a lot of it. Of course, there might also be a sweet spot in between where it has some coverage from click data but not enough that your global representation does a good job. So maybe, you can use that. Yeah. You can definitely use that. The second part about looking at previous documents in the same session. Again, I've not looked at it in the context of document ranking. But one of the things we chatted briefly before that I was super interested but haven't really made a breakthrough there is, when I was training the DSSM model on spares of queries from session, the intuition that I really had was kind of starting to think about such sessions as effectively a path in an embedding space. And if that's going to allow us to do something interesting in that direction. Right? And that was trained on queries but it could also be trained on queries and click documents and whatnot. Again, I haven't done anything on that. It's something that's been on the back of my head. If anybody's interested to chat about that or has any ideas about that, I would love to talk about it.  Quick follow-up to the follow-up.  All right. Last question.  The 2,000 documents you are using helped your ranking or?  This was based on Indri. So, he was Fernando-  -type data.  Not in here.  You wouldn't use a full search engine, right? You would be incorporating-  No. This was just based on retrieval and just something simpler model. That's an important question, by the way, which I'm not going to touch upon is how you evaluate these models. Especially the deep neural models on top. There's this whole discussion about telescoping evaluation versus other things. But I'm going to refrain from that. We can talk about it separately. But it's actually an interesting question.  All right. Thanks for speaking again. 