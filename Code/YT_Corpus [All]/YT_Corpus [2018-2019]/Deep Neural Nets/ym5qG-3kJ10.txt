 [Music] so we have seen sequence learning problems now we are interested in the question of how to model these right so we look at something known as recurrent neural networks and our question that we're interested in is how do we model tasks which involves such sequences okay so here's the wish list that we have whatever model we'll come up with should account for the dependence between inputs because that's a strong case that we have made that the output actually depends on multiple inputs and not just a single input it should also account for variable number of inputs because a video could be 300 seconds it could be 20 seconds 25 seconds a sentence could be of arbitrary lengths and so on and it should also make sure that the function at each time step is the same because at every time step we are trying to do the same activity okay so we will focus on each of these items from our wish list and then try to arrive at a model for dealing with such problems so first let's ask this question what is the function being executed at each time step while the function being executed at each time step this should come after a delay you have an input your ability to go blank on me is just amazing actually you have a hidden representation and then you have an output the first time we are seeing this situation in the entire course where you have an input a hidden representation and an output what's the function being executed remember the output is always a function of the input lecture two or three I don't know definitely not lecture 14 so what's the function can you write Y I as a function of X for my sake if not for God's sake you can okay what's it okay first tell me what is s 1 US 1 no non-linearity is no bias all that plus bias then no non-linearity who cares about non-linearity okay and then what's Y 1 okay some output function okay for some reason I have written Sigma here oh right we just call the output function as always maybe in this case Sigma would work but o is what I will call it it just to make the distinction clear okay is that fine so this is a function being executed at this every time step you can just write it using these two equations for the first time okay and I is a time step since we want the same function to be executed at each time step you should share the same network at every time step that means what do I mean by share the same network share the same parameters good so this is the same as this because you V and B and C are the same okay so that's an easy way of taking care of the requirement that I want the same function to be executed at every time step okay and this parameter also sharing also ensures that the network becomes agnostic to the length of the input because now whether I have a word which has 10 characters or 20 characters it does not matter because at every time step I am going to execute the same function that's why it's important that at every time step you have the same function so since we are simply going to compute the same function the number of times it doesn't matter and we can just create multiple copies of this network that we have and for any arbitrary length n we can still compute the output okay still not quite there we still need to take care of a lot of things but we are just slowly addressing each item from our wish list okay now how do we account for dependence between inputs or rather actually the right way of asking this is how do we account for the case that the output actually depends on multiple inputs and not just the current input okay how do you account for that feed in the okay good so let us first see an infeasible way of doing this okay so you are given the first time step x1 you have a network which predicts y 1 from X 1 you know at the same second time step you also want to look at the previous inputs so why not just feed it X 1 and Y X 2 both and then try to predict Y 2 at the third time step feed in X 1 X 2 X 3 and predict Y 3 and so on forever is this fine that was already the word infeasible is there so why is so what's the problem with this you are good so I'm looking in terms of the conditions that we have on the wish list which condition does this violate what is the function being executed at each time step ok so let's see the function being executed at every time step is different so y 1 is function of X 1 Y 2 is a function of X 1 comma X 2 this is not just saying that you're passing two inputs everything changes because you know you need to have u1 and u2 here you need to have u1 u2 u3 right so everything changes it's not the same function I mean if you get this it's a different function being executed at every time step so now if I have a sequence of length hundred what happens I need 500 which takes F X 1/2 X hundred as inputs and as how many parameters you want to you hundred right you can you could share you one to you 99 for Y 99 and 100 but we still need those many on that right so the network is now sensitive to the length of the input and at the length of the input grows you will have to construct more and more functions right and imagine that if at training time the maximum sequence length that you had seen was 25 and suddenly at test time you get a sentence which is of length 30 you don't even know how to compute that because you haven't trained any parameters for doing that so then the final solution is actually to add a recurrent connection in the network why does this work okay before that now can you tell me what is the function being executed at every time step assume there is a s 0 here these are a s 1 s 2 s 3 up to SN and there's a s 0 now what is the function being executed at every time step can you write it down if you it would help if you think in terms of y2 and not in terms of y1 y1 is the boundary case so a special case the think in terms of y2 or any other of the Y's and first think of what is - is from s to Y is straightforward how many of you can write the function so s I in general si is you into X I plus W into si minus 1 plus B how many of you get this right and then what is why I again this has to be output function ok but how does it solve our problem or does it take care of everything on the wish list one the way we have written it in terms of I which is the time step definitely the same function is getting executed at every time step there is no doubt about that right modulo this boundary case of s1 where we will assume that there is an S 0 ok so the same function being executed at every time step can you deal with inputs of arbitrary length as long as you ensure that the same function is executed it's fine does it ensure that the output is actually dependent on the previous inputs how through a si minus 1 right so that's an interesting thing that this guy actually depends on this guy this depends on the previous input and also on this guy which in turn depends on the previous inputs so recursively you can see that you depend on all the previous inputs that you had ok that's a very neat way of ensuring that your output depends on all the previous inputs and you don't blow away the parameters blow up the parameters by sharing this recurrent connection and that's this is a compact way of writing is that your Y is now a function of X I si and it has these parameters wuv and B and C so si is called the state of the network at time step I and what you see here this just for the sake of completion this is known as a recurrent neural network because of this recurrent connection and si is the state of the network at time step PI's as when you start working in deep learning and it is when you're dealing with sequence problems state of RNN or state of lsdm or state of gru is something that you will be hearing or reading often so this is what you mean by the state of the recurrent neural network this is the current state which kind of encodes everything that has happened so far and it has an encoding of all the inputs that you had seen so the parameters of the network are W U and V which are shared across time steps I always forget the biases and the same network is getting executed every time step so I don't need to worry about whether I am computing y1 y2 y3 or 100 right so everyone agrees that this solution takes care of all the things that we had on our wish list how many of you agree with that and this is a more compact way of representing that that you say that you compute si and then you're feeding it back so this is just a more compact way of representing a recurrent neural network so let us now revisit the sequence learning problems that we had seen so now just correct each of these networks what would happen each of these things I was thinking of all the inputs as independent so now what will I do what's what's the only thing to be done just add the recurrent connection right so once you add the recurrent connection now you can go back and relate to all these problems that when I am trying to predict the character which appears after E I also have the information of the and and same argument you can make for all the other examples that you have but I am trying to predict this final state I have the information of all the previous inputs here okay [Music] [Music] 