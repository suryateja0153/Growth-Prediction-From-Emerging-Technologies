 [Music] welcome to lecture nine of CSN zero and five today we will talk about greedy layer by spree training better activation functions better weight initialization methods and batch normalization so today's lecture is more like tips and tricks to make deep learning work right so when you are actually experimenting with deep learning in practice what are some of the things that you need to take keep in mind and it's also a my way of connecting the history that we saw to where we are today right so there were certain things which we saw in the history and now I will try to bring those back and connect to where we are headed from here right where we have reached today and where we are headed from here okay so that's with that in module one I'll do a very quick recap of training neural networks and not take more than five minutes and I need it for a specific purpose so we already saw how to train such a very shallow neural network what is the learning algorithm gradient descent and this was the update rule right and particularly I want you to notice that the gray gradient actually depends on the input okay so when you compute the gradient formula you have this multiplication by X so it's proportional to the input and this is one fact that we will use it at least a couple of places in the lecture today so this was a very shallow single neuron Network what if we have a wider network still which algorithm gradient descent okay and we just have these three different formulae and for each of these formulae note that the gradient or rather this gradient depends on the input that you are feeding in okay I didn't keep this in mind and what if you have a deeper network so we saw a very shallow Network we saw a wide network and I am showing you a deep network what will you do again gradient descent but you will apply the chain rule for computing the ingredients and again here in general you will notice that for any of these weights w1 w2 w3 the gradient formula will have this H I minus 1 what is H I minus 1 input from the previous layer right and it's zero is the actual input so the gradient at any layer is actually proportional to the input from the previously and this could either be the input from the hidden layer or the actual input okay and finally we saw this thin so we saw a wide network we saw a thin network now we will see a wide network and a deep network right sorry we saw only we saw a wide network and a deep network now we see a wide and deep network and here again you have compute the gradient by applying this chain rule across multiple paths and that's what we use and we call it back propagation and remember again they're the same thing holds that the gradients at some point are proportional to the input at that clear everyone remembers that okay so this is important so what we have is things to remember from what we have seen so far is that so training neural networks is basically a game of gradients right so you compute the gradients and everything depends on those how will you update the weights and everything from there on is about the gradients and these gradients actually tell you the responsibility of the parameters to other laws and you appropriately update them and we saw a variant we different sorry various variants of how to use the gradient so we saw the gradient descent we saw an AG momentum and all but in all of these the underlying core thing was to compute the gradient and then do some manipulations based on that ok and the other key thing is that the gradient at all a particular layer depends on the input to that layer ok fine so now let's go back and just retrospect a better and see what is it that we have learned so far so so far what I have taught you gradient descent oh sorry back propagation is something which was proposed way back in 1986 right so in fact it was existing Biffen before that but it was popularized by this work of rumelhart and others in 1986 right so but then in the 1990s or early 2000s if backpropagation already existed and we could train deep neural networks then why didn't we hear so much about deep learning at that time of course you guys were busy with school and all at that time but but why did the others or older people like me not hear about it hey computational power is that the only thing computation and memories are the only thing who said convergence okay good so actually what happened right in the late 80s and early 90s and even early 2000s when you used back propagation to Train really deep networks it was not very successful and what do I mean by not successful actually what are the two things that could happen someone gave the answer already it doesn't converge right that means you don't reach the optimum solution right in fact till 2006 it was very hard to train very deep networks okay and typically even a laughter large number of epochs these networks did not converge that means they were still at a very high loss and although in principle everything is fine you have a deep neural network you have an algorithm that can train it but you are still not being able to train it properly and you're not being able to make any practical use of that right so that was the story till 2006 so today is about what happened in 2006 what it led to in the next few years and then where we are currently right so that's the journey that we need to make okay and that's why we started off with this quick recap of back propagation because that's what I want to tell you that why did it not work earlier and where are we today [Music] you 