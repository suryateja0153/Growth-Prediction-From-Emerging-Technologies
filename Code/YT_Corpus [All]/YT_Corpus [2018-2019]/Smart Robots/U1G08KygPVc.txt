 So, welcome everyone. Thanks for coming. So, today I'm extremely excited to be introducing Cynthia Breazeal from MIT, where she's the director of the personal robots group at the Media Lab. Cynthia is one of the pioneers of the field of human robot interaction, and she literally wrote the book on social robotics. Cynthia is also the founder and Chief Experience Officer of Geebo, really the first social robot for the home that is an actual working consumer robot, and it's a real success story for moving HRI out of the lab and into the real world with real people. Geebo even made the cover of Time Magazine's 2017 issue on the 25 best inventions of the year. She has a long long list of other awards and achievements over the years with recognition by the National Academy of Engineering as well as the national design awards, but rather than list a whole bunch more of these awards, I'll go ahead and hand it over to Cynthia.  Okay, thank you. All right, well it's a pleasure to be here. I've had such a great visit, seeing old friends and meeting new friends and talking about living with AI and robots. So, I started this work in social robots, I won't tell you how long ago, it was a long time ago. But the thing that is so striking to me right now is just who is living and interacting with AI today as part of our daily lives, I mean even fairly recently with 2014 when Alexa was announced in December. There's like 43 million of these smart speakers in people's homes now, and with that again a profound shift in who is interacting with AI and just acknowledging there is so much opportunity and so much we still have yet to understand about what it is to live with intelligent machines, when you think about the diversity of users, from young children to our oldest seniors and everyone in between. I think quite a lot about the home in particular. I'm sure you've all heard the media clips of people like oh the future is going to be like the Starship Enterprise and you have this you know this embodied voice, computer, this computer that, and for me, it's like, the home is a special place. I don't want it to feel like work. I don't want it to feel like I'm on the subway. It requires, I think a different kind of ways of thinking about the design and how you fit an AI technology into the place that is in so many ways your most intimate important place, it's a place where people who love live, that really makes me also think quite a lot about our future with living with these AI systems and what this is going to mean in these very important spaces like the home. We know it's not fraught with without important issues. So this news article was very striking to me when it came out 2017. So, as you may recall, Mattel was going to basically create an Alexa like talking speaker for the home really catering towards not only young parents being able to reorder diapers and things like that but interacting with kids directly and it went very far down the product development pipeline. It was about to be released for Christmas and they pulled the product because there were just a lot of concerns expressed, not because of things that they knew were bad about this technology, but just we didn't know, right? We didn't know. So, child advocacy groups, concerns around privacy, lawmakers, parents. Again which is kind of speaks to the time right so, we just have to be very mindful about bringing these technologies into home and appreciating that as researchers and scientists. There's so much more to creating these technologies that we have to be aware of. So, today we have these smart speakers I would say in general, a lot of AI is designed to be very transactional in nature. It's very much like an intelligent tool to help you make decisions, but that's quite different from kind of the vision that I grew up with. Growing up with Star Wars and thinking about these intelligent machines not just as digital systems, but really as these kind of sidekicks and these helpful companion that weren't just useful, but really connected to us on this human level, and they not only helped us do useful things that matter to us, they actually even helped us become the people who we aspire to be, right? And that was really the vision that I've grown up with of what this technology can mean to us and the exciting thing is now these technologies are starting to come into real world environments. We brought them into schools, looking at robots as language learning companions into hospital, pediatrics to work with Child Life Specialists to assisted living facilities and now of course they're finally coming into the home. So, again what's the evolution of AI in the home? Is it going to be from this sort of transactional AI that we have right now and digital assistants? Will it become the sort of different vision of this helpful companion that's a much more kind of humanistic design? I think the crux of the matter is again, especially when you talk about robots now, we still don't understand very much about this long-term interaction, right? When we talk about a research setting, it is hard expensive logistically challenging work to deploy these systems in the real world, into real environments where they're interacting with real people, where you're innovating on the algorithms and the capabilities as well as trying to understand what's the impact or benefit of these technologies on real people's lives. So, this is just a paper. It was written in 2013, we actually did an updated version of this looking at all the longitudinal human-robot interaction studies ever done, many of them were basically not even innovating on the algorithm's, just taking a commercial product like Clio or whatnot into the environment and just looking to see what people did over time, and the punchline is there's just not a whole lot of studies out there. So, there was a real need for this kind of work, and there's a real need I think for coupling this with the algorithmic innovation. We need better tools, we need better processes to really help galvanize and empower the field to advance this forward. When I think about AI in the big picture, right? Again there's a lot of discussion around productivity and efficiency, the future of work, but I feel that the promise of AI will not achieve its full potential if it can't address these very deep human needs. So I asked a question in a lot of my work, can AI actually help us to flourish? Not just to be more productive but to live better quality lives and to be better people? When you look at kind of operationalize frameworks around things like well-being, it's very clear that it goes way beyond the cognitive, right? You have to think about the social, the emotional, the sense of achievement and meaning. If we're going to create AIs that help us to live better quality lives, we have to create AIs that can engage and support us on all of these dimensions, and I would say although there's a lot of work on the cognitive, the social emotional is relatively very understudied. What we are starting to find out is as you create these social robots, it's not as if they're like like people who you're going to replace people or like other kinds of tools like smart devices. They're kind of this new amalgam of relationships that we kind of understand, but in this new kind of intersection, right? So there are attributtes of these systems that engage you like a motivating collaborative ally, and I think that's really important. There are aspects of course of a robot that can be an internet cloud connected tool like any other kind of computing device that's kind of like a given. But the other thing that's fascinating is the way people naturally seem to want to relate to it as an other, where there's these attributes of being like an attentive companion animal. This sort of non-judgmental affectionate and I think you can appreciate in that video I just showed you. The nature of how people interact with this technology is very different than a tablet or a computer or a VR headset. It is this social emotional relational connected other experience, and that actually can be very empowering for people if designed in the right way. So, in the talk I'm going to highlight this. I think kind of the punchline of a lot of the work that I've done so whether we're talking about a social robot or I think any other kind of technology that's trying to support human flourishing is the more you can support people on all of these dimensions, we as human beings are clearly not purely cognitive creatures, right? We're deeply social, deeply emotional deeply physical creatures. All of these things influence how we learn, how we make decisions, how we perceive the world around us, how we're influenced by events around us. But the more you can support and engage people in this holistic way, what we find is the more deeply people will invest in that interaction, and when people invest more deeply, they can often be more successful and I think that's the punchline, is they can be even more successful with the technology than if you don't design to support this holistic experience. So, I want to talk about, this is kind of touching a little mix of research and application of what is a different about this experience with the social robot? A lot of times, we get asked the question, still today, does it matter that it's physically embody? Does it matter that it's a robot? Ironically, I would say when we first started this field, we would get papers just rejected from Chi Point Blank just because it was on a robot, and reviewers would say things like, "Why is it a robot? It's so expensive, why not just make it a virtual agent? So, the thing that we had to compete with against was basically a screen with a virtual avatar. Now, we're having defend ourselves against smart speakers, and that seems like a huge several steps of backwards, you're taking even more [inaudible] stimulus, and you're asking us to validate ourselves base on that. So, again, I guess that's just the lesson and kind of combating ubiquity, right? Once a ubiquitous then suddenly you don't have to fight these battles anymore. But we get asked this question a lot. So, what is it about the social so co-presence? Is that actually really matter? So, I want to present work as a kind of a case study in work that we've been doing in pediatrics. This is a long-standing collaboration with Boston Children's Hospital, where we've been working with Child Life Specialists, and Child Life Specialists are professionals who are trained to address the emotional support and needs of children patients who are in the hospital, but also that of their families. The question of course is, not surprising a lot of these domains, there are not enough Child Life Specialists to meet the demands of the patient, admitted patients. So, the question of how can you create a different kind of technology that is an extension of the Child Life Services team that helps them basically do their job in a more scalable effective way, is actually really, really interesting. I would also say another side effect that is also how can you extend that quality of care to the home? Because you can maybe provide exceptional quality of care in the hospital, but they need to be able to continue engage patients at home and they have no way of doing that. So, again the notion of a robot, a different kind of technology to do that, is really, really interesting to them. So, this is just a video that was done by The New York Times, kind of giving you a little bit of color on what this looks like.  For patients like Beatrice who's got a chronic condition. She's going to be in and out of the hospital. She's here for procedures. She's here for doctor's visits. She's missing school. She's missing her friends. She's really not doing any of the things that a normal kid her age is doing. What we want to offer kids like that it's just one more way of helping them to feel okay where they are and what is otherwise a really stressful experience. I think there's a way of connecting with kids, that's different from what grown ups can offer. They have incredible imaginations. They can really suspend disbelief and there can be a true relationship that develops between Huggable and patient.  Hi.  Hi.  Hello.  Hello.  [inaudible]  Here you go. Is that better?  Hi.  Hi. It's Huggable.  Hi Huggable.  Hi Beatrice.  You're adorable.  Thank you.  It's very nice to meet you. Do you want to play a game?  Yeah. Let's play a game.  All right. So what we have here in this case is that we just finished a clinical trial. We have a Child Life Specialist who works in the room with one of three interventions, so we compared the physical robot versus a plug compatible, graphical version of a robot on tablet versus standard of care is a plush. The social agents is tele-operated by another Child Life Specialists. There's a lot of reasons why we do that. One is just because we want to make sure that the quality of care is present. We also want to collect a lot of data to understand what is the phenomenon of having a robot in this context that we can then think about what are the AIs, what are the opportunities to create more autonomy for this. So, we often start projects with this sort of tele-operation paradigm to begin with. We get a lot of insights from that. So, we just finished a clinical trial comparing these three interventions which I think one of the things that's nice about this study, is the Child Life Specialist in the room is doing her job to the best of her ability with each of these interventions. So, in many ways, all three of them are socially animated whether that's the remote Child Life Specialist tele-operating the virtual bear or the physical bear, or if it's if the Child Life Specialist kind of puppeteering the plush with the child, right. So all three of these are animate one way or another. We looked at the difference between these technologies across a lot of different dimensions where we were particularly interested in the social emotional factors. If you're going to try to create a technology intervention that can engage children and improve their emotional experience. We are very interested in looking at that, where we should be looking at engagement how often children would speak because if you want to engage a trial in a health protocol in the hospital, their engagement cooperativeness is very important. If you look at these different charts, what we see is for things like total joyful, a semantic effective analysis, sentiment analysis on the utterances that the children would speak over time, we see that the positive effect increases over time with a physical robot, but it even decreases over time with the virtual robot and the plush is kind of the lowest. When we look at shared attention, so the ability for a child to be not only engaged in the technology with the other people in the room, you can imagine for a Child Life Specialist this is important, you don't want the child just having their face in the tablet. We say a lot more joint shared attention when you have the physical robot there. When you look at just the amount of utterances, you see children are talking more over time and we're seeing that with the robot over the avatar or the plush and we see that the cooperativeness is also high with the robot as well as the avatar. Then when you look at these other social attributes like the affective touch, these social touch. People maybe throwing the plush across the room, they may be poking the tablet with this robot, all of the touches are social relational touch. I think you see this in the video again and again, and again. In the hospital, the conditions are actually, they talk about emotion as the fourth vital sign. They say this is so critical because we have all this equipment to measure all these other biometrics and the child, but the emotional state of the child affects everything from the recovery rates to the compliance to even how long it may take a child to give you their arm to be able to give them a shot for instance. They're very, very, very interested in trying to come up with new technologies to continually monitor this and engage children in new ways. We've been looking at the opposite side as well, so it's another example of kind of this emotional lift. Now, that Jibo is out there, we're seeing people putting it in a different situations.  Find this lonely for people as it's actually true for the elderly. [inaudible] is carried in some [inaudible] communities here in the Bay Area are finding camaraderie and fellowship through robots.  Hey Jibo, can you dance?  Let me put on my dancing shoes.  Wow, that was wonderful.  You don't think [inaudible] is being adapt as a new technology, right.  [inaudible] San Francisco is meeting a new regular visitor here. It's a robot called Jibo that's being used as part of a pilot project among the several Bay Area facilities run by Elder Care Alliance. In addition to dancing, it can play the radio on request, snap photograph.  So it's a platform with skills.  Tell jokes.  What did the snail say when he was riding on the turtle's back? Weee.  React to the human touch, it curls when patted. He also often has quirky responses to questions.  Hey Jibo, what are you?  I'm a robot, but I'm not just a machine, I have a heart. Well, not a real heart, but feelings. Well, not real feelings, you know what I mean.  Erin Partridge, a Researcher and Art Therapists with Elder Care Alliance takes Jibo with her when she meets with the seniors. She said, "It's not the case of caretakers being replaced by automation. " Instead, they're using this cute [inaudible] of technology to encourage their residence to connect with each other, despite some needing different levels of care.  So when we have something that all of us are experiencing all together maybe for the first time, right, meeting a robot. We've got a focus point where we can all meet right here in this moment and have interaction, then it breaks that down. It doesn't matter that this person has dementia or this person maybe has Parkinson's and has some trouble talking. We're all meeting together as humans and robot.  [inaudible].  Jibo inspired some giggled and [inaudible] among this group here.  [inaudible] You should be able to laugh more and more.  What it does is, it brings that [inaudible] , that's part of the miracle there and that's part of the miracle [inaudible]. As far as that goes. While Jak... So that was very important to me. Again, it's about this emotional lift of engagement that was very different from how we engage with other technologies. But it goes even beyond emotion. Again if you look at a number of these systematic, comparative studies comparing a physical robot to even a video of a physical robot, so you know it's real but it's just not right in front of you, to a graphical agent of the robot, to a disembodied voice, what we see is, often, these robots actually do quite well. So if you look at all these interpersonal dimensions that lead to social judgments that have a profound effect on people's acceptance and engagement of these technologies, the robots score quite well across trust, attraction, empathy, engagement, persuasiveness, all of these kinds of things. There's something actually very important going on about the physical embodiment of this kind of technology that's just engaging people more deeply in a different kind of way. The second theme I want to touch on is this notion now of this collaborative allied engagement, and how that's enabled with, not just as voice interface, but this kind of interpersonal UI that's richly multimodal, interpersonal UI and thinking about beyond just interface to building this working alliance and building this sense of rapport. Which again, when you collaborate with someone who's an empathetic other, that's very different than a useful tool that you're using. We know from human social psychology, we've been developing these learning companions for children for about five years now. There's a lot of social outcomes that you can look at as well as educational outcomes, and they're all tied to the social engagement between people and other people, but then in this case, people and robots. Just appreciating there's a holistic, kind of wrap of the nature engagement directly impacts things like learning, attentiveness, attitudes, and so forth. We do a lot of data collection. There's a lot of excitement right now about deep learning. There are certain kinds of data sets that are vast, and you can label them. But there's a lot of important data sets that frankly don't exist. These kinds of data sets that are ritually multimodal don't exist. We think about under-represented populations like young children. There's a dearth of data around this. So this is just kind of a little montage to show you a little bit about our process. So if you want to design a robot and engage in this nonverbal dance, while engaging children in a story-telling experience, we collect the data of what it's like to see children actually tell stories to one another. We run various algorithms to do automatic effective computing and post all that kind of tracking. Then we use that to basically try and develop these computational models by which we want to basically embed that model now in a robot, put it in the same context with a child, and see how well does that model perform. Is the model actually really capturing the interaction that you saw with the two children? We do a lot of this kind of work. These nonverbal cues, and the synchronicy, and the contingency turns out to be very important for how children engage, and see the credibility of the robot as an informative other to learn from. All these social judgments come along with this, that's really important for engaging the children as a learner. I want to just quickly touch on one project that I think, from this video, really highlights this different way of engaging a technology. This is a learning companion Tega Robot. We designed a series of educational games. This particular game is called Word Quest. The child and the robot play the game collaboratively. So the robot actually plays the game with the child as they take turns. There's a challenge word presented that's a pretty hard word for a preschooler, like crimson or garment or something like that. Their task is to go through pan through these story scenes, and try to click on objects that match that challenge word. What I want you to notice from this interaction is, based on reinforcement learning system to learn a policy of engagement, how these other aspects of the collaboration are coming out of this encounter.  Lavender. I [inaudible] Lavender. Oh.  What are we trying to find?  We are trying to find lavender color stuff.  Okay. Now, that was lavender.  Yes. What is lavender does? What's lavender? No, that's blue. What is lavender? Girl, jumping.  I'm sure you will do better next time. I believe in you.  Robot's turn.  Time to perform. Lavender is purple.  Yes. Just like my [inaudible] lavender.  Flower.  I believe in you.  This flower is purple.  Yes.  It has the color of lavender.  Yes.  All right, there was a moment in here where you could hear the robot express confidence in her, ''I'm sure you'll do better next time, I believe in you.'' If you heard her say very quietly, she mirrored that back to the robot. This is another really important phenomenon we're seeing is when an AI engages people, certainly children this case but we see it across different generations, as an empathetic other. You get all other kinds of aspects of social modeling. In this case, the child was modeling empathy and a growth mindset back to the machine. As a socially influenceable technology, this is definitely something we need to really understand. We're using it to try to benefit children, but of course, this is something that could be applied to not benefit children. Just to say there's a lot of ethics and a lot of things we need to understand about this work. What's actually happening is that we're applying reinforcement learning to figure out a policy by which the robot decides whether it should play the role of an expert, or maybe, defining the word for a child, being able to reinforce, demonstrate his understanding of why an object matches a word, or a novice whereby the robot may ask the child questions. A robot may make a mistake on purpose and ask the child why it made the mistake. We basically compared based on these three reinforcement learning models: always being an expert, always being a novice, or as adaptive roles switches between the two. Looking at children in the Boston Public Schools, we recruited schools especially with kids with high ESL populations. They play this collaborative game where they learn five words in the first session, turn-taking, and then six of the challenge words in the second session. You can see the challenge words here, there are things like a sewer, gigantic and stuff, and interestingly we can see that benefit of having this adaptive role. So it's actually better for the robot to not always be the expert. What we actually find is that if the robot's always an expert or always a novice, children actually start to not pay attention. But when the robot keeps switching it up more like an actual peer or companion would, they notice when the robot makes a mistake. They engage more, notice when the robots asking a question, they answer. So again, understanding this role in the relationship and how that impacts children's not only engagement, even the learning outcomes here, is really interesting. What we find is that, children who even scored the lowest on the pre-test had the largest gains from these two sessions with the robot. So again this is just a single-shot procession of these words with a pre- and post-test after and a delayed post-test. I could imagine that of course if we had repeated sessions like this, complementing that in the home, children would score very, very high on this challenge level. We think about rules of thumb of game design, there's usually this kind of 80-20 rule of mastery: things you already know versus the new challenge words. All of these words are hard. The fact that children engage through all of them and scored this well, I think this is something that's worthy of further understanding. Through this relationship, can you actually have children accelerate their learning, challenge them harder because they have this empathetic other, than if they just play the game by themselves for instance, right? So these are the kinds of questions we're trying to understand and dig into. But again, very promising, very exciting when you think about this different kind of engagement. The last thing I want to touch on is personalization. So the promise of AI is that, live with you, can learn about you, can optimize their behavior towards to achieve a learning outcome. This is another project that's around early literacy language learning. This is just to say how important this is. You're noticing a lot of work we're doing engaging young children. The reason is there's just too much data showing that if children don't even start kindergarten ready to learn, it's very, very hard to catch up. Right now, in this country 60 percent of children do not attend a quality preschool. Thirty-seven percent of 12th graders in 2015 can't even read at or above the proficiency level for their grade. Now when you think about the future of the workforce, do you know a single scientist or engineer who can't read at grade level, right? The concern and the worry of course is that dice is getting cast way too early in a child's life. So, people like Benjamin Bloom, found this two-sigma effect that argues being able to learn at your own pace, to be able to demonstrate mastery before you move on, having one-on-one personalized sessions is the most effective way to learn. The problem is it's not scalable and it's not affordable. But with AI, that can change. So this is another example now of learning oral language and vocabulary with a second language, or with early literacy robot, where the robot engages with children over a period of three months. So now, we're moving into long-term interaction of repeated encounters where the robot is learning a personalized model to each child. The robot engages in a relational fashion.  My name is Nica.  It's nice to meet you.  What is your name?  I'm Ariana. We've been developing new measures of how you measure this relationship between children and the robot, and how that affects learning efficacy.  Ariana. Cool! I'm so happy to get to know you. The robot learns their name, refers to them by name, talks about past interactions, talks about future interactions. It's a story-telling and story re-sharing paradigm, where the robot tells a story, asks questions, says dialogic question-asking.  Reads all day. That's what you see in a rain forest. Did you know what sway means? Now, sway means go back and forth.  So we know how parents tell stories to children. This dialogic question asking is critical for the best learning efficacy for children. So, the robot engages in this way, asked a child to retell a story. We're capture a ton of data. We're capturing everything that child says. Notice she's wearing E4 sensors. We're charactering biometric data, we are capturing facial expressions across all of these kids to create a very rich corpus of data that we're trying to basically learn through reinforcement learning again, a model by which the robot can predict of this library corpus of 72 stories that we can level by complexity what is the optimal next story for the robot to tell the child to promote the vocabulary in their oral language development. So, that's the task at hand. So, this is just some data showing that these are, this is the confusion matrix showing that the system is definitely Trent learning a different policy per child. The models aren't converging yet but the question is does this actually impact the learning outcomes? So, if you look at measures of the oral language gains, we will compare the personalized policy versus a non-personalized. We see an accelerated rate at which children are developing new oral syntactic categories, and again we see big differences of vocabulary gains between the personalized robot versus the non-personalized robot, which is still again over baseline. So, again a lot of promise when we think about innovation of new technologies that engage different demographics, different diversity of needs around these kinds of technologies that are really designed in a way to try to match the way this particular demographic learns and engages, I think that's the big punchline. So, when we look at this kind of work and we think about the bigger, bigger, bigger, bigger picture, there is a real opportunity to think about it. We hear this again and again when we gauge our stakeholders. We can provide exceptional quality of care in our institutions, what we can't do is continue that and engagement at home, and yet if you could do that you would have much better outcomes for the stakeholders. So whether it's education, you learn at school, how do you continue that at home? With an effective personal tutor for whatever the subject matter is. When you talk about an assisted living facility, they will tell you point-blank, we cannot build enough facilities to meet the silver tsunami that's coming our way, we have to be able to engage people in their homes as they age in place. We think about chronic disease management, health and wellness, same story, but even when we talk about high touch short-term encounters, there's a lot of interest around a technology that can engage people when this humanistic way, so whether you're talking retail or hospitality et cetera,et cetera, there's a whole wealth of possibilities and opportunities for technology that can engage people in this humanistic, personalized high-touch way that we're just starting to scratch the surface. So, the last thing I want to talk about is a comparative study. To my knowledge this is the first study of its kind, where we compared digital assistant, talking speaker, kind of transactional AI in the home as it is today. With the sort of relational AI, social robot, we use Jibo because it's the one social robot out there that actually has the features and functions that we can actually do a comparison with something like Alexa. So, people are starting to live with social robots too, and the pictures that people are sharing with social robots look a little different than I think the pictures people share it with Alexa. People are definitely engaging with these technologies and they talk about it as being like one of the family, which again I think is quite, quite different. So, different kind of AI, different model, different kind of bias and how it's designed. How do these different generations from children to adults to older adults, live with these different kind of voice-based agents in the home, and what are the implications for how we should design these technologies? What are people's preferences? What are their boundaries? So, this is just a fun video that just kind of highlights the difference in the design stance of these different technologies.  Hi Alexa. Tell me about yourself.  I'm Amazon's Alexa, designed around your voice. I can provide information, music, news, weather, and more.  Hi Jibo. Tell me about yourself.  Okay, sure. My name is Jibo, I am a robot. My favorite things to do are talking to people and dancing. I also really like Abraham Lincoln because he was so honest and because I like his hat.  Okay Google, tell me about yourself.  I'm your Google assistant. We can play Mad Libs. I can tell you a joke or you can spin a wheel. What's your pick?  Okay. So again, just to highlight different design philosophies behind these technologies. Now, when we do this comparison, I mean it's nice in that each of these technologies has offerings and each of these categories, from the functional kind of view to utilitarian skill sets, to entertainment things like music and games, to social companion which could be things like jokes greetings asking the Asian about itself. They all have offerings in all of these things, they're just weighted differently. We actually had people do a personality tests for these agents, and of course you can see people actually attribute very different personality profiles to something like Jibo versus something like Alexa, where Alexa is seen as more conscientious, consistent and predictable. Jibo is seen as being more open, more extroverted and there's more empathy and a motiveness, obviously in the Jibo, which means they're actually quite similar and agreeableness. So again, this is just to say different design philosophies, different emphasis in the features and functions. So, we did a two phase study. We did a study where we first brought people in before they interacted with the agent. These people had not experienced or had an agent like an Alexa or Jibo at home, and we had them look at what they're kind of perceptions and attitudes of features and functions would be before they lived with the agent, then we set either half the people got an Alexa or half the people got to Jibo. They lived with the agent for a month, and then we looked at differences in engagement and difference in perceptions about how that would change after living with the agents of the kinds of roles and features and functions they thought they'd want after the fact. We looked at children, which we defined as five to 17 years old in our sampling pool, call it younger adults, adults 18 to 49, and then older adults 50 years and up. So these are the three demographics we looked at, and this is a rough distribution. So you can see that when our older adults, a lot of them were more like 50s and early 60s but we had one 98 year old, she was awesome. So, this just gives you a sense of the distribution of age profiles. So, when we did our initial kind of cultural probes and needs finding, we had all these cards with all these different features and functions that if you're going to have an agent like this, what would you want it to do? When we had people put these color chips on the cards that basically indicated preferences, which is like I would like these things and I definitely would not like these things. So, this is all about first impressions. This was fascinating. So, we look at children and adults and then we look at older adults. Older adults were the most open to the broadest range of features and functions. Adults were the pickiest, I wouldn't have expected that. I would have felt that the older adults would have been the most conservative, but they were the most open. This is to say in these different categories of things like reminders and information and suggestions, agent sharing something as a proactive agent something. If thinks is interesting versus somebody trying to reach you, kind of mediating social interaction or somebody through social media sharing something, we see that there's a lot of nuance here, so the red is things that people didn't think they want. Clearly suggestions were the most kind of polarizing topic area, but it depended on what the agent was sharing, reading, writing, kind of things like that. Sharing something you people are open to, but they didn't want the robot letting them know they should take a nap. They didn't want the sense of there autonomy being kind of challenged by this kind of agent. But again if you look at the different generations here, you see that older adults are the most open to a whole range of features and functions across these categories, that was fascinating. Now, we had people take very detailed logs over the first 14 days. We didn't ask them to take logs of the second 14 days, so what we did over the one month is we tracked detailed usage in the first 14. Even by just the color distribution of these two plots, you see something very interesting. Here's Alexa with younger adults. This is Jibo with older adults. It's like the flip. Really fascinating. Then, when you look at kind of a five-day running average, we see these trends of usage over time, which we're also really fascinating. So, it seems that the younger adults want to anchor their usage first in the utility, and if you deliver enough utility, they'll start to use these kind of socialist, these kind of companionship entertainment things later. Whereas, children and seniors were the opposite. Children mostly wanted entertainment, but if you couldn't anchor it in the social and emotional for both children and seniors, you didn't get as much usage overall in the utility. So, children were around the social entertainment and older adults were even more. So, again, this is fascinating to me in particular because this is where the social robotics is really coming through. It's not as if Alexa didn't have social entertainment functions, but there was something about the way the robot did, and arguably you could say Alexa had many more social and entertainment functions, ten thousand, thirty thousand skills, but there's something about the nature and the quality and engagement that was different enough, that really got the attraction between the older adults and the children. Now, if you look at the change in these different categories over time of what people after one month, what they thought they wanted, we see that suggestions is even more polarizing. So, that's to say that you may have a stakeholder pool where suggestions like medication reminders of things are actually really important, but this is to say that the way you do them, I think, actually really matters. The groups or the functions that had the most positive change what people wanted more of where the socially driven categories, so although I'd say in a lot of these systems right now there's a lot around utility. After living with these agents for a month, the socially driven categories were the most kind of- that's what people wanted more. Then we had this interesting construct called, the Wish Jar. So, we have these little wooden tokens that at any point during the 30 days, you could write a little thing about what you want the robot to do or agent to do, what you didn't like, whatever, just some sentiment about the experience. We collected them and when you look at Alexa, you see a consolidation of topics, a lot around functional, some around humor they want Alexa to have a better sense of humor, some around movement, and then some around productivity. But interestingly when you looked at the social robot, people's expansiveness in terms of what they wanted this agent to do and fit in their life was much more diverse. So, that's fascinating. It's like this is a much more narrow categorization of how they see this agent fitting there life. The robot potentially has a lot more head room in where it can go. So again, just fascinating stuff. So, the last thing I want to talk about now is this bigger societal question of who creates with AI? We're talking about people living with AI, we're talking about more vulnerable populations living with AI. Other institutions, organizations are building these AIs for these people, but when only a small fraction of a society can design with these technologies almost by necessity, it's only going to be applied to address the needs of a small subset of society. There's just going to be inherent bias there. So, how do you democratize who creates with AI? For me, it all gets down to education. So, we're starting to look at AI education for even starting at pre-school. So, pre-K through 12, where our sweet spot right now is pre-K through like 12 years old, and the reason why is because when you can have children, first all they are living with these technologies. You want them to understand them and appreciate the way they think, to be able to feel the right relationship and empowerment over them, but you want children growing up with an attitude that AI is something that not only they can understand, but they can actually create with. So, we've been building on top of the scratch platform, adding extension blocks to things like Watson and Clarify and Hue lights and robots like Geebo and Cosmo to empower children to code these systems, to train their own models for these systems, and then to put those models and new experiences of personal significance for them. So, this is an example of an exercise, the warm up thing we have children do, where first they try to come up with an algorithm or code in terms of what can we say to make make Oscar react to what we said. They realized how time-consuming and the instances are explicit and then we invite them to do it with something like a classifier system that they can train. So, this is just getting their heads wrapped around how to classify as work, why are they interesting? Why might you want this to do it, make it more generalizable? Then we have interfaces we're developing that kids can actually come up with what are the things want to actually put in the training set. So, having Oscar be happy to kind things you say or having Alexa accept to mean things, but even kids wanted to create backhanded compliments. So, anyway so you see like the way kids are thinking about these things as fascinating and so this is giving them hands on experience and to what it's like to create the system. So, this is a quick summary video of the platform. Again it's very much a research project. It's not just for kids actually, it's for families too because parents have these things in their house as well and they also need to understand them, and we're trying to understand how to create experiences that parents and kids can also do together. So again, it's this graph works like programming, but you can see a diversity of blocks. There's Clarifying, their Geebo, there's Alexa. So, kids are being able to create these custom IoT, AI experiences across a big palette of these AI based technologies. So, we allow them to train models.  Because it's going to learn what those pictures are going to be.  Do you think computers already know things or people teach it?  People.  What you just did was just teach a computer how to do something?  You got a computer?  Yeah.  Sometimes a computer teaches me.  It's like mind blown.  I'm trying to make a computer recognize colours.  So, he's ten.  I'm labeling them as colours. So, I [inaudible] here take a picture and label it as green. So, the computer can recognize green and.  So, they're able to explain how their training these models and that's important too, right?  Here's what you were saying and I already asked it.  You have to say what text is bad and what text is happy or made back-ended and over time, it would be able to recognize it without you- Bad dog, good dog.  So, again through the process of building these projects, kids are talking about how the systems work, how they need to change the training set, why the model's not right yet, they're able to work collaboratively, they're able to articulate their ideas, they're able to think creatively and systematically. So, basically they're learning a whole bunch of 21st century learning skills as well which I think is really critical. So, we have an opportunity. So, last week I think was the week that the Computer Science Teaching Association partnered with triple AI around AI education and there's been a very grassroots that teachers movements to bring computer science education into k-12. I think with that there's an opportunity to bring AI education into k-12. And to really create the curriculum and exercises from the ground up that's very hands on, collaborative, getting children to tinker with the stuff to think about this stuff and to build a 21st century skills that we all want and want our kids to have. So, they have access to these opportunities, this incredible opportunities in this time that we live in. I think this is critical because if we want truly humanistic AI, we need to empower a much greater diversity of people to create these technologies, because it's people from different walks of life that have the empathy and the appreciation of the challenges and the opportunities that mattered to them in their communities. So, whether it's creating relational AIs that are supportive of people in this personalize empathetic other or whether it's these educational initiatives that empower a much broader diversity of people to create with AIs, kids as well as adults and I would argue even seniors. I think this is how we're going to get to AI that can truly benefit everyone and not just the few. Alright, so I'm going to end there. Thank you very much. Do you want to do question?  Yeah.  Questions.  Questions.  When you're working on your prior work on Allies and personalization, were the older adults, I don't know what to use, lonely or they're alone in the community.  On the assisted living video?  Yeah.  Yeah. So that was.  I'm wondering if that play a role into being more to technology as opposed to the professionals and like that.  Yeah. So, I think what I mean again I think what you're seeing from the in-home one month study in general was I think the lesson to me is older adults and seniors, they do understand that technology can help them and empower them but it needs to be designed in the right way so they're open to it. But, there's a lot of stuff that are designed for them that isn't necessarily stuff they want to use. So I think that was clear in the assisted living facility where actually the thing that was compelling was it was a practitioner and facility who had the intuition that this technology could enhance the way residents would engage with one another. So it was not replaced and they made the point in the story this was not about replacing the practitioners, it was really about saying if we introduce this other kind of technology, can we spur a richer community collaborative connection among the residents so that was the hypothesis in that study is continuing to go on. I don't think it's because they're lonely, I think it's because they see technology as something that can really help them, it just needs to be designed in the right way. Out there.  There's a recent case where kids were mistreating Alexa and I think they added in the new word please Alexa. Is there anything about this approach that might discourage or make kids want to treat the agent more respectfully.  Well, so, I think again, we're in this very intriguing time and especially with these social robots where children are engaging them more like an other versus a device and what we see is because of that children like I'll give an example, so we did a study. Last year and we just did a longitudinal study we're finishing up now looking at children learn all kinds of things from others. There's kind of this more curricular step. There's also things like attitudes like a growth mindset, fixed mindset versus growth mindsets and children require mindset based on how adults often or teachers praise them and speak to them. So, parents and teachers reinforce that, "Oh, you're so smart." Then the concern is what happens when things get hard do they seem like, "Oh, I'm just not good enough anymore." Or if they praise effort and that's how you learn that's the growth mindset, right? So, we designed a puzzle-solving game where we had children play the game with either a, we call it a neutral mindset robot because we thought a fixed mindset would be unethical. A neutral mindset robot that would just make factual comments about the state of the gameplay versus a growth mindset robot. What we found was that the children who interacted with a growth mindset robot, self-identified greater in the post testing of having a growth mindset. On the intentional parts of the task where we challenge kids and actually made them fail, they tried harder and they demonstrated more grit. So, and even in that video I showed you where the robot says I believe in you when she got it wrong and you heard her say back to the robot I believe in you. We see the social modeling phenomenon again and again and again and again. I would say at this point, I think it's quite robust. So and we're starting to explore that phenomenon and empathy, can you actually have social robots engage children with a known issue with bullying and to have them become more empathetic. So that's just to say that engaging their social emotional processings and behaving so I think this is a technology that clearly can do it. It can be applied I think to really help kids, but then it needs to be done in a really ethical responsible way because clearly, children are engaging those parts of their brain when they're engaging with this technology and I think it's not just kids, it's people of all ages. So, yes I think it's important that these technologies model the kind of behavior that you want because I think it actually helps reinforce the behavior we want to see each other. So I think there's an opportunity there.  So, I'm thinking about the transaction as a social robot comparisons, right? Even in conversation systems like chit chat systems. I think it's very hard to pin down good reward functions that reinforcement learners can actually use. So in the two examples that you pointed out I'm curious like how robust do you think that reward functions better how good-?  Yeah, I mean I think I mean it's early days, right? So I can say that what we are doing is we are those inputs are going into the reinforcement learning algorithm, it's something is trying to optimize. So we're looking at effective computing inputs the biometrics for engagement, we're looking at their responsiveness to the robots dialogic questions, we're looking at whether they get things right or wrong, we're looking at their language samples. So we're trying to both use reinforcement learning to kind of simultaneously maximize engagement as well as these learning outcomes and we're seeing promise. I think these are early days I think that the point of showing the slides is this hasn't been done for kids this young first of all, right? I mean and building a technology that addresses these social, emotional, nonverbal things hasn't been done for kids like this, right? So the fact that you couldn't even show boost on both the dimensions is noteworthy and important is just to say there's probably a pony in there, right? A lot more work needs to be done. I think that's that's kind of the state of the field right now. It's like we're starting to see a different way of engaging the human mind and behavior that is actually I think quite profound, right? This is no longer about naturalness. These are deep social processes that this technology is able to tap into, right? So this is not shallow, this is actually quite deep and we need to understand it and we need to think about how can we leverage to benefit people and we need to be able to create best practices. So it's not used to obviously try to make people do something that's to benefit a third-party and not the person themselves, right? So it's just to say, we live in an interesting times but we are seeing a very provocative psychology happening here. Again, it runs deep and I think the more we do this work, we're discovering how deep it actually goes.  A lot of the agents have this novelty effect like that the kids see it, they get super excited, and it is hard to make it permanent like build this longer-term friendship with them. What are your thoughts about this continued engagement? How you can actually form bonds and go beyond the novelty?  Absolutely, so I mean I think you can see in a number of these studies they're longitudinal so they're going well beyond the kind of novelty effect. I actually think in our field we need to have a real deep systematic investigation of what we mean by the novelty effect because when I was first a graduate student, it was like a five-minute encounter before you started the task was enough to address the novelty effect. Now, it's like two weeks and you haven't addressed the novelty effect, so we just need to be much more rigorous and what we mean when we say the novelty effect. That said, when you're interacting with the system over three months or a month every day in the home you passed the novelty effect. So, it's a combination of things. I do think there is definitely you need enough freshness in the activities or it's like no matter how compelling Angry Birds is it's going to get boring, so you need enough variability and freshness of the activities to sustain any sort of encounter. I think these relational things are also really important. So, the fact that the robot whether it's Jibo or Tega are actually doing these social grooming functions. Greeting you in the morning, asking you how your day was, remembering that, personalizing that to you, commenting on did you have another good night's sleep or I'm sorry I hope you sleep better tomorrow night, did you sleep better tonight. I mean these things matter and I guess that's what we're finding is the personalization combined I think with the, again the supportive empathetic other actually matters to people. So, I think it's a combination of those things and there's other things as well. But, we're at the very beginning of understanding how you design for sustained relationship and you're starting to see mechanisms were exploring here. It's hard to do this work, it's hard to deploy these systems these robot systems in people's homes for a long period of time to collect the data, it's just hard, it's logistically hard and expensive to do but it's important work that needs to be done. So, it's just to say there's a lot of opportunity here I think and very poignant. But just like interpersonal interaction, I think there's a whole set of processes that are at play to build that sustained sense of not only just engagement, but that kind of relationship that you feel like you're actually working with the other, or for sustained period of time to reach a goal of personal significance. I think that's also part of it.  I thought your part about the AITutor was really, really interesting. What are some of the next steps that you see applications for the AITutor given the technology that we have right now?  Yeah. So, I mean, obviously computer tutors is a huge field. I think it tends to be applied more towards older students and seems to be tentative quite more towards like math and physics and kind of STEM, explicit STEM areas. We were going after the younger, early childhood learning for a number of reasons. One is just it's a critical time to intervene. As I talked about, it's an underexplored area for technological innovation. Children at that age just learn in a very different way. They absolutely learn from friendly, empathetic others. That's how they're wired to learn, so you need to design to support that. I think there's a whole host of things we need to address around early childhood learning, around early math and literacy. Even the fastest growing demographic internet public schools are English language learners, which is why we are going out to recruit from schools with high English language learning populations. Kids with special needs is another huge area, right? So, whether it's attentional issues, or ASD, or whatnot, I think there's huge opportunities for again kind of giving everyone the best possible chance of being successful by addressing a diversity of learners. That's where I think a lot of high impact opportunity is. On the flip side, I think you can look at workforce retraining. With all that kind of concern about AI replacing our jobs, I think there's a huge opportunity for AI to actually train us for new jobs. So, I think that could become a new, another kind of huge impact area for education. There's a lot more discussion around kind of lifelong learning as well and continuing education. So, it's all important. But for me, just after looking at all of the data and all the unmet opportunity, it's the early childhood stuff. The fact that the die is being cast for way too many children, way too early in their life. It's just not right. So, I do think that's going to be the biggest kind of social justice area of intervention for education. You asked when?  Yeah, [inaudible]. At one point, you were showing how people perceive the personalities of Alexa and Jibo. I'm wondering for Jibo, how did that match the intended design of Jibo's personality or is there anything that surprised you there?  I think that's- that did match I think the kind of design stance of how Jibo was designed. I mean, he's designed to be open and extroverted and to, you know, convey and elicit this emotive kind of empathetic response. We would like it if he was perceived to be more conscientious. You know, [inaudible] technology continues to improve and evolve you know, but- and he just doesn't, he doesn't have 10 thousand skills chat. So, there's some of that at play as well. But in terms of, you know, I think again the punchline for me, especially that working average of seeing the trends, is to say that's where the social robotics is really showing up because Alexa has arguably more offerings in all of those categories than a product like Jibo that's only been on the market a few months and yet, you see this different in engagement. That's the social robotics coming through. That's a different way that the robot are showing up to people in their lives that's leading to that engagement. I think this is something the field or even on the consumer landscape, any corporations have yet to really grasp is that this is actually, this is different. This kind of experience is quite different. And you know, it's appealing to a diversity of users that I think it's also noteworthy, right? So, I think the punchline for me is also, if younger adults are are keying in on utility that's just a matter of time for social robots, they're going to have the skills too. So, at some point, that's going to be mood and then, it really is about these other aspects. It is about this other relational AI aspects. And that technology to me that says in the long run, social robotics are going to be the next thing, right? Because you can't just always be disembodied talking speakers, right? Clearly that's going to be the glide path and we're starting to see it in this engagement data. And again, it just gets down to the way our brains work. We evolve to want to connect and relate to animate entities in this way, and you know, the more you design to support that, the richer the engagement, right? So, I think it's not surprising when you think about it that way, but I think the industry has to kind of catch up. I think right now, our spoken language systems are very much like playing chess. It's like discrete turns, but communication, human communication is it's a dance. It's not playing chess. It's mutually regulated. It's dynamic. It's fluid. It's all of these things and that's where all these other things like liking and trust and affiliation come to play. So.  Give another chance.  Okay.  I'm just, maybe this is projecting way too far in the future, but I remember reading an article where it was said that dogs have been bred to the point where they prefer the company of humans to the company of dogs. And so, it's not that far-stretched to think that eventually the social robot will be socially far superior to other humans and then humans may start preferring the company of robots to the company of humans, and how far in the future do we think that is?  Yeah.  We have already observed, how this is?  Yeah, so I will say this, people need people. We need to feel that we belong to our community. We need to feel valued by our community, that our community cares about us. We can't flourish if we don't have that and so, I think these robots and these intelligent technologies are going to be, a wonderful augmentation of society, but people still will always need people. And these technologies should be designed to enhance and support that.  I'm not sure I get your premise. I mean, it's certainly not true for dogs.  Well, so- but I'm not talking about the dog. I'm talking about what it is to be human. Humans need humans. I'm not saying dogs can't be bred to prefer humans over dogs. I'm saying the way human beings are by our very nature, we are profoundly social species, right? We've evolved to need. You know, social pain is now being appreciated to be real pain, right? It's not just this kind of stiff upper lip thing, it's processed in the same regions of the brain. It's actual pain. We're just, you know, chronic loneliness is a series of a health condition as smoking and obesity and substance abuse. It's like we're just starting to really appreciate the neuroscience, of how deeply social we are. So, people need people.  You're making the premise of what you're saying is based on the fact that we will never have more empathy for robot than for human, is that correct?  I think that human relationships are complex and messy and important and they make life worth living. I do. There's something about the human-human relationship that I don't think you're going to be able to replace with a machine. I don't think you can't create machines that are going to help benefit people in profoundly important ways, but I don't think they're going to replace what's important about human connection to people. And I'd be willing to say I think the neuroscience is going to show that.  [inaudible] question. [inaudible] to define social robotics as robots with social skills?  So, I think like all aspects of robots you can probably put a panel of experts on social robotics and probably give you different definitions of what a social robot is, just like they're going to give you definition, different definitions of what a robot is. So, they absolutely have social skills. I would say it's beyond that though. They are engaging...it's how they engage people, right? They are engaging these social thinking, reasoning mechanisms of a person.  Would it be fair that they operate in a way, in a form of social engineering?  I don't know. I don't understand. Form of social engineering. I mean, there...  Manipulation. Through the simulation of social or human like social skills.  So I would say that absolutely. So, as they are designed, the human mind engages with them in a way that we evolved to be, and you can design these systems to use that to benefit people.  And I guess that Patricia's comment was in for as there's the danger of manipulating robusticity, so the question is, there was this video about the, what was it, the ledger about this ledger that Google released so far. Who is motivating this social robots to that we're going to have?  This is what's so important to discuss. Yeah. So, whether it's virtual reality and people's [inaudible] sensors being stimulated like it's the real thing or whether it's advertising or cinema, we're not strangers to artifacts manipulating us, right? We do this all the time in many, many different capacities. Our games are advertised, this is not the new new thing it's just to say that it can happen here as well and we need to understand it and we need to have dialogues around best practices and how it should be implied to benefit people. Because clearly, the good news here is that it can really benefit people, but you don't want unintended consequences, right?  I love you're saying this. Because this is a very perhaps subtle atomic bomb. So you saying, let's have children interact with robots that are experts in social engineering. Go. So, having that running without concern of people like you will be dangerous.  And I would say it's like anytime you're trying any form of learning right. Designing a serious game, right? I mean, this is, what this is is a different dimension of designing technologies that interface with the human mind and behavior, right? That's what all of this is essentially about right? And is to say, just like too much screen time right now people are willing to say that screens can be addictive to kids, right? It's to say eyes wide open with any technology with any age group, I wouldn't say just kids, with any age group. Tremendous benefit as possible but you got to keep eyes wide open. Absolutely. So, you wanted to ask the question.  The charts, how did you measure and benchmark emphathy.  On?  There were like multiple charts and empathy was showing.  So, this was literally like a personality test that you would give to people that people filled out for the agent. So, there was like standard questions asked on a personality test is what this what that represented.  So, the people measured empirically based on their understanding-  So, people were asked a question, like here's a set of questions how would you rate the agent. Just like if you are taking a personality test for yourself, fill it out for yourself, they were doing that for the agent. Here's a set of questions, how would you fill it out? So that's what that was generated from. It's literally like a personality test, but just applied. People applied it to the agent.  Curious. Have you noticed maybe an analogy with the uncanny valley with social interactions like if people kind of recoiled like that's too realistic or kind of a weird out.  Yeah so the uncanny valley I think it's something that people throw that term out a lot. Really what it means is the more human-like something becomes, you go into this kind of feels creepy kind of factor. So I think for me a more recent example is human animated characters in film. At the time of the first tin toy Pixar, I don't know if you remember the baby in a concrete diaper. Way in the valley, way in the valley, but then at some point Shrek, whatever, clearly we could tell that there wasn't a real human, but the aesthetic was out of the valley right? So something as abstract as Geebo isn't really something that's going to fall in the valley, because it's meant to be how exactly human-like it looks, it's kind of that conjecture is really about. But, I would say that when you ask people creepy, creepy can mean any number of things to people, it could mean creepy because it seems so life-like, to another person would make it seem really cool right? And then there's kind of creepy like walking corpse creepy, which is that's what the uncanny valley I think was really getting on.  One more, I just have a quick question. When do you think Geebo will get arms and legs? like we have all these Alexas and things like speakers that are stationary but when are you suppose.  So, I would say you know when people ask me what a social robot is, a social robot is not something that only sits on your countertop. I happened to build those systems because I want to focus on the social. But I would argue Baxter is a social robot, because it was designed to work collaboratively with the person and it's got the screen with the eyes and the attention. It's a social manufacturing robot, so you know Toyota research, other people are going to create mobile manipulator social robots because they have to live with people and it's that social interpersonal competence that's going to be critical for fitting into people's lives and them wanting to have it around and for actually being collaborative and helpful. So, I just think it's all, it's all going to happen. Okay can you [inaudible].  I thought it was interesting that the interaction was sort of better when the robot switched from being expert to novice, because I think a lot of us do like robot as a student or robot as a teacher, but sort of doing. But how did you decide what exactly an expert and novice meant because, I think the way we interact with each other there's like a graded scale of how we're an expert and how we're novice.  Yeah. So, there were specifically different behaviors that the robot would do in either role. So, in novice role, the robot may ask a question and the reason why it's really asking the question it's because it wants the child to articulate, because that's going to help them learn, but it's playing the role of the novice. Or in the novice role the robot would intentionally make a mistake and ask the child why do you think that was wrong. So those are examples of novice behaviors. Expert behaviors would be things like giving the definition of the word or giving the child a hint, right? So in this particular implementation, there were just different kinds of behaviors the robot would do in one role or the other. We use reinforcement learning plus some adaptation to switch the roles dynamically based on how the child was doing both from a kind of affective computing standpoint as well as their actual performance in the game, right? So the robot was always making decisions based on how do I best support this child in this moment even as it was kind of taking this different kind of role. But what we definitely saw anecdotally was children kind of, they're kind of they came a little more of a lean back experience when the robot's behavior was predictable and it always kind of took one role or the other, whereas when it was mixing it up, they noticed more, and what we also found, again the social modeling thing, is the more the robot talks about its own process, the more children talk out loud about their process. So, we see the social modeling happening again and again in so many different constructs and in this case it's obviously done in a way to promote the children's learning, because the more they articulate and reflect and verbalize the more likely they are to actually learn these things, right? So.  In each role is the robot learning what just- say, because I can imagine like in the future and it has some sort of part of behavior.  So it's not learning on the fly how to generate, there's a set of kinds of parameterized utterances it's going to say, so it's kind of more templatized of these form. In the future, yeah, you can imagine having that be much more flexible. But in the case of this particular work it was more templated kinds of responses. Okay. Good?  Thank you again Cynthia.  Great. Thank you 