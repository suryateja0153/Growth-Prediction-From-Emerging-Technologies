 [Music] we're here to talk to you today about designing AR applications so Google has a long history in designing for AR we've been doing mobile AR for the past four years and working on other augmented reality projects before that which changed most recently is that mobile AR is really starting to take off and Google's AR core makes it possible for anybody to create quality AR content so I'll share some fun facts about AR core the first thing is that we released 1.0 at the end of February and that made a our content available to more than 100 million devices and we're already seeing rapid growth with more than 300 apps already available in the Google Play Store so you're here because you want to learn how to design for AR and what we found is that once you understand your users and the type of experience you're trying to create design principles for getting started fall into five different categories which we call the pillars of AR design first you want to understand your users environment where will the users be experiencing your app think about the surfaces that are available and how your app can adapt to different environmental constraints then you want to consider the users movement and how much this space the user will need in order to experience your app and when it comes to initialization and onboarding you want to make the onboarding process as clear as possible so that users understand exactly what to do in this entirely new medium when it comes to object interactions design natural object interactions that convey the affordances and also the feedback so that users understand how these digital objects fit in the context of your real physical space and when you're thinking about user interfaces balance on screen UI with volumetric energy interface design so that you're able to create an experience that is meaningful and usable for your users so we asked some examples that showcase the different guidelines within each of these pillars and the thing that we want to point out is that this framework can help anybody get started with a our content creation so throughout our talk we're going to show you some different demos that you'll be able to play with very soon through an app we're launching on the Google Play Store called a our core elements and many of the core interaction patterns you'll see in this talk are already available in scene form or will be available for unity later this summer all right so let's start out talking about the users environment the the first pillar of a our design so Eric where's the relatively new technology so just to begin let's talk about what eric worre can actually do so it does lots of things the first thing that everyone's familiar with is surface plane detection where I can understand surfaces tables floors those types of things it can also do walls vertical surfaces and then of course what's what's better than horizontal and vertical surfaces it can also do ankle surfaces with oriented points being able to place an object on any any angle Eric or does light estimation this is really important for having objects sort of look realistic in a scene and there's also some other fun things you can do with that that we'll get into and announced just that IO yesterday cloud anchors which is available now for Eric or both on Android and also on iOS plus we do multiplayer experiences an AR with two people viewing the same thing and also announced yesterday augmented images the ability to recognize an image but then also not just recognize it but use that image to get 3d posed data off of it so you know where it is in space so that's the the current set of AR core capabilities and this is of course growing over time and what we found is the more your app integrates with users environment really the more magical your app is going to feel so let's look through a few examples here first for a surface plane detection a lot of AR apps currently only use one surface you know say you're playing a game you know game board will appear on a surface what we found is I mean there's really no reason that you have to use one surface you could use all the detected surfaces and at moments when you have your game interacting with multiple surfaces those can be really kind of like breakout moments in your game where it feels very magical even something like if say you're playing a physics-based game and you destroy your opponents you know castle or something and the bricks fall onto the floor at that moment when you see the items on the floor it can be really quite stunning all right light estimation this is critical for making objects look realistic in the scene here's an example that we are working on sort of test out some different techniques there we have three fake plants on one real plant although I think it's too actually a real fake plant and we have unlit which is the most basic lighting you can do really not very realistic than dynamic and a combination of dynamic and baked and what it's great with dynamic is you get real-time shadows and then when you combine dynamic with baked you start to see things like self shadowing where the the leaves of the plant are actually a little bit darker and it's picking that up they can sort of see example of house looks with some movement and lighting is there's definitely a lot of a lot of innovation that's going to be occurring in this space as we try to get more and more realistic lighting you can see where we are right now and especially as like as the scene gets darker you start to see that the unlit object just doesn't perform as well so it's really important that you're using the the real-time lighting api's that are in Erik or the other thing you can do is you can actually use lighting as a trigger to change something so here in this example when you turn the light switch off in the room the city actually glows and it responds to that change and these types of moments can really feel great and magical for users where you imagine you're playing you know sort of a city simulation game and it's having these kind of you know significant meaningful changes based off the environmental light all right oriented points this is actually a very new feature so we don't have a whole lot of examples here but here's one of the more basic ones I filmed this when I was when I was out skiing and here I'm just attaching androids to the side of a tree and you can see that you know as I as I placed them they stick to exactly that point on the tree at the the angle of where the branches were like cloud anchors announced yesterday here's an example of that being used I here both players see the same game board in exactly the same place and they can play that game together and this is really tremendously fun you can actually try it out in the sandbox if you want to stop by later today and again this works with both Android and iOS alright augmented images there's a lot of different ways you can use this one demo that we have in the sandbox you can go check out is actually an art exhibit that was built using augmented images [Music] so we're really excited about what you can do with talking about damages and there's really lots of possibilities from artwork to even something like you know just having a toy sort of come to life you know the surface of a product box or you can you know see sort of 3d models of what you're about to play with alright so now that we've got over some sort of you know basics on the core capabilities of a our core let's talk about how you'd actually start to design an app for a are so one of the first things you're thinking is okay where do I actually start you know blank page and ready to start having you know brainstorming and then yeah new ideas for AR and one of the things that want you to first focus on is air exists outside of the phone so your design work should really exist outside of the phone as well so something I found if a lot of people have done you know tremendous amounts of mobile design as they they tend to be very attached to sort of the phone frame and sort of flows of screens and you know they've been doing that for for so long that one of the first things you need to do when you're starting to think about AR is to actually you know put away all of those you know to D UI stencils and don't really think about the phone at all instead what you want to do is you want to sketch the actual environment that the users in so you should be sketching living rooms and and tables and outdoor spaces and then as you sketch the users environment then you start to sketch you know the various AR objects that they're going to be interacting with in that environment in many ways you can sort of think of AR as having a lot of the same challenges as a responsive design for the web in terms of different window sizes but it's even more complicated because now you have responsive design for 3d spaces that are you know the users actual living room so you want to sketch the user for scale to get a sense of how you're going to start crafting this experience the user could be very large relative to the air objects are very small and then you want to start thinking about how that user is going to move around in that environment and that brings us to user movement so now that we understand how to design for the environment let's think about how to design for user movement and as Alex mentioned it's completely okay to design beyond the bounds of the screen and what we found is that in many ways this can make the experience feel more delightful and even more immersive because when you have an object that begins on-screen and also extends beyond the boundaries of the phone to support it can make the user feel like the object is really there and beyond that it can also motivate a user to organically is being moving the phone around their environment so that they can appreciate the full scale of these digital objects in their physical space and that brings us to our next observation because users are more familiar with 2d mobile applications that don't typically require user movement as a form of interaction it can be very challenging to help convey to users that they're able to move around so many of users don't because it just doesn't feel natural based on how we've used to do in the past but what we realized is that characters animations or objects that convey visual interest on screen and then move off screen can be a natural way to motivate users to move so here we have a bird and it appears in the middle of the screen and when it flies off screen it's replaced with a marker that moves around and slides along the edge to help users understand the bird's location in relation to the user another major thing that you want to think about is that whenever you have an experience that requires a user to move you also want to think about how much space a user needs so we found that experiences fall into three different sizes there's table scale there's room scale and there's also world skill and when it comes to table skill what we found is I your experience is able to scale to the smallest of surfaces so that many maybe many users are able to enjoy your experience and with room skill it expands the impact of a are so that content will start to feel life-sized and you're able to do a lot more with the space that's available and world scale has no limits it allows users to aaarr in whatever area they see fit and this is an area we're particularly excited about because what it means for procedurally generated content in world scale so no matter what size your experience ends up being just remember to set the users right set the right expectation for users so they have an understanding of how much space they will need because it can be a very frustrating part of the experience if the user is playing a game and in the middle of the game they realize they don't have enough space to enjoy it and when it comes to how much how much movement your experience requires there's no one-size-fits-all solution it really depends on the experience that you're trying to create for example if you have a game that requires user movement as a core part of interaction that can be a very delightful experience you can use proximity or distance to trigger different actions so that as a user gets closer to the sprog it can leap behind the mushroom or maybe the mushroom can disappear and that can be really cool to see however if you have a utility app where the core purpose of the app is to help users understand very complex data and information then requiring users to move might be a really bad experience because what it means is that users who have different movement or environment limitations won't be able to get the complete app experience so allowing users to manipulate the object to rotate it to move it around in a space that's more appropriate will ensure that all users have easy access to the data that they seek because there's relatively new the actual process for users to flow from 2d parts of your app into 3d can be at times a bit awkward amber is just starting to create some sort of standards around that so we'll talk about initializing into error one of the first things you can do is you can leverage a standard view na are a material icon so users when they see that they they know that when they hit this icon they're going to be going into AR you can use this and sort of you know all the normal places that icons appear like a floating action button or on top of cards as the indicator that you can actually view this object in 3d in the in your environment one of the next things you'll see if you've been playing with lots of AR apps is something that you might not initially understand I want to talk about the concept of how understanding depths actually requires some movement so you'll see these types of animations where as trying to get the user to move their phone around so why is that actually happening basically we you know we perceive depth because we have two eyes but we actually get a lot of our depth information by actually sort of moving our head around and being in the scene and for the case of a our core current most current phones on the market only have a single camera on the back so the device only has one eye and if it hasn't moved yet it doesn't necessarily know what's going on so this is the first thing the phone sees it's gonna say alright well that's that's interesting but I don't totally have a sense of you know where these objects are yet and once you just move just a little bit then it becomes clear i as soon as you bring in a little bit of movement then you have an ups of that data on different angles onto the scene that it can start to build up a model of what it's seeing so that's why we have these animations at the start of the app to try to get that movement to try to get aircore to have enough information to recognize the scene next thing you want to think about is deciding if users are able to easily move the objects after they've been placed or if these are really more permanent objects and there's again like no right answer here so you know more persistent object might be like a game board or something that itself takes input but we want to recommend that you use these sort of standard icons to set expectations for users they know as they're placing that object if that object is going to move around later on as they swipe on it so some examples of that let's say you're placing like a city game and here you know as you're swiping on the city you're actually going to be interacting with the game itself so we recommend using an anchor icon for these more sort of persistent object placements and you still want to enable the user to move the you know game board later perhaps through a menu screen or some type of reentering flow but to set expectations ahead of time that this city actually is going to be sort of stuck to the ground there for a while as you interact with the game versus you know something like say you're shopping for furniture and you're just placing a chair in the scene here the chair itself isn't interactive so you can actually map swipe gestures onto the chair and just easily move it around so using the plus icon to kind of set expectations ahead of time that you know you're not really committing to exactly where your place in this object alrights no we're talking about object interactions there's actually quite a bit of details there now that we understand how to onboard users let's start thinking about how users can interact with objects in their space one of the things that we challenge you to think about as designers and developers in the community is thinking about how to solve problem solve for user behavior even when it's unintentional so one of the things that we recommend is giving users feedback on object collisions and this solves a huge problem that we see in mobile AR where a user will be moving the device around and once the device collides with an object in AR that object might disappear the user has no feedback in terms of how to fix it so what we recommend is we recommend providing feedback in the form of camera filters or special effects that helps users understand when object collision is not an intended interaction and this tends to work really well the other thing that you want to think about is how to give users the right type of feedback on object placement and it's really important in this case to think of each stage of the user journey even as it relates to surface feedback so service feedback in AR is very important because it helps users understand how AR core understands the environment it gives the users a sense of a sense of the surfaces that are available the range of the surfaces that are available so we recommend including feedback on the surfaces when the user is placing objects in the scene the other thing that we recommend is maintaining the height of the tallest surface as a user drags an object from one surface to another and once they once an object is suspended in the air make sure they are always communicating of visual feedback on the drop point that way it's very clear to the user at all times where the object is to land and once an object is placed into the scene we also recommend providing feedback in the form of a visual feedback on the surface or even on the object itself just to communicate the objects entry into the physical environment so now that we know how to play with objects in your scene let's think about how an object might get there we recommend using gallery interfaces in order to communicate user to users how they can take objects that live on screen and drag it out into their real world so here you see we have a gallery strip at the bottom bar and as a user selects an object they're able to drag it on to their space and not only that we're able to support both selection States and also very familiar gestures that allow users to manipulate the objects so you can use pinch to scale twist to rotate and even drag to move and you've seen many examples in our talk and how dragging objects is a very common and expected behavior but another alternative for object selection and object movement is through a reticle so reticle selection is also very effective in that it allows users to manipulate objects in their scene without covering too much of the users view so we have an example here where reticle selection is being used to select Iraq and that's triggered via the action button in the bottom right but what allows users to do it Allah is that it allows users to see the many surfaces that are available and as you can imagine if the user selecting an object with their finger and dragging it across the street at the screen you don't have as much screen real estate to see all of the surfaces that the user might want to place the object on so radical selection is very very impactful here the other thing that you get with radical selection are rate casts so ray casts are very effective in helping the user get a sense of the virtual waits apply to each of these objects so here we have another example where the user is able to pick up a better and once the feather is picked up you'll notice that the raycast has that very little movement and very little bends on it and for the most part it remains straight however when the user picks up the rock you're able to see a more dramatic Bend applied to the raycast that signifies the larger amount of mass the heavier weight of this object in relation to the feather alright so let's move on to the final pillar which is volumetric interface design I think one of the the first things you want to consider here is that the phone is the user's viewport they're actually using the phone to look out into the scene and see the application and because of that you don't actually want to place a lot of - DUI on the screen that's actually going to obscure the users view onto your application so it's showing a sort of quick example it's obviously a lot nicer to have you know a limited set of controls as soon as you start to clutter the screen it really gets in the way of the users ability to enjoy the AR app and the sort of counterintuitive thing that we've even found is that users are so focused on the app out in the world that often designers will place a control on a screen level because they they want to draw attention to that control but it's actually having the opposite effect that users are actually more focused out in the scene so they'll actually just miss controls that are drawn on the surface just kind of tune those out so really you want to be very mindful of when you're making decisions on if you're gonna put a control up on the screen versus out into the scene itself for not just you know obscuring the view but also for discoverability of them then finding that and that's not to say that you should never put a control up on the to the screen but you want to be there considering a few different metrics on it so our recommendations is that you only really leverage on screen surface UI for things like your controls that have a very high frequency of use or controls that require very fast access so like a camera shutter button is kind of the perfect example of something that you know hits both criteria where you know in a camera you're obviously taking lots of pictures and also you want to take pictures very quickly but imagine if you were like playing a game and there's you know some ability to like fire or something that would be you know a good candidate for an on screen control because you're both hitting that button a lot and also you need to get to that button very quickly so we talked about using the view and AR icon to get people into the experience and transition from 2d and TR but you also want to be very careful about the opposite of when users are now in AR and they're actually transitioning back to a 2d experience and one thing we found is I if the user is not initiating that action to go back into a 2d experience it can actually be pretty obnoxious because they're so focused out in the scene so the users you know during the application and then suddenly a 2d UI shows up and blocks the entire viewport that can be pretty annoying so you're really you know depending on even if these are is you know exiting or there you know customizing an item in the scene or you know whatever these cases you want that flow back to 2d screen level UI that's covering most of the screen to be something that the user is actively doing and not something that happens by surprise so you also you know common common thing with multiple application design is you want to maintain you know touch targets that are about the size of the users finger for 3d this is of course a bit harder because the object could be any distance away from the user so quick example of some things you can do here here we have two tennis balls and when you tap on the tennis ball can feet eve is out of it because in error you can do whatever you want and we're showing the the touch target size with the dotted line so one of these tennis balls is actually maintaining a reasonable touch target size as it gets farther away whereas the other one is not it's just mapping to the virtual size of the object and of course it's a lot easier to interact with the one that is maintaining a large target size we've also found for interfaces where you're manipulating objects if you're not doing tricks to kind of maintain target size you often get these problems where you swipe an object so they're very far away and then it's actually hard to bring the object back because it's now at such a small target so you have to actually walk over to the object to get it which is a little bit frustrating maybe you could say it's very immersive but either way it's nicer to be able to actually bring the objects back as well so on the whole you want to be thinking about you know what controls are going to be on the screen versus what controls are going to be out in the scene and kind of a mantra the the team has had is to say you know seeing over screen obviously we talked about some you know sort of boundary cases of when you'd want to put something on a screen level but I found it's it's many people's initial reaction to design everything for the screen level because that's you know the type of design work we've been doing for 2d applications they you really want to start thinking more about volumetric UI and having your UI out into the scene itself so give a quick example of this this is actually one of the demos the ships of scene form it's a solar system simulator loads of fun also it's missing a planet right now we did fix that for the public release in case you notice that in the video but imagine now you need to design the UI for this so you know a lot of people would initially think oh I'll have you know a gear menu up in the corner that will you know throw something up on the screen you know the problem there is then you're not going to be able to sort of be as immersed in the simulation itself as you're interacting with it so you know alternative way of doing that it's actually leverage these objects you know on the scene itself so as you're tapping on planets you'll get feedback on what planet that is just you know kind of nice for educational use cases and then in this particular demo when you tap on the Sun that's how you start to control the entire solar system so here the users tapping on the Sun and that brings up a panel this is actually an Android view so in scene form you can map just standard Android views I into AR and here you have controls like you know changing the orbit speed or the rotational speed of the planets themselves and it's really nice to be able to interact with these objects in the scene and not to have that sort of sudden loss of being able to see things and being sort of taken out of the experience that kind of brings me to final point which is this idea they are presents so we we've actually seen this coming up in and user research studies where people would be looking through a phone and then they would kind of step outside of the look outside of the phone to like see this you know something was placed correctly and then you know of course we're recording it so they laugh and they're like oh yeah right of course you know I can only see it through the phone and you know we always laughed when we saw this happen and then I was testing out nap it was he sort of you know plastic interlocking bricks and I had instructions of what I was building and I was playing it for a long time and at one moment I looked over so the instruction book and it wasn't there and I like you know had the reaction that you normally have of like an object just disappears in real life and of course then immediately I'm like oh that's silly you know like yeah it's AR but I was so immersed in the experience in the application and I've been playing it for so long that I was no longer kind of mentally tracking like what was real and which was what was virtual and I was just sort of buying that the experience was happening and you're gonna start to have this experience as well as you're interacting with these applications and I would say that's the moment when your application is performing really really well because it means that the user is just completely immersed and engrossed in the application so if you ever have these moments where people are you know looking at a phase and you know through their phone and then they look down it disappears and they react that's good that means the app is performing great alright so we've gone through the five pillars of an AR design which again include understanding the users environment planning for users movement onboarding users by initializing smoothly designing natural object interactions and balancing on screen and volumetric interface design and again this framework we believe will help anybody get started with creating apps that everybody can enjoy so we have a quick video for you some amazing content that's many designers and developers like yourselves from the community have created we hope you enjoy [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] we're very happy to share with you everything that we had today and we look forward to seeing what you create please fill out our survey and check out our resources online thank you [Music] 