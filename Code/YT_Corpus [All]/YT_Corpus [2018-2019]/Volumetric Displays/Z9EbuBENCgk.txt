  ADITYA SANKAR: Welcome everyone. My name is Aditya Sankar, and I'm with the UW Reality Lab. And welcome to another UW Reality Lab Lecture, that's being jointly held with the 481V VR Capstone. Today we're very lucky to have Paul Debevec. Paul is a senior scientist at Google where he runs a team doing various kind of interesting things with computational photography. He's also an adjunct research professor at USC where he leads their Creative Technologies Lab, and happens to be a Board of Advisors member for the UW Reality Lab, so it's very nice to have him come and visit. So Paul did his PhD at UC Berkeley with Jitendra Malik, in computer vision, and since then done a bunch of interesting projects, which we'll hear about some of them today. And also, a small personal anecdote, so while I was making a video for my first SIGGRAPH paper, I asked my adviser, Steve Seitz, you know, how do you make a good video? So Steve told me a bunch of things, and one of them was check out this video from Paul Debevec, called "The Campanile Movie", which is essentially Paul running around the Berkeley campus with a 3D model of the tower and then stopping and then the camera spins around and becomes this cool aerial view. So it's like, oh my God, this is like, amazing. I wish he could recreate that, but it was a very inspiring moment. So very happy to have Paul here and look forward to see what he has to say. So thank you, Paul. PAUL DEBEVEC: Great. Thank you, Aditya. [APPLAUSE] Wow. Excellent. So first of all, thanks for having me. It's a huge honor to be part of the UW Reality Lab. And I think that's an amazing thing that started up here, and hopefully, many of you are already involved in some of those projects. I think magic is going to come out of this. And it's great to be here to catch a little bit of a glimpse of things. It's nice to be back at the University of Washington. I think I gave one of my very first academic talks here right after The Campanile Movie. I think actually the first place The Campanile Movie was ever shown publicly was in April of 1997 here at UW. Like within 48 hours of submitting it to SIGGRAPH I had it on a kind of a Hi8 analog tape with the composite video plugged in to show on the thing. So maybe a little bit coming full circle here. I am going to, in fact, get to talk about some projects that come from both of the institutions that I worked with recently, and one of them is Google VR. I'll talk about the end and some work we've been doing on light fields for virtual reality. And I'll also be talking about some of the work that we've been doing having to do with kind of digital faces at the USC Institute for Creative Technologies. And I'm very lucky both of these places have buildings that are across a park from each other in Playa Vista, California, down in LA. So keeping up the activities there has been OK. And one theme is also going to be movies and kind of finding inspiration for movies for developing technology, then finding ways that that technology can maybe help out on another film. And so growing up one of my favorite films was Back to the Future, which I presume lots of people have seen. I think it's still popular. My favorite scene of it was at the end where the DeLorean time machine car has been outfitted with new kind of capabilities and such and it can fly. And there's this great visual effects sequence where it just goes down this suburban street or levitates in the air, goes over the trees, and flies back out at you. And this image really kind of stuck in my mind. I think right after Back to the Future II came out it kind of was back on my mind. And everyone wants to kind of imagine themselves doing that, that being their car. I imagine that my own car looked something like a DeLorean as well. It was at least the right color. But it was actually a 1980 Chevette. And it wasn't the most exciting one. This is me about to drive off to college and saying goodbye to my dog and my grandma before heading out there. But I wanted to see my-- I got the idea my car would feel more like it was that DeLorean in Back to the Future if it could fly, too. And I knew that-- I actually did talk to an aerospace engineer about mounting rockets to it at some point. I recall that happened at the University of Michigan at some point. But it seemed like CGI would be a better way to maybe make that a possibility. So we go forward. We've got-- and I had a summer job where I had access to a video digitizer. So I could take photos of the car from different angles. I parked it next to a parking structure and I thought, let's make a 3D model of this car from the photographs so that I can virtually watch it flying across the screen. And I knew that just getting the shape of the car wasn't going to be enough. I should also try to get like, you know, the coloration of the car. You can see it's got chipped paint on the roof and a bent license plate. And so all these things that kind of made it specifically my car were important. And the technique that I ended up doing, again, for the summer job-- not technically what I was supposed to be doing for the summer, or at least those two weeks-- was kind of volumetrically intersecting the silhouettes of the car and then using these photos as the texture maps for all the front-facing, side-facing, and top-facing surfaces. So this interior view is kind of like a novel view synthesis in between those different photos I took from the front and the top and the back. And then using a little thing, I could do a low res 64-frame animation and play it back in real time. And that was really exciting because it kind of looked like my car. Looking at it today, I notice there's even some view dependent shading effects. So like if you look at the reflections in the windows and such, those kind of change as it flies past because it's actually effectively showing different photos mapped on depending on as you view it from different directions. And that kind of gave it a little bit more sense of realism as it went by. And at that time it was the moment to kind of write essays for what I wanted to do at graduate school. And I wrote that I wanted to kind of take some of this technology and apply it to architectural scenes. And I was very happy that one of the people who read my application, who actually read my application was professor Jitendra Malik at UC Berkeley. And he taught me a lot of computer vision stuff first, and then he put me in touch with a fellow named CJ Taylor, who was a postdoc who had been working on reconstructing scenes based on line segments instead of point correspondences. And got to talking with him and we realized that a really potentially useful way to reconstruct architecture would be as series of geometric primitives rather than just tons of little points out there. Every single point gives you another three dimensions to solve for for where it is, but if you know that part of your architecture is like a box, then you can parameterize that as a height, a length, and a width and only solve for three parameters, and you get eight vertices out of solving for only three unknowns. So based on marking line segments and photos, building a prototype model where you haven't set any of the sizes and positions, this system we called "Facade" could solve for where all the pictures were taken from, the size and shape of all the architecture. And then you could generate novel views of the architecture from anywhere that you wanted. It seemed like that would be kind of maybe a useful filmmaking technique, that if you took pictures of a somewhat larger scene, like the Berkeley tower and all of the images surrounding it, you could reconstruct a 3D model of the surfaces there. And then if you projected those photographs onto that geometry, it might start looking like a relatively photoreal version of that scene as well. And since Silicon Graphics actually donated a RealityEngine2 that had 16 megabytes of texture memory, we could even generate these kinds of renderings in real time, you know, more than 20 years ago now. And that was pretty exciting to see stuff that looked pretty photoreal. Of course, it looks photoreal because it's really based on photos. We're just using the reality of the photos to kind of use what was already there in reality and then transition back. And we tried to do this film Aditya was talking about for the SIGGRAPH computer animation festival and put that together. And one of the people who actually saw it at the SIGGRAPH computer animation festival was a fellow named John Gaeta, who was trying to figure out how to do some virtual cinematography kinds of things in some shots for a movie that they were trying to figure out the visual effects for. So at some point they hired one of the grad students I worked on this film with. He actually called me before we went to Australia to shoot some photographs on top of a building that they were shooting there, too, so that they could do kind of a photogrammetric reconstruction of a scene, and they used this kind of modeling and rendering architecture from photographs for the virtual backgrounds in the bullet time shots in The Matrix. And so while it seemed like a lot of people saw The Campanile Movie at SIGGRAPH, probably more people would have seen kind of these shots taking place in the film. But the movie industry hadn't stopped innovating in its work as well. Another film that I was very impressed by, I have to say, was Jurassic Park because, you know, it brought creatures to life that like-- first of all, maybe in 1993 you could believe that you could have a computer-generated car, right? And you could, like, visual effects people could find a way to render and shade that and it would look totally real. Organic shapes, like dinosaurs that move and have muscles that bulge and stuff, we hadn't seen a lot of that yet. It just seemed fundamentally hard. Like this thing isn't at all like a computer or like the plastic, metal kinds of things that we'd seen before. And not only that, they made these dinosaurs look like they existed in the same space as the characters of the film, the actor. You could see like, you know, the lighting, you know, coming from behind and matching the lighting direction, and kind of the quality of the bounce light from the grass here that's giving them some of the coloration. The lighting integration of this, you know, that you see on the actor and the character are very consistent. And you just don't question whether this thing was actually there. Because if you question whether that was there, you wouldn't believe that this guy had all that much to worry about and then you'd be missing the whole point of the film at that point. So I looked at this and I just thought, well, I'd love to do something like that, too, or at least I want to know how it works. And I didn't get very satisfying answers from the couple of people that I knew at Industrial Light and Magic, who were aware of how the work was done. Because apparently, it was done much more based on artistry and artists trying to put lights around manually until it looked right. And that's extremely impressive that they got it to look as good as it did, but it didn't feel like a very repeatable or scientific process to match the lighting. So that was the idea behind trying to actually use cameras to record illumination in scenes and shooting panoramic photos that look in all different directions to basically observe the color and intensity of light coming from every direction, and then also based on some other stuff that I got to work with Jitendra Malik on, you know, bracketing the exposures to capture the full range of illumination from the dim stuff all the way to looking straight into light sources to capture what we called light probe images, or HDRI maps, and basically, having the correct color and intensity of light coming from every direction. Then the other thing that helped make this possible is that one of the people in the UC Berkeley environment was a researcher named Greg Ward from Lawrence Berkeley Labs, and he had written a lighting and rendering or a synthetic illumination system called Radiance, which already supported high dynamic range images where pixel values don't go 0 to 255, they go 0 to a million or whatever they need to in order to represent the full light of the scene. And he was able to show me what I needed to do with Radiance in order to actually use it to have these images of the illumination light the CGI objects. So basically, what you do is you take some surface surrounding your object, you texture map it with this 360 degree high dynamic range image, and then you ask the lighting algorithm to light the objects not with individual point light sources or area light sources that you've put, but actually just let this image as an emissive texture map be the light source for the object. So it'll trace rays to the object. It traces rays out looking for where light is. If it's somewhere bright, it adds a bunch of light coming from that direction. If it hits somewhere dim, it adds less light. And then it calculates for every point how much light is visible to it, and then you can get some pretty realistic renderings from that as a result. So this is like the first animation that I got to work with some students on to make. And it's pretty simple. It's just some computer-generated objects with different material properties lit by a single HDRI map in the UC Berkeley Eucalyptus Grove. Something that helped the realism here is that I get these high dynamic range images out of Radiance and it felt like a shame to have to clip all the pixel values back down to white, even though there were some really bright parts of the scene that still were much brighter. So I put a bunch of glare filters on top. Basically, every image you see is the sum of four different Gaussian blurred versions of the image, one with like a 50-pixel blur, one with a 10, one with a 5, and one with a 2-pixel blur and a bit of the original image. And you add those together and then it produces these kind of light wrap effects where it feels like the light is kind of almost reaching around and pulling the objects back realistically into the scene, which is something that'll happen in your own eye and in real cameras as well. So that helped the realism. There's also like a vignetting effect going. We're darkening the corners of the image down to about 0.25 compared to the center of the image. And when bright stuff goes in there, it still can saturate that out because some of these pixel values are 50 times brighter than the white point that we have here. And so as the camera spins, you're kind of exploring the dynamic range of the scene. And you don't notice that consciously, but maybe subconsciously it looks like there's more going on than what you would see otherwise. And then we used all of those kinds of tricks plus some dynamic simulation work that was inspired by work being done by Brian Mirtich and John Canny at Berkeley, and a photogrammetric reconstruction using the Facade system of the interior of St. Peter's Basilica to try to imagine, you know, something kind of interesting happening there. This was sort of all about kind of a abstract interpretation of the conflict between Galileo and the church. And even if it didn't communicate well on that level, at least you got to see some big dominoes falling over somewhere nice. And so that kind of worked out. And to my surprise, this kind of technique of going out there with a mirror ball or taking a panoramic, shooting HDR and lighting your objects with it, it actually did pick up in the movie industry as a way to do this kind of thing. And it continues to this day. It really is sort of a standard process for lighting a CGI character that you're going to add to the scene. So this is Hugh Jackman shadowboxing with his robot boxer friend here. And you know, what was really there is I think there was a guy standing on like a wooden box wearing a motion capture suit, and he got edited out and then they have to render this robot there. Like the whole scene only really works if you really feel that that robot's really there just as much as Hugh Jackman is and that they're having this interaction. So he has to be lit right. So Digital Domain is one of the visual effects companies that can go out and shoot a proper HDRI map, and then you can use just about any renderer, V-Ray or Arnold or Octane, to basically do image-based lighting on these, and it helps a lot of these effects happen. And looking at these kinds of results, sort of the next thing that presented itself as a research problem was, like, we know how to light the robot if the robot wasn't really there. What if we had to light Hugh Jackman if he wasn't really there? Like, what's the right answer for that? What if we only had him in the movie studio on green screen because, you know, he's busy or something and we have to do a reshoot? How could we get him lit just the right way so it looks like he's in the scene? And so the thought there was that maybe we could do image-based lighting on people. So like instead of like what we had on the previous slide here, of trying to surround the object with digital illumination, maybe we could kind of come up with some way of having a face and lighting it with real illumination. So I knew eventually this light would have to come from the whole sphere of illumination. But I actually found this photo recently. I think this is like late 1998, and it's me in the computer science building at Berkeley just trying to take pictures of what a person-- unfortunately, me in this case-- looks like lit by images of illumination reflecting back on them. And so this is the thing I projected on the video projector, and then I was interested in like maybe eventually finding a way to light somebody with light that would bounce back from all different directions in order to create some kind of lighting sphere there. And nothing really presented itself as a practical technique just with the contrast ratio available of video projectors and how bright they were and the fact you'll get bounce light inside these spherical things until this product came out, which was the Color Kinetics iColor MR Light Source. It had red and green and blue LEDs in it. And it was really designed that, you know, if you could afford $120 a piece for these things, you could plug them into these things and there was like a USB interface to your computer, it's controlled over DMX, and you could program these light sources. So if there were some way to build a whole sphere of these light sources, we could actually start lighting people with images of illumination that we'd captured somewhere else. And that was like the original idea for a light stage. And the one, the device that actually finally did this was the third light stage that we built. Earlier ones kind of had fewer lights and we had to move them around to get them to all the directions. But finally, we could surround people with color LEDs from every direction. So we'd take our HDRI map, we'd resample it to the 156 lighting directions in the sphere, light the person with it, and then composite the person into that background plate, which would be a little piece of the environment from the background. And if we paid attention to, you know, all the color balance and such, it should look relatively realistic. One drawback here is that somehow we had to get the alpha channel or the mat for the person in order to stick them onto the background. And I didn't really want to put a green screen back there because green screens are still a little bit tricky to compute what all the edges are properly, particularly for things like blond hair and such. And then they also can bounce green light onto the subject. And we're trying to be really careful to have the light on the subject be just right and just so. So instead, we put black cloth back there that reflects infrared light, and then there's some infrared illuminators on that. And we film them with two cameras. There's a color camera here, there's a piece of glass, a piece of glass here, which is a beam splitter, and then a second monochrome camera that's sensitive to infrared light. So we're actually shooting them both in color and in infrared. And in infrared, they just look like they're a dark figure on a lit background, which is pretty much the right image that you want if you need a traveling mat for them. So the compositing is done based on the infrared channel without having to do the green screen. And that worked out pretty well. And we thought we were terribly inventive doing that until some very friendly person pointed out that some experimental filmmakers in Russia had done this in the 1950s. So we had a cool citation to make in our paper, but we realized some people had also come up with the idea and actually gotten it to work with far more primitive film technology back in the day. But this seemed like potentially a useful result. I remember one of the SIGGRAPH reviews came back and it was enthusiast-- it was clearly somebody from industry-- and said, oh, this is going to really help how we light actors, and such. It took a while for this to actually catch on in a significant way in the motion picture industry. Like for The Matrix stuff, we showed it at SIGGRAPH '97 and it was in the movie less than two years later in the film. Here the gestation period was a little bit longer. The film that really ended up making use of it in a significant way didn't come out until 2013, and that was the film Gravity. And Gravity is really fun. I just realized I wanted to watch this again. I haven't seen it in a while and it's a great film. It's only 90 minutes long. This is another good lesson I've learned is the shorter you can make your film and still tell your story, that's almost certainly the better way to do it. And the way that the visual effects worked, basically, almost everything in this scene is CGI, is computer graphics. This is a computer-generated planet Earth. And people at Framestore visual effects, they're actually doing like an atmospheric scattering model. They can create sunsets virtually. There's all sorts of great volumetric stuff going on to create all the different Earths that you see. It's all rendered in Arnold by the way, which is a global illumination path tracing commercial product that a friend of ours, Marcus Fajardo, wrote. And the Space Telescope, they're trying to fix up the Hubble Space Telescope here. And that's all computer rendered. And all the tools are computer rendered. In fact, the space suits are all computer rendered. These are CGI models and they're animated. They almost could have submitted this film for consideration as Best Animated Feature because of how much computer animation was in it. And the helmets are CGI. The visors are CGI. The only thing that's not CGI is the faces of the stars of the film, Sandra Bullock and George Clooney here. And in fact, when they first brought the project to us in 2010, they were really asking us to try to figure out, like, how should we do the effects? How can we do these effects? Should even the faces of the actors be totally digital? And they actually did do scans of Sandra Bullock and George Clooney in one of the light stages to get digi-doubles of them. So some of the shots, you know, make use of a CGI model of the face. But they kind of knew that, at least at the time they were making the film, it wasn't a sure shot that we could get extended performances for a lot of the film time to look realistic with kind of the facial animation and rendering technology of the day. And so they thought, no, we're going to shoot these faces live action and then we're going to composite them into the suits. But the big question is, this is a hard compositing job, right? The face has to somehow be filmed from just the right angle and lit just the right way so that when you lay it into this shot, it looks like it's being lit the same way as the rest of the space helmet and the visor. There's all these other things that's very obvious how they're being lit. That needs to look consistent with that. So that ended up being something that looked like it might be a job for Light Stage. We did a test up in Burbank with Alfonso Cuarón. The director himself actually came and helped with this test. He's very, very hands-on, very smart fellow. This is Tim Webber from Framestore, who would eventually be one of the people who got an Oscar for the visual effects for the film. And we had some stand-in actors. This is before they had the final cast. So this is an actress named Marguerita, and she's in the light stage at OTOY, which is built by a collaborator of ours, Tim Hawkins. And here's Alfonso Cuarón getting used to directing actors who are otherwise caged by LEDs for that. And we're basically doing tests of playing back lighting patterns on them dynamically in a way that we can either light them or relight them with different environments. And here Marguerita, the test actress, is reading in lines to this fellow here, who's standing in for George Clooney. And for some reason, one of the big questions that the production people had was that they weren't quite sure if this process would work particularly well for stubble that people have on their face. Right? Astronauts don't necessarily shave every day, so that's kind of an astronaut look. And so they decided to kind of test both cases at the same time. So if you look closely, you'll notice he's kind of shaved on this side and he's-- [LAUGHTER] --still got the stubble on the other. And hey, it's a great day of work anyway. So in the film, what they eventually bill built over in London-- they had to shoot in London because there's a lot of tax credits there-- is that they rented, or as they would say, hired many LED panels, which have now become available, to basically create the light stage device. And then they put an instrumented little basket below to turn the actor to just the right angle. And they put the camera on really high quality, strong, industrial robot arm to move the camera to just the right position. So when the actor got in there, they close it all up except for a vertical slit, and then they could get the right angle horizontally by twisting the actor, the right angle vertically by moving the camera up and down, and the right distance by moving the camera back and forth on a track. And then they would play back animations of the lighting of the environments that the astronaut suits were going through. So they actually had to do the CGI of the movie first, and then shoot the live action so that the CGI lit the live action. And usually it's kind of, they'll shoot the live action first and then add in the CGI. You can see here there's a bit of the International Space Station going around. This helps a lot if you need to show the environment rotating around the actor quickly because physically moving lights around would be difficult to do. They even use it for interior scenes. You can see this is the interior of the International Space Station. And at some point, Sandra Bullock kind of is floating through. And I think that her legs were computer added, but her torso and arms were all real. And they were actually lit by the CGI model of the International Space Station, which was all a computer model. And it all looks completely real in ways that I find basically pretty unfathomable as well. And if we take a look at some of the shots here, you can see-- here they are. They kind of put some diffuser paper over there, the LEDs, because the LEDs weren't super high res. But this is the CGI part, and then they add in the faces and such. And then you get a final shot where hopefully it all looks like it was there at the same time so you can believe the shot's really happening. That's various forms of the [INAUDIBLE].. Then probably the hardest shot to have done any other way is this shot where she's kind of spinning out of control but the camera's focused on her face, so the whole lighting environment just needs to spin very rapidly. And they actually did do a study of what it would take to put the lights on the camera arms and move it all around her on the robot arms. And that was going to be too tough, particularly for that big area light source of the Earth. So they ended up getting the effect pretty well here. So we've actually continued to work on some of the lighting things because it looked like there were a couple other problems that we might want to address. And one of them, one things that bothered us a lot-- oh yes, Steve. STEVE SEITZ: So that was the first film that used the Light Stage? Or-- PAUL DEBEVEC: That was the first film that used the lighting reproduction aspect of Light Stage in a big way. There's I think four or five commercially-relevant processes that the Light Stage has. So the first film that actually used the Light Stage was Spider-Man 2 in 2004. And that used Light Stage 2, which was a series of strobe lights in an arc and it would rotate around in eight seconds. And so they brought Alfred Molina and Tobey Maguire over to get scanned for basically facial reflectants, to get relightable texture maps of the face. So there's about 40 shots of the film Spider-Man 2 that have digi-doubles made using image-based relighting technology. And that was kind of helpful because you could get good skin reflectants without having to do subsurface scattering because it was just all baked in to the relightable textures and it still looked like the actor's skin. And then eventually, when subsurface scattering algorithms got productionized and fast enough, you kind of needed texture maps and high resolution geometry and then good shader writers, and then you could kind of do it that way. And so the high res Light Stage facial scanning process got used for that. And that'll be the next part of the talk. And actually, for the lighting reproduction process, Gravity was the first film that used it in a big way, but there was another film that was being figured out at the same time that used it in about five shots. It was a film called The Social Network. And this is the film about the early days of Facebook and Mark Zuckerberg. And the people who are the nemeses of the film are the Winklevoss twins. And they cast-- they actually cast two people to play the Winklevoss twins. They cast two actors. One's Armie Hammer and the other was Josh Pence. And they didn't have a lot of money for the film. The Winklevoss twins are actually supposed to be identical twins. They actually are identical twins. And they thought they could just get away with, you know what? We'll just have two guys, we'll just pretend they're brothers instead of identical twins. And at some point, I think they realized the Winklevoss twins are famous enough that people actually know they're really supposed to be twins. And so after they'd cast Armie Hammer and Josh Pence, they had to tell Josh Pence, yeah, we still want you to play the twin, but you're not really going to be seen in the film because we're going to face replace Armie Hammer on top of all of you, too. So he still went through with it. And a lot of times they would just do the traditional, you know, Michael Keaton Multiplicity thing where they'd shoot the scene multiple times and then join it together so it looks like there's two Armie Hammers in the scene. But in some other shots, for a variety of reasons, they'd only shot it with Armie Hammer as one person and Josh Pence as the other. There's one where they're both rowing like a long canoe/kayak thing for the rowing club, and they didn't shoot it both ways, probably because it's impossible to like switch positions when you're actually out in the river on these things. So they had a composite Armie Hammer's head over Josh Pence's head. And fortunately, they had shot HDRI maps, so they were able to use the lighting reproduction technique to have Armie Hammer in the studio doing his lines, even pretending to row a boat, and then that would get composited over the face and the lighting would match. And so I think five shots of The Social Network used-- that way. Thank you. Good question. Yeah. So I was mentioning on the Light Stage that something that bothered us a bit is that the illumination that we're reproducing is being done with red, green, blue LEDs. And since we're shooting the HDRI maps with cameras that record red and green and blue, there's like a nice one-to-one matching that's all convenient. But in the real world, the spectrum of light is like a whole function of wavelength from like about 400 nanometers to 700 nanometers in the visible range. And we know from our color scientist friends-- because they kind of don't stop talking about it-- is that a lot of interesting things are going on across the spectrum. If you just approximate as RGB, then you're possibly opening yourself up to grievous error in terms of color matching and such. And they have a real good point, because if you look at the spectrum of, like this is a red LED spectrum, this one's the green LED spectrum, that's the blue spectrum, there's big gaps in wavelengths that just don't get generated between the red LED and the green LED. And those are things that worry you as a color scientist because if your surfaces are reflecting light in an interesting way in their spectral reflectance curves, then you just won't pick up on it around here. In fact, generally if you light somebody with white light that, like, lights a white piece of paper pretty well, but it's only made out of red, green, and blue LEDs, people just look weird. Like the color of the skin looks too saturated and maybe twisted toward magenta a little bit, even if it's lighting your white shirt exactly right because things are going on under the hood of the spectrum that just aren't well approximated with red, green, and blue. So it turns out you can also buy LEDs that are other colors, like amber and cyan and white. White's great for scanning faces anyway. So we actually built our latest generation Light Stage with six spectra and we thought, OK, now we'll be able to get much better color rendition on the skin if we go out and capture natural illumination or fluorescent illumination or, you know, incandescent light. Then we can reproduce it much more faithfully using six spectra than the three spectra that we had. And then we've just walked straight into a research problem, which is that, wait a second, our cameras only capture red, green, and blue. Now we want to drive our Light Stage, all six channels of it, with data somehow to magically make it so that if we've captured incandescent light, it actually reproduces the way they look like under incandescent light, or if we've captured daylight, it looks like daylight in there. And the technique that we came up with we published at SIGGRAPH 2016. We're happy to work with a grad student who now is doing some work with us at Google named Chloe LeGendre. And the thing that we did is we added to our light probe not just a mirror sphere, we also added this black plastic sphere. That's kind of nice for getting some of the high dynamic range a little bit more quickly as a result, because this is about 4 and 1/2 stops down from the chrome ball reflection. So you might not have to bracket your exposures as much. And then we added all these little color charts kind of aimed in five different directions. And it turns out a color checker chart like this has 19 independent different spectral curves of the different colors that are reflected because they actually intentionally make it with something that's supposed to reflect light the way that, like, leaves do, or reflects like when daylight reflects off and it gives you the color of the sky back or the spectrum of the sky back. And they have one that's designed for darker skin tones, lighter skin tones, like dark, white, complementary colors, all of that. And so usually, if you're trying to assess the quality of light, you put a color checker out there and it will look qualitatively different under incandescent light than daylight, even if you color balance the white square to look white. And so it's kind of a good record of that. And what we realized is that if we simply take the color chart, put it in the Light Stage, and see how the color chart looks to our camera under each one of the LEDs, that this forms the basis that we want to match. That we can drive the LEDs to optimally match the color chart's appearance any way that we want just by solving a non-negatively squares problem to figure out how much of the red LED, the amber, green, cyan, blue, and white that we need to turn on to make the color chart look the same. So now we have two problems we want to solve. We want the RGB color of the light source to look the same color that it does in RGB and intensity as we observe off of the mirror ball, off of the HDRI map. And then we also want the color charts to look correct. And if we have, like, five independent color charts in different directions, then we can optimize for all five of those directions matching in the same way. So it all ends up being a linear system for non-native-- or a non-natively square system. I won't go into the details of that. But the result is that we were actually able to get pretty good results of photographing somebody in the Light Stage and somebody out in the real world and having the results match without having to tweak the colors at all ourselves. This looks like [INAUDIBLE]. But anyway, the thing here is that these are our friends Jessica on the top, Jeanetta on the bottom. This is like late afternoon light. I believe this is her actually standing outside the Institute. We took another photo after she stepped away to get the background plate. We took a picture of the light probe. We processed that. About two hours later, we had it ready for her to step into the Light Stage and get lit that way. And then we quickly-- we didn't use the infrared matting technique. We quickly flashed strobes on a piece of dark gray card stock behind her to get her silhouetted for a good mat without using a green screen, and then we did the composite there. And hopefully, it looks, you know, like you wouldn't think this is a fake photo, hopefully, as to whether she's really there. And then with Jeanetta we did kind of an overcast sky, or maybe she's just standing in shadow. And you can actually see this is kind of like the warm color of some of the building bricks reflecting in her forehead. This is the blue sky reflecting in her forehead. And you can see that there. And then all of that gets reproduced in the Light Stage pretty effectively as well. So I think we have a good tool for if you need to film somebody in a studio, that you'll be able to get a decent match. And actually, we just completed building a Light Stage in Beijing, which is 26 feet wide and has all six spectra. And they're going to be using it for filming movies and TV shows where they need to add an actor into some footage that they had to before. So again, all of that's kind of just the original idea for the Light Stage. But after we realized that we had these devices that surrounded people by LEDs-- did you have question by the way? All good? OK. That we could do some other stuff with it as well. And you know, some of the earlier image-based relighting work for some digital characters and movies, like I mentioned from Spider-Man 2, was all nice, but it kind of felt like it was sort of a clunky process and the ability to relight faces with subsurface scattering looked pretty successful. The question would be, could we use-- could we figure out a way, maybe it uses the Light Stage, to get high resolution models of people's faces that actually have geometry? Ideally like down to skin pores and fine creases, and then also good diffuse texture maps, specular maps, normal maps, all the things that people really want to use in order to build digital characters in the more modern era. And so since we had these devices that had people surrounded by lights, we started playing with other illumination patterns that are more like computational illumination patterns that might let us measure these maps off of the face a little bit more directly. So kind of a natural thing to think about lighting people with was, you know-- well, first of all, just turn all the lights on. That gets you a nice flat, lit image of people. That's almost valuable right there. But as far as getting fine-grained surface orientations of skin, the thing that occurred to us is something that was sort of a generalization of photometric stereo techniques to spherical lighting, which is to light somebody with the first four spherical harmonics. And that basically means light from everywhere, and then kind of a gradient of light coming from right to left, a gradient of light coming from top to bottom, and a gradient of light coming from front to back. And if you light people with these three conditions, then basically, parts of the face that point more to the right are brighter in this image than compared to that image, and points of the face that point up are going to be brighter in this image compared to this image, relatively speaking. And it kind of reads out the surface normals. Another thing that we did is we realized that specular reflections have a much different reflectants model than the subsurface scattering of the face, which comes out as a pretty diffuse reflection, almost kind of lambertian. So by putting polarizers on the Light Stage and also on the camera, you might have seen that before you can cross-polarize out illumination if you light somebody with a light source and it's got a horizontal polarizer on it, then you take a picture through a camera that has a vertical polarizer, all the shine off of the skin disappears. The specular kick is just gone because that light keeps the polarization. It's like a little mirror reflection. It stays polarized. And it can't get through that second polarizer 'cause it's still horizontally polarized. But the light that gets into the skin and bounces around and goes through multiple scattering gets depolarized. And so depolarized light can, at least some of it, make it through the polarizer so you still see the subsurface scattering. So all these images here are actually cross-polarized and the shine of the skin is gone, whereas these images here, we flipped the polarizer on the camera and then the shine comes back. Nowadays when we build light stages, the lights can actually switch the polarization. So we leave static polarizers on the camera and the lights switch. So basically, our lighting patterns-- and we also stopped putting dots on the face since we did this project a while ago. But you notice here she is lit with very flat illumination from everywhere, but you don't even see the lights reflecting in the eyes. That's all cross-polarized out. If you flip the polarizer really quickly, then the specular reflection comes back. And you can see that the difference between these two images is the specular reflection coming in and out. And since you can do math on images, you can literally compute the difference between these two images, and it literally is the specular reflection. And this was like a new image that, you know, if you're trying to get a map of what the specular reflection of the skin is, if you can light it from all directions and observe just the specular reflection, that's a great image to have for that map. But if you then start lighting with the gradients of illumination coming from the different directions, you can watch how the light plays off of the skin. There's all these little bumps on here. So like just pick some little area of skin and watch what it looks like when we light it with the three gradients. And I'll go back. You'll see that it's basically lighting it from different directions and it brings out the shape based on how the light plays off of it. And since this is the specular reflection, it's pretty sharp detail. It's not the light that got under the skin and blurred with the subsurface scattering, it's this first surface reflection that really reveals the shape. And so if you create a surface normal map based on what this implies about which part of the Light Stage is reflecting in every pixel, you get a high resolution surface normal map that looks like this. And then if you start with some other kind of millimeter accurate 3D scanning technology, like structured light or high-end photogrammetry algorithms, that'll get you maybe to a millimeter of detail of the skin. You can then use these high resolution normal maps to up res that to about 1/10 of a millimeter. And so we were finally able to start getting scans of faces with, you know, it would take at the time about five seconds to shoot this with our cameras to take the pictures that we needed. Nowadays, we can do this in less than half a second. And you know, we get skin texture. We get nice little creases in the lips. We even were able to see almost like vein structures under the eyelids and such. And when the face went to different expressions, we'd see all sorts of crinkles and wrinkles and crunches and such. And if we have an actor available for an hour, we can get a whole set of blend shapes for them. And that's what we shot for a company called Image Metrics back in 2008 that was trying to create one of the first, hopefully, believable digital faces for an actor. And all of our data went over to them and they created this from that. So they actually showed this at SIGGRAPH 10 years ago, and the face was completely computer-rendered. It was composited on top of the relatively easy case of the actress herself. So it's still her real ear and hair and clothes, but all of this stuff was completely computer replaced. There's the diffuse. Here's the specular. It looks a little creepy. Maybe good for Halloween. And you know, the underlying animation mesh that they used their technology to animate. If you take it all away, that's her original face. And we didn't match it perfectly. Side by side, you can see there's differences. But we brought the actress, Emily O'Brien, back three months later to see what we'd done and we showed her and we were very proud of all of this. And she said, OK, so what was that? Because she actually didn't realize it was CGI either. She just thought we were showing her video that we'd successfully gotten onto the television in three months. [LAUGHTER] So then we showed her and then that was a, that was a good thing. So we realized we were on to something. Now that face probably wouldn't have looked nearly as realistic if we were to have zoomed in on just a little part of it. And at some point, just because this is what happens when you're doing research is you start to focus almost obsessively on certain details. And we got interested in even the hundredth of millimeter detail of skin. At one point, I was stopped in traffic, which happens in LA-- apparently here, too, I'm told-- I had, like, sun hitting me on the forehead, which happens in LA-- not here as often, I'm told. And I could see the reflection in the rearview mirror of this highlight in my forehand. And as I like-- I was really bored-- I was raising my eyebrows and pulling my eyebrows down, I could notice that the reflection of this highlight in my forehead was just really changing shape and lots of just fascinating-- compared to the traffic-- ways. And what really was happening is there's skin microgeometry that kind of gets tugged at and squished and it forms anisotopic reflections because the microgeometry actually changes shape in a significant way. And so I don't know, for some reason we got a proposal funded to look at this and we built this really friendly-looking device that was going to let us measure skin microgeometry under different amounts of tension. So we're inside the Light Stage and we put a camera inside. It's got a big lens, but it's not blocking so much of the light that we can't still do our gradient illumination stuff to get high resolution detail. And we have a little machine vision camera here. And this macro lens is focused on about a square centimeter of skin. And what we've done is we've 3D printed these little things to stick to somebody's forehead. There's double stick tape on the back. And we found that actually the medical community also uses double stick tape when they need to do things like this, too. So it's totally legitimate. And then we have a little gauge where we can get the shape of the skin. We can squish the skin a little bit, then take another scan, squish it a little bit more, open it up, start stretching it. And at some point, the tape just pulls off. And we got data of what the microgeometry of skin looks like under these different tensions in different directions. And so some of that data looks like this. And look at this amazing stuff skin does. Look at how it gets stretchy and you get all of these, like, vertical striations and such. That's exactly what produces anisotropic reflections. If you tabulate the surface normals off of the gradient illumination, you can see that as you swish it and compress it, that all changes. And so working with our grad student, Koki Nagano, we actually came up with a approximate way to add this kind of hundredth of a millimeter surface detail onto skin scans that have displacement maps where it actually changes dynamically according to the stretch and the squash and the angle of it as the skin deforms, and then you can get these kinds of effects of really looking like the skin is under tension here. So that's actually just showing it in a game engine with real time rendering. If we do it with proper illumination-- here's Emily's, that same Emily performance, but we've zoomed in a lot. And hopefully, the skin looks relatively realistic. And a little bit of this is that we're actually using our model of the stretchiness and the anisotopic reflections you get off of the skin microgeometry to do this animation. Sorry for the tearing across there. And so we talked about this at SIGGRAPH and then we got the very validating response that some people at Weta Digital were interested in adding that to some of their digital character pipelines. And mentioning Weta Digital, they're actually the first company to actually come to us for scans of actors using this polarized gradient illumination facial scanning process. So we had some initial results that I talked about with a fellow named Mark Sagar, who worked there when we were at SIGGRAPH 2006. And he flew back through LA on the way back to Los Angeles. He looked at some early results. And within a few weeks of that, we had Zoe Saldana and Sam Worthington from the Avatar production showing up to our lab and Weta asking us to do scans of them. And in the case of Zoe, right, they wanted scans of her so that her character, Neytiri the Navi, could basically have her same facial shape and skin pores and structure. So she'd look a lot like Neytiri did under all of her facial expressions. So obviously, the nose is different and the eyes are enlarged, but basically all the other parts of the face come directly from Zoe Saldana, as they do for most of the other Navis who are based on people. And here's a scene where, you know, it's a totally digital Neytiri here. Sam Worthington's character is also completely digital in the shot, too. So we did scans of Sam Worthington and those got used for his avatar, but also for a number of shots of digital doubles where he is a person. Now certainly, even though it was his first movie, they were probably paying him enough to just ask him to lay down for a bit while they moved a camera around to shoot a green screen that they could have put in here. But since Neytiri needs to lean down and then there needs to be lots of shadowing in between the two faces and such, it's actually much easier to get a realistic result if he's completely CGI too, just like Zoe Saldana is, or just like Neytiri is. So in this scene where she leans down and kisses him, it's two totally digital characters. And at this point, the Light Stage technology has been used in about 40 different movies to do digital characters. And I obviously won't go through all of them, but just a couple to highlight. One of the more kind of weighty jobs that we had to do was for a film that we thought was just going to be a silly action movie, Furious 7. And they actually came to us the weekend before Furious 6 came out and they kind of already knew it was going to do well enough that they wanted to do another one, and they were going to do even crazier visual effects for it, and that they'd want to have digital doubles for the characters. And so I went and saw Furious 6 tried to get into the whole thing. I'm glad I saw it with a crowd that really liked it. And then we started scanning some of the actors. And then a terrible, terrible, terrible, tragic thing happened, which is that Paul Walker was killed in a car accident. He was a passenger in a car that was going fast and it lost control. And halfway through, they'd shot half of his scenes, they'd shot in several of the locations, and he was no longer available for the film. And basically, they shut down production and everybody just assumed that the movie wouldn't go forward. Months later, we heard that there was some interest in actually completing the film. And the reason there was interest is that Paul Walker's family was supportive of finding a way to finish the film so that Paul Walker's last work that he did, you know, could get out there. It could be in the film, his fans could see it. But they've only shot half of the scenes. And so, obviously, they can rewrite the film a bit, but they couldn't use all of those scenes unless they had some of the other scenes that would match into them. So turns out Weta Digital got the job, the same folks who did Avatar, to try to create a digital Paul Walker. And Paul Walker's two brothers, Caleb Walker and Cody Walker, came to our lab with awesome people from Weta Digital and they got scanned. As it turns out, Caleb Walker is only a few years younger than Paul and kind of looks a lot like him, at least from the cheekbones up, and then his face kind of gets narrower. And Cody was quite a bit younger and has maybe less of a resemblance, but his jaw line is pretty close to Paul Walker's. He could sometimes be mistaken for his brother, like if you see him from behind over the shoulder. And so basically, these two fellows, they worked out with Paul Walker's trainer to get in the right shape, they drew the stubble out in just the right way, and we'd scanned them with and without stubble, too, and we got very extensive documentation of those faces. We gave it to Weta Digital, they did tons of miraculous work as well, and they were able to put together a digital version of Paul Walker for a film, which came out in I think 2015. And I think this stands as the best, most extensive use of a digital character to date in a film. And then the publicity for the film actually really didn't talk about the digital character. Like, the executive producers didn't want that to be the story of the film coming out. It wanted to be, you know, understandably, just much more about enjoying Paul Walker's final performance and not realizing that hundreds of shots were actually created using a digital model and it was a big achievement in computer graphics. But if you look at the kinds of stuff where a totally digital face-- so a lot of these scenes, this is Caleb Walker, his brother, acting it out. He actually had to learn how to be an actor to some extent for this. And then they face replace Paul. Vin Diesel's real. He's actually there. [LAUGHTER] And another digital-- wherever possible, they look at reference. Like if they had some reference of Paul Walker doing that character, doing something related, they'd kind of animate to that as much as possible. But you know, a lot of unexpected digital character work that had to get done for that. And now they can finally talk about it, although it's well past the visual effects Oscars for that year at this point. We had another project, which was kind of interesting, which is that I'd given a talk at the Smithsonian Institution in Washington, DC, about some of our scanning work as it could be applied to cultural heritage, and they saw that we'd done some scans of the actors for Avatar, for example. And then in early 2014, they actually asked my lab if we would be able to potentially do a scan of they said a VIP somewhere over on the east coast. And we thought, OK, well, yeah, sure. We can do it. So I just told them, yeah, yeah, we can do it. Didn't at the time really have a portable version of the Light Stage. In May of 2014, they then said, OK, well, we've got a date. We've got a, you know, we'll tell you who it is and everything. And can you show us data from your portable Light Stage by next week so we can just kind of see what the quality is? So we had a very busy kind of Memorial Day weekend building a portable version of the Light Stage, which was kind of a mobile structure, and then pulling light sources off of our Light Stage, pulling cameras off of our Light Stage. We got all of our best cameras. We got 50 of our lights on there. We had to build a new control system. And this was designed so they could potentially open up to go through a single narrow doorway, close up to go into a shipping crate that could fly across the country, and figuring out how to get that all shipped over there. We did a couple of tests over at the Smithsonian and then we had to figure out how to get it over to where it needed to go, which looked something like this, which was a little bit amusing. And then on June 9, 2014, we got up in time, we did one test, and at noon we had our VIP available to sit down. We had to dress up a little bit better than we usually do for Light Stage scans. And we actually got all the expressions that we needed done in about, in about two minutes. So usually, like when we have an actor, we have no compunction about telling them they need to just wait for 10 minutes while we focus all the cameras. Here we had to kind of go a little bit faster. We were happy, very happy this worked out. This is the photo that I wanted the White House photographer to take. And I really hoped to meet with him before the whole thing happened, but you know, there was never the chance. Turns out the guy's a pro, turns out, and totally got it. It was nice because they originally thought we were going to do it in this kind of like, somewhere in some small room in the West Wing, but then they decided that this might be kind of a nice fitting thing to do under the portrait of Lincoln in the State Dining Room because Lincoln is the only other president that we had a measured 3D record of his face while he's in office. So there's actually a plaster cast of Lincoln done about a month before his assassination. And so that's one connection between the two presidents here. And you know, there's obviously others. And the Smithsonian people had one extra light in their kit that they weren't using for anything, so I asked if I could use that and I put it on the back of the scanner just to light the portrait of Lincoln. So Pete Souza, you're welcome. And we got the, we got the photo we wanted. These are the photos that we took of the whole thing. So we had our Canon 1D X cameras from eight positions. We added a couple of Rebels to get a little bit more. And these are our polarized gradient illumination lighting conditions. You can see kind of like the cross-polarized gets the texture map pretty well, and then all of the kind of light from the left, above, below, from surface normals. And at the time, this was the model that we processed. We didn't have access to the data for all that long so we couldn't do a full V-Ray subsurface scattering rendering. We're using an approximation called hybrid normals here. And that ended up producing this result. Nowadays, Ira Kemelmacher and others can probably do this from YouTube videos at this point. So fortunately, we got the job to do it a couple of years before that word came out. And they were also able to use it for a nice 3D print that they made. And I think it was 11 days after we did the scan they actually headed back to the White House to have the person who really matters take a look at it. And it seems like he thought it was reasonably cool. All right, so I'm using a little bit of time. Let me just quickly go through a last thing. Kind of the most recent cool result that I liked, we worked a bit on Blade Runner 2049. There's a scene where these two actresses, one who's kind of a hologram and one who's a real person, kind of merge into one person. So they actually did lots of Light Stage scans of both of them to build a emerged version of them that they can cross dissolve into, which was cool. And then they also made a completely digital face of a young version of Sean Young, the way that she looked in 1982 when she played Rachel in the film. And this was a combination of scans of Sean Young and scans of a stand-in actress to basically get kind of the skin texture and the makeup reflectants and such. So Loren Peta was sort of the, we can say the skin donor in this case. And you can see our grad student Chloe measuring microgeometry of her. And then Sean Young herself came back and helped reprise her role as Rachel so they could get basic bone structure from those scans. Tons of artistic work on the point of the visual effects industry to do it. This is a making of shot. It's a little creepy for a second when they're kind of showing the bone stuff there. Again, good for Halloween. And then eventually, there's the, there's the face. And it's not in the film for all that long, but it's really good when it's there. And you know, subsurface scattering, specular reflections, all those are really important. In the end, the most important thing is, of course, the performance. And I got to talk to some of the people who really scrutinized every single frame of what's going on with this in order to make sure that the emotions read, that the thought appears to be occurring behind those eyes. And even though I had to wait like a year and a half to see the shot after we did the scans, the performance was compelling enough that I bought it, too. It's like it actually pulled me out of analytical check on the face if it looks good mode to back into the story. So those folks definitely did a good job of that. This film, like many, had plenty of holograms in it as well, which, of course, are pretty trivial to do in visual effects because you just paste them onto the scene and then you can look at them. Another thing that our lab has looked at over the years is kind of trying to create versions of kind of three-dimensional images that feel like they're sort of floating in space. So kind of inspired by the long range of these holograms, going back to Star Wars and the Princess Leia thing, I got to work on a project with Mark Bolas, who's now at Microsoft but he was at the ICT then, and his collaborator Ian McDowell on creating this rapidly spinning display surface that we would project 5,000 frame per second video on top of and it could actually reflect 288 independent views of some CGI model in all different directions in order to basically give the appearance of kind of like a floating 3D automultiscopic character there. And like the coolest thing about this is that you could be looking at it here, your friend could be standing on the other side of the device also looking at it. And the magic was just, like, you were both seeing this from the appropriate perspective and it would become like a social interaction where the two of you are looking at the object at the same time in a way, you know, from across the different directions that you just normally get with digital imagery. And one of our sponsors saw that and saw the head and thought, hey, could you turn that into a 3D teleconferencing system? Could that base be something that you're actually talking to and it's being transmitted from somewhere else? And whenever you get a suggestion that's that good from one of your sponsors, you say, perfect, OK, yes, we're definitely going to write a proposal for that. And so we came up with a system that had a high-speed projector here doing different stripe patterns and then a stereo pair of cameras. And this person here was basically getting their face digitized live in 3D and transmitted to a slightly larger version of the display so they could have a 3D teleconference. Now we didn't get it to the point where this person could see their friends in 3D, but we tried to line up the angles so that they'd produce all the right eye lines of sight and everything. And if a couple people come up to talk to kind of this transmitted avatar version of somebody, it looked like this. So this is this face, and she's actually, I think in this case, she's just down the hall. But we did do one of these about 10 miles away from main campus back to our lab. And on this display, she can make eye contact with them because we've calibrated the screen so she's looking in all the right directions. And then multiple people can see her at the same time and look back at her, and you can kind of really see who she's trying to address in the teleconference, and that seemed to add quite a bit. But of course, it was just sort of like a little floating head, so we had to keep pushing past that and we thought, well, if we could do a full body, that would be great. We do have a design for a full body spinning mirror. We'd want to wait until a real engineering company can build it because we'd probably just cause a big mess if we tried to build it ourselves. So until then, we've kind of unfolded the optical path and we've put lots of video projectors behind a kind of a special screen that spreads light vertically, but keeps it focused horizontally. And we have kind of a full body automultiscopic display that you can come up to. And the content that we show on it that's sort of the most interesting application that we have are these interviews that we've done with the USC Shoah Foundation with survivors of the Holocaust. And at this point, we actually have about 12 different interviews, each of them with about 1,000 questions and responses with the idea that we are hopefully preserving these stories for the future. These are about half of the cameras that we have running when we recorded the people. In fact, actually, the first one of them that we recorded in the last five years unfortunately passed recently. So we're glad that we got to it at least when we did. And we can either do view interpolation or 3D reconstruction to try to create a 3D version of this. And then this can be displayed on the display so that if, for example, Koki has a question of Pinchas Gutter here, he can sit in front of him, see him life-size, and see him speaking and answering questions. There is an AI system built by David Traum at the Institute that recognizes the question, knows what responses are available, and then plays back the most appropriate response. Since this is historical material, we wouldn't even think of trying to change the story at all or try to answer the question more directly by guessing at what they would have said based on what we do have them said. But in other applications, you could imagine that people are asking questions about some product that they're thinking about buying, that you might have a database of responses that you try to mix and match from, for example. So that's nice. The biggest thing that came to life seeing it in automultiscopic 3D is really the hand gestures. It's the way that the human communication uses a volume to kind of communicate. Because in the 2D, it just always, that doesn't feel as fleshed out. But being able to see how people speak with their hands makes a big difference in terms of the realism. So ultimately, if we wanted vertical parallax, we'd need to put projectors going up and down as well as left and right and use a kind of slightly different kind of screen. It was kind of images like this that made us start thinking about not just projecting light fields, but recording light fields with arrays of cameras. And then the very last thing I'll talk about is some stuff we've done with synthetic light fields at Google VR, or captured light fields that we're doing, where basically the original idea was if we want to give people a VR experience of being able to move their head and see something from every possible angle, we could record it with an array of cameras trying to capture all the rays of light coming into a volume, and then in real time maybe we'd be able to reconstruct the views that you'd get according to the two viewpoints of where you're moving your head to. And at this point here, you know, this looked like it'd be pretty expensive to build. So you know, there's like 126 GoPros. So of course, we want to be frugal with what we do at big companies that we're at and things like that. So we're actually very lucky that Steve Seitz's group had built and created this really neat and well-engineered and useful device called the GoPro Odyssey, which would go into their jump stereo stitcher. And it was an array of 16 synchronized GoPro cameras. So since we got to Google, they generously gave us a couple of these. We sort of less generously tore them apart and started building them into other things. And so the idea is that these single synchronized GoPros here will rotate in a about 30 seconds or so as they film video and will capture all the rays of light coming into the volume for a still scene using that. So here's kind of a long exposure photo of the GoPros going around. These are two virtual viewpoints that you might want to create in a headset based on these images and using light field rendering as presented by Microsoft Research and Stanford University back in the mid '90s. You can actually generate proper, true views from anywhere by grabbing pixels out of the cameras that saw those rays of light as those rays of light came into the volume. And you might have to do some view interpolation stuff in between here to get the intermediate rays. But since the cameras aren't too far apart, that's a tractable problem. And I was very lucky to work with some people in Mountain View in San Francisco who were working on light fields at the time for the rendering parts. And so we just kind of built the rig and then since we had our friends at the Smithsonian, they gave us a pretty good object to try to shoot light fields of. So the same folks who asked us to do the scans of President Obama asked us, would you like to shoot light fields in the Space Shuttle? And you realize, wow, that's exactly what I want to do. And we actually got to bring our device over to Washington, DC, again, spin it around inside various fun places in the Space Shuttle, and then do light field rendering. So you can see basically each one of these little disks represents the pixels that we're going to use coming from one of those original cameras, and we've shrunk the disks down to the point where they don't overlap and you can kind see where all of the imagery data is coming from. The camera's actually spinning around, but it's also translating through the scene, generating 3D parallax as it goes. And if you expand the disks out a little bit using algorithms developed by Ryan Overbeck and Dan Erickson and others, then it actually produces a pretty good looking image of the scene as you move around in it with six degrees of freedom, including specular reflections and motion parallax and stereo that works well if you look up or down or twist your head and such. And so that's one of the scenes that you can see if you try out some of these light fields. And we also experimented a little bit with portraiture. These are two artists who decorated their house with a bunch of shiny tiles, which already suggested a pretty nice light field. And we brought our light field device there to kind of do a portrait of Sherry and Gonzalo here. And we realized when we took pictures of people with this that there was a problem is that, like, well, where do you tell them to look? People like knowing where to look where you're going to take a picture. And so the first thought I had is I put a piece of pink tape on the middle of the device and said, look at that. Stay still while the cameras go around, but just try to focus on that. And we got some light fields that way in testing. But the problem is the subject always just sort of were like almost eerily just staring off into space. And so you could look at them but like, you know, it's like you move into their gaze, you can move out of their gaze. They just kind of looked a little creepy and frozen. So we had another idea, which is let's have them follow the middle camera as it goes around. So I put some green tape here and on the front and on the other side and I said, watch the green tape. And it turns out that's actually way easier for people to do because it's easy to-- like we're designed to just track stuff as it goes ahead. It gives you something to pay attention to so you're not fidgeting anywhere else. And the result then is that it-- basically, all the photos, it looks like they're looking back at you. So if you play this back in the light field, then their eyes are basically tracking you as you move back and forth and they're paying attention to you. And so we found a different way to create something that's creepy and-- [LAUGHTER] --and a little weird. But of the two, I would choose this one I think because you feel more present with them at this point. So that's the last thing I brought. I just wanted to say if you want to try out any of these light fields in a Vive or a Rift or Windows Mixed Reality, go to the Steam store and there's a free download of Welcome to Light Fields. It's got a 7 and 1/2 minute experience. We got to go to like six different places and meet Sherry and Gonzalo. Have them stare into your souls. And I hope there's time for at least one question. Here's a couple of websites, a lot of collaborators, and some thanks to the people who made the talk possible. Thank you so much. [APPLAUSE] ADITYA SANKAR: So we're a little bit over. If you need to head out, that's all right. But otherwise, we can maybe open the floor for some questions. I got one. So there's these amazing huge rigs that you build, obviously, are electronics, lights, microprocessors. Can you talk a bit about the engineering that goes into building those things? And how do you diagnose issues? Like how do you fix these things when they break? PAUL DEBEVEC: Yeah. Well, so it's a great question, and a lot of it comes down to one person that we've worked with named Xueming Yu, is his name. He's kind of our electronics person in the group. And I started working with him about 10 years ago at the USC Institute for Creative Technologies. He was a master's student at USC. I hired him as a master's student. And the embarrassing thing is I hired him as a software developer because he was actually studying software engineering. And once I got him there and he saw that we had some of these what must have looked like terribly clunky light stages to him in terms of the electronics, he said, you know, I have some electrical engineering background. I can actually, I could probably do a circuit for that. And I said, oh, OK. Maybe you could do a circuit for that. And made a nice circuit. So we just moved up to the point where now our Light Stages are, like, every light is a custom circuit board with a microprocessor, an FPGA, we can put any color LED anywhere that we want thanks to Xueming for this. So he also comes to the Light Stage shoots. And so like, you know, we have an actor there and we want to look like we know what we're doing, we want the Light Stage to work. Sometimes like, oops, the cameras are misfiring or the Light Stage isn't advancing the pattern. And so we have to have him look at a circuit board, move a little thing, change a cable. He's good at that kind of stuff, and he's gotten us through a lot of shoots for that. Fortunately, in the movie industry, it's, you know, it's expected sometimes. Like, OK, the camera's having trouble so people wait around. Like A-list actors, they're totally used to sitting around bored while people mess around with lights and cameras. It's like part of their job. So that works. In the case like when we were at the White House, that that was a bigger worry. And we had circuit boards that Xueming had soldered days before in there. And unfortunately, there's like a certain process to get people approved and into the White House. And so we're trying to bring two Americans, a Canadian, and Xueming, a Chinese citizen. And in terms of the timing, the one person who we really should have been, if we only had one person there, you know, was waiting down Pennsylvania Avenue at a Starbucks. We were so sad that like he didn't get to be at the thing. Turns out that before he came to the scan-- and this is like the first time anything like this happened-- Obama walked to Starbucks. He walked to that Starbucks and went and got a coffee. Like totally surprised the Secret Service and everything too and so they had to go and deal with all of that. So anyway, Xueming still got to be part in a little way in that way. We're very lucky to have him. ADITYA SANKAR: Maybe one more question. AUDIENCE: Just a question of the infrared in the-- is it possible using the IR Cloud 3D [INAUDIBLE] scanning of a model using the Kinect? [INAUDIBLE] PAUL DEBEVEC: That's a great question. So can you use the Kinect depth sensors to help out with the Light Stage scans, right? Absolutely. So there's actually a really nice piece of technology that Microsoft Research came up with. Rick, you probably tracked a little bit. This is like the project Steve Sullivan took over with the holocap-- holoportation, holocap. And also, yeah, Shahram Izadi, who's now at Google, used a bunch of Microsoft Kinect sensors to kind of digitize somebody in real time. I wonder, like when we did the 3D teleconferencing project, the Microsoft Kinect wasn't out. We probably would have tried it out as the way to kind of scan the face in real time to transmit it if we were doing it at that point. We have on occasion put Microsoft Kinects into Light Stages, but kind of the level of geometry that they're designed to get is much lower resolution than even the base mesh we can get through photogrammetry on the Light Stage. One technique is just to know if the person's in the right place. So you can kind of scan the face and then know if they're-- you could even tell people, OK, move forward a bit, move to the right a little bit, and now you're in the right place for the scan. But we haven't directly used a depth sensor for any of the Light Stage scanning that we've done at this time. AUDIENCE: Thinking about putting the infrared on each one of those LED lights that you have. PAUL DEBEVEC: That's another super good question. And it's kind of funny because every time we're designing a new Light Stage light, at some point we plan to, OK, now this is the one we're going to put an infrared LED on there. And then at some point, we realize, like, well, we're just not sure what we're going to use it for. And then gets replaced with a white LED or a lime-- there were new lime LEDs this time, and like OK, so we have put lime LEDs on, right? But at this point, we have yet to put an infrared LED. And then we end up regretting it at some point. So we need you to tell us what to do next time. That'll help. Thank you. ADITYA SANKAR: I think Paul may be here for a few more minutes after the conference. Questions off-line, but besides that, let's thank Paul for a great talk. [APPLAUSE] 