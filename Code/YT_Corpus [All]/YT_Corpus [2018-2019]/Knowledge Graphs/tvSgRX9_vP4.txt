 welcome everyone my name is Michael Moore I'm the executive director of be wise knowledge graph practice I operate across all sectors and I'm joined here this afternoon by Omar Azhar who is from our financial services organization who is one of the principal scientists in our advanced analytics and machine learning center of excellence right here downtown in five Times Square so today we want to talk a little bit about knowledge graphs we're going to share some perspectives that are based on our engagements and and we're going to give you a little bit of a how-to and maybe a little bit of a roadmap for thinking about how you can incorporate graphs into your infrastructure how you can take those data graphs and turn them into knowledge graphs and how you can leverage them in the long term so our agenda today is I want to give you a brief overview of Ernst & Young we're going to talk a little bit about why we are so excited about graphs and graph technology we're also going to spend a little bit of time on how knowledge graphs are being used across a variety of of use cases within financial services we know that there's a lot of folks from the industry here today so we picked financial services but keep it keep in the back of your mind that a lot of the use cases we're going to touch on are actually broadly applicable and then finally we're going to give you some tips and tricks on how to get started how you can run some conversations with your own IT organization and and get moving on deploying graphs so we are very pleased to be able to sponsor graph connect I've been working in graphs for four years I think this is the most exciting transformative technology that's come around since the advent of sequel I firmly believe that within ten years probably 50% of the sequel workloads will be all running on graphs at Ernst & Young we're a services organization we work closely with emerging technologies we have the ability to you know help organizations around the world be successful with those technologies and solving real business problems and we offer a number of different services just like any other consulting firm but I think what we like to differentiate ourselves on is that we work on really hard problems we tend to have small teams working on hard problems and trying to drive high value of outcomes are our data and analytics practice globally is deployed in over 150 countries we have well over I don't know what is that number six thousand or so analysts and developers working across a whole number of capabilities from strategy to analytics to transformation implementations and then just running business operations as managed services um we do things all the way from you know designing data warehouses to moving data to building bi and visualizations to doing things like robotic process automation where we can you know apply machine learning to documents to make sure that they're being processed more accurately and efficiently things like that if we look at some of the tools that we're using this is you know this is the typical group of tools we we do a lot of work with large systems I'll show you in just a second some of our alliance partners but you know we're we're fully stood up around using all the best practices around note booking we do a lot of work we have many are developers Python developers Omar's group makes extensive use of tensorflow I make a I make a lot of use of h2o AI and then of course a lot of capability around all the common bi tools and so our whole our whole goal around around our data analytics practice is to quickly find value out of data and then push that higher up into the organization we are our mix of onshore versus offshore we tend to be about 30% onshore and near shore and then we also have some global delivery centers that we use for some of our managed services or more straightforward migrations and things like that we have a number of Global Alliance partners some of the notable ones are Adobe we have a global alliance with Adobe so we have a number of consultants that are very well-versed in the entire mobi stack we also have a big practice around sa P or a global alliance partner also with Microsoft and we are actually involved in some Co development work with them and and then and then we have a number of other interesting alliances so for example I'm one that's of interest to me is we have an alliance with IBM for example and and which opens up some interesting possibilities about large server on Prem deployments of graphs and things like that all right so that's enough about UI how many people are here at graph Connect for the very first time raise your hand excellent how many people have actually downloaded neo4j and built the movie graph okay that's a relatively smaller number all right so I'm gonna be gentle we're gonna start slow off and so the first thing that I want to talk about is what is a graph okay and and this is something that actually this is probably the Mayu most used slide and all of my decks so when I we go talk to business leaders we always start right here but so what is a graph so a graph is a is a visual representation of how data is connected how different things are connected and graphs are very useful for describing processes and you know this because anytime you give somebody a dry-erase marker and they and you ask them you know tell me how your business works or how this system works or whatever within minutes somebody is drawing bubble with arrows two more bubbles and boxes and so forth and they began to use a graph as a construct to explain how a complex process works and one of the really fascinating things about graphs is that you have this interesting dichotomy between nodes and relationships sometimes nodes are called vertices sometimes relationships are called edges and what's interesting about this is that from a design perspective you can actually leverage this in a couple of different ways so I can take I can take my graph representation and I can create a database in something like neo4j and the schema of that of that database can be an almost exact replica of what was originally described and one of the most important features for driving graph adoption in businesses is they have this very high degree of what I call semantic fidelity and so as a data developer you can have a very very informed discussion with your business counterparts and you can say you know here's how I understand your business works here's the schema I've built may I show it to you and in the synchro world that would be a horrifying statement right but in the graph world that's actually kind of fun and you can sit right down with the business leads and you can say this is what how I understand it works and that business leader can basically say no no it doesn't work that way there's an arrow missing and you're like great will create another relationship and so there's a very nice agile flow around graph development and so this slide here note graph that we've described and you guys are have been following along it's just a simple ecommerce scenario where we're trying to drive somebody to a website and so in that in that scenario I I'm sending email to people I'm trying to get that person to visit the website and when they get to the website I want them to buy a product that's sold on that website and so that's what this little graph shows and in this schema we've implemented a graph database that does exactly that and in the graph database I have my nodes for my emails and so think of a whole bunch of individual emails each that each will be labeled as an email but the contents of that node might be the specifics of what that message was I have a bunch of customers so these are this is my person node and I have I would have a whole bunch of person nodes and every one of those nodes would carry the details of that individual I would also have a website node and maybe that node would have a bunch of pages associated to it and then I would carry all the details of the pages I'd have my product nodes I'd have all my product records and now whenever we do one of these executions I have the opportunity to set a relationship so we did send an email to this person so I can set that relationship now I have a con now I have a concrete connection between a group of emails that were sent to a discreet individual did that individual take action did we see them on the website if so I can set another relationship yep they visited the website right now did they buy something and and and and what was it that they bought and then I can actually connect the product node so I know what my inventory on the website is because I can see that it's sold on the website and so the real thing that I'm looking for is did they buy the thing that was sold on the website now if you were to write a sequel query and you wanted to actually find out how many individuals went through this pattern you'd have to touch at least four different tables you'd have to do a lot of recursive query to figure out how many paths existed across those relationships in in a graph world instead of figuring out what tables you're trying to put together and what keys you want to joint use to join those tables you declared a traversal path and I know this sounds very simplistic to those of you who are used to graphs but it's important to remember that the biggest shift around thinking about graphs is to begin to imagine how you traverse the graph and so here we see some example code and in that code block this is how simple it is to query this graph and come back with this pattern and so here we're basically saying we're going to send this email to this person and because neo4j has a property graph we're actually gonna filter for only the emails that were sent to a guy named Steve Newman and we're also going to apply the constraint that Steve will have had to have visited the website and we're going to apply another constraint that he's bought a product that's been sold on that that's been sold on that website and now given this complex cyclic query which is a it's a correlated sub-query right represented in a single line of code then it will return all the records that satisfy that pattern that traversal pattern and so and so graphs are very very good for doing these kind of complex traversals across large landscapes of data and one of the interesting things about graphs is that all of the logical possible relationships that would exist between two data entities would have been pre computed and stored in the storage structure this makes them very very fast at query time so a little slower to write to but they're very very fast to query much faster than sequel the other thing that's very important about graphs is that you can query through traversals many many many different entities and still get a very linear consistent response you'll never have the graph database just simply fall over and throw up its hands and say I'm out of memory which happens all the time in sequel and anybody who's a sequel practitioner you know you do a lot of work with query optimizer and hinting and the ordering of the tables and all this kind of stuff so that you don't run out of memory not a concern in a graph so what are some of the use cases for graphs this is a slide I'll admit it it was a better slide than I could make myself and so I stole it from neo4j awhile ago you'll recognize it but this just gives you a really nice view of some of the some of the use cases around graphs and so one of the most common requests we get from our customers our customer 360 recommendation engines marketing attribution Enterprise search and a host of others and so we'll touch on some of those in a second so why are we excited about graphs several reasons first one they're super super fast on query time which means that you can use a graph and run a really high digital experience directly off your graph they're easy there are great development targets they're iterative they're forgiving you can figure stuff out with a graph you can load your nodes set a bunch of relationships and scratch your head and say you know what I don't like the way those relationships look so you just blow away the relationships and redo it no big deal they're very impactful and so there's a there's several ways of thinking about graphs so neo4j is an OLTP optimized in-memory graph it's wonderful for real-time applications and and then you can use graphs when we talk about this a little bit later for use it for doing machine learning and you can do that actually inside the graph you can do it using other OLAP graph structures and all of that is a really great ecosystem for driving really impactful use of your data and I believe they're fully transformative and so a lot of the work that we're seeing as people start off with proof of concepts in a single data domain and they very quickly realize the power of this construct and they want to add additional domains and it really just becomes a matter of rolling in and designing additional new edges and nodes for that graph and bringing them together and so it's very it's it's well within the realm of possibility and there's and there's many use cases out there where large enterprises have basically said you know what we're gonna use this graph to span many more data domains than we typically would with say this reporting system or this operational application and then finally I look at them as being highly strategic and they're very very good at unifying and mobilizing data and bringing that out to front-line experiences and so in my personal opinion you know data work is great but unless you can mobilize that data and bring it out to customer experience you haven't been as successful as you could be and so let's talk a little bit about data leaks I love data leaks they're so beautiful and clean and it's such a great vision like all your data in one place it's like wow that's really where I want to be right but then you start to very quickly hear what's probably the most common core lie in the data world which is we'll have the data like done by next year and I hear this every year right and the fact is of course a data leak is never gonna be done right and as the business of Olives and grows more and more data sources come in there's lots of unstructured data now being held in in data lakes you have whole topic areas where no one's even gotten around to thinking about it and so they take a whole snapshot of an Oracle database or a terabyte ah you know and then they dump it into the data Lake and like oh yeah we have an s3 you know bucket that has all of the data from our Oracle system and I will have you looked at it and I know we can't get around to it this year and then of course there is some there's often a lot of attention on specific data domains they tend to be highly conformed and curated and that's great a great place to start working and then of course there's streams and so you have streaming technologies that are being stood up all the time around core processes and this is a great architectural model now the publish/subscribe model so the reason we like graphs and I'll say it again is that we believe that graphs are a terrific accelerator for mobilizing data and so this is kind of a functional architecture here and what we're really talking about is putting a graph layer over your data warehouse or your data Lake and what that gets you is it puts you very quickly into the world of real-time queries you're out of batch mode it puts you into the world of being able to iteratively and in an agile fashion evolve your data your data presentation with how the business is changing and typical consumers of graphs would be things like mobile and web applications real-time bi and scorecards your data science teams and you know the another truism is that you know CIOs and those folks they don't necessarily care about the data itself they're more concerned about the containers the people that really care about the data the quality of the data what the data means what are the implications of the analytics these are all of the end users across the business you know your marketing departments your ops departments the folks are involved in privacy and data it's product recommendations and development your sales teams these are the people that need to see accurate data and they need to be able to leverage all of the other things that graphs can do in turn in terms of providing wider scope for their queries machine learning etc and so our vision for advanced analytics rests on basically four pillars so our first pillar is natural language processing and we have a lot of we've done a lot of work in this area and we believe that this is a really important area of focus because it's where you understand intent it's where you understand sentiment it's where you understand the nuances of how humans are interacting with each other which you can't typically get out of structured data machine learning is the the usefulness of machine learning is dramatically improved in scenario when you can expose it to some NLP sourced information the other thing the next pillar is we've done a lot of work around open platforms and we've actually built fully containerized platforms in Azure that that have the entire Apache stack from top to bottom and we use that we use those kinds of environments to do large models and we also use them basically as test beds for customers to come and try out new ideas without getting into a lot of technology risk and of course knowledge graphs were big fans of knowledge graphs we believe that ultimately this provides the best path for a full 360 view of your customers whether you're involved in b2b or b2c if your customers are external or they're internal customers and and furthermore what really constitutes a knowledge graph is when you take a data graph which might be just a graph based representation of your sequel environment and then you begin to apply machine learning to that data and then you write those results back to that graph and then finally you have to get the data out of the data environment and out to the front edge and so we have we've placed a lot of bets around technologies like nodejs micro-services streaming because we want to be able to set up very very low weight microservices so that we can actually do things like put a recommendation it's very intelligent recommendation in a widget that might be being rendered in a relatively unintelligent web experience right so rather than doing a broad you know IT transformation we're really looking for narrow of narrow pipelines where we can push really smart analytics right out to the front edge and so with that I'll hand it over to Omar thanks Michael all right so a little about myself right so I started off as a quant and Finance you know financial engineering and applied mathematics and pretty soon I started getting the machine learning right the linear algebra and the optimization theory the same mathematical frameworks they use in financial engineering are pretty much used in machine learning as well right and so within advanced analytics what we do is we help our clients create build and implement practical AI and Big Data strategies right and kinda revolves around three key pillars the first is you know understanding the customer to drive the growth agenda and customer experience taking out a process end-to-end and applying the right rpm machine learning frameworks for intelligent process automation and then better signal for wisdom control right and so with machine learning AI Big Data all the big buzz words every executive wants to know how do I use this is this just smoke and mirrors just buzz words how do i leverage this to get an edge and create process efficiencies or gain better insights right and so one of the big things is obviously naturally expressing every day a whole slew of unstructured text is created from all the various of communication channels that exist right especially in financial services you know you can have a call center if you're getting thousands of calls from clients every single day you want to know what they're talking about what products are you talking about what's the type of conversation what's the sentiment you know is my agent handling that call properly or trying to get them additional training in wealth management you want to know how your financial adviser to talk to their clients right how are the clients related to one another in sales and trading you could be a desk that's getting thousands of emails a day from institutional clients these could be anything from collateral Margaret margin obligations or price verifications or just some sales guy yelling at you and so natural language processing has immense use cases within just the financial services industry from back office in front office no matter where you are right and so using natural language processing we can extract a lot of the features from the conversation right what are people talking about in terms of context or topic the entity is being mentioned is it an apple stock McDonald's the country of USA or Russia the sentiment and the intent and you and you pair up these features that you're extracting with your domain ontology and domain hierarchies you actually get very good practical use cases and a lot of companies are now starting build you know it's very targeted NLP pipelines across their businesses but those are great tactical use cases for NLP right how do you actually create those features that you're extracting in the use case that you have how do you make that into an actual strategic asset well we view this is once you've got your anthing pipeline start pushing those communications into graph and now you're actually building that communication channel as a natural asset right you're now building a network of communications all your clients are having your editors now relate your clients just based on the topics that they're conversing on because the graph makes that communicate that linkage natural right and as you keep expanding on this and you add more and more domains you start getting to very intrinsic like link natural language about just your own enterprise all the different communications are having within your business you're starting to build the conversational intelligence platform and now all those NLP pipelines that you had they're recording going through your call centers going through your emails you know going through your FA to client conversations you're actually starting to build that up as an actual strategic asset that you can use going forward to better understand what's happening is your business and where your clients are actually talking about your customers are talking about so now we're starting to build essentially a communication graph and you're trying to saying how your customers are laid to know one another one another just based on what they talked about but then you can start adding in additional things right so in this particularly very well managing client wealth wealth management business you know you've already started building your communication network you know how your FAS are talking to their clients do you know exactly what the points of conversations have been what products are talking about you know what are the different trends but in disturbing an additional data sets right bring in your transaction data bringing your account level data bringing your fa data bring in a different career aspect that you know about your client starting this 360 customer 360 view right with a knowledge graph a customer 360 view is very natural innate everything you want to know about that one customer is right there on that note a few traversals away right and then from that you're connecting how that customer is related to the other customer as well if they owned the same stock they're automatically related just with a few of traversals in the graph but this 360 view that a graph arrives is more than just for customer 360 and trying to build a better understanding of your customer building next best action frameworks and recommendation engines you can also use it for the other three pillars we're talking about a big one will be conduct surveillance right collusion or conduct and sales and trading or even in sales you want to know if sales reps are pushing stuff that they shouldn't be pushing or this talking in ways that they shouldn't be talking to clients you know how do you detect an anomalous behavior amongst your traders if you start using this 360 view of the knowledge graph and your sales and training as well start putting in all the different communication channels to have or who are your trade is talking to what are they talking about right what are the different actions that they've taken you're now starting to build a sub graph of each trader and using anomaly detection you can see okay I have nothing but a group of equity blue chip stock traders I would expect their sub graph to be relatively the same right so you can start doing some anomalies anomalous spacecraft detection to start detective media collusion behavior or insider trading or you know any sort of abnormal conduct that you should maybe look into and that 360 view that the graph provides you know makes that very natural for you to look at in terms of your data but now let's go back to the wealth management sample right so now you've started adding in all these different data sets you're adding all these different domains and you're building this enterprise knowledge graph which includes all your customer 360 views the various different products you have you know all the different customer conversations you know and then how do you make it smarter now right it's already pretty intelligent it knows everything it knows about your business all the information you want to know is at your fingertips you can build search engines what have you well you can make it even smarter and more proactive by bringing external data right so something like external news data sources you can start appending that to your own internal graph in this example your own internal business knowledge graph might end up at Apple as the farthest in terms your product domain right you know every single customer that owns an apple stock and all their FS that are related to them but that's only your internal business data once you start bringing an external data sources you can start knowing okay what are the external events happening around Apple and then how does that impact maybe my customers portfolios right is there some sort of action I need to take knowledge graphs make that traversal information very easy and completely natural right so in this example if we start appending external news that's happening and start putting them into the relevant products that I own within my business I can start using natural image processing or other machine learning frameworks to detect significant events right if a significant event happens in one location of my graph I can add in business rules and domain logic to automatically tell the graph hey here's the specific type of event I want this information to be passed over to this particular node in this particular domain of my knowledge graph so in this particular example an event happens on Apple stock you can immediate you can make your knowledge graph intelligent and actually take actions on its own by putting in the business rule okay a significant event on this stock I want you to send information to all the FAS who have clients that have positions in this stock and that I think it's a notification and it says ok this here's an event maybe you want to look into it here's a news article right you're starting to make this graph now intelligent and now you're baking your business proactive and all you have to do is really you're going to start with simple rules right this information needs to be sent here whenever something like this happens you only need to go into complex machine learning or AI or deep learning frameworks because every piece of your business is now connected and the way it should be all you need to do is say hey this information seems like it needs to go here well you can do more than just information retrieval force a customer and actions you can also build a knowledge graph as a massive calculation engine right in banking we have what we call C core stress tests where every single Bank has to build these massive number of models to stress test across the variety scenarios that the Fed gives them this is generally a months-long process and banks take you know anywhere from 8 to 12 months to complete these stress tests all the way from identifying their risks building the scenarios that they think are going to actually stress their institution build the models that act there accurately capture their business and then also then aggregating those model results into different line items that they have been sent to the Fed or the SEC or to the shareholders in terms of their revenues their balance sheets and whatnot that's just an information flow right and a graph can actually capture all that as well if you start graphing or your risk inventory that you identify then connected to the identified macro variables or market variable that you said Drive those risks then those variables are the ones that are also are the drivers in your models and non models and then those models also didn't have the results of those model also aggregated up to financial statements that's just an information flow that you can start putting in as a network of models using a graph database and now you've built yourself an Enterprise calculation engine if you're a CEO you put all your models into this not models and you link it up into a knowledge graph you can say hey you know let's see what if VIX goes up five bits tomorrow morning and adverse scenario what's the impact to my balance sheet right now if you ask someone that that's about a three to four-month process but if you put it all your models and use the graph as just a natural information and it's just a calculation engine where one models input goes into the other as it should and then they all get aggregated that's the very relatively quick calculation right you just put that into whatever app you build for it and if you change one note and the example if I change a little information about the VIX at time series of VIX all the downstream nodes will be impacted and then I can then just look at the people art which stands for pre provision net revenue so say the net revenue line item and say okay this was the impact from that change in notes so knowledge graphs can also be used so anywhere where information needs to flow and data needs to be linked right whether it's building customer 360 views whether it's doing anomaly detection cyber or even like linking in a massive network of models I mean for some of these banks these are like 800 to a thousand different models that can range from anywhere from regression to ratio based to complex Monte Carlo and stochastic calculus models and then you can also do a lot of cool things right once you've built your entire business in a knowledge graph you've kind of built this layer it sits on top of your data systems you start building very intelligent enterprise search engines one of our large private wealth management clients came to us recently and asked you know I'm just a business guy I've got my data spread across multiple different data systems and I get my weekly reports from the tech teams that's nice and I get that to my clients where every now and then I want to just retrieve information and make some ad hoc reports but I can't do it because a I have to go talk to the tech team get them the variables that I want and then they pull it for me it's the cumbersome process I just want my data when I want it how I want it right and we said you know what this seems like a classic knowledge graph plus natural ends processing problem for you you've got your structured data across maybe a dozen or so systems if we just put a graph database on top of that using natural language processing we can give you a search bar you just type in what you want right using natural language they'll be able to interpret the intent the named entity that you're talking about the timestamps will have you convert that to a cipher query and pull the rel information from a graph and that's just the beginning of how you would go about building an enterprise search right and then from here it'll output I mean what I've shown you is just screenshots of the proof-of-concept that we built for them no they just type in what they want and it'll give you an excel file that they can to go put into tableau or Spotfire and create their ad-hoc reports when they want now I'll hand it back over to Michael we'll talk about you know how you would actually go about starting building an enterprise graph all right Thank You Omar that was terrific okay so the next few slides are basically a little bit of a cookbook on how you can get started so the basic process in getting to a data graph at enterprise scale what we've found as a general method that works well is we will after we've done some you know we've we've done some locating of where the data is etcetera the basic process is we'll we'll build a bunch of queries those queries result in what I call graph form tables so these would be deep tables of either nodes or relationship maps between nodes and then what we'll do is we'll just do a simple select star query from each one of those tables now I get to why we do it that way and then we create a zipped a compressed CSV file and then we load that into neo4j and the neo4j loader there's a terrific fast loading tool that can directly consume zipped csv files is highly highly efficient it and this is used for essentially your initial graph hydration what we found is the hardest part about building a new graph is everything that happens before the graph so you know when you're starting to deal with really high volumes of data and you're trying to do things like map relationships and things like that you have to observe all of your key custom key constraints you have to make sure that all your string handling is done properly and there's a bunch of things like that and so what we end up finding is that we have to iterate a lot on the core tables they're going to go form the graph and so we'll spend you know weeks working on those first tooth you know those first two major steps but then when it all comes together as you can see there on the little black screen that's real output from a graph that I built about a year ago on Azure 500 million nodes 2 billion relationships hour and a half just terrific performance and of course all of that is being validated as it's being loaded and being a database neo4j properly will complain very very loudly if it can't find a key right to create a create a relationship between a node or if you violated a uniqueness constraint all right so the next part that's important to realize is that as you're talking with teams around your company so in this first model if you're fortunate you've got a pretty good data Lake pick some data that's already been worked on a little bit and then you're basically in this business of creating node and relationship tables this is a good accelerant for your projects typically that data will already be cleaned someone will have thought about it and so forth another common scenario that we see is our clients will have legacy databases and what we find here is that this is a little bit slower and I'm sure your folks are aware of this but you'll have to do an individual extraction from every one of these databases and what's probably the slowest part of it is just negotiating access to that specific system and then finally we love working in streams streams are very easy to bring into a graph we typically start thinking about streams after we've done our initial graph hydrations and we're talking about updates there's a variety of ways to update graphs you can use you know incremental CSV loads you can have applications updating graphs through the various drivers for example that neo4j supports but streams are a great way to go and very scalable now let's talk about how you go from that data graph to to a knowledge graph and there's several patterns so the first pattern and which is probably the most common pattern is and we do this will extract data out of neo4j it will typically need to be extracted in some kind of tabular format that will have all the features that are necessary to do the model build we'll push that into some modeling environment like our or h2o or tensorflow and then we'll take the scored results and write that back to the graph little clunky it works it's very powerful leverages you know the best abilities of each platform this is another pattern that's starting to emerge and this is the pattern of using a distributed OLAP graph processing and so we've had some really great discussions here at Graff connect about the great work that neo4j is doing with spark integrations and so I believe that this pattern will become increasingly prevalent and so here you're talking about basically doing doing extracts from neo4j pushing those into graph frames inside spark running a Scala based modeling procedure and then updating from that output back up into neo4j now my favorite pattern and the one that I'm hoping will see the most progress in is of course in in graph in memory machine learning and there's been some really really nice work that was recently done by Michael hunger and team around a whole set of extensions procedures they're basically a plugin that you can put into your neo4j server and this this exposes for you a number of highly parallelized graph machine learning algorithms and this is a really powerful model because now instead of moving the data around to go where the modeling is we're actually bringing the modeling into the graph and then this can be very very fast as well and so where do we go from there so once we have the model scores what transforms your not your data graph into a knowledge graph is how you choose to write your predictions back to the graph and so you've seen this little graph here that we have on the side I did this for a POC this is a marketing attribution graph all those yellow dots are different marketing messages that have all converged on a single individual and so the question on the table is what marketing message should should this individual be exposed to next so that they will convert to a lead and if you look at this graph you can see that there's a similarity relationship so I'll get back I'll get to that in just a second but your so there's basically four ways you can think about how you want to make your graph smarter so you can take your predictions and you can just simply add them as properties to existing nodes this is commonly done another interesting thing that you can do this is good for things like clustering algorithms is you can actually apply additional labels to nodes in your graph so you can actually have as many different labels as you want in a neo4j graph all a label does is it just declares a set of nodes and so those and those labels can be fully overlapping and so that's kind of interesting one of the interest and one of the things that you get by applying a label is you basically get a free index and so you can immediately sub-segment your graph by labels of course you can take you know complex model results and and just push them in as nodes and connect them into the graph but well I think one of the most powerful models around pushing scores into graphs is to leverage the relationship and one of the interesting things about neo4j it is a property graph and you can put properties on relationships and because you can put properties on relationships right I can have my two data points and I can have multiple model runs and with each model run I can set a new model relationship a new predicts relationship that references which model made that prediction it may be a probability score or a confidence score a similarity score and that's essentially what we're doing here with these similarity relationships and so in this in this calculation we're computing similarities across every single customer to every other customer in this graph and that's and that's a very accessible kind of calculation in neo4j and then you write it out to the and you write it out as a as a relationship and now you can keep track of multiple versions of your models and of course because of directionality you could have different different predictions for the same two notes as you can see I've got a I've got some blue arrows where a is predicting B but I could just as equally have B predicting a and now I come up and then this becomes very compact language for representing knowledge in the graph and the most important ask all of this is that now that they are instantiated as relationships I can actually Traverse using cypher and pick out all the predicted nodes and you could have multiple linked predictions and this is what Omar was referring to earlier you get a multiple linked predictions extracting highly related data that you're essentially imputing using this combination of the graph and machine learning and that's a very powerful construct so getting started this is a typical sandbox this is implemented in AWS asher has very similar services and so typically what we'll end up doing is we run it you know we'll do stuff at hive will create a bunch of a bunch of those graph form tables in s3 and then we'll import those into say a an ec2 server that's running standalone in the virtual private cloud and what we always like to do is throw in a bunch of other tooling so that so that the data science teams can be working directly with the graph using the packages of their choice so we're going to finish up right there so how do you know you need to use a graph and I think rather than thinking about this as a technical problem I think it's much better to think about it as a business problem so if you are starting to hear questions like I have a large volume of incoming customer communication how can I better and understand what my customers are saying that's a graph problem how can i how can I get a better understanding of my customers to give them a better experience right how can I improve the way that I directly interact with my customers another good graph problem how can I be more proactive how can I separate signal from noise again a great graph problem my data is spread across many many sources how can I easily get access to it and come up with a better view without having to go through so much difficulty excellent work workload for graphs and then finally and I think this is probably the most important question what's the next best action I can take and in my opinion a knowledge graph a data graph that's combined with iterative machine learning is probably going to be the best pattern going forward for answering that specific question so with that I'd like to say thank you Thank You Omar and if you have any questions let us know [Applause] 