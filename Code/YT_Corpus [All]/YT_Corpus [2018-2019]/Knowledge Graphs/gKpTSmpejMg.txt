 so welcome my name is Federico Acevedo I'm a postdoc at the CBMM and today is my pleasure to have here some more report to file a former colleague of mine from the Max Planck Institute for Biological cybernetics so basically some where it'll be his PhD in the U at UCSD in computer science and then he moved to Tuebingen where he joined the department of bernard shaw cup and there we met and after his postdocs there he he came back to the u.s. as an assistant professor at the Toyota technological institute and after that he became an assistant professor in Princeton in the operations resent risks operations research in financial engineering center and in general he is going to be moving to Columbia in New York and today he's going to tell us everything we can know about model size estimation using kim ik-keun and graphics and their applications to clustering so without further ado thank you very much thank you thank you for the introduction and thanks for having me so yeah so the talk today let me just address this so yeah so my talk is going to be about clustering and in general but a particular form of clustering called density based clustering it the talk is actually based on the series of work over the last four or five years or so with Sanjoy Das Gupta Comerica Chapter II or recap on Roxburgh henry chang and jennifer Jiang so hopefully this thing is working okay okay I guess my clicker is not working so so I'll start quickly by defining what density based clustering is about so closer in itself most people here know what clustering is about clustering itself let me see if okay thank you so clustering itself is just about grouping points together you have a you have a data set you have a bunch of points and you want to group points into the closest to most similar into the most similar regions of points in density based closer ring the idea is how do we define similar we defined similar by just saying that the points belong together to a regions of very high density so in density based clustering a cluster is essentially just the region of very high density of the data and we might view grouping all the points together into this maybe four regions this is another cartoon where we have a ring here another ring another ring and those will be essentially my the Centers of the cores of the clusters and so the idea will be try to find these regions of high density and then bring all the points together assign all the points to regions of high density of the data so that the the base the basis of density based clustering the reason why it is quite appealing is because it's very flexible in a lot of those who are familiar with clustering here if you think for instance of a K k-means clustering you need to specify a certain number of clusters a priori in density based clustering you don't need to specify number of clusters a priori and it's very flexible in that sense that the number the regions of high density of the data could be any number of regions and so that so that flexibility is very appealing and it could also be that the regions of either city could have any shape and so you could have clusters of any shape and that's also very appealing in in a number of applications another appeal of it is that it allows you to define a mathematical object a clear ground through the mathematical object that you're trying to estimate from the data most of the time in clustering it's very hard to define what we even mean by cluster right what is technically a cluster if you define a cluster as a region where points are closest to each other you have to define what you mean by closest right so here by just defined in terms of high density we can actually mathematically define what it is that we are trying to estimate so we can then talk about the performance of the estimation problem there are many applications in medical imaging in text mining in speech in computer vision and at the end of the talk I'll try to get into some of these applications so the main motivation for this series of work for this line of work for us was that in density based closer in there are tons of heuristics that people came up with because of its appeal there are tons of your wrist extreme up with and the heuristics worked very well in practice however it's very hard to say anything theoretical about these heuristics it's very hard to give guarantees are they truly finding the regions of identity of the data it's very hard to to say so however there are tons of theoretical methods for which we can give this type of guarantees they would certainly with high probability find the regions of high density however when you try to apply the theoretical methods they don't work as well as the heuristics so that was our main motivation can we come up with a method that is theoretically grounded with guarantees that complete with the heuristics and so that's what I'll try to talk about today a series of work that allowed us to eventually arrive at that arrive at a method that is theoretically grounded and works just as well or better than the heuristics so first I'll talk about the various formalisms of density based clustering the various approaches that people have used so far in the series clustering the common idea to all these approaches is you cluster the data around so-called high density course you first define what you mean by a high density core you find a high density course and then you closer all the data around the high density course the distinction between the various approaches is really in how they define high density course what common to all the approaches is that we assume a priori that there is an unknown density let's say this is the unknown density there is an unknown density from which your data is generated you might view it as the population your data is sampled randomly from this population density and you define your clusters with respect to the population not with respect to the data you observe with respect to the population and then you try to estimate with respect to that population so here is a simple cartoon and let's say the data is sampled iid from some density F the density itself is not known it's a density over the space it could be a Gaussian it could be anything you want it's a the DES it is not known all we observe is the point from the density so one line of work is essentially the assumption in DB scan DB scan is one of the most popular heuristics for density based clustering and the main assumptions made in DB scan and similar algorithms is that the course what we're going to call the Centers of every cluster the course of a cluster are given by level set of this density so I have the density I cut it off at some level and then whatever point correspond to that color of the points whose density I above this lambda would be my course will be the core of my cluster so those will be the points of very high density so DB scan and a bunch of other algorithms use this sort of formalism for what high density course would be the idea wouldn't be estimate this high density course in data and then once you've estimated that I had the high density course you assign every other cluster to the high density course and please feel free to stop me at any time if you need better explanation so the problem with DB scan right away is you have to specify the density level at which you are you looking so this here is one specification of the density level but if I change the density level if I put it here all of a sudden the clustering changes completely so that's one problem with it is that the practitioner has to specify this density curve and it's not a quite clear a priori another line of work is a min shift and in me shift the cluster course essentially just mode of the density by modes of the density the maximum the maximum of the density the points at which the density is maximum the base can mean shift sort of assumes that the density is maximal at exactly at some points in in in practice it's not necessarily maximal at some point but it could be maximal on a whole set right but this is the main assumption within mean shift its maximal at some point and the problem is that mean shift can be very unstable for very general Maxima in practice a Maxima of an unknown density does not have to be at a single point the unknown density can be flat or approximately flat in some regions of space and whenever that happens if you think about this region for instance whenever that happens then the density the high density core is not well specified it could be any of the moonship min shift procedures could be picking up any point in this region and in which case you generate a whole different cost story so this is also not very well specified and in the end it gives you different answers depending on which sample you're getting from from the unknown density so that's a sort of the motivation here the other problem is that min shift even though it's a quite popular practice is very hard to analyze it's very hard to give guarantees to min shift it's also fairly hard to give guarantees to 2 DB scan and these are the two method that we are going to try to compete against we are going to try to do much better and compete against this method the key idea for us was simple try to fix this instability within Minchin so the idea for fixing the instability is rather than assuming that the the unknown density is maximal at some point we allow the fact that the unknown density could be maximal over some very general spaced subset of the entire space and that place were the density is maximal we're just going to call that a [Music] sorry that puts where the density is very much is maximum is what we are going to try to to estimate and we'll just call that a model set as opposed to a mode okay and so the hope a point enough for us is come up with a practical and optimal estimator of model set of the unknown density so here again in the cartoon the density here is maximal on this Rings here and so the idea would be fine try to discover the Rings and then close to the rest of the point around the Rings in this other cartoon here the densities maximo here try to discover all this point and and I sign everything else to the point now the difficulty here is that we don't know the location of the Maxima we don't know the shape they have we don't even know the dimension of the Maxima and yet we want to do this very general estimation the side benefit of this type of estimation is that it applies much beyond the problem of clustering it also applies to the problem a very similar problem called manifold denoising so in manifold denoising i have am observing a manifold plus noise and this is sort of the case here I'm observing manifold plus noise and I want to discover the manifold itself I want to do now is the manifold and we'll get the benefit of that by just understanding how to estimate model set any question up till this point all right so this will be the main goal for us once we can assume a model set will just fall back to simple clustering by clustering everything around the molar set so the rough intuition as to how we're going to estimate model set the key problem as I said earlier the key problem is that we don't know the location of the model set we don't know their shape we don't know anything at the very least will try to isolate them we'll try to get their location that's the first that's a first test that try to get their location and so to get their location the idea is a isolate point around the Maxima if we can find roughly approximately all the points that are close to a Maxima then we can within those point try to estimate the model set so how do you a how do you isolate this point we can isolate them if we can estimate the level sets suppose we can estimate the level set of the unknown density then we can isolate we can isolate the the regions where it's maximum so how the set here is defined simply this way it's all we considering an upper level set this is the unknown F we're considering all the points that I above a particular level so as here if we can estimate all the points above this level these are the points we can then infer eventually that there is a single mode here if we cross isolate all the points here then we could save actually that there is actually a whole flat mode in here so first step for us is isolate these things and then learn the Maxima in each of these level set the subroutine that we are going to use to do the isolation is called a K nearest neighbor graph okay and the K nearest neighbor graph is a simple graph easy to to describe it starts with it starts with a parameter K the number of neighbors and for any parameter K let's say that K is 2 then I take every point and I attach it to the two closest points to the two nearest neighbors and over time they build an entire graph called the K nearest neighbor graph it will turn out that the K nearest neighbor graph is the main object that we can parse quickly to obtain all the level sets of the unknown density and from there isolate the mode or the molar set ok ok so the outline of the talk is the following and this outline sort of follows the outline of our research program so first thing for us is understanding how nearest neighbor graphs relates to the unknown density once we understand how the nearest neighbor grass relate to the unknown density we can then think about ok what if the unknown density only has point modes so we're going to look at a simpler problem how what if the unknown density on response mode can we estimate them all once we are able to estimate them all we then go to the harder problem of estimating more general model set marginal Maxima of the density and I hope that that sort of makes sense once we get intuition here it becomes quite easy to understand how we do this last part okay so before I move on because I'm going to get into much more technical stuff in a minute I want to give you a small spoiler what do we get by doing this right what sort of clustering result we get by doing this and so here a result on a few you see a data set quick shift plus plus is the closer in procedure we get from this from this approach estimating model sets first quick sheet plus plus is the procedure DB scan is one of the heuristics means shift is another listing a quick shift is another very popular heuristic and only a lot of these data sets we can see quickly that the approach works really well we actually do much better than all the heuristics here by just doing this as this model set estimation first and here we didn't actually go and pick the data set so that I can show this result we just tried a lot of different that are certain the things kept working very well so well that we thought at first that there might be a mistake in the in the code but there was no mistake and I'll show you a lot more such result in the end the the scores here are called mutual information and then rank index shows us the two most common scores in in assessing the performance of a clustering procedure I can talk a bit more about them later so first thing is how do what do kanan graphs and code about the unknown distribution the unknown density F all right so I'll start off by giving you a bit of history so how can we characterize the density or the clustering structure given by unknown by audacity so we have a density over Rd over a general space and Hartigan in the in 81 try to apply characterize the proposed characterizing the closer structure through the so-called cluster tree of the density and the closer tree is fairly easy to explain I look at every level set every upper level set of the density and then in this level set I'll say if I pick this level set I'll say there is one cluster I just look at the poetic component given by the level set and there is this one connected component if I look at this level so there are these two core data component look at this one there I decided to and the whole collection of disconnected components per level set forms a tree it forms a tree in the sense that every other component here a subset of the components at the lower level right and the whole structure the whole continuum is called the closer tree of the unknown density F all right and so first question that how to gain post back in 81 is okay once this structure is defined is it as dimmable can we estimate it from data alone he showed that it is possible to estimate it from data if we are in are on are on the line but soon as the dimension of the data is - he didn't have an answer so chale chale durian Dasgupta in 2010 then showed the first consistent estimator by consistency I just mean that as the data size goes higher can we at least approximate the closer tree as well as possible right and child in the school time 2010 showed the first consistent estimator based on a clustering algorithm called single linkage then with phone Duxbury in 2011 we showed that a nearest-neighbor graph actually sub graphs of the nearest neighbor graph also estimate the gloucester tree and we show a slightly stronger notion of consistency and I'll explain that also in a minute so here first I'll give you an intuition why nearest neighbor graph can estimate the closet tree so here is a cannon graph built from a built on data from a mixture to gaussians okay and so I don't know if it's possible if you can see here all the way here so we just took a mixture of two gaussians in RD and you imagine a mixture of two Gaussian already has these two modes if you draw a point from the mixture most of the points will be concentrating on to the model and you'll have a few points in between right and that's essentially all this is and then we build a nearest neighbor graph on this now once the nearest neighbor graph is built on this the whole idea is the following start removing point in regions of low density and the way you do it is you pick any point and you look at you start with point whose nearest neighbors are farthest right you start with those points whose nearest neighbors are farthest those points must be in regions of low density so if I go back to the drawing I did here there are very few points here so if I wanted to look at if I build the nearest neighbor graph on this the points there are a lot more point here and so the point whose nearest neighbors are farthest will be the point in the middle somehow right so I take those foot and I remove them first and I keep removing it in that I keep removing point in that order as you're removing point you can imagine that the graph will start breaking up into sub components and the sub couple component would actually correspond to the kinetic components of the original density right so again here if I had looked at a level lambda and I start breaking up here after some time these points don't exist anymore and I have these two components and that's exactly all we're showing there over time as you keep going you might eventually get false breakups in there but the false breakup see themselves in some sense can be identified so quickly we have this theorem the theorem says the following it says that let F the unknown density be uniformly continuous plus some additional conditions or mouth conditions and let K the number of nearest neighbors be greater than log n n being the number of points you have in your data set right the reason you have this this here is simply that you can show that the original graph in order for the original graph to be connected you need K to be at least this large okay and then the the statement says it says this let's see 1 and C to be two disjoint kinetic component at some level of the original density then with probability going to 1 all the points coming from c1 and all the points coming from c2 at this joint in some sub graph of the original kanan graph one of the sub graphs constructed this way so in that sense they can and graph or sub graphs of the kanan graphs level set of the unknown density so that's the that's the main result does that sort of make hopefully that sort of makes sense right however I told you a second ago about these breakups here that you could get and that could become a problem we have to be able to identify false breakups and we also have a consistent consistency result that says that there is a simple way to identify when the breakup is is false in some sense when the break-up is just due to the variability within it but how exactly we identify it I can explain it a bit more later but the key idea is that if we are here in the if we get to this sub graph here we can look back down and see that in this procedure these things were connected earlier work on it ago and then whenever we have something like that that it's connected not so long ago we reconnect but I'll explain it a bit more later the key idea is that we can identify these things and reconnect them and that's actually the stronger form of consistency that I talked about earlier so in words let me put it this way if our looking at this mixture of two gaussians this is the closet tree it looks like this it's just a tree that goes up and has two branches the first theorem says that we'll recover a tree that might have a bit more than two branches however a subtree of that is the closer tree the second theorem is saying that I can actually identify this as a bad branch and remove it ok so after this work there were many refinements by various authors and some of them are very recognizable in the industry in the statistics community and but that's as much as I'll say for now about nearest neighbors the nearest neighbor graphs the key idea is that nearest neighbor graphs and code the clustering structure of the unknown density so that's the key thing to remember so now getting into this problem how do we estimate all modes of the unknown density ok so how do I submit all modes and to make it clear I have to first tell you all that a little bit about all that people have in order to estimate modes of an unknown density so what was known as the following so suppose this is the unknown density and this is a few points from the unknown density much of the work on mode estimation so estimating the Maxima of a density was started by I mean it was there before but much of the theoretical work started with this work by Sachi back overnight 1990 and in this case here he tried to understand how hard is it how hard is the problem of estimating the modes of a density but he considered only the point at the case of a single mode that the density had single mode so it has a single Maxima it doesn't have so many right and much of the theoretical work after that consider the case of a single mode and there is no theoretical work that I know that considered a case of multiple modes because estimating even a single mode happened to be that hard and here we want to estimate all mode and estimate all modes optimally the practical procedures such as min shift actually estimate all modes the theoretical procedures estimate a single mode and assume that the density has a single mode the practical estimate all mode but they are hard to analyze as I said before there are some consistency as some consistency result and it's very hard for us but we still don't have 4 min shift for instance we still don't have a finite sample result so for a finite sample how well do we estimate all mode we don't know but the consistency results say that if the sample size goes to infinity then eventually wind shift would estimate all mode so what we would is a rate optimal estimator that would be based on the nearest neighbor graph be the program of construction is the following a first thing for us to understand is to understand how well can we even estimate the unknown density so there is the density F we don't observe it all we observe is a sample from the unknown density and we all view F hat a whatever the F hat I mean an estimator of this intensity and we'll consider estimators that are based on nearest neighbors since we're looking at the demographic and the question for us is how good of an estimator is this estimator it turns out that the rate there were tons of analysis of this near this density estimator but the analysis were not quite satisfactory for our purpose the reason why is because therefore the differences try to understand how well globally does how well does F hat approach F on average and here what we want to is understanding how well F hat approaches F at every point simultaneously because we don't know the location of the mode and we sort of needed to understand simultaneously how well we could estimate this unknown F so that's sort of the first step that was the first step in our analysis the next step was can we even estimate a single mode can we estimate a single mode with a procedure that is practical the usual estimator for the usual estimator for a single mode is this theoretical estimator it says that let's start with some estimate of the unknown density F and then maximize this estimate over the the entire space over all of Rd this is an optimization problem which in itself just a pure optimization problem if even if we know if hat well this pure optimization problem is a tough problem is a hard problem on itself estimating finding the the maximum over the entire space so a practical estimator is the following X tilde is just the maximum over the sample just pick the maximum over the sample right this one is known to be an optimal estimator however it's not easily implementable this one is easily implementable but we don't know if it's optimal so we know that it's consistent but we don't know if it's optimal and here in this work we also show that this simple estimator this practical estimator is also optimal then we get into the problem of Scimitar of estimating multiple modes and I said before practical procedures are to analyze and there is no theoretical procedure for for case here all right so let's start with which rate we could get for cannon density estimation this part I'll just go over it quickly because I just want you to have a flavor of what the result is so here what is a nearest neighbor density estimator what is the a the density estimator defined at any point X let RK of X be the distance from X to the case nearest neighbor in the data to it Keith's nearest neighbor in the data right the density estimator is based is defined in terms of our key of X and all it's doing so it's n times the volume of the ball XR k of X and all it's doing is it's taking the mass of this ball divided by the volume of this ball which is so in some sense what the density is and and then therefore in order to understand how far is FK of X from f of X if you look here you see that the only thing that depends on f of X here is our K of X ok so we just need to understand how our K of X behaves in terms of f of X and that's the main idea in the from the analysis so this is a fairly complex right but I'll just say quickly what it says it essentially says that F K of X is as close to f of X as about 1 over square root K where K is the number of nearest neighbors ok but it says that this happens uniformly over all X simultaneously as long as K is greater than logon and is less than some quantity what is this quantity the main thing you need to understand about this quantity the quantity says how fast a changes in a small neighborhood that's all it says ok so we can characterize how far FK is from F at every point in space just in terms of how much F changes in those points in space ok so that's the key thing to remember from here and then from and this is a pretty general result it happens that this result also would with high probability and this simple result can cover what's known to be the optimal rate of estimation for unknown known density and the optimal rates were known from before but not using this not using this particular estimator okay all right so single mode rates how can it have how well can we estimate a single mode and can we estimate a single mode without looking over all over Rd remember the commonly studied estimator is this one that looks over all of Rd there is also some other type of estimators called recursive s estimate but the key question for us is let's look at this simple estimator which maximizes FK which I described earlier which maximizes FK over just the sample can we show that this thing is optimal in some sense and it turns out to be out to be optimal so the assumptions this is a form this is a bit more technical the assumptions are the following there are very general assumptions however the assumptions that the following let X be the Maximizer of F and we also assume some regularly the assumption about we also have some regularity assumptions about F in a neighborhood of X it says that the Hessian of F the second derivative is well-defined in a neighborhood and not only its well-defined in a neighborhood it's a strictly negative definite okay that's all it says and this is a common assumption in a fairly common assumption in optimization and it and then there is some simple technical assumption a global assumption but I can get into it later if someone want to understand it better so the result says the following the result says that X tilde Let X tilde be the estimator here that being the estimator from the sample it says that X tilde minus X is that most K minus one half X tilde is at most K minus 1/2 away from X and this turned out as long as K is between log N and n 4 to the 4 plus T and this turned out to be optimal it turns out to be optimal for K being set to n for fo plus T and that was the optimality of this rate was shown by typically 90 but were shown for the procedure that was not implementable and here we show that ok this rate is also optimal for this procedure so we now know how to estimate just a single from data and the key idea here a in the proof and I won't get too much into the proof the key idea is always she had to show go ahead you had a question the squiggle here just means up to a constant there is something so the key idea for to show this was to simply show that there is always with high probability a point that false if you if I have a random sample that there is with high probability of points that falls very close to the optimum and that falls at a deep within the optimal rate of the optimum and and then the other part is that the assumption this assumption but people in optimization recognizes that this assumption just says that F behaves like a quadratic near the optimum and that's good enough to have enough curvature and enough like order to get enough signal and I submit the Maximizer alright I won't go into the the math but let's get into the multiple mode rate but here I'll stop for a second what we've seen so far is we now know if I have an unknown F and observe a bunch of data we know how well the unknown F is being estimated so that was the first part how well is unknown F being estimated just using this nearest neighbor approach the next thing is if I have a single mode if the unknown F had a single mode can I estimate it with a practical algorithm with one that is implementable can estimate it efficiently that was the that part that this simple estimator that just speaks the Maximizer of FK in a sample is optimal it's rate optimal so that was the first thing that was the second thing we can estimate a single mode well now the question is can we now estimate multiple mode F is continuous and uniformly continuous over the space here yeah yeah so you're asking what if I had on a different estimator yeah will get the same thing because the key point was to understand that there is a point close enough to the mode then then for those I could adapt to higher orders of smoothness in it turns out that I don't need exactly a phonetic control for the rest of the result because I sort of need to have good enough estimation at the mode and okay estimations in the tails [Music] so now the setup is this I'm assuming that F has multiple mode and I want to be able to estimate all the again with a procedure that is implementable that's the key T because we want in the end remember we want in the end to compete with the heuristics which not only are implementable at very hard to beat okay so the assumption that we're going to make at every mode is similar we assume that ad but we assume that at every mode F is twice differentiable with enough curvature right then the idea is the following remember that for a single mode all we did was we look at all the points in the single mode and pick the one with highest F K value okay now as long as we can isolate the points around every mode we can do the same thing isolate a bunch of points around every mode and pick the one we with highest estimated density so now the key prom problem is how to select the points around every mode so that's where the main intuition comes suppose this were the unknown F this is what I will do I'll start with the highest possible density level if I start with the highest possible density level then I get this single kinetic component and I estimate the the mode just in this component the key problem is I don't know the location of the other modes but I don't really care as long as now I bring lambda down I bring lambda down and as I bring lambda down another mode appears and I'll just tell myself here I've already estimated a mode here because I know this one is contained in this I've already simulator mode here I have an estimated mode here yet and so I submit another mode in here and I keep doing this thing and eventually all the modes up here and I seem it just a mode at every location where there is a mode I hope that sort of makes sense right the key thing however is I need to know I need to add any let the component and how do I identify the kinetic component that was the first part of talk the kinetic components I embedded in the nearest neighbor graph so I can just parse the nearest neighbor graph and from parsing the nearest neighbor graph identify all the connecticut components and i can implement this simple idea this procedure now here is another question the reality is that if this were the unknown f the f hat that our compute is not going to be so nice and smooth it will be jiggly right so again that goes back to having having false branches in here so being jiggly means if this were that the real thing maybe I have this okay maybe I have this in the effort this thing is the real F but I have these false bombs you have had all I have to do is identify the false bombs the good thing is from the first part I know how far F hat is from F at every point so I know this bomb is at most about 1 over square root K by just knowing how big the bombs are the bump that is below 1 over square root K I view it as a false bomb and I remove it so it just allows me to smooth out the process and recover just the tree not not the false ones all right now there is a small point I'll have to make here which is the following which is that in order to recover all these things I also need a bit of separation between the modes right I need a bit of separation between the modes because if there is no separation between the mode then it's very hard to identify a mode by itself so all I mean is if I have two modes that are that the result that turned out to be very close to each other then it's hard to identify them from sample because I would only have a few samples or samples in between to sort of identify it so the result relies on a separation and we say that a mode X is our salient if there is a valley of with about are between it and any other mode so the results is that for the following the theorem says the following that if Tilian then there is a sample size large enough that depends on R such that every such mode that is Arsenal is recovered at optimal rate okay and so just through the procedure we now have a first a first algorithm that is optimal for all modes simultaneously at once any question up till here yeah so you need to be deep also that's a very good question the valley need to be deep also but because we made the assumption of curvature around every mode that curvature assumption that gives gives us the depth ok and here we just here is just a theorem that says what I said earlier that we can identify all the false modes and remove them okay above a certain level right any other question at this one okay so what we've done so far is we now understand essentially how to estimate all modes of unknown density but the assumption here so far has been that the density is maximized at points or the space not maximized over general regions of the space not maximized over general set and now what we want to understand is how can we estimate this model set which by definition is set on which the density is maximal is locally maximal how can we identify this from data alone and so all we'll do is follow essentially the same intuition as here but we'll have to do a bit more because we no by default a priori what is the shape of this set on which the density is maximal or what is their dimension and such thing okay so here again to remember this is the the generic cartoon here is data from some density the density is maximal on these two rings okay and the density is maximal these two reason the first thing that these are my model said to the two rings and the first thing I want to do is being able to defy the tories point mode what we've discussed so far can be viewed at zero dimensional model set well the more general one higher dimensional model set so the key changes to the previous procedures are simple let s a model set so here this is s this is a place where the density is maximum so s is just this segment this line segment the way we estimate s with s hat is the following let's say this is the density estimate of this density okay I'll start off by just let's say I've isolated this mode the points coming from this area I start off by just first identifying the highest point the point with the highest estimated density value once I've identified that point I just look at other point that is within about 1 over square root K of it why 1 over square root K because that's what we said is the maximum fluctuation other points between the density estimate and the known the true density okay and then I take all those points that are within this fluctuation distance and throw them in and call them as hat that's all and now for multiple mode model set you can repeat what I explained before I go down the levels isolate them on on and do this procedure all right so there has to be some key changes to the analysis to the previous analysis in the previous analysis I assumed that the unknown F was twice differentiable in a neighborhood of the mode of the point modes now we are looking at more general optima and I cannot talk necessarily about differentiability at the boundary of the Optima and so this is a technicality but I cannot talk about differentiability at the boundary of the of the Maxima however I can still talk about curvature and smoothness and it resulted as long as F is uniformly continuous in some neighborhood of the Maxima s they resist upper bounding and lower bounding functions that act as curvature and smoothness for for this and then we can express now our result adjust in terms of these functions all this is saying is I guess I'll draw again here all this that is saying is that I have F like this and some Maxima s this is s they resist two functions that envelope my tyre and those functions I know and serve as a curvature and smooth so just the existence of those two functions is enough for us to establish a rate of convergence and the rate now is in terms of the hausdorff distance between the two sets now we can say that the House of distance between the two studies in terms of this L goes to zero so we have consistency and goes to zero so we can estimate as consistently with s hat provided again that K is greater than log n but less than a function of you the lower a function of the upper bounding function a function of the upper bounding envelope okay and then we get similar pruning guarantees as before by the way if I let the lower bound and the upper bound be quadratic I recover exactly what we had in the case of the point modes right the whole reason what I'm through here is because if I have a set of this type they might not be differentiability at the boundaries of this set okay all right any question here okay so the key is that if you want to forget everything and just remember this they keep on so far is that we can estimate all model sets very general set where the audacity max is maximized we can estimate all this model set at optimal rate that's the that's the key points is just the dimension of the space is the interesting it's the full dimension of the space right because here by default which is assuming that the density exists in the full space so the the support of the in his full dimensional okay however that's a good question whether we might let D be just some dimension of the the neighborhood but in this analysis no because we're caring about neighborhood of the set and the neighborhood have full dimensional alright so the resulting procedure is the following quick shift plus plus I'll just add the resulting clustering procedure which is plus plus I'll explain it quickly first you estimate model set s1 through SK of the unknown density you estimate the dis model set of the unknown density and then once you've estimated the model set you're going to cluster every point around the model around each model set and the way you cross every point around each model set is just by doing a gradient to the model set so I by doing a gradient ascent I assign every point to the closest model set okay and rather than an exact grid innocent you do I made gradient I sent an approximate gradient our Center says that the following I'm not going to do a gradient I set over the entire space but I'm going to do a gradient I sent just over the points in my data set okay so it says follow a sample path starting at a DX 0 I'll follow a sample a sample path to si to some model set si but just seeing whether f hat is growing along that direction and that's all and so that whole procedure we call it quick sheet plus plus and that's sort of the results that I was showing earlier right and this is a very recent work ICML with GN okay here again is now an expanded version of the result that I had shown earlier and this is DB scan this is mainship this is quick shift and this is quick shift plus plus and again we throughout and with reading peak we find that I set on which we are working well we just picked a bunch of data set from this you the UCI database and we keep doing so much better than a lot of these heuristics a lot of the competitors in a few cases min shift did somewhat better in a few other places quick shift it essentially just as well in some of the cases such as page page blocks all the procedures for some reasons were not doing so well at all and yet we still do reasonably well in terms of clustering right and so what this is saying is that the additional stability that is brought up this approach because rather than just trying to find single point at which the density is maximal we are finding first hole regions where the density seems to be maximum and then we cluster around those regions what means she would have done is find single point where the thing the density is maximum close around those regions most ability here is giving us a lot a lot better performance sorry what is the dimension of these samples and these experiments so the highest here was about 200-300 dimensions that's a very good point the rate I exponential in dimension that's what you're getting at right yeah the rates is financial in dimension and they are sort of the worst case rates for the assumptions that we made it turned out that that in practice even up to 200 to 300 we did fairly well but I believe that the reason why is the following the rate of estimation at the rate of estimation of model set however the problem here is that of clustering even if we don't estimate the model set that well as long as we estimated approximately well it's good enough as long as the clusters are far from each other there's a sort of mix n so that's why we believe that we're doing well even when the dimension is relatively high it could be it could be I haven't thought about that but my first impression was that potentially the closest are far from each other and I don't need to do an exact jannat the model set so here is a different picture that I want to show why is this picture ok here I could offend some of my colleagues but the in clustering the picture is the following is that in a lot of result on closer in people just give you this they tell you this is how we do compared to the competitors right they don't tell you how the tune did it's very possible that I tuned this as well as I could because I publish my paper and I didn't tune these things so properly right and so this here is actually just showing you the B behavior as I go through all the tuning parameters all the ranges of the tuning parameters of the other algorithms right so I take DB scan and the main parameters is epsilon I go through the entire range I behaves same thing with mean shift same thing and same thing with quick shift prospects I go through the ranch and I could see that the best what I'm showing on the other page is the best possible for the tuning parameter of the competitor sorry the problem in unsupervised learning right we cannot do cross validation because we don't have a ground truth a priori that we can cross validate against the score the scoring function themselves are not easy to cross validate against I unlike in classification and such things so that's why I did this I didn't do cross validation I know to have we do have labels labels for the testing data and so for the labels from the testing data we're computing the score but we're going through all the possible choice of our parameter settings but we don't know how to choose the parameters a priori right and so for all the possible primary settings I'm reporting the best possible that was done the good thing with quick shift plus plus is that in a lot of cases the range of parameters on which we are doing well is very large okay it's just the number of neighbors and it turned out that when we look back at the theory the theoretical range is also very large it goes from logon to a root and so for that whole theoretical range and for a lot of the for a lot of the data set we get that that for a whole theoretical range we plateau somehow the performance remains fairly good so the procedure in itself is very stable in that sense we started also investigating as few other applications in medical imaging image segmentation in in computer vision and also problems having to do with other detection in smart cities Internet of Things and such things here I'll just show you some quick results for image segmentation quick shift okay before I even start there we all know that today the go-to procedure is deep neural net for almost everything right however in image segmentation deep neural Nets don't work as well unless you have also labels so there is also what's called semi-supervised image segmentation in which the neural network very well but in purely on supervised image segmentation the one of the best things is still shift I am in shift and so we're comparing against quick shift and what we're reporting here is the best rate of in over segmentation and on the segmentation and some of result here are very qualitative but we get pretty good results of our in image segmentation for this problem sorry the density function here you actually just pretend but in the intensities a form of density so you you could you could use caller but you often you do a first transformation that gives you a general intensity over seven dimensions about and then you perform this thing on there but in black and white it's easier to understand alright so some future questions are adaptation I think you were asking such questions can we a Optima can we automatically choose the parameters teacher even though the procedure is very stable to which choice of parameters we will still like to be able to just look at the data and make the optimal choice of parameter so that one is a fairly hard problem so far high dimensional clustering would like to be able to go beyond 300 dimensions to much higher and in those cases we need to come up with some notion of feature selection within unsupervised learning or maybe go through spectral and projection approaches but the right approach is not so clear so far all right that'll be all thank you [Applause] 