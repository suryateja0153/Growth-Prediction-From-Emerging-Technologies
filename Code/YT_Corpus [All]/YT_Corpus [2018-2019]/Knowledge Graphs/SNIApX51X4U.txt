 hi good morning and welcome to the webinar in which we're going to be talking about knowledge graphs and in particular the power of graph based search within this context and and thanks very very much making it this morning so I'll just introduce myself quickly my name is Petra soma and I'm a member of the query languages standards and research group and what be essentially doing this group is identify design and then specify new features for cypher these will features that will eventually make it into the query language and on the back of this what we actually do is we write many many documents detailing for instance the syntax semantics ie behavior and so on of these new features and what we also do is collaborate with academic partners so that we actually get what section means is that we are then at the forefront of graph Kring developments in general so just in front of you now is the outline for this morning's talk and what i'm going to start with is the property graph data model and I realize probably most of you are very familiar with this but because it's very foundational to the rest of the presentation I'll just introduce it briefly for those of you who are maybe not so familiar with it I'll then actually talk a little bit about here for Jade itself and then we'll actually dive into the meat of the session ake a knowledge graph so I'll begin by setting the scene of it talking about the background also my particular take on what a knowledge graph is what is it how is it used why do we need it that sort of thing also how we envisage using it within the m4j and race tools and techniques you can use to actually a miniature knowledge graph and how to basically take it forwards into the future and how to use it effectively and then then after the last part of the talk will be about a graph search through Seifer that will introduce a small introduction to decipher for those of you who perhaps are not familiar with it on a day-to-day basis and after that I'll be going on to various extensions that the query languages standards and research group are looking into adding decipher at in in their future so I shall begin you begin quickly with your property graph data model refreshment so the idea here is that obviously the underlying construct is a graph and it consists of three main three main facets if you like so the first one is a node this essentially represents entities of interest with unit domain so for example a node can be a person the node can be a building and node can be a vehicle these sorts of things it's also analogous to those of you come from a mathematical background to the notion of a vertex we it's a synonymous term the next facet is a relationship and that essentially is the connection between your nodes and essentially what relationships do is by the very nature is to add structure to the graph and they provide semantic context for your nodes it's not only do you have a node which is labeled as a person you can also see that this particular person actually lives exemplified by lives relationship in a building when the building is perhaps another node with a label of building on it and by actually bringing relationships like this to the fall it actually gives much more structure and semantic meaning chill graph the final facet is properties and essentially these are just essentially the actual data items the attributes if you like of your nodes and relationships and these are represented and stored as key value pairs which actually present the year the name of your property so perhaps a property literally called name and then you would then be the value of that property so for example cynical John or LAN or something like that so it's showing a bit more information here so a node may have 0 more labels and either spectrally mean that a node can actually depending on the amount of labels your science or no this gives an even more m which semantic context so for example a node can be both a person and a teacher or you can actually either no that's just a person only because maybe you don't have any other information about that particular person we also do allow the fact that a node may not necessarily need to have labels but we do find that this is quite an odd thing to do and we actually see it very very rarely in practice moving on to relationships relationship absolutely has got to have one and only one type and also direction so we actually have a this is basically a directed graph and things like a type could be something like nose likes etc and in further one this is what I was talking about a bit earlier a nodal relationship may have zero more properties so again this is an example so a a node for example could be a person node and in fact if you see the diagram on the right hand side we see that there are three nodes in there and if we just focus on the leftmost one you see that that node has got a person label also he loves an C he loves node on the right hand side he also drives a car down on the bottom and they're also very start going in incoming relationships and we can actually see that there are a number of properties associated with this person so we see that the name is Dan we see there was born in 1970 etc etc we actually have noticed as in contrast to the label story that nodes and relationships frequently or not frequently rather it may sometimes not have any properties and this is actually perfectly okay there are actually applications that use the property graph model to model am just the topology so for example Aiko water flow Network or something where you have junctions and pipes and things like that and in that case maybe properties are not that relevant or useful but in general we actually do notice that by and large most nodes will have properties and most relationships as well so I think what I'm hoping is that the idea that's coming across here is that you can very quickly get a very rich description on your domain by using a graph so in this picture over here I've actually have not explained anything to you what you're actually able to very quickly see just by looking at this graph sods here very visual construct you can very easily see what the entities of interests are how the information flows and you can very very quickly see what the context is all about just diving into some use cases that we starting to see crop up one more in an industry in particular are things like impact analysis so which are the most dependent upon components in your data center for example logistics and routing so what's the most efficient way of say delivering a parcel delivery or something like that recommendations access control if I'm actually to talk a little bit more about those later on in the talk and I just wanted a round off this section with a couple of very well known use cases actually are both using now under the hood and the first one is NASA and essentially what they are doing is using neo to store their mission history and essentialness encompasses knowledge of every launch that they've actually undertaken over the past few decades and what they're actually doing is in their current research and the current missions that they're planning for what they can actually is traced back to situations where those problems that they're encountering not actually occurred in the past and what this allows them to do is just to see very quickly through root cause analysis what went wrong on those missions that were similar to missions that they're perhaps planning today and also perhaps to see what alternative configurations or components and things like that are and previously it used to take them a very long time to actually search through their old system using tables and records so you can imagine if you're going back all the way to the seventies you know this is the sort of taken quite a lot of them digital archaeology and forensics what they actually can denies find an information incredibly quickly and the takeaway actually is that this apparently has saved them about two years worth of research into the decision-making process that's actually really very exciting the second case I'm sure most of you heard a lot about this last year in particular Panama papers and as you can see how they won the Pulitzer Prize last year and this essentially was tracking where and money was flowing and as you recall this ISM the leaked data from mossack fonseca in which various organizations and individuals were evading tax and as you probably recall many newsworthy items and investigations came out of it and paradise' papers was kind of a sequel to this to Panama papers and it continued in the same vein and essentially what's happening and what's still ongoing is that the original Panama papers graph if you like which was then enriched subsequently with the Paradise Papers information is still being added to and I'm sure over the years we'll actually see many more stories come out of there but he made the idea with both of these applications was that the whole underlying technology was near for Jane graph database and the idea was that even non-technical people could very easily get an idea of how and what was happening how things are related tracking things through searching etc so just a I'm going to just in the next few slides talking a little bit about neo4j in particular so we like to now consider ourselves the graph platform for connected data and essentially lying at the heart of all of this is a native graph property database and what this essentially means is that we've developed the database from the ground up to handle graphs natively so we don't actually sit atop any other layers that may actually am cause interactions and non graph operations to prevail over the graph operation and what this basically means is that we store nodes and properties and relationships as first-class citizens and this actually makes things very very quick very very fast queries execute very quickly we also ensure that we comply with acid so that essentially means the actual data is safe so once you commit the data to your to the database it is written and it's not ever lost or corrupted and we also have always try to ensure that the product is easy to use so we're very much began by looking at the transactional graphs safely look at the ERM that triangle in in the image we began by actually concentrating on developers and DevOps and transactional graphs and what actually has happened of the last euro so is we've started to expand this picture so we moving away now just from solely concentrating on real-time OLTP and moving over into two other spheres if you like so on the left hand side you can see we're actually Wilson are considering people such as data scientists and obviously these are the guys who use graph algorithms to perform very complex analysis and essentially what they're doing is whities for this group of people are doing is looking at the graph as a global structure and actually finding sub structures within that so this is unlike the transactional graph story where generally only bits of the graph are modified reviewed at any one point in time and then of course on the right hand side we also are also spending much more time catering for the business user this essentially is that they can get a very high-level overview of what's happening so we're bringing in loads of visualization tools and techniques to better to bring to beyond that just to dive a little bit more deeply into the broth platform as you can see the in the circle linear Forge a cylinder that you can see in the middle that's actually our database and all the bits around it are now considered our extended graph platform so as you can see there's actually a lot more going on now so we essentially to reiterate we're trying to drive the use of connected data beyond just the developed area and I don't mean just the developed area but we're just trying to expand the focus there be shifting focus away from them to also include other facets of the organizational Enterprise such as business intelligence and as I say the young analysts so areas such as graph visualization and graph analytics and these include algorithms such as cluster detection PageRank all those sorts of things we're not taking much more of a prominent role nowaday essentially is that non developer personnel can actually get a much more robust view of the data and that actually everything evolves in that way so I'm not coming to the actual and meat of the talk and knowledge graphs and first off set the scene a bit by talking about the background of it and so initially a few slides back as you may recall I was showing you a few use cases so I'll just talk a little bit more about them so in here you actually see loads of things that probably most of you are aware of they've actually been on becoming one more prevalent over the last few years so for example they've got recommendations and these are generally undertaken by retailers we've got fraud detection so this is where people are able to detect the flow of money and they're able to keep an eye on any suspicious patterns emerging and then act in real-time upon that they whoops it looks like you know there's some some fraudulent activity occurring in nottingham sake or something like that as I mentioned before the network case is actually a very very popular one mostly with root cause analysis and also as I said seeing what's what the most dependent upon a component in a data center is for example so for instance if you have a component that most of your services rely on and should that go bang one day very sadly what are the effect be right so it's also basically a way of actually minimizing risk in that particular sector what's actually not on this diagram is a sector I'm very interested in myself hadn't come from that area and that's healthcare pharmaceuticals and bioinformatics so essentially by pharmaceutical industries are now looking using graphs as a way to actually help them reposition and repurpose drugs for example and to also get other discoveries coming out of their data and bioinformatics applications for example a beats actually model and analyze protein interactions so these areas actually growing and growing a lot and I've seen a few an industry but certainly a lot in academia it's certainly become quite popular to use graph databases to actually feed that because this data essentially is very graph II right so to take a bit of a step back so what's the problem we're actually looking at so if you just look at the slide you're probably thinking to yourself hmm actually none of these problems are really new they've actually had these are issues bubbling away for the last few decades well it's only really now they've actually are starting to find solutions or maybe partial solutions to these problems it's essentially what we have here is the fact that any healthy business obviously needs to grow and evolve and obviously this process presents challenges and certainly the point I can resonate with a lot is the last one which is the aging infrastructure and siloed information where all of these different silos spread across an organization may actually contain really great nuggets of information and the problem is of course you actually can't get it and that's actually a what knowledge graphs seek to actually solve it's the negative consequences of having a siloed information or at least information you can't actually act upon is a as exemplified by the list shown and actually I would say the biggest one I think is the one I highlighted which is essentially you don't really know what you don't know this is one of those huge uncertain things in life in general and the idea is that graphs are actually a great way to bring those sorts of gaps into the light and basically leading through two new insights being made and obviously as well you know it's obviously never a good thing to have bad information to be spread on through them a company and also it would be a huge pity if market conditions change to the point that a great opportunity makes itself known but that an organization is unable to act it just simply because they don't have the information to hand or they're not able to act on it and quickly enough so just to take her very much an eagle-eyed viewed on knowledge graphs so essentially have sketched out a picture below what this looks like see if you ever look at the diagram there are a number of silos in this we can actually see there's a consumer data database Product data database etc etc so each one of these departments or sections of a company if you like has got its own data and the idea is that the knowledge graph if you look on the UM top half of the image actually joins all of this data together and in this way you actually get a cross-sectional view so Matt I'm not going to give you my take on what I think knowledge graph is and why it's a useful thing so I'm going to actually walk you through a few steps the idea is actually in rich grass of more and more data or derived over time and what this actually will give you is it will be a grow it will result in a graph that has more detail than the original graph or context ie maybe there are richer connections that have not come to the fore or that have been added more truth by truth by the way I mean that the data is correct so maybe the original data was not you know hundred percent verified maybe was incomplete anyway the idea is that as the graph evolves by being enriched the processes are actually augment the data state becomes more more correct and this very much is driven by relationships as first-class citizen and because of that we now actually have a graph that is more intelligent than the original one and also furthermore has a much better and more fine-grained semantics not being in this position what this means is that this enriched graph and bearing in mind of course set this is probably a you know could come over quite a few iterations the idea is that eventually your your shall domain I what is important to you is actually captured much more precisely and one thing I'd like to actually bring out here is that property graph data model does have an optional schema so what that means is when you begin with your initial graph you may not be quite sure of you know what what your data model looks like maybe just experimenting or exploring and the idea is that over time as you get more and more data as your graph becomes more and more rich you can actually various invariance in your model may start to make themselves known so you may actually start to see that people can only ever be connected to buildings or vehicles other people but that there's no way there's no sense in connecting people say to a bathroom or something like that top seats not a great example but you get the idea so as your graph gets richer and richer you actually start to see what parts of your data model you want to start locking down and then in which case this is where you can start bringing a scheme and one more to pin these things down to make sure that your data is a more strict and in fact that's one of the things I'll be talking about towards the end of the presentation it's anyway so assume now we've got this fantastic graph it's very rich it's it has information in it that we can trust there's a lot of them information we can get out of it it's all very well having that but if you cannot get the information out of it it's a going to be a little bit pointless so it's very very important that all with some good information that's in your graph can be searched for and therefore extract it in a meaningful way and the reason why we'd actually want to search in the graph is obviously to get knowledge that's what we want at the end of the day and the idea is that we can get direct and indirect knowledge so you can not only get information that you're seeking directly but maybe connection also get new insights he may be searching for one thing but in the course of doing that you discover something else that you were not aware existed so you know like a serendipitous discovery and yeah that's essentially what my take on a knowledge graph is and how and why we build it up and essentially what we want to get out of it at the end of the day so just again to take a step back the knowledge graph essentially provides a 360 degree view of any to your interest these may be things like customers and things like that auxiliary entities so these may be stores at which the customer perhaps did their shopping and also customer reviews and things like that things that basically support the main entities and then also the processes that actually surround these so maybe this is how I'm there is some film and processes worked or how the shipping works out or you know delivery and things like that and to take even a further step back essentially if you really really want to summarize what knowledge graphs should do and what their value is essentially as they give answers about things it knows about and of course this means that the better knowledge graph is or the richer it is the more it can give answers to and very importantly also explain how and why the answer was returned and the the great thing about this is that you can actually see what other facts relate to this being the answer why was this what was this the case right and this is something you can very easily get from the context and structure and this again is something that we get with a graph and just to really really emphasize this there must be searchable and as I say much more about this later right so the next couple of slides are a bit more about how we envision envisage knowledge graphs working neo4j so it's quite actually a lot going on in this slide but if you would like to I'd like you to focus on the AI workflow loop sets and the leftmost part of the diagram and essentially what's happening here is I'll just walk you through a process very quickly imagine we have a developer and right on the left hand side building a system service could be something like a recommendation system and the system goes into production and all is well and good and the system is being used and as it gets used one more it essentially gets for once a bit of words thirsty for more data the idea that I'm adding more data to the graph which powers this application would actually be a good thing and in the way this is done and this is the idea of their growth platform idea I was talking to you about earlier is that a lot of the information after that application as well as other applications within the enterprise get put into a data lake or a data warehouse and then the idea is that we actually have people have data scientists and business analysis it's actually I'm going through this data and coming up with any algorithms for example so the need for these algorithms are triggered because of the rich data coming from the recommendations app anyway then the data algorithms if you can imagine bring up even more data and more good things that will actually be good to add back to that original graph that's used for the application so imagine them that's not feed back into the graph the algorithms cluster the new data that it brings to there and basically you can then see it's a feedback loop essentially as the application gets used more and more one more data is generated which means the data scientists are motivated and triggered to produce and look at more algorithms which in themselves go back into enriching application so over time this graph gets imbued with extra intelligence and generally are now looking on the right-hand side of the screen across the enterprise obviously other people also have a need for this knowledge to get back to them somehow and essentially the idea is hopefully that better decisions are made based on this knowledge submitter just walk you through an example that probably a lot of you have come across and that's certainly very very privileged in these days is we have here two sets of fairly disparate informations on the left-hand side we have information about products so it's very much product centric they have a catalog we have an purchases all that sort of thing and then on the right hand side we've got pretty much the customer journey post post purchasing something and while all of these bits of information are in silos it's actually be hard to obtain information spanning the entire universe or this particular domain so for example we can't answer questions like why did a customer buy something you know was this something else they're looking at at the same time are they likely to return all these sorts of things it's very hard to actually get a sense of what's going on because of the fragmentation of the data so the temptation may be inserting this is what actually I'm it does work to a certain extent and has obviously been on the status quo for quite a few decades to pull out all this information from these various databases into a data warehouse or a data Lake and the idea then is that you know you've got this pool of data and the you then the idea is then using a core set of tools this is you know Spock those sorts of things to write business intelligence apps that the ends that to consume that data and into obviously spit out a useful information as I mentioned this is actually rather good up to a point however you may come across a couple of stumbling blocks one of them being that the a lot of the data fishing nowadays maybe rather unstructured so it's kind of hard to in this sea of data pan down what the entities of interests are and obviously what they what their connections of interests are and the second facet may it is that this process may also be quite slow which means that what you don't do is you don't get the information in real time so you've lost that operational flavor to it so anyway and the format to summarize may not be very suitable for putting into action and you there's no way of actually drawing out actionable info in real time and real time insights as well it's the solution to this we think all maybe one of the solutions to this is actually the knowledge graph story and we are seeing much more of this actually happening amongst our various customers is that essentially relevant data is taken out of each of the silos and maybe you take out some of the purchasing information that you deem relevant similar product catalog information etc etc you take it out of each of their silos and you materialize it as graphs and then these graphs I essentially joined into one large super graph and you can actually see that in the top half of the diagram so what if actually done here is stitched together the facets all the important entities and connections of each silo and this essentially is what our platform allows you to do very easily so for instance we actually do provide the capability and tooling to extract relational data out of relational database and we provide various ways to allow you to define how that how that data in tables is manifested as entities and connect we'll save this similar thing for CSV files as well it's anyway assumed it so I went to the platform providing the wherewithal to draw your data up into the graph you're actually able to as I said materialize entities you're interested in and any interesting connections and you end up with this larger graph that essentially provides you now with a really great cross sectional view of the enterprise what you can now do is query this graph in real time and this basically means you're going back now into that operational mindset so I'll just walk you through very quickly through a slightly different point of view so I'm just with an example so as you can see in here we've got three very different graphs this is talking now more about the EM stitching together or joining together of information so we have on the Left a customer graph for a product graph and a supply graph and the ideas generally just actually meld it all into one super graph so what we now actually have is no longer a bunch of graphs but we rather the knowledge graph for that particular organization and idea is that this data now can get enriched over time and that's because of us joined up in nature right we've where we've joined our customers product supply graph it said all of this information we can actually possibly see how to enrich this graph even more over and above what was basically there in the first place so for example information that you can find in the left hand side of the graph could actually be used to provide more information in the right hand side so this leads to you perhaps being able to add more information that would not have been obvious to add if they're still been in in their original data silos so we can actually for instance identify material as different types of customers for example by that I mean basically identifying different segments of the market things like that we could also perhaps have an a notion of say bad weather alerts relating to delivery of items say so I said there's a dreadful storm coming on the way and we can see the number of our customers live in that area we can actually forewarn them look we won't unfortunately be able to deliver your widgets by the time we initially said we would you're going to have to wait a bit and I think you know there being more responsive to these alerts and creasing the level of customer service in that way can only ever be a very good thing so other things it can do for example is because you're enriching your graph with customer information you can actually have all these extra auxilary applications sitting around your knowledge graph and what you can for example also do is record normal buying patterns for a customer so that's if you start seeing anomalies for example you may start to wonder you know what's going on there you know is there's some fraudulent activity going on as they've been there maybe some cyber crime going on so all these things can actually not be done so the essentially the idea is that provided that we actually have a searchable and consistent and detailed intelligent repository the idea is that actually this will serve corporations doing lists and essentially the idea is that such corporations will actually always be one step ahead of the competition and in a nutshell what's required to actually get there essentially is is what I was alluding to earlier is that actually over time you enrich more and more and more through any means that you can see including them using information from other parts of the enterprise through to graph algorithms through to analysis etc cause that to enrich your graph and also to ensure that there is a way of actually getting information out of the graph just very quickly to end of the section is we also are just looking peeking a bit into the future we foresee that a knowledge graph could end up being the foundation for AI related activities in the future so for example this would include data scientists developing new algorithms to utilize the graph so I says the huge um pool already off theoretical knowledge about graph algorithms and how to execute them efficiently but obviously because this ear is now getting quite hot even more resources are being put into this to research even more interesting ways of undertaking these sorts of algorithms the idea as well is that as the graph gets richer that age scientists can drive new insights to enrich the graph even more so for example if you use a deep neural network you could explain how when and precisely where decision making a code and also materialize that information and then ideas that could be exposed to future similar searches so things are essentially learning over time and if you recall earlier on I was saying that your graph gets more truth and this basically links back to that so you can actually record you know how there is things were derived or how the course to come into being or why they were course to come into being this actually adds to the truth of your graph so I'm just going to walk you through a few ways of actually how you can enrich your graph I'm going to begin with something probably mercy I've heard of and that's natural language processing so essentially this is understanding human language and the idea is that a block of text would come in and this would be passed using various parts of libraries to extract entities and various attributes associate these entities maybe we can actually for example in some sentence get the idea that this thing called Daniel Craig is actually not a thing he's actually a person and moreover because of other various words or clauses that sit in that sentence or in that paragraph in which he is mentioned we can also infer that he's not just a person but he's also an actor to continue a little bit with that actually a couple of side topics I suppose you could call them related to NLP and in fact there's a lot of academic research going into here at the moment so certainly not the case about ten years ago but because of this son becoming quite a hot topic certainly a lot of researchers have taken up the baton so entity disambiguation and this is a really really difficult how do you actually tell apart Emma's in the rain forest versus Amazon the online store versus Amazon the tribe of warrior women if I recall correctly how do you tell these apart it's actually much more difficult than you think it's a usually from the context but it's it's never as simple as it sounds also another facet is sentiment analysis and that's trying to identify from a statements what's perhaps being infertile what's not being said directly but he's being meant things like polarizing statements negative statements very much that sort of thing so sort of skirted around but indirectly and this is a what I was meaning about when I was talking about the truth of data and that's around the area of provenance and lineage this is again I come from an academic background this is certainly being researched a lot lately and it's also Aria what what actually brought it to the fore is particularly in scientific communities is the need not actually provide traceability so it's all very well providing a results that have been discovered but that's no longer enough you now need to be able to show how you derive those results what was the raw data what's processes and experiments did you undertake on that data what were the derivations you made on that data how did you actually eventually end up with your conclusion this all needs to be traced through so essentially this is a down to being able to undertake repeatable experiments anyway so this may have obviously as I say come from that particular domain but this is certainly something that would be very valuable in an industrial context as well and from this you can actually see that Brazil eree functions such as snapshotting graphs and versioning graphs would actually play into this a lot in fact I'll talk about that a bit more later right so inferencing and actually deriving new facts from a machine learning in algorithms so this is actually obviously provided a couple of Link's as you can see on the screen these are free to use out of the box so am I do urge you to actually have a look so the first thing is actually I'm around 450 procedures and functions and I had to look yesterday and I believe that was a number and essentially these are a whole bunch of excellent utilities there to help you with everything from data integration various graph algorithms data conversions etc everything that people have stumbled across in the past they're actually enriched or encapsulated into very easy to use procedures and functions so lots of documentation around there is also I do urge you to have a look the second facet is actually our graph algorithms library and again this is a free to use and because we've actually linked with various academic institutions and also industrial institutions who's a speciality is there is in this sort of thing we actually have ensured that these are very highly parallelized and therefore run very super efficiently and you've got everything there from you know centrality is PageRank between us community detection pathfinding etc so I would be very surprised if you did find what you're looking for in that list also to mention these both these areas are being enriched constantly over time as well so new things always being added things are being made faster as we learn new things so another way of actually enrich the data through the inference or derivation of new facts is something called the deductive method so I'm not sure if any of you here from the the knowledge base or M ontological world so to send him to a lot RDF and SPARQL things like that that's actually to derive new facts through actually using rules so for example I've just sketched out an example down there so the idea is let's make node X more meaningful all right so assume node X has a person table and also it's got an outgoing teacher's relationship what we can then encode in here is that node X should not just be a person but there can also be a teacher and as you can see this essentially makes node X much more finely described and as you can imagine and there all sorts of variations on these sorts of simple rules and you know these things will get quite complicated as well but sickness is another area that is well worth looking into so continuing with that you actually can also use external data sources to obtain new facts so for example and they have noted concept net and word net so for example word net in particular contains lots of information about synonyms and very similarly meant terms so say for example you want to treat a teacher in the same ways you treat a lecturer and you can reflect this in your knowledge graphs that win and teachers search for it can also get information about lecturers as well because maybe the user won't be aware of the fact that there's this distinction in the graph they're just you know after anyone who teaches and obviously that encompasses both of these terms so and it's all very bare domain as specific obviously so yeah so also um you're not sure of your graph are not sure of what the rules are you can actually am as I say hooking to these external sources I think there's also one for geographic information as well as sets or something to potentially look at right so this is the final section of the talk and it's all going to be about a graph based search so I'm hoping that most of you know what cypher is so I'll just a very briefly given introduction for those of you who don't and this essentially is our graph query language it was developed by us by neo4j but it's no longer actually neo4j only things so we actually a couple of years ago open it up because we actually would like other graph vendors to also use cipher we think it's such a fantastic language it should really be in the language that people use when searching in grass just like sequel is great for searching in relational data we think ciphers should be in the language to use when getting things out of your graph and the idea around that is it's declarative so it's very much like sequel you stay to what you want not hard to get it and the beating heart of cipher is the pattern matching part of it and this essentially is just how to transcribe patterns in your data and telling you're getting data that matches your pattern out of the graph so what we actually have there so it's a full language so we have dqr for reading data and that's actually what I'll be focusing on in this section will serve DML for actually mutating the data and also DDL for creating constraints and indexes but as I said I only focus on the first bit and what cypher gives you and you may be thinking those of you coming from the sequel world what's special about it apart say from the pattern matching aspect it also actually allows you to very easily do recursive query and the certainly is something you can do in sequel but not very easily so essentially what you can do is things like transitive closure and in fact there's more about that in the next slide related to that is actually a variable length relationship chains and also the ability which is a very different and plays very much into the knowledge graph story the ability to not just return you know properties or notes it's actually the ability to return whole paths and as I said to you before this is actually a really useful thing to do to see why a particular answer was returned you can actually see exactly what the path was that later that answer being a being the answer it is so just to show very graphically what's actually going on so what we have is the match clause and this is the clause that essentially expects a pattern and the pattern is given by the EM the rest of the query this we actually have a a node pattern and the node pattern is actually the pattern that's enclosed in the round parentheses so there we actually see that we're looking for a node with a label of person furthermore we're also expecting that this particular node has a name property and that the value of that name property is Dan then we're going on to something we call a relationship pattern and that's that's in the the gray text there and we're actually saying that that node has an outgoing relationship so you can see the arrows arrows pointing outwards so it's going from Dan to the other node and furthermore that that relationship has a type of loves now it could be that that relationship there there are relationships in the graph with type loves and that also have properties but because we haven't specified any property and predicates in this pattern that's not taken into consideration and then just end off with on the right-hand side we actually are then searching for any nodes that are connected in that way and because no information is given in that node so all we're doing is just assigning it a variable we're not specifying a label it needs to adhere to our property it is going to return any node so it will return people to return dogs cats etc anyone whom Dan loves and then the actual return statement pretty much is analogous to a sequel select statement and that essentially just specifies the data to be returned it's in this particular instance we're just going to return all the node information they're matching whom so for example what this would return to you is the labels of the node as well as all properties of the node but we could have also said whom dot name and then what they would have returned is the name value of all nodes returned that fit that pattern so there's quite a lot of detail going on here so I'll just go through this very very quickly so as described we have the beating heart of cypher which is the match pattern and in there we actually have transcribed a pattern we also like sequel does pretty much we have filtering with predicates so we have the where clause and things like that we can project data values and expressions using the return clause and just like sequel again we have things like ordering and also things like slicing and things like that going into the middle of the screen this is where it gets real much more ciphering away and this is this variable length relationship pattern remember earlier I was telling you about transitive closure so if you look at that first query match me friend star both what that he saying she will tell you is traverse from me some me node one or more friend relationships that's what will actually what what so actually do is it will traverse for as many friend relationship types as there are in the graph it's assuming the node me starts at somebody called Dan it will then go to his friend his friends friend his friends friend etc etc etc and for essentially and all the nodes related in that way have been returned and that's essentially a transitive closure it's also a way of bounding transitive closure and that's exemplified in the second example we've actually specifying their Traverse two to four friend relationships that will traverse all friends that are two friend relationships way three friend relationships wait for different relationships way and it will stop basically so this is actually way of limiting the results returned so you can imagine using these structures in particular for tree data or like a department you know you've got departments containing departments containing departments that sort of story so you can actually extract a whole subtree out of there by using things like that very important to note is that cipher today takes as input a property graph and it outputs a table so if you're wondering what these are what the return statement lecturer turns to you it essentially as a table it will be a usual M columns and rows they look more about that in a moment and then the other very cool Seifer is the path binding way actually you can return paths so if you look at the very last crate towards the bottom you have something called match P equals and then some pattern and in there actually we can see that we can assist me I hadn't mentioned you can actually change together as many nodes and relationships as you like so it doesn't need to just be no relationship node and that's it you can actually go on for a bit further so in there we have got a that represents a node with relationship through to an anonymous note that's one that the first empty parenthesis then going out with a to relationship and so on and so on and so on so you can actually chain it quite quite in a quite lengthy way what's in returned by P is all paths that match this particular pattern and what this will actually consists of will be in the order that that is described all the nodes followed by their relationships and for each node and felishj relationship all of the other information will be materialized in your in your table so let's see for the nodes all the relation all the em labels and properties of all the relationships you'll see the type as well as padmi as well as any M properties that are associated with that relationship and so as I said before this actually this particular facility tells you why the answer was returned this is going back to the UM the context that a knowledge graph can give you so you don't just get him on so you can actually see you know you can track back to why was it answer given what was the path between what I'm you know where I started from and through to where I ended up so I'm not going to walk you through what the particular group I work in has been working on so these are extensions to cypher and they're about four of them if I recall correctly and and the first one is actually talking about multiple graphs in query composition and don't be worried I'm going to and explain precisely with these what is meant by these terms this information before cyber today is essentially a single graph model so what happens is you you basically come into neo4j and you start writing a cipher query and what it will query is a single implicit graph and I'll say implicit is because this graph doesn't have a name it's just there it's the graph anyway the idea is to actually move that away from this single graph model through to multiple graphs model right so where we actually have the property graph database contains multiple graphs and so you may actually be wondering now okay that's that's that's quite a change why is that actually a good thing so the implications of actually having this is the following we'd actually accept multiple graphs as well as a table as input and you may be wondering why note them why would we want two tables input we've actually seen in many potential use cases and through conversations of customers especially in the more complex sectors that actually have an input table can be really useful as a driving mechanism for generating graphs right so the idea is um in this brave new world not only would we return a table as we do today but also we'd return graphs so that's probably something that maybe many of you who've used neo4j have always wondered it's great that you know I'm able to query my graph but I can only ever get the data out as a table that's maybe a bit annoying well in this new world would actually be able to return the graph as well and not just one graph but also multiple graphs depending on what you need what this actually means is that this would give you the facility to do many many things with this and that one of them is actually to create new graphs on the fly either from scratch or actually from existing graphs maybe you want to remember house talking earlier on about how you'd be looking at your customer data and destroying officer the subset of entities you're interested in so basically you know just to draw out a sub graph you can then also combine in transform graphs from multiple sources you could also have things such as views this is a very good way to actually abstract logic and this actually feeds directly into the sub query support as well a number of other things snapshots rolling up drilling down operations although some sort of business intelligence type operations but also things like access control which is certainly very very important for many many many businesses so this is all about security right so you don't a knowledge graph for instance which obviously will probably have some sensitive information being accessible to anybody within your company you obviously want to maybe limit this to only people with them correct clearance and an idea around this is you'd actually create a view over that objects you notice I'm going quite short in time so I may just need to step up a pace of it and the adjunct to this isn't called query composition so this is possibly one of those are tricky terms so what I've actually tried to do is to describe what we actually mean by that and what it basically means is that the output of one query can be used as an input to another so essentially if a query can return a graph in a query takes as input a graph you actually have composition and you can actually see in the image on the right-hand side that the first create X has inputs two graphs and a table and it forms them the blue graph in the middle and that's actually formed by some logic where the first two graphs in the table was somehow combined it's fed then through to another part of it another query which then spits out a final graph maybe it's an aggregated graph or something like that as well as a table and their day around here is that actually you can reuse logic you can actually replace parts of this um pipeline if you like without affecting other parts and you can actually in this way build very complex workflows the next flavor is actually something called complex path patterns so for those of you who are familiar with regular expressions I've actually just don't know a quick little summary up in the top left it's actually comes from academia again something called regular path queries we can actually specify a path by means of a regular expression of the relationship types so I've just given you a quick example on this so what this actually do is we follow a nose relationship and then one or more of the following either likes followed by eats or drinks so that's basically the idea of of this particular function anyway that's actually was research undertaken on Plessy grass eye grass about properties so in our world we actually need to make this a bit more complex for it to become useful so insight we'll be calling these paths paths queries and the ideas that we not only look at the relationship types of the path you are interested in but also we're looking at the node labels of the nodes along that path maybe we want to exclude all intermediate nodes that have say a dog label or something like that and also to introspect properties along nodes and relationships along that path and as a final flourish we also would be looking at actually being able to specify cost of paths as well maybe this would be bothered you know length of the path would be some cost function that the user would express much more information actually is available in that link down at the bottom and here's an example actually so in here we'd actually in and I do actually am must point out this is a very much illustrator syntax only it says by no means the final thing so other than up on the top we declare a path pattern you can actually see there's a predicate on there it's just a friend relationship between two nodes the idea then is in the match clause what you do is you'd actually insert this path pattern and that's indicated in the orange font where that occurs one or more times and then we go on from there very briefly so probably those of you who are familiar with the cypher today it's actually we use something called relationship isomorphism that's a hugely horribly and tongue-twisting term meaning that no relationships are repeated in any one pattern this particular example is actually showing for example that when we navigate out from Jack so we go to an when we go to Tom where she will then not backtrack so for example an r1 and r2 cannot bind to the same relationship within the same pattern say essentially everything ends after tom and only tom is returned and we actually have found that this has been this approach was actually never really thought about it was just the way it was developed and this is just how it's how things have happened it's actually proven to be quite useful and that customers get the results they expect to find but it has actually come to our attention that sometimes you may actually want to have repeated relationships being allowed so the idea is that we'd actually be able to allow for Christ to be configured to use whichever one of these are morphisms or of use so as I said relationship isomorphism is what we have today but you can also maybe specify that you want homomorphic pattern matching that is where the same nodes and same relationships can occur more than once in a path but also maybe people want to go more restrictive that also we don't allow the same node to peer narrow paths more than once that's even more restrictive than what we have today a couple more things as well we actually would be looking at quantifying the pattern so seeing how many um matches are actually returned so you know maybe you just want to see all these two things actually connected at all you know like an existential and pass a search or maybe you just want to get all of the matches and then also there'd be some length restrictions such as shortest and the sort of mentioning earlier as well consider only shortest paths and that's determined by the length of the path loss maybe cheapest and that's by that um due to that cost function I spoke about earlier and restrictive just be you know give me everything and finally just to end up with the schema remember how earlier I was talking about the fact that a.m. as in Rachel knowledge graph more and more over time you may actually find that you'd want to start locking things down if you want to ensure that your data quality remains high and in fact it continues to go higher we actually are looking at enriching our schema model much more so what we actually are looking at and this is just some examples there aren't many more is things like endpoint requirements so we'd actually have the requirement that then owns relationship can only ever end at a node labeled with either vehicle building sake or never end at say I'm shopping center or something like that and also cardinality constraints so for example we saying that between any two person labelled nodes you can have zero one two or three knows relationships but not more than that and as I said they a couple of other things we're looking at as well bettan yeah but that's I should like to thank you for your attention and maybe we have time perhaps for one question we've gone over a bit yeah not a problem thank you very much Patrick for the presentation and so if you do have any questions for petrol please just type those out in the questions box so we have one here when you talked about the extensions and you'd mention a graph in a really relational table can coexist so do you know of any converter tools to kind of translate a relational database to a graph database or vice versa so I'm not aware of the vice versa so I'm not aware of how you translate a graph data into relational form but so he would have tools it's called our ETL tool which allows you to actually look at your relational schema so you dog sis your bunch of tables and your keys and everything and in there you could actually pick out what what information from those tables would form your nodes and also what columns maybe say would actually also be the properties for those nodes and also what the name should be for the label that should assign to those nodes end at the same goes for relationships we actually do have an ETL tool and also we have a a tool that allows you to load CSV as well and through the use of Seifer for the loading of the CSV files you can very atomically and a define granular level specify what columns in your a CSV file will be the nodes what will be the properties of those nodes and how they'll be related to other data within that spreadsheet within the M CSV file ok can you compare kind of master data management with knowledge graph is that a similar use case or how is it different yes I think the most of data management is probably knowledge graphs - one in a way in that master data management I think first would arose from a different age it arose very much from a business context and there was just basically the way you actually are bringing data out of your silo and stitching it up into kind of the one super graph the knowledge graph then takes it one step further in terms of the enrichment sexually using things maybe not like in LP those deductive rules graph algorithms to further enrich that knowledge graph over and above what was actually initially sucked up or extracted out of the original data silos 