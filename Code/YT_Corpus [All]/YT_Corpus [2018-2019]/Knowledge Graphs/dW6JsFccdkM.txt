 welcome to Graf connect 2018 fantastic this right here right now the morning of graph Connect is my favorite moment of the entire year we have an absolutely amazing day ahead of us as Lance mentioned extraordinary content fantastic speaker really interesting all that every time at every single graph connect the best part has always been the ad hoc conversations and I think we're gonna have a lot of those today a couple of practical things before we get started this is the fifth floor this is graph Connect we talked about graph databases neo4j if you're here for the Google conference you're in the wrong floor should go to the third floor and then your learn everything about the Google cloud platform it's a great platform my personal favorite this by the ways of the marketing do it gets really nervous when the first thing I start saying and I start marketing another conference in the same building that's why these guys tell me to stick to the script but the conference are actually related as it turns out as probably all of you know the key innovation that created Google in the first place was PageRank a graph algorithm right so the way to think about it is actually if you want to use the tools and the technologies and the amazing products of the current Google go to floor number three if you want to learn how to build the next Google go to floor number five and stick around here and learn that today that's how to think about it so um I keep it very simple for my talks I have one really really strict ground rule which is that I do not want your undivided attention we are online we have hotel Wi-Fi here which we all know is a really high quality product use it test it tweet Facebook WeChat snapchat let us know how we're doing and bad the only thing that I do ask of you is that you used either the neo4j or the graph connect hashtags because we monitor those obsessively so a couple good things that Lance said one of them was know your audience I thought we would start there hands up if you have actually used neo4j hands on please raise your hand well most people fantastic so keep your hand raised if you're currently developing an application with the Oprah Jane and keep your hand raised if you're you're currently using neo4j in production probably 20% or something like that fantastic actually think we had some a little bit of fake news going on there at that at that question because they actually think that every single one of you in this room have used neo4j at least this week is anyone staying at the hotel hands up if you're staying at the hotel a few people every single hotel booking reservation that you're doing at either a Marriott like am area proper like this one or courtyard or any of the Starwood properties or like the ritz-carlton anything owned by Marriott is done using neo4j right did anyone fly here today a few people mostly local but now it's a fair amount 99% of all airfare calculations in the world are used with are being calculated with neo4j 99% it's pretty amazing did anyone take out any cash be used an ATM this week and raised so in Sweden there would be like no hands raised at this point so 20 of the top 20 US banks used neo4j today so chances are very very high that without you knowing it you've been using neo4j today so what that's was a couple of examples of people who used graphs to solve a specific problem like be it hotel reservations or air our calculations or for our detection or something like that and that's actually how we got started in the first place way back in the days way back when dinosaurs ruled the earth and I first started dabbling graphs we had a very specific problem we happen to be enterprise content management where we had a lot of connected data and we were held back by our existing data infrastructure and we thought a lot about what was going on what was holding us back why development was so slow why performance was so slow and and we realized that there was this mismatch between the data that we had the shape of the data and the abstractions that were exposed by our standard traditional data infrastructure and this is the relational database right and we realized after I was like hey if we could just have a data model that is very very simple you have nodes and then relationships between those nodes and then properties on both of them we can model everything so we invented this data model that was exactly perfect for this problem that we were solving right so it's kind of one driver for it we wanted to solve that particular problem a second driver was that it also made a lot of sense to us like it just felt like this was a very natural way of organizing data just instinctively that's how we thought about data and about the world it's it's its entities and they're connected to each other right and then finally the third thing was that we elevated connections how things are connected to be as important as the data itself right not more important but also not less important right like it had been treated in all previous data models so we elevated that and we felt like putting connections front and center the world is very likely going to be increasingly connected over time so it seems like this data model is that this approach data is kind of on the right side of history this was over 15 years ago right when we first made that those decisions fast forward today to today and that data model is being used everywhere I gave you several examples about hotel bookings and whatnot I'll tell you a couple more stories that inspire me right now the first story is in the field of medicine so a year ago at Graff connect here in New York in 2017 we talked a lot about cancer research and the 12-plus independent project are using the fpga to try to find the cure for cancer today we're going to talk about a different field of medicine and a different disease specifically diabetes diabetes is a pretty horrible disease it's a very prevalent today over 400 million people apparently suffer from diabetes and not only that it's increasing as well so back in nineteen eighty four percent nineteen eighty four percent of the population suffer from diabetes it's now twice that eight percent today one point six million people die everywhere every year from from diabetes and fortunately there's been a lot of research in that area for for diabetes for decades now which is great it turns out that you can imagine kind of clinical medical research is actually today a very data-intensive thing and you can kind of think about it how you have some kind of clinical study you study you know someone with the disease and they have some symptoms and you study the cause and effect and you get some kind of result and you store that in some kind of a database right and once you reach critical mass you start analyzing that to try to find causation well correlation probably initially and then ultimately some kind of causality right and that's great however the challenge is that the body is a very connected thing and all the various components that some parts of the body affect each other right and in the kind of the way that it's being done today if you're looking beside of these medical research institutions is that they all have their data in silos right and that's great if you want to look at something very locally right so you might have a human being right who's participating in some clinical research right and then you look at that and you study that and you find you know those correlations and Kosala T in that local data set and then in a completely separate data set you have maybe some experiment Association some studies of mice right and you look at what that looks like right and then you study that and then locally you can try to find what's going on there but never do you correlate the two separate data sets right and there might be that there are some biological pathways and some metabolic pathways that are not dissimilar between human beings and mice and if you can find that all of a sudden you can see how this thing over here actually affects that thing over there right and this is the insight of a research team in in in Germany at the German Center for diabetic research and they realized that hey actually that diabetes now have gone beyond a data problem it's actually now a connected data problem and they're using neo4j to do use just that so we have the head of data for the the German Center at Diabetes Research here today and is giving a talk and exactly this topic so you think that is as exciting as I do please check it out I think it's gonna be a very very interesting talk and with massive impact if they're successful so that's my first story my second story is about Adobe Adobe so I'm squarely in the in the software space obviously so I don't know how many of you pay very close attention but I've been very impressed with how Adobe has evolved over the past few years where they've gone been on this very big journey of transformation where they you know their origin is as a classic on-prem desktop software vendor right and they transformed into in to a cloud-based subscription software service platform all right in a very impressive way as a public company all right you know as a as a software CEO myself I've been very impressed with having done that and Adobe is temporarily you know a somewhat bigger company than we are and managed to go through something like that is it's very impressive at the core of that is what they call the Creative Cloud and one of the aspects of that is what's called B hands B hands is is I think of it as a community for creative professionals right where you have people who are have users right and you can follow other creative professionals that maybe inspire you and when you follow them you get update if they publish new content the gear you get updates if they comment on things or you know things like that right and then you can you can follow them and tag along and see what's going on right if you peel behind the layers they were struggling a few years ago because they had based this architecture on a very popular non-relational no SQL database and the functionality worked great people responded really well to it bought because of their data architecture they ran into a bunch of problems and those problems and manifested themselves in several different ways they had to batch update write so if someone published something you would only get it some amount of time later maybe five minutes maybe ten minutes maybe 15 minutes so you could just think of Twitter or Facebook or you know what would happen to that if you couldn't immediately see what everyone was posting right so that was kind of holding them back the recommendations were poor our engagement was what was was lower right and it was also very very resource-intensive it required a lot of servers it required a lot of hand-holding and ops ultimately they realized that hey this is actually a connect the data set right you can you can just see it here it's of course very center around connections what if we replace this there's no sequel database with with the graph database which is actually optimized for managing connections like this so they did that they immediately were able to move beyond the batched updates in the god a real-time view of their feed which improved recommendations improved engagement they also did this in a much more scalable and performant way which reduced Hardware right so they actually went from 48 server I believe down to 3 servers using neo4j and still performing a scaling scaling better it's a great story David Fox from from Adobe is here today and he'll give you a lot more details if you're interested in this I think it's a it's a fascinating case study and Adobe is not alone it turns out that seven of the top ten software companies in the world now use neo4j and then the final story is the story from the world of health insurance right one of the biggest healthcare insurance companies in the world they are they're active in over 30 countries they're massive here in the in the US all of you would recognize the name their entire business is actually managing networks but it's networks of people and patients and hospitals and facilities and things like that the challenge for them is very very similar to the previous stories in the sense that they ha their entire world was managing networks but their data backends was silo so all that data was trapped in Dustin's of vertical silos what they are doing now is that they're using neo4j to create the next generation data platform when they realize that hey our oldest networks what if we could look at our data as networks as well right the silos held them back cost operational inefficiencies you know slow claims processing I'm sure none of you can relate to slow claims processing in healthcare insurance but they had some problems with that and by moving to this new architecture where they were able to look at data the way it was actually naturally shaped and formed they're able to optimize and make that a lot more efficient zooming out from them I actually think that the entire world of insurance is based around networks and I think over time it's gonna be completely we are connected around connected data in fact even today eight of the top ten insurance companies in the world are using neo4j so looking a little bit more at that domain model over there this I think is pretty intuitive one of the things that we've talked about from day one with with this is that graphs are whiteboard friendly that was the observation that we did way back when when we showed our data model to domain people people who are experts in something else but are in computer science geeks it doesn't they don't know anything about third normal form or er diagrams and and stuff like that and they look at this on a whiteboard it makes sense to them like there's a patient and they're treated by a doctor who works at a hospital this this makes intuitive sense right in the graph world we call these circles we call them nodes we call the arrows between them we call them relationships and then on top of that we have this key value pairs the the properties and you attach them to both the nodes and to the relationships and with those three super simple building blocks it turns out you can model everything right and that's the core of what we're doing here today all right so let's take a step back and talk a little bit about the industry right so this were some examples of what's going on right now in the world of graphs in terms of customers and users what's going on for us as an industry right so there's a sight out there that I don't know if you have heard of it I pay close attention to it it's called DB engines and it's a little bit inside baseball but it's basically a site that tracks all these new databases and they track actually over 150 database projects last time I checked which that in and of itself is an interesting thing when I got started as a programmer in the 90s there were four databases that you could choose from right there was like Oracle and there's David - and the receive four makes and Sybase and that was it this was even pre MySQL right and arguably they're the same right they're like they're say at least they're the same type of database so really your choice of database in the 90s was like a vendor choice right that is it fast forward to today they tracked over 150 right across many different categories so that alone says something about the value of data today right so they do a lot of different things one of the things that they do is that they compute basically a popularity score or a buzz score where they they look at Stack Overflow posts they look at tweets they look at LinkedIn job postings LinkedIn scales indeed com job postings and a number of different signals like that and they compute a score around how much buzz there is so much talk there is about a particular database and that's interesting but what's what I find even more interesting is that when they break it down per category right so with all the various different categories in in databases here's how they've evolved since they're since 2013 and it turns out that graph databases have actually been the fastest-growing category in all of databases since 2014 which i think is pretty amazing we see that at neo4j today 76% of the Fortune 100 are either you sing or piloting neo4j which is a I don't know the exact number but if I if you if you've been at Graff connect in 2015 I think that number would have been zero right this is happening very very very fast and in fact many of the leading organization that's some of the biggest verticals in the world or using neo4j I mentioned 20 of the top 20 US banks 20 of the top 25 global financial services institutions right three of the top five logistics firms seven of the top ten retailers it's pretty extraordinary graph adoption in the enterprise is no longer a theory and I hope it is very much here so what are people then doing with the graph databases well we've talked about the number of different use cases and and the challenge is use cases is that in some way they kind of limit us right because what we have is fundamentally horizontal technology the use cases are wherever you need to work with connections in data when you don't want to ignore how things are connected you can use a graph database right that's the that's the ultimate expression of the use case right but how does that impact the business right and we've identified top six and this is probably today I would imagine between 60 and 65 percent of all graph adoption at least commercial graph adoption belongs to these six use cases and the first one is real-time recommendations and that's basically if you're a big retailer again seven of the top ten retailers in the world are using neo4j and of course your entire it's kind of the tectonic shift in your industry right now I was going online right when you go online Amazon sorry to mention if there's anyone in retail to mention it forbidden word but Mazon has taught the world that you need to do recommendations so other people about this also bought that type thing right and it turns out that that's really very well-suited to connect the data right you know if you look at like a big purchase list a tabular form is one way of looking at that if you take that exact same data and you actually look at it in a graph form you see that this is mo Emeril has bought these three items and here's someone else who's also bought those three items so let's let's take that and take the fourth item that Emilie's bought and recommended back to that person right because probably they have similar taste it's a very very simple collaborative filtering recommendation or an open triangle recommendation it's one or two lines of cypher in India for Jay it's a very popular use case fraud detection is also hugely popular I'm sure here in New York there's several people from the financial services industry fraud detection is very popular in financial services it's basically being able to capture fraud rings where you have a number of transactions that are in and of themselves or okay but they're connected in a way that isn't okay that is fraudulent right if you can't work with connected data that's really hard to detect right and graph databases allow you to do that Network and IT operations it's kind of obvious in a sense right if you're a big telco your entire world is about managing networks it's kind of the equivalent of that healthcare insurance thing right but it's physical networks and you want to make sure that they're up and running so you want to do root-cause analysis you want to run what-if scenarios hey if all of a sudden I have a dashboard and 10 devices stopped working at the same time is that because all of them mysteriously fail at the same time or they may be connected to one power unit or they're connected to one firewall or something like that right figuring that out requires to have you to have a model of how things are connected and that's obviously you know a good fit for for graph databases master data matters which is a very broad use case it typically breaks down into customer 360 being able to get a consistent view of how your individual customers connected both the internal systems and to external systems like social media and data lineage which is also very popular in financial services and in also in government in heavily regulated industries knowledge graphs is the topic that we'll spend some time on later so I won't dive too much into it right now and then I dented in Access Management which is about keeping track of how you relate to content and assets that you may or may not have access to you and how which groups you belong to and and so on and so forth so it's been interesting to see in in recent years it used to be that we were brought in and maybe not 100% but absolutely most of the time we brought in as a very specific point solution to a problem typically one of these problems that's typically how adoption started you have a recommendation engine project you have a fraud detection project or something like that what we're starting to see now is people having success with that first use case and then we start seeing more and more use cases inside of the enterprise and it's sometimes the same use case right it's sometimes recommendation but for another part to the business for example but more commonly than that it's another use case right so today almost half of our customers actually have one two or more projects which i think is pretty interesting another thing that we see now is where we're getting brought in not to solve a specific point problem but more as a generic data platform technology when people realize that hey there's a lot of value if I put put data in this platform where I can maintain the connections and we get brought in not to solve just an arrow thing but as a broader strategic platform so one example of this there's a number of big global communications companies right now they're using neo4j as the platform to back their entire smart home initiative and obviously there's a lot of slices of use cases in there but ultimately it's the one data back in that connects all the data this is something that has been very interesting to see evolve because as you get more data into the into these databases it enables more use cases and in fact it leads to what's called a data network effect and for all of you here in this room we know that network is as a synonym for graph and graph is a synonym for four networks that's obviously very related to what we do but it's this phenomena that when you have a product that becomes better than more data that it gets but it that product in itself also generates more data so you get this virtuous cycle that ends up driving a lot of product usage but also a lot of our deep penetration within these customers cool so what's then enabling of this so a year ago here at Kraft Connect in in New York I wasn't staged and I announced a broader vision for the company up until that point we'd been squarely focused let's call it the OLTP of graphs right the Oracle of graphs maybe you know it connected there because database would connect the data but we felt that what we observed was that when we talked to many of our customers both big and small we saw that many of them actually ended up reimplemented a very similar stack right to put neo4j in there but neeraj a very seldomly exists in isolation we're part of an entire ecosystem of data and technologies and typically you want to have some data in your relational database and some data in your data Lake right and so then people end up building this integration tooling right this glue to get data from your maybe your data lake into neo4j right and then a lot of people realize that a that will whiteboard friendliness that that dude with this Swedish accent talked about that actually not being true so even if I have this database which is a very back-end oriented thing if I could just show that to the business like it would make sense to them right and that's really powerful and so then people add some kind of graphic realization on top of it right from a link curious or key lines or a Tom Sawyer right and they add that on top of it and then we saw that hey what if we could just take that package it all up in one cohesive package and offer that to our end users I think of it kind of as the the lamp stack but for connected data it should be like the one-stop shop for everything that you need end-to-end to get up and running with with connected data we announced that a year ago we said hey this is a multi-year vision this is what we're going to drive the company towards over many years to come this is not something that you that you then spit out a quarter or two later but we've been really hard at work trying to implement to get to as close to this as possible and I'm really excited to be able to tell you some of the the progress that we made in this year and I'm gonna invite up to stage the the person who is the absolute best on the planet describing this journey that we've been on and the the progress that we made for the graph platform on the graph platform which is none other than you know Jay had a product feeling roughly please give him a warm applause thanks Emma welcome thanks to all of you for coming here I'm gonna give you a quick tour of what's happened in 2018 with the graph platform at the center of all this of course is the database and May we released a new version of neo4j version 3.4 lots of great features in here I'm going to focus on just a few one of one important one is Multi clustering something we're quite excited about with clustering something you've always gotten is the ability to handle high volumes of reads high availability and high performance and when I say high performance this is one of the things that you love and appreciate the most about neo4j is I'm hearing every day is this we often use the term minutes to milliseconds to describe queries that take minutes or even hours and other platforms and run in milliseconds or seconds in neo4j so clustering gives you all these three things together what Multi clustering does is it builds on this concept for the specific case where we have multiple distinct graphs being managed in your database what it lets you do is to spread those databases out across different parts of the cluster so that you can have not only Reed scaling high availability high performance but linear right scaling as well this is something we feel is becoming quite important and useful across a number of regulatory scenarios we saw gdpr come out in the last year and there are lots of cases and lots of reasons why you would want different data to be stored in different places and never the twain shall meet Multi clustering gives you that another feature that's been getting a lot of traction is bringing together the worlds of graphs and the world of 3d 3d geospatial the way it works is you essentially have a new point system so this is a new data type in neo4j that supports XYZ coordinates and you can basically take that and bring it directly natively into your cipher queries what the way you would use it is you would say here are a set of points that I care about now of my data's tagged with coordinates and in a particular query I can simply say what is the radius of the particular things that I'm interested in and that'll either filter or you know cipher will find the right way to optimize the query and bring back the points of interest inside of that particular set so this is very powerful I can take a recommendation and bring into that not only 3d space but also a time where we made something from it improvements in 3/4 as well so a way you might use it is let's say you find a building and once I have the building well now I have the Z coordinates so I can have the elevation I can find the particular floor of interest in the building but once I get to that floor I can actually rehome the coordinate system around the things in that floor and get to the particular rack or the particular place on the floor that's of interest a cool feature I'd encourage you all to check it out and use it finally you can never get enough performance as much as performance is something that's valuable that neo4j does well we're never satisfied our engineering teams never satisfied you can never get enough a number of areas in performance improved with neo4j in fact one one of the multi-year journeys that we've been on is taking the single biggest bottleneck for data ingestion and making it very very fast eliminating that bottleneck and specifically that involves using Lucene in an appropriate way which is which is not for for graph it's not well-suited for acid high volume graph transactions so what we've done is we've created our own native index structure we did that about two years ago and with every release we've steadily been moving different kinds of schema indexing into neo4j native indexing this improve the performance somewhere between two and a half and 10x so on average you can expect a 500 percent or so improvement in going from a non in going to native in index back neo4j data big improvements there and really across the board including with cypher we've gotten a lot of unsolicited remarks from customers we've upgraded to 3/4 saying all right my application is suddenly running roughly twice as fast across the board so very happy about that so that's three four in a nutshell let's continue around the graph platform and talk about graph analytics this is an area that's gotten a lot of attention and it's not surprising once you have your data in a database it's only natural to look at that data and one and mine it and say what one more information can I get out of it and graph analytics is actually a quite a rich area in some ways you could say that a lot of the transactions that you run are analytic taking things that previously had to be done in batch running of them in real time but here I'm talking about true as a data scientist or data analyst going in understanding analyzing the data and using it for new kinds of things one of the three areas I'd like to highlight here is graph analytics this is a whole new area in data science well not not a new area but a newly exploited area for many like I will mentioned earlier with with Google and with PageRank that has the potential to change companies so this what we've done in the last year is we took the graph algo library that we announced here a year ago actually and expanded on it with a number of new algorithms and we took the algorithms who've already in here and we made them faster there are a couple talks on this today and I'd really encourage you to go and check it out and learn about this area and this new capability another one which is not ready yet but we also announced it last year and it's there's been ongoing progress on this is bringing Seifer querying bringing graph query directly into your data Lake through SPARC another area that we are very excited about and that we're getting very close to you'll also find a talk on this today about taking graphs and tables and being able to interchange views between the two using spark with cypher on top of all that again very exciting work and last but not least and I'll say the least about this because you'll hear a lot more about it in the keynote and throughout the day is the integration between graphs and AI so lots here to talk about there are a number of sessions and I won't say more continuing the walk around the graph platform I like to talk about this new product called neo4j bloom it was freshly freshly released this summer and it addresses something we've been hearing about for years which is making visual code free navigation available not just to technical users but to non-technical users I've seen many of you go to the business and have the business look at a graph in the neo4j browser and say oh I really want this this tool can you give me this tool and of course you can't do that with a neo4j browser you need to learn to write code and write cipher what you really want is something that's code free that smooth that's visual where you can just explore and sort of you know not worry about code but just engage in interact directly with the data this tool is really meant to inspire users including non-technical users to understand the data better as it applies to their business and directly experience the power of graphs we're really excited about this we have a bloom booth and some some talks on bloom throughout the day one of the things that's most valuable about the platform is not the neo4j technology it's what you can build with it and it's not just what you build with it as an end user it's what other technology vendors and communities and projects build that you can then use so you can actually you know in addition to your data network effects get some software network effects Lanson ml have both mentioned and I'll repeat it again we have lots of great partners here at the conference today who can talk about the services and the products that they offer including different perspectives on visualizing data particularly you have lots of users and you want to do more complex things so please check those out as well that's one of the most important considerations as we build out the platform is building it in a way that's extensible and leverageable and reusable now on that note none of this would be possible without the language and one of the so one of the foundations for all this has been the cipher query language which is which becomes a shared language for accessing graphs across all these different vendor technologies and we've been working hard on making cipher and making graph querying in graph pattern matching universally available and when I say we I don't just mean we neo4j I mean we the technology community and by which I mean other other technology vendors academia and the end-user community and there's been a lot of work happening over the last few years around this and quite a lot this year and we're learning better and better how to work together and what you the community needs it's very clear that you need a universal language for this to become an increasingly useful category and there's been a lot of a lot of great progress on this in fact more than 1,600 hundred of you voted earlier this year and told told us that what you want is the vendors to really get together and start moving this towards a process of formal standardization very happy that we're you know making a lot of progress on learning how to do this both to evolve the language and to agree to to how to do this together if you want to catch a glimpse of where this is going and how this might look in one to three years I'd invite you to attend a couple sessions one is by one of our guests Keith hare who's the chair of the international ISO sequel Standards Committee for the for the international division and the co-chair of the American Association sorry committee and and also the talk by Alistair green and Stefan plenty call about the bring together cipher into the world of spark into your data like an exciting area and there's a lot of movement happening here and a lot you can look forward to wouldn't be a proper graph connect if we didn't talk about what we're currently working on and what's next so without further ado let's talk about neo4j 3.5 there are lots of great things in this release I'll highlight three the first is possibly the most highly requested end-user database feature that I've heard about in the last few years it's also about bringing together different functionality different languages different communities and capabilities and this is full-text search we've been baking this little by little and happy to say that in three five you will be able to bring the full power of full text into your graph not just for your nodes but also for your relationships you can full text relationship properties as well next is we're finally completing this multi-year journey with moving off of leucine for the things leucine isn't good at of course we're using leucine properly for the things it is good at which namely is full text and so this is going to make a massive difference for accelerated data ingestion so you can get more data in to solve more kinds of problems and to benefit from those data network effects and then finally last but not least again you hear community as the theme of the day we're excited to make neo4j more accessible and useful to the NGO community and with an official go driver and the way we've built this is Ashley the go driver uses a new C connector that we call C bolt that's been designed to be the basis to be a low-level driver Nuvi be the basis for new drivers and this is something we'll be talking to are talking to our driver community about in our tech community about at the ecosystem summit they were actually doing this Friday and we can't wait to see what the community the driver community does with it and what new community drivers might emerge so that's coming out before end of year but you can go to our website now in download alpha 9 which is the very latest work-in-progress version of neo4j 3-5 we're just about all these features that are going to be in the release have landed all right that's that's a quick review take advantage of the sessions today there's lots of deep dives across all these different areas check out the booths and mo I'll hand it back to you Thank You music all right cool so lots and lots of stuff there and that kind of the big thing that ties it all together is this graph platform umbrella if you even though that is like really compressed and a lot of information and a lot of features and functionality whatnot that actually just scratched the surface is he found any particular piece of this particularly interesting there is going to be a talk a deep dive talk on every single feature that that Phillip mentioned there's going to be a talk about today here are some examples of some of the talks about various pieces of the graph platform and what what Phillip just just talked about today I do not expect all of you to memorize to read that right now but it's all in your little brochure in your in your badges switching gears a little bit I'm going to talk and close with that with a topic that is near and dear to my heart and that you've all heard a lot about which of course is AI which many of you may actually be working in and specifically of course how graphs relate to machine learning right and this is a field that you know we've been paying close attention to last year I was in stage and I showed this graphic or which I thought was a was a very useful I found it to some Social Feeds somewhere but I thought it was a useful taxonomy of very parts of the AI landscape but of course what struck out to me was that the way that they visually described all these different subfields was as graphs right so there is something very intimate about going on between graphs and and machine learning and the big then begs the question for all of you who are working with some kind of a machine learning like how can you use graphs to to make that that better right and the answer in a nutshell is very simple what graphs provide for AI and machine learning is what it does for your normal software applications which is the power of context and the power of connections right last year we spoke about one very central use case for for AI and grass which is knowledge graphs right here are four speakers who spoke in about their knowledge graph use cases at graph connect before it since 2015 and and onwards I think grab knowledge graphs there's a lot to be explored in there but there's a lot of content out there already it's starting to become a fairly well understood field if a very well understood intersection of graphs and AI so I'm not going to spend too much time on that today I'm gonna talk about more specifically how graphs impact machine learning and in fact later today there is no more than 13 talks about machine learning and graphs so you can you can look that up in your and in the in the agenda and there's lots more information for you to to to look at to to consume today when it comes to graphs in AI but so take let's take a look at kind of a standard machine learning pipeline that I'm sure many of you work with today right and it starts out with some kind of data sources that that you have right and then you extract data from that you know some kind of data records which we like to call features right you feed that your machine learning you train a model that model hopefully predicts some really good things and make some kind of decisions and you know off you go right very very standard of course for all of us here in this room we know that data is awesome but like what connects individual data pieces is also awesome and in fact the world seldomly looks like that top row it more frequently looks like this it is connected as I think we've said a couple of times on this stage and you will hear throughout the day and so what's interesting about this is that so we're training these models to try to predict things but we're kind of giving them let's call it a black and white view of the world we're missing an important nuance which is how things are related and I think that's pretty crazy and in fact I'll go out on a limb and say that's scientifically crazy to do that and I do that based on one guy who has scientifically proven that that is crazy that's probably me stating what his research is I don't know if he would state it that way but the guy that I'm talking about is a gentleman by name of James Fowler and he's social science professor at the UCSD I wrote a really interesting book called connected he likes being on shows this is him at some show and this is him that's a much more important show which is Graff connect in 2012 the first ever Graff connect he key noted it and he talked about his research and his research is super interesting what he's proven out is that he's researching various fields of social science right so one of the things it looks at is his smokers versus non-smokers it also looks at election turnout what can get people encouraged to go to go to elections and vote and happiness that has happened as spread in in society and one of the things that he saw that I blew my mind in 2012 this is quite a long time ago right he said that if I want to predict whether you are a smoker and I have two options I can either get all the facts about you from us graph people or the properties directly on the node like all the facts about me name age you know medical history you know demographic all of those facts or like I get to know whether the people in your graph or already are smokers if I have those two options I will be able to predict with much higher degree of confidence that whether you're you are or will be a smoker with the second set with the graph in fact not even just your direct friends but not even friends of friends but friends three dead three three three hops outs your friends of friends of friends so if all he knows is the friends of friends of friends and whether they're smokers he will be more accurately be able to with higher degree of confidence to predict whether you are will be a smoker than if you know knows everything about you right that's pretty mind-blowing I think pretty pretty fascinating right but then going back to why are we training our machine learn language like the goal here is to be able to predict things and make decisions right so at the end of the day this is probably not what we should be doing what we should be doing is more akin to this right we should train our models and and make them see the entire world not just individual data records but how things are connected right this is called connected feature extraction and there are three distinct techniques on how to do this and I'm happy to say that there are talks and each and every one of these techniques today at the conference there's also a great overview summary talk by Amy Hodler and jay crane that talks about broadly speaking how to do connected feature extraction using using neo4j this is one of the four pillars of graphs and machine learning together with knowledge graphs there's two more graph accelerated AI I don't have time to go into them in in detail but basically the observation is that there's a lot of sparse matrices involved in in machine learning and what I remember from my math class way back in the days is that you can look at any matrix as a graph and then the graph is a matrix and sparse matrices are really really powerfully and more are easier to compute and more resource cheap to operate on as a graph so that's what number three is about number four is I don't choose my favourite children here but like maybe the most fascinating topic to me which is around a I explain ability there's that's about I'm sure you've all heard of it like taking that black box of AI and machine learning and make it more of a gray box try to figure out why did it make the decisions that it did why did make the predictions that it did provide some transparency and visibility into that it turns out that there's a lot of decision trees involved in that and I think there's a huge role for us in the graph community to play there you saw bloom and graph visualization that's one way of visualizing these decision trees which obviously a tree is a graph so those are the four fields that where we see currently graphs being used in AI and machine learning and you're gonna hear a lot about all of them if you so choose throughout the day so that's kind of how we look at that and that's a perfect segue actually to to the next speaker who has been a huge hero for me for many years I first heard about her as the chief data scientist at bitly she subsequently moved on and founded her own company fast forward labs and is currently the the GM of machine learning at Cloudera she's done some of the most amazing I think riding out there on on machine learning and again not to choose your your favorite to learn but maybe what I at least found that some of the most interesting is her riding together with DJ Patil on AI and ethics I'm sure she can tell you all about that so without further ado please join me in welcoming Hilary Mason onto the stage [Applause] good morning everyone thank you oh you're awake it's really a pleasure to be here I've actually been a fan of neo4j for quite a long time I've used it in some of my own projects and I'm really excited to to join this community here this morning I hope you've all had your cup of coffee and for those of you who traveled to be here those of you who are watching on the livestream I put a picture of Times Square up here so you can get a sense of where we are and I'm sorry for those of you who came to New York that you're here in Times Square all day the rest of the city is really nice I grew up here I'm allowed to say that um so uh yeah we're gonna get started um this is actually a talk about machine learning and AI but also about metaphor and if you take one thing away from my talk this morning I want you to think about the way the metaphors we use to think about problem statements and the world that we're creating Drive the architectural decisions that we make they drive the design of our algorithms and they in fact design they drive even our creative solutions to the problems that we want to solve and so throughout this talk though I am talking about AI and machine learning I've chosen to highlight metaphors that have led to certain design decisions and and try to give you some context just to why things are the way they are so if you take one thing away from me today it is that metaphors control the way we think about the world and there are powerful tools for creating new kinds of solutions and opening up new opportunities so before I move forward how many people in this room are actively building something you would call a machine learning application okay we have maybe 30% and so for the rest of you how many of you think you may build a machine learning or AI application in the next couple years hopefully that's the rest of you okay so so I think this is a good topic for this room for those of you who you think machine learning is a buzzword I think you're going to be disappointed it's my opinion all right so the first metaphor I want to share with you is is this one this is on the right side you have people on a train in the 1800s so you might wonder why it's because that was the metaphor that led to the creation of the system you see on the left side so Herman Hollerith was a New Yorker in the 1880s and he realized that we could not count the u.s. census using the technology of the time so they actually would send people to do it by hand on paper it didn't work and so he was inspired by the way that train conductors would punch holes in tickets on trains to create this machine that was an automated counting system and you can go google it he wrote a patent which unlike the patents of today you can actually just read yeah it's clear language anyone can understand it and it was a patent for this machine to count the US census the metaphor led directly to the design of the technology and another thing I'll point out here which is a funny little side note is that the punch cards that he created were the size of US Treasury in the 1880s because he got a bunch of postal or currency boxes really cheap those punch cards by the way his company was called the tabulating machine company it you know my favorite joke is from Vint Cerf who once said if computer scientist named Kentucky Fried Chicken we would have called it hot dead bird so his company through some mergers acquisitions and marketing talent became IBM though that original punch card design led to the the later design that you saw in the 1930s which has led directly to the architectural metaphors that we all use today in our computing environments so here's one example of metaphor we've been doing this data thing a long time right and today if you look at headlines about a I'm going to show you a few of my favorites this one is about somebody who made a rap a bot that raps like Kanye West so what a guy does this is one I actually every so often I go to Google News and I type in artificial intelligence and I look at the it's about half doom and gloom and half you know the other side of things optimism you know AI is curing cancer and by the way it's also going to destroy a society and if you can see on the lower right of the screen and by the way this is from yesterday so this this is not this conversation is not progressed we have a lot of work to do and here's another one that I like we're reporters set out to become BFFs with a chatbot and the way I'm the reason I'm highlighting it for you here today as he said you know the chatbot the a I forgot my name and of course it says in the chat transcript if you can read that you know hey what's my name and it says hello undefined and of course nerds know why that is but um but to normal people that might seem a little bit weird um the reason I highlighted this example is because it shows the way that the vocabulary we use changes the way we think about the technology so we've moved from talking about AI and machine learning as computer programs things we are all quite intimately familiar with and nobody really gets excited about except perhaps the people in this room too talking about them like these magical boxes or these creatures that are going to emerge from nowhere take our jobs they're gonna rap like Kanye West and then they're gonna forget our names right and so what I'm hoping to share with you today is a way to think about this practically and I am relentlessly practical in the sense that I get very excited about things but then like many of you I actually have to go home and build them and they have to work and that means that we need to have robust metaphors for talking about what we want to do before we can even decide on the solution and I often find that in machine learning it is posing the question that is the challenge the answers are either trivial or impossible so so when we think about the vocabulary today that we use around data 10 years ago we would be having a big data conversation and that was really just the ability to get all of our data in one place and count things in that data and that was it that was I mean it sounds very simple but that was absolutely transformative at the moment it was not transformative because we couldn't have done it already if people have been doing this for a very long time it was transformative because something that became inaccessible it was inaccessible and incredibly expensive and it became cheap and accessible and that opened up this entire piece of work new use cases new applications creativity around the technology that was ten years ago in big data and we see the same thing happening today in graphs once you can count things you can count things for business purposes you can do analytics you can spread out the kinds of things you want to count you can put that technology more people's hands you're counting things for a reason when you can count things cleverly you have data science so you can model things you can predict you can start to look at things that haven't even happened yet and build representations of your data using different metaphors to explore the world and when you can count things cleverly with feedback loops in systems that appear you know useful and magical that's when you have machine learning and AI but again it's a peer that's important the technology is just moving up the stack but you cannot do it without being able to do data science without being able to do analytics without being able to do big data or have a fundamental data representation so when you think about those metaphors I would not dismiss AI out of hand excuse me but I would think of it instead as a new label for this technology where we've moved up the capability stock where things that were previously expensive or out of reach have become possible in reach and useful to a wider variety of people and this is why we see this flowering of interest in machine learning and AI today and I'll add also it is up to us as technologists to make sure that people understand what is real and what is not so what does it look like when we do it well well I'm gonna share a couple examples one positive this is one of my favorite machine learning applications of all time and this is Google Maps with the traffic view turned on just to be clear I don't work at Google I'm just fan of it the reason I'm a fan of it is what is special about this it is really really boring right so you can be driving down the road you have to make a decision about which route you're going to take you can look at your phone while you're driving a deaf machine by the way I lived in New York City I don't own a car and haven't in over a decade and it seems totally nuts to me that anyone would and you can make this decision while you're driving using this app and you can figure out the best way to go where you want to go and you don't think about it at all you do not need to be even a little bit aware of the technology behind the scenes to power this you don't need to know that every phone running Android going down the street is sending data back to Google that they're doing incredible predictions using historical data with that real-time data you don't have to know anything about the cellular tower network that is coordinating to stream that information down to your device and you don't even need to think about the visualization because everyone who grows up in our society is trained that green means go and red means stop when you're this big and you can just look at this and understand what it means so it's quite a successful machine learning product and the most important thing about it is that we don't think of it as a machine learning product at all we just use it get where we're going and then turn it off and that's what success will look like when we stop getting excited about AI and we start getting excited about what the applications we build actually can do for us but this is really hard and it's hard you know even for the best people in the world added so I'm going to share a quick personal example of being an edge case in this data data enabled universe in 2007 there was a startup called cool and you can tell it's 2007 because Google Reader was in my tabs at the time and they said we're gonna defeat Google and we're gonna do it by pairing images with web page results so images with text Google didn't do this at the time it was a big deal but that is my bio at the moment this is the actress Hilary Mason who happens to share the same odd spelling of the name Hilary in her later years playing the role of ugly hag in a movie so this is really not very cool as far as I'm concerned and you might say okay that was 2007 it's a decade later Hilary why are you still on about this thing I'm not that mean so 2006 I think we are at this point Microsoft being another company that has some of the top talent in the world in this domain rolls out there you know sort of celebrity visualizer inside of Bing search and I by the way I'm not the celebrity I am the nerd who happens to share the name with the celebrity so you can look at her bio that is my photo you know for everyone I am not dead just to be very clear for everyone in this room so even again some of the best people in the world at this you know struggle to being the right data at the right time and then a couple years after that once again I was looking up this movie robot jocks which is like a 1991 movie about a robot punching another robot and it is awesome and of course you know there I am thank you google and I actually I loved that photo so I'm glad to see it pop up there and of course what do you do when this happens you complain on Twitter and it got fixed but then I was giving a talking year ago and I thought oh this is a really funny example I think I'll use it again and you know okay there I was so the reason I bring this up is it's not just to talk about about how hard this is but to say that I'm even the people who are best at this in the world struggle to do it well today and that is why it is exciting that is why we need these new metaphors new ways of representing and thinking about our data and we're waiting for the way that we think about technology to catch up to our capabilities so this is the moment to be where we are sitting today this is why we should be having this conversation and thinking about the interplay between metaphor how we think about the world the architectures we decide to build and many of the big opportunities to do this well are right here and so one of the big advantages I have from where I sit is that I get to work with a wide variety of people who are working on really interesting machine learning and AI applications and everybody thinks all of the cool stuff happens in startups but I actually want to tell you that while a lot of the cool stuff happens in startups it is people who work in businesses that have operated for some time who generally have collected a large amount of data as a side effect of operating those businesses who are also creative and ready to embrace these new metaphors for development who have the biggest opportunities and these people may be in Fortune 100's they may be you know researchers tackling really interesting challenges not all of the cool stuff is in little tiny companies that are highly resourced and highly data constrained so if you come from a company where I met someone in the hallway this morning who's saying you know oh I work for a supermarket chain that's you're the right person to be here you're the right person to be thinking about this you have these big opportunities all right so I'm gonna change tactics a little bit that was my intro and now hopefully you're thinking right this is machine learning now is the time I want to do this but I'm also supposed to tell you what the future looks like and if you do work in machine learning you know that predicting is hard and predicting the future is really really hard and so when I got signed up to do this I thought okay you know I'm speaking to a really smart audience here's what I'll do instead I will tell you how we in my applied machine learning research group try to see the future I'll tell you what we see and then I'm gonna let you take that methodology and bring it back to your own work and so I hope that you will tell me what you see as well so at fast forward labs which is now part of cloud era so clutter at password labs we do quarterly reports on emerging machine learning applications designed for real business use cases which means that our team reads 30 papers write some code to try to figure out what will actually work at scale on the kinds of data that people use in the real world and then write that all down so our goal is to be our customers nerd best friend to accelerate the pace at which they can take advantage of many of these technologies and that means we have to see what's coming and we try and aim six months to two years ahead of what you will see in production so given that here's our secret first step drink coffee have ideas you can tell I've had a big cup of coffee this morning but this is pretty much my usual state what I want to point out here is that you need to have a lot of ideas and that means deliberately trying to have bad ideas when I visit a company and they show me the machine learning projects they're working on and they are all good ideas I get very worried and the reason I get worried is that if you only pursue the things that are obviously good ideas you are missing out on a lot of the opportunity for things that might be a little bit risky but could have a huge potential payoff if you are able to accomplish them so have a lot of ideas and then validate so go is broad and wide as you can and then once you have that collection validate against criteria that can be fairly quantitative and these are the ones that we use so first looking for active research activity that's relevant to a particular machine learning application that means are people publishing papers that are relevant are their papers and one academic domain that could be moved into another one of the advantages of having a team like we do where people come from so many different backgrounds so we have computer scientists physicists neuroscientist electrical engineers and so on is that people have solved these problems in one field but they didn't bother to tell everyone else and so when you get that kind of diverse thinking in one room you have a lot of creativity and a lot of potential knowledge that can move from one place to another a second is there a change in the economics of the systems required to architect a solution meaning what is the cost of say GPU CPU compute I could draw this same graph for pretty much any of the systems we rely on and this one I found on Twitter which i still find to be so compelling which is the micro SD card where the capacity has increased orders of magnitude over a decade same price same cards same form factor right this world we live in is one in which if something is out of reach for you today don't throw it away because a year from now two years from now it might be cheap there might be some service you can just use to do it um we look for capabilities becoming commoditized particularly in libraries and open source Hadoop itself is the core example of something that was very hard and expensive to do even though people widely knew how to do it once the open source project became reasonably widely adopted you could take for granted that you could have that infrastructure in one place and you could count things and you could get an answer fairly easily but we see this happening like word Tyvek is another great example in the machine learning space where word embeddings or something very mathematically complex and if you want to write your own it's going to take you some investment to do that at some time and energy but now you can just sort of download them off of models you and you know go off running in a couple of hours commoditization is a moving wave and it's hugely powerful for our ability to execute on machine learning and the last thing is that data becomes available and this may be data that's internal it may be data you're generating because you launched a new product or launched a new feature it may be data you can collect from the world it may be data that you purchase wherever the data comes from you need data to be available in order to pursue machine learning the reason I have the Wikipedia page on data science appears for two reasons one is that Wikipedia is the dirty secret of every NLP application at least run by startups because it is widely available and license allows you to use it for commercial purposes the second reason is that the data science Wikipedia page for a long time cited the creation of the data science Wikipedia page as a major milestone in the evolution of data science as a profession and that is the most Wikipedia thing I have ever seen and once you have your your criteria you go through that set of ideas and you say okay these things might be possible these things probably not you can progressively explore these capabilities and so in our group we do a three-hour lit review which is really just googling and reading abstracts of a bunch of papers to say whether we should or should not look further once we do that we pick a subset do a read a couple papers come to some individual point of view on whether it's worth the investment once we do that we take the subset that passed that filter and actually go write code and so that kind of progressive exploration allows you to consider things that are weird or risky and that might otherwise not be worth your time because you've bounded the investment of time you're gonna waste on it and to it lets you have a way to repeatedly and more importantly depending on who is doing this work you know get to the same answer no matter how many people are involved in the process just to show that predicting the future actually is hard these are postcards from France from around 1900 predicting life in the year 2000 and they're largely self explanatory but we still don't really have any of these things so there's a like children in a classroom in the middle wearing calendars on their head just getting the the knowledge pumped right in there on the right side there are firefighters with wings flapping their way to it to put out fires now we do have technologies that address these things in ways that the people of 1900 could not even daydreaming about and yet you know predicting it exactly is a challenge so if you get the direction right you're doing pretty good alright so I'm gonna share with you a few of the actual technologies that we see I've selected a subset here so they're a lot more but again I'm talking about what machine learning can do you today and in the near future all of these things we converged on based on that process I have just described so now you know how to see the future and I'll tell you what we see there one of our reports is on natural language generation and it's one where if you see it in the news you tend to see stories like this it is true that the Associated Press actually has somebody whose job title is automation editor and his job and I've met him is to actually oversee the algorithms so he has know people who work for him he oversees the software system that generates a fair set of their stories we built a prototype that generates real-estate advertisements so you tell it the kind of apartment you want to find in New York City or you want to sell in New York City and it writes the ad for you it gets really funky when you tell it things that don't exist so you say like I want to sell a one-bedroom 16 bathroom apartment on the Upper East Side with a doorman and it'll come up with things like this sun-filled home has a lot of bathrooms you know um but the point of this technology is not that that robots are writing articles but rather that we can take when we think about how we as humans interact with data you can imagine columns of data or graphs of data and only a rare set of people get excited about that you can imagine a graph like in the middle here and that's something and we expect every professional to be able to read a graph to be able to use Excel if not more but most people really intuitively understand a language based interpretation of that data and that's what this technology gets us we can go from structured data to a language based representation and that means a couple of sentences that say what it is that's where the power is is in bringing the understanding of kind of structured data to a much wider audience through the power of language and we've seen this actually go on to be implemented I'll tell you two customers so one is a bank that use this to automatically generate compliance filings there's a whole interesting story there the second one is a celebrity fashion magazine that already had a structured JSON feed of celebrity clothing which believe it or not is the thing that exists and they were able to quickly create a mobile app where you could you know get two paragraphs about Kim Kardashian wearing a sweater and then you could buy that sweater using this technology so the same math very different forms again metaphor here probabilistic methods for real time streams we think there are new architectures that are necessary to understand the world in real time when you think about this from an engineering point of view you go from a metaphor aware perhaps we keep our data in some sort of batch environment and let's say we want to take an average so we just count up everything in the set and then we divide it by the number of things in the set and we have our result it was pretty straightforward but depending on how large that is or how distributed it is it might actually take quite a long time to calculate but what if we live in a world where our computing device is something the size of my thumbnail and we only have a highly limited amount of memory and our data comes in as a stream of thousands of events per second perhaps how would we calculate an average in that environment and so the answer here is of course that use an algorithm called reservoir sampling where you have a constrained amount of compute a constrained amount of memory required you have a certain number of buckets that you're always probabilistically updating and you can take the average at any time and it's just n because you're taking the average of and buckets what you do have instead of a correct answer is a correct answer with error bars now this is a metaphor for thinking about the design of systems that I like to imagine is the way we will finally build that Star Trek tricorder that we have yet to do that instead here's a demonstration of running it across the entire corpus of comments on reddit running on one small ec2 instance and you can see things like similarities and language clean um it's like shower thoughts and why I'm single people talk about the same things and those little corners of the internet um but this is again a metaphor and of course the graph metaphor here that allowed us to build a system that just would not have been possible if we were constrained by the the older metaphors of design of architecture and so there's a ton of power in these probabilistic techniques that we are broadly as a community just really starting to exploit and then of course I can't give a talk about AI am machine learning today without talking about deep learning and deep learning for those of you who have not been hit over the head with it in the last year is really an evolution of neural networks that is neurons that looks something like this many of them together and then in layers and the deep is how many layers they are it's not really a technical term because I've even seen plenty of deep learning papers within one layer deep networks um the reason I mentioned it here is again metaphor the brain does not actually work exactly like this but it was inspired by the way we thought the brain worked in the 1940s and 50s in fact the first neural networks were Cornell aeronautical laboratory they were hardware and they could recognize a light turning on and off and what do we do with them today well they've enabled a huge amount of analysis of rich media that was previously entirely out of reach so here's a demo where you can actually put in an Instagram filter and it tells you oh this person likes to take pictures of in this case Ireland I'm gonna show you it going wrong because I think one of the things that we as machine learning developers and scientists have a responsibility to do is to show the boundaries of technologies also I love cheeseburgers so this is bleaker burgers the cheese burger restaurant here in New York it classified everything pretty well so the ones on top it says though these are burgers and you'll see they're all photos from the side the ones below it says this is food but it might be a meatloaf might be a hot dog not entirely sure there's a flagpole in there which is the only thing they take pictures of that isn't burgers and there's crabs so what is crab let's go look at that well visually I think we're all pretty smart we can tell that this neural network has learned that anything that looks like french fries near water or a dock is a crab and it's pretty sure that it's got some crabs here and I share this with you again just to say that it is our responsibility to not only understand the power of these techniques in these metaphors but where do they go a little weird and how we as individuals are responsible for making them not go weird and I've two more to run through really quickly so if you apply these same kinds of deep learning techniques to text using tools like word embeddings and sentence embeddings you can start to build things like a system that will take an article and then extract the sentences in the article that intuitively contain the same information so the sentences on the left here contain the same information is the entire article more or less that's all done automatically it works for any article in the English language because of the way the models were trained that you could pretty easily develop it for other languages as well so what does this give us this gives us a new way to look at not just single pieces of text content but corpuses of content where you might have 50,000 documents about the same thing and you want to know what the the set of viewpoints are so cluster them and find the viewpoints and that corpus and then summarize each one and this is now something that you can do pretty easily the last one ties back to something Emile said earlier today on algorithmic interpretability so when you build all these black box models how do you look inside and first why do you look inside so you look inside for two main reasons one is that the government makes you compliance and regulatory adherence often requires you to be legally able to explain why a system did what it did and so for those of you who just said okay great now I don't care because I don't work in finance or I don't work in a health care the other reason you look inside is because sometimes as I just pointed out these systems do really weird things and you want to be able to know why it allows you to build better systems overall and so to give you a bit of an into in here this is a set of algorithms that you put on top of your black box algorithms that permeate the inputs look at how the outputs and the classifications change and then infer which features were significant in the black box model and this gives us the ability to you in this case go into a telecom churn analysis and figure out not just the probability that a customer will churn but why and what action you can take and you can play with actions and see the probabilities changing to change that classification and therefore that customers fate so this is a really useful set of techniques not just if you're worried about compliance and regulation and so a couple I'm gonna end with a couple of great my favorite examples of graphs and machine learning one is actually from an article that Gilad Lowtown the chief data scientist at BuzzFeed created after the inauguration of our president he did a huge analysis of emoji use on social media and created this representation where you see on the right people who are happy about that inauguration and on the Left everybody else and you can see that a you know the way things cluster and this graph visualization tells us the human story again it's the right metaphor for analysis of that particular very emotional moment in time at least for those of us who are American another project we did was helping one of the top accounting firms in the world figure out some automation around tax codes so understanding when the tax code changes in the United States and it does so in a fairly complex way at the federal state local level through law changes as well as through changes in judicial interpretation of existing code we were able to build this model that helped them automatically build a machine learning tool to support their CPAs in their workflows so they didn't miss anything this was also really exciting this is a fundamentally new capability in the firm again graphs and machine learning and a similar analysis of how we trade commodities so news coming affects things that you have in your investment portfolio you need to be able to see those connections this is an example of of using the power of that metaphor to build tools that enhance the capabilities of human professionals in doing their job even better by getting them the right information at exactly the right moment and so I'll end with with a few points of caution for anyone who's gonna run out and do this it is hard we don't entirely know what we're doing best practices are emerging if you are building something with technology I encourage you to think very deeply about the impact of what you built on the world that we live in and here's a short book I co-authored with DJ and Mike Luke a tease about the practice of ethics and data science again this is a book that poses questions it will not give you the answers but I just encourage you to spend a moment I think this is the big question that data science practitioners are thinking of of our moment like think about this as you build these technologies and again my favorite metaphor which is that technology and machine learning in particular is actually giving us superpowers because it is giving us the technical ability to do things that are out of reach of our cognition as unadorned unaided human beings and it's a very exciting moment to be working in this space so with that thank you and I hope you enjoy the rest of your day here [Applause] awesome that was absolutely fantastic absolutely fantastic I think there's so much exciting things that you all can do when you combine machine learning and graph so really look forward to being back a year from now and talking more about what all of you have built with the-with the-with these two technologies so um where this concludes the keynote and the normal sessions kickoff in half an hour at at 11:00 so I'll just conclude with what one thing which will be by 4:00 actually the most important thing you'll hear at least from me today which is that I really want Graff connect to be the best data conference on the planet but equally I also want it to be the warmest and the most inclusive and the friendliest conference on the planet and that that starts with me and and all the neo4j folks out here that's also your job that's also your job we will graph people here we know that relationships are important don't be tragically isolated documents the graph people talk to one another strike up a conversation just talk about the keynote your latest thing what you've been doing with neo4j a what you want to do is something like that enjoy all the amazing talks that we have enjoy the hallway conversations learn but most most importantly connect thank you everyone have a fantastic Nick 