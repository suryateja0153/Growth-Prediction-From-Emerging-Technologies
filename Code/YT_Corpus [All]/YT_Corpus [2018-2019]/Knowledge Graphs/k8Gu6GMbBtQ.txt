 good evening everyone and welcome to our talk it's the last talk of the day so hopefully it will be interesting enough for you to all stay awake for the party after this one so my name is LuAnn mesquita I'm principal consultant at Graff oval and today we're going to talk about knowledge crafts and relevant search using elastic search this is really based on one of our live customers that we did this for presenting with me today is dr. alessandro negro our chief scientist and he'll take over for the later part of the talk so today we're going to cover the rise of knowledge crafts the application in relevant search we are also going to talk about specifically knowledge crafts in the e-commerce domain because this was really our customer case infrastructure and then combined search approaches so first knowledge graph so the Forrester wave for master data management said that knowledge graphs provide contextual windows in the master data domains and the links between domains and this is really important because there's been over the last few years very huge leaps from data to information to knowledge and then finally automated reasoning and what's key about knowledge graphs is that they play this fundamental role because they gather data from a variety of data sources and they link them together in it mostly a very organic form so you're also able to grow it query it easily maintain it easily and yet keep it very relevant and up to date you heard in the morning's keynote from ml that knowledge graphs have really been on the rise for the last couple of years and we are only going to talk about a few examples in this before we dive into e-commerce so in e-commerce of course you have multiple data sources one of the primary primary use cases for knowledge crafts category hierarchies typically you never have a single hierarchy of products you have products that the same products that actually form part of various different hierarchies and this is quite a difficult problem to address if you don't have a graph and finally you have the combination of not only products categories data sources but they are also influenced by marketing when you're out to sell something you don't sell it just on the basis of having a product but there are always marketing strategies and promotions that influence what you sell so the combination of all these three really emerged well into the concept of a knowledge graph enterprise networks as well for a long time being trying to connect the individuals in their network so partners customers employees opportunities providers together and knowledge cross helps you discover relationships between all these connected data and therefore you can discover new opportunities finance the textual corpora such as financial documents these contain a wealth of knowledge and you will have seen many case studies specifically the Panama papers which actually link together a lot of such types of documents and what you end up with is a very structured graph of entities and how these are connected health is a very very important or use case for knowledge graphs and this has actually had many many practical use cases over the last few years because it unifies your structured and your unstructured data from multiple data sources and it integrates these into a graph based model with very dynamic ontology ok where data is categorized and organized around people places events and this enables you to generate a lot of knowledge around based on the co-occurrence of relationships and data and so really patterns in dizzy his progression causal relations involving the disease syndromes okay and the treatment paths for all these diseases new relationships have been discovered that were previously unrecognized before okay simply by putting all this data together and applying a context around where this happened who was involved for the natural events that might have happened around this later and finally criminal investigation and intelligence have had very many papers published recently about how knowledge crafts have really helped them to investigate cases one of the most familiar cases has been around the area of human trafficking and knowledge crafts have helped because this is a very vague field to track and a lot of the information is very obfuscated so you have coded messages posted on public forums which form a pattern and here's where tools and techniques such as NLP come into the picture to really decode these coded messages make sense of them apply context around them and another very key part of the whole knowledge graph is that especially with law enforcement you really need to have traceability from your knowledge graph back to your source of information otherwise none of this would have would hold up in a court of law so with the knowledge graph you also maintain the traceability of your data from source okay to we're to the point at which it really provides you the information or the evidence that you need so now we're going to switch a bit to some of the problems you see around data sparsity when you talk about collaborative filtering one of the classic problems is your cold start you can you you cannot really recommend products or things to other people if you do not even have other people in your graph or other purchases in your graph and this is commonly solved through things like tagging based systems or trust networks and alessandra will later talk about knowledge graphs and how it helps solve all these if you don't have collaborative filtering even content-based recommendations suffers a lot of problems due to missing data or wrong data if you don't have an enhanced graph to pull all this data together these are very hard problems to solve take search as well with sparse data you end up with very user agnostic searches and the world is moving away from anonymous agnostic searches to very relevant context oriented searches similarly relevant search is a problem so these are some of the areas that we will touch upon in the later part of the presentation but really knowledge graphs are what we used to know as knowledge basis but on steroids they provide you an entity centric view of your linked data they're self descriptive because the data that described is really described in your graph and therefore you're capable of of growing this data in a very organic fashion enhancing it as you discover more and more facts about your data and really you have an ontology that can be extended or revised and its supports continuously running data pipeline so it's not a static system that's once built remains that way forever it grows as your domain expands and the last point is that again tracing back to the law enforcement cases it it provides your traceability into the provenance of data and this is very important in many many industries so a knowledge graph is really a convergence of data from a variety of places of course you have a lot of your actual data sources which could which could come from all over the place and all sorts of formats you have external sources to enrich the data you have you have user interaction so as users play with your application or your domain you learn more you feed it back into the knowledge graph and therefore you improve your knowledge graph and then you have tools such as machine learning processes or NLP which really in the end help you achieve your business goals so I'm going to hand over to Alessandro now and he will cover the rest of the material thanks Ron so let's make this presentation a little bit more practical by considering a specific application of a knowledge graph a specific use case that is implementing an advanced search engine that delivers relevant search capabilities to their end-user relevance here is the practice of improving search results so this final day information needs in the context of a very specific user experience while combining how our balancing or our ranking impacts on business needs I'll walk you through an implementation that combines graphs and more in detail knowledge graphs with the elastic search in order to deliver services to the end user and you will you will see a real architecture and a concrete infrastructure where combine them both it's possible to deliver a level set of services to the end user and even notice how the knowledge graph called help in this direction not only because they can solve the issues related to data sparsity that Bren was talking before but also because graph represents direct model for delivering these type of services to the end-user so accordingly to the definition still it before relevance moves along the four different dimensions resolved around four different elements that our text user context and business goal first of all a search has to us to satisfy an information need that is expressed by the user using the textbook query so in this context informational retrieval and the natural language processing are key to provide search results this mostly satisfy the user intent expressed by the search query but relevant search moves toward a more user centric perspective of the search that means that we can't deliver the same result set to the same user even if they perform the same query so in this in this way user modeling and the recommendation engine could help to customize the result set accordingly to the user profile or the user preferences while on the other hand context expresses the special conditions under which that specific search has been performed so context information like location time or even the weather could be helpful to further refine the search results accordingly to the needs of the user while he was performing the the query last but not least the business goals drive the entire implementation because a search exists only to satisfy a specific need over an organization in terms of revenue or in terms of a specific goal that she would like to achieve moreover a relevant search has to store a lot of other informations related to search histories or for example feedback loops and all this data has to be accessed in a way in which they shouldn't affect the user experience in terms of performance that could mean response time or the quality of the results so far so good so here is where knowledge graphs come in because we will see our knowledge graphs represent the right in terms of information structure for providing a relevant search because in order to provide a relevant search a search architecture has to Endel an eye quality high quantity of data that are heterogeneous in terms of the schemas in terms of sources in terms of volume and even terms of rate of generation furthermore they have to be accessed as a unified data source that means that they have to be normalized at access as a unified schema structure that satisfy all the informational and navigational requirements of a of a relevant search that we saw before so more in detail graphs are and we will see how our direct representation for all the issues related to the relevant search that our information extractions recommendation engines content context representation and even rule engine for representing business goal more in detail information extraction attempts to make the semantic structure in the text explicit because you know generally we refer to do the text as unstructured data but they have a lot of structure related to the I mean related to the grammar array to the laid constrain of the language so you can extract these this information using I mean natural language processing and then you will find it from a document you can extract the sentences and for each sentence you can extract the list of thoughts and then the relationships between these thoughts based on type of dependency or mentions on whatever else so this is a enact connect a set of data that can be easily stored in the graph and once you store this data you can easily extend the knowledge that you have about this data ingesting information coming from other knowledge graphs like consummate five for example it is a an ontology structure that can be easily integrated in this basic set of information just adding new relationships in the Intergraph on the other side the recommendation engines can be built using user item interactions that can be easily stored in a graph as a bipartite graph for example but also can be built using content-based approach in which you need to use again information extraction or I mean in general a description or a list of features of the element that you would like to recommend in both cases the output of the process is a could be a list of similarities that can be easily stored in a graph just as a new relationships between items for example between users and then use them to provide a recommendation to the to the user that combines both approach for example context by definition is a multi-dimensional representation of status or an event mainly it is a multi-dimensional array it is a tensor and a tensor can be easily represented in a graph where you have an element that is one node and all the other indexes that refer to that element that are other nodes the point to that specific node in this in this way you can easily perform any sort of operation on the tensor that will be slicing for example or whatever else and you can even easier add the new indexes to this tensor just adding new node and a new node appointing it to the 2d element finally in order to apply a specific business goal you have to implement a sort of rule engine and even in this case a graph could help you not only to store the rule itself but also to enforce the rule in in your system so I have to be fair because I promised you that this will have been a very practical presentation so let's move further and consider a specific use case that is a search engine for an e-commerce site because any search application has its own requirements they say has its own dramatically specific set of constraints in term of over expectation from from the search that means for example that we in the case of a web search engine you have a lot of documents and that our pages obviously and these documents are completely different one from each other and even the source of this knowledge cannot be trust on the other side when you think about an e-commerce site you can think that okay this could be simpler this is not true at all because if true that the the set of documents is more control at and is even in terms of number is reduced but in an e-commerce site the the cattle gravitation and the let's say the text search are the main salespeople of the say of the e-commerce site because it is not a way for just searching for something is also a way for promoting something for shorten the path between the need and the buy so they are very important so in terms of expectation a search in a ecommerce site has a lot of of things to do but even more in this case you can gather data coming from multiple data sources that could be sellers that will be content providers that could be marking marketing people that would like to push any sort of marketing strategy or marketing campaign in the system like promotion offers or whatever else and also you have to consider the user so you have to store user item interaction or even more using a store or user feedback in order to customize the results provided to the user according to their history in some way and obviously again business constraint so let's see an example of a I mean I mean this is a simplified example of a knowledge graph for an e-commerce site and we will notice all obviously the main element in this case are represented by the products here we have just three different products so an iPhone a cover for the iPhone an earphone so for every single product we have some informations like the list of features that describe that specific product but obviously this list of features could change accordingly to the type of the product so it will be different for a TV or for a know a pair of shoes and and so on so forth and starting from this information you can easily I mean process for example the textual description and destruct the list of Todd's as we said before we are talking about information extraction in this case and once we have the Todd's you can for example ingest data from consummate five and know that a smartphone is a personal device and we will see how these is available information later on when you arrive to provide a relevant search but apart from the data that are let's say common for this type of application you can also analyze your data and create new relationships between items for example so this is how relationships like usually both together jump home and also you can even morally add relationship like the relationship between a coder and the related form so in this way you are I mean augmenting the amount of knowledge that you have step by step and use all this knowledge during the search during the the catalog navigation so with all these ideas in mind we we designed this infrastructure for one of our customer where you can see how the knowledge graph is I mean the core is them the main source of truth of this infrastructure and we have a several data sources that just a feed these knowledge graph and we have a machine learning platform that just process this graph continuously and extract insight from the knowledge graph and then again store this insight in the knowledge graph back and and there is also another part that we will discuss further more later that is the integration with the elasticsearch that allow us to export multiple bills with the multiple scopes in elasticsearch so that we can avoid the front-end to impact too much on the knowledge graph but just use I mean the elasticsearch as a sort of cache but also as a sort of a powerful I mean search engine on top of our knowledge graph more in detail may say how we design the data flow because we created this sort of a synchronous data ingestion process where we have the multiple data sources just pushing their data in multiple queues and then a micro service infrastructure that just react to these events process them and store this sort of data that are intermediate data in the in a one queue that then is processed by a single near Forge a writer element that just read this data and store this data in the graph and this is why we would like to avoid any issue in terms of concurrency but also in this way we can easily implement a sort of a priority based mechanism that allow us to assign different priorities to the element that we would like to store in the knowledge graph and finally in order to store multiple views in the elasticsearch we created his event based notification system and it is ugly customized customizable and that you can customize any way in which you can just push several types of event and then the the elasticsearch writer just reacted to this event read data from the knowledge graph and just create new documents or update assist existing documents in elasticsearch so it's clear how I mean the neo4j is the core of this infrastructure because I mean it's a store the it's towards the the knowledge graph the Tisza as I said before the only source of truth there is no other data somewhere adds all the data that we would like to process are here they converge here in this knowledge graph and so neo4j is a available tool because allow you to store several types of data users products detail about products or whatever else and also provide an easy to query mechanism an easy to navigate system for easily accessing to this data model net in the early stage of the I mean advanced at search process the relevant search implementation it can help the the relevance engineer in order to identify the most interesting feature in the relevant in the knowledge graph that will be useful for the implementation of the related search in the in the system moreover once all the data are in the graph they go through a process of I mean of extension or that may mostly comprises three different operations that are cleaning existing data augmentation and also the data merging because you can imagine that you can have multiple sources so they can express the same concept in different ways so the emerging is also important but they say that some of these some of this process can be accomplished in neo4j itself with some plugins or with some cyber queries other need to be externalized because they are heavy in terms of computational needs in terms of memory so we created these machine learning a platform that thanks to the neo4j spark connector allow us to extract data from near forge a process them using the natural language processing or even a recommendation model building and then store as I said before again new data in new 4j that will be useful later on for providing some advanced feature to these a relevant search so here I would like to say that we are trying to use the right tool for the right job because we can use neo4j for storing the new the knowledge graph and maybe it is the the most valuable tool for doing this but obviously in terms of text research who could be an issue if you would like to perform more advanced of the technical section near 4j so we added the elastic search on top of near 4j in order to provide this fast available and easy to tune textual search so as I said before we again stores multiple views of the same data set in elastic search for having for solving several scopes observe for saving so serving several functionalities that are faceting for example that means any sorts of aggregation of the result set and that you can have you can serve product details page or you can provide the product variants aggregation moreover you can also provide easily with the elastic search of the completion or suggestion but it's worth noting here that it is not another base and we don't want to use it as a database so again I would like to stress on the fact that it is just a search engine that I mean neither in this case data from near 4j and provide the most available tool for textual search in this case so let's see now how we can use elastic search for providing a relevant search on top of the knowledge graph that we have just built using the infrastructure that I discovered before so in this in this context I shall define a simple concept it is the signal a signal in in terms of relevant search is any component of a relevant score calculation corresponding to a meaningful and a miserable information I can say to simplify that is just a filled in in a document in elastic search but obviously the most complicated part is how you design this field what I mean you put inside this field and now you can extract from near from the knowledge graph and store it in the in the raster search it really matter because when you think about I mean relevancy you have two main techniques for controlling relevancy the first one is senior modeling that is how I said this is how you design the list of fields that are composed your documents in order to react to some textual query and the other one is the ranking function or the ranking functions that are mainly the let's say how you compose several signals and assign to each of them a specific weight in order to obtain the final score and then the rank of the results that you will have this is not a simple task is called because you have always to balance between let's say the precision and recall where in this context of position is a receded pressure touch of document in the result set that are relevant while recall is the percentage of relevant document in the result set it's complicated but to simplify we can say that in the worst case if you would like to return back to the user all the documents you will have a recall that is 100% but because you have the relevant document in the result set but in terms of position we will have a very very small precision and even in this case you can have multiple sources in terms of data in this case from the knowledge graph that you can use for modeling your signal and let me give you a couple of examples because we would like to I mean to explore outside of the usual approach to the research otherwise we cannot call it a relevant search and in this sense the first approach is the depersonalization search the personalized search and this is I mean one of the directions that we discussed before about the relevant search here the idea is that we would like to use users as a new source of information because we would like to provide the customizer the result set we would like to customize the visit set accordingly to the user profile or the user preferences so here we have two different approaches the one I mean the first one is what we can call profile based approach where we create a sort of user profile that could be created manually by the user just asking him to fill a form or automatically inferring the user preferences from past searches and the other approach is a behavioral basis in this case is more related to a sort of recommendation engine that just analyzed a user item interaction in order to I mean make explicit the relationship among users and the items so and once you have the users profile order I mean behavioral information you have to type this information in the query and you can do this in three different ways at query time that means mainly changing the query accordingly to the recommendation that you would like to do or index time that means changing the documents accordingly to the recommendations that we would like to do obviously the third one is a combined approach let me give you an example if you would like to customize the results accordingly to the user and we would like to do it at query time you have to change your query specifying the the list of the products that the user could be interested on and in this way you can easily boost the results if for some reason in the results set there is some of the products that the user could be interested on the other side at index time means that you have to store the list of users that would be interested to every single product and then again you can easily boost accordingly to that specific matching in that case so another interesting I mean a extension of the classical search is the concept search in this case we are moving from searching for string to searching for things because you know the user called user different words for expressing the same concept and this could be an issue with the classical text search environment where you have to match the specific ward that appear in the text so using the techniques that we described before in terms of data enrichment you can easily I mean extend the knowledge that you have about every single words and then you can easily store them in elastic search and react to I mean to the query of the user even if it is not writing the right term that is present in the specific I mean in the specific text so even here we have different approaches try to summarize in this way because the most basic one is just to I mean add the new fields where we can manually or even automatically add the list of tags that could be AB full in terms of describing the content or you can write the list of synonyms so for example for TV you can have other synonyms like t dot v dot or television or whatever else and in this way even if the in the text you have a TV you can easily react to I mean so provided results if the user right for television for example obviously there is a more advanced approach in this case in which you can use machine learning tools for up minting I mean the knowledge so in this case you can use a simple co-occurrence as well as you can use a Latin digit allocation that means I mean cluster I see your document set and defined the set of the wards that to cook better describe that specific cluster and so far the entire documents in that cluster so finally I'd like to show you two different approaches that I mean we used a longer way in this sort of a path for integrating neo4j and elastic search on the left side we represented the first approach that we had in which mostly we have a two different elements that are the the documents in the elastic search and enter graph database so in that case when a user performed a query the query go through two different of phases the first one is the classical textual search query that is performed 100% on elastic search and then the first basic result set is a I mean is manipulated accessing a to the to the graph and then boosting the documents or boosting the results accordingly to whatever rule you would like to have this could work but in terms of performance could I mean could suffer a little bit when you have to do complex boosting fat boosting operation so we moved to a second approach in which there is more I mean connection between elastic search and the knowledge graph in this case because we can see and this is exactly what we discussed in this presentation that the knowledge graph is used for exporting multiple views in elastic search so part of the of the worker is a is done at the index time and then when we are the user perform the query the query itself go through a first stage that is an enrichment so accessing the graph we can I mean change the query and then we will perform the query in against elasticsearch and this appeared to be a more performant approach because this operation itself is not related to do every single element but it's based on the context or on the user on whatever else so it is very fast and then we can use a let's say elastic search for what is supposed to be just textual search but at that point we already have a very complex query that is I mean created by using the graph and even the index are already changed accordingly to the graph structure so to wrap up we can say that the knowledge graphs are a very important buzzword let's say because I mean we are using graphs for representing complex knowledge and modern data we are using a model I mean we are storing them using an easy to query model the Tala was easy to get their data from several sources that means that we can store users we can store users item interactions we can store whatever else we want and we can even add further extender importing data later on or changing our knowledge graph adding a new information and on top of this we can implement we can develop a search engine capability using a search engine like elastic search that to provide a fast available and easy to tune text search engine and the totes are provided a lot of other interesting feature like faceting or auto completion so it is a combining them that you can achieve I mean the tooken deliver high level set of services to and user so what is the takeaway of this representation that the idea is that you should use it the right tool for the right job in this case you can use the graph representing complex knowledge and then use a simple tool like elasticsearch to provide I mean textual search capability advanced search capability to the end user that's all thanks 