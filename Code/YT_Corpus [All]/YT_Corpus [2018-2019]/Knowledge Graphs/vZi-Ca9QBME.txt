 hello everybody thank you for joining us today my name's Amy Hodler I am the program manager for analytics and AI at neo4j I'm here with my colleague Jake Graham who is the product manager for artificial intelligence and analytics and we're going to talk to you the next 40 minutes about how grass enhance artificial intelligence in particular about context and we're also going to give you a few examples to walk away with and so I like to start with a little bit of a level set around machine intelligence in general and so there's a lot of different terms out there that you you've probably heard a AI machine learning deep learning a lot of different definitions but it's a really high level what we're talking about are different ways to solve problems in particular with AI a way to solve problems and apply learning that mimics the way humans make decisions and what we're really trying to do is develop probabilistic methods to use software to make predictions now usually that's either to predict what comes next based on features or the information we might have at hand or it's about predicting how to classify things am I looking at a cat or a dog should this community be classified this way or that are they similar or dissimilar to another community now if we think about intelligence in general and especially if we're trying to mimic the way humans make decisions context is really important for that it's really important for understanding it's really important for predictions now I have an example of doing that wrong actually from my past I'm developing a logistic solution for a company that shipped yoghurt we made the assumption that F OB for a logistic solution of course would mean Freight on board it turned out for this company f OB meant fruit on the bottom and that was very significant to them it meant for them that that those yogurts needed to go in different containers and different shipping methods and so our solution failed them even though we we had done a very clever job we done a lot of due to plication for that FOP classification and we hadn't taken in the context of the current situation we had only learned from our past and if you think about how humans learn we in general maybe not always are pretty good about trying to understand from our past and then applying that in variety of solutions variety different ways and so if we think about how humans can do that and trying to mimic that here's an example of how a child might learn to do something or rather not do something I don't know if you all know what's about to happen I actually did this as a child I burnt my hand on the stove now what's remarkable or maybe not remarkable I didn't need to do it a second time I didn't need to try another time now it an AI solution or machine learning solution might need to try a thousand different times try the the spot in the back try the spot in the front try it a different time a day I didn't need to do that I didn't need to try it again at my neighbor's stove when we got a new stove I didn't need to do it again because I had learned very quickly from this traumatic experience that anything at counter height that you cook on with metal that felt warm as you approached it was not something you put your hand on a second time now if we want to do that with a machine learning type scenario right now that that can be very difficult if you don't provide thousands of different examples so another example taking this context idea a little further is you know I've learned this as a child in this scenario with a stove but another example my father used to race motorcycles and the pipes and exhaust on motorcycles can get very hot now you would you would hope that I wasn't going to I'm not going to show you another traumatic event because as a I had learned from the first event that something that I approach that's metal that's warm that I'm not going to touch again that I'm not going to try this again and taking that context from very specific to more general to even more general is that expansion is considered contextual learning I don't need to have an exact same situation I've learned by context to apply that the what I can now recognize at that event now we would like to get a I machine learning closer to that now unfortunately we're not quite your there yet there are some things we need to do now it's probably pretty clear from the the previous example that missing contextual information is a big deal AI is very good at very specific well-defined tasks but it struggles sometimes with ambiguity with missing information machine learning often doesn't use some of the most predictive elements that we know of which are relationships especially if those relationships are hard to reach or process the other thing is that processes that we have are are mired in these older layers of complexity a lot of this math has been around for decades we've done very good recently with optimizing some of the hardware and Jake will talk about process optimization but we haven't used context to optimize that higher-level process overall there's also a lot of missing context and knowledge systems they're missing that adjacent information getting the right data for the right decisions if we think about that fo B example it worked the software work just splendidly but it was the right solution for the wrong problem it wasn't appropriate for that situation so the other area that is becoming more critical critical is explained ability so we often can't explain especially when we're talking about deep learning why or how a decision was made and so without context that can be really difficult and that's an area Jake is going to talk a little more about as well and how that can really help us move forward so graphs enhance artificial intelligence by providing context so if you take nothing away from this talk take away the idea that adding in that rich layer of information that's all around us into your AI solutions so that you can have better AI outcomes and so there are four different areas you heard about this morning that we're going to talk a little more about and the first one is connected feature extraction how do we get connected features when we're talking about predictive features how do we get those connections so we can have better accuracy we'll talk a little bit about graph accelerated machine learning so model optimization so you can have context for better efficiency we'll talk about knowledge graphs as well which I think there's a lot of information out there this one I kind of get excited about so it's content for decisions and I always like to add appropriate decisions in in decision support so there's a difference between accuracy and an appropriateness it's good to have both when you win again and a I explain ability so that's bringing context for credibility so we're gonna dive into connected feature extraction first there we go and when you think about this it's kind of fun I mean I always think about you know sometimes it's who you know so I think Hillary must have stole our example on this so I'll give you a different example so if you have a lot of relationships the second degree third degree relationships with people that perhaps have have committed some kind of a crime that's a better indicator of you potentially having a criminal activity than some simple demographic so you know very simply I think with Hillary's description on the smoking it's also been proven with with other other aspects as well that you wouldn't think would would naturally with relationships unfortunately current machine learning methods use vectors matrices tensors that are pretty much built from tables and I'm sure you've heard in other lectures the difficulties of tables displaying and holding up relationships with the multiple joins especially as you try to add in more and these methods pretty much try to abstract simplify and sometimes just completely leave out entirely these predictive relationships and network data I just wanted to point out one thing in the visual you're seeing what you're looking at are not tensors right you're looking at tensor spaces so the spaces in which a tensor could occupy you can have single one-dimensional tensors that basically vectors moving into multi-dimensional tensors but I've got my phone up here I wanted to make sure I get it exactly right that the Wikipedia definition of a tensor is a tensor is a geometric object that describes linear relations between geometric vectors scalars and other tensors I think the important thing to focus on there is linear relationships right vectors tensors matrices are good at showing one degree of relationship but that's not the entirety of what a predictive relationship is right it doesn't take into account more complex networks it doesn't take into a into account multiple hops of relationships but those can be very predictive and where a graph fits in is to help one of the places at least and Louis hanging fruit is to help deliver those predictive relationships into your current tabular or tensor base model yeah and you can do that without actually having to alter your model and your other algorithms that you're using as well so adding those in without having change what you're your pipeline might look like graphs also are pretty good at inferring relationships so I'll talk about that in a moment where relationships don't exist or are very sparse by using those second third degree hops you can infer relationships even if they're distance that don't directly exist so very good with that and we have a couple different methods that you can use to do connected feature extraction you can use engineered features so those are looking at labels or inferred relationships where you know what you're looking for I want to know how many four hops how many people have a fraudulent account and I want that I want the count and I need to abstract that and then use that in another way graph algorithms are really good at finding things where you you kind of know what the structure is but you don't know exactly what you're looking at so an example would be I want to find the node in my network that can reach every other node the fastest so I know what I'm kind of the structure I'm looking for but I'm not sure exactly what the node attributes are another example would be influence centrality algorithms are used quite a bit where you want to know the influence but you don't know exactly what you know what that's going to look like or what the nodes gonna look like once you find it and then with Grantham beddings that's kind of all the way on the on the other side of being more unstructured on what you're looking for in that case you're taking all of the relationships of the nodes and trying to abstract them numerically and then use them in something like like a tensor now an example we're going to show is in a financial crime scenario of fraud prevention scenario and Finance is a great area to look at because there is just so much data and is so highly related and especially in kind of fraud or criminal scenarios when the attempt is actually to to hide using multiple hops and multiple network relationships so with this example this is a very toya example it's a very small example we might have a couple of accounts of course you have hundreds of thousands probably accounts but somebody perhaps is is looking to make some kind of application a financial application whatever that might be and the immediate thing you'd want to do is well let's look at who they're connected to and you can really quickly look with just these couple hops and start to see some interesting some interesting indicators people using the same social security numbers same phone number you know it's kind of looking a little suspicious and so at four hops or excuse me two hops we can see four connections already that might be marked as as fraudulent or potentially fraudulent and if you go out just another pop we've added three more four hop connection so pretty simple you can imagine you want to do this with a lot more features you want to do this with a lot more attributes but you can see very quickly how it would be easier to follow the hops and find your connections than to do a complete brute force now this is kind of an interesting scenario once you've found this or you've identified this fraudulent potential Network you can then do further algorithms on it to look at well who is the biggest influence here what features are the most influential in this you could project out to a subgraph and then do some analysis on that we have several customers that use PageRank actually to look at their models and try to figure out which feature is the most influential in their model so they don't even care what the feature is itself they're just within that find me the most influential feature and then those are the top ten or whatever we'll look at and we can disregard the other features and another thing that you can do is you move from investigation into a pipeline right the the goal of data science in general is to understand the predictive relationships and the data and then move it into something autonomous really we want to be able to detect these things before they happen a simple example of an engineered feature in this case is simply running a count at every connection how many known money launderers are there right and in this case what you might find is you're not connected to any known money launderers at one hop but at two hops there's multiple and at three hops you're seeing this pattern and what you can do is engineer features to build in to be it a rules-based engine be it a random forest spit any other model to look at how many known fraudulent accounts at one hop a two hops at three hops and those tend to be highly predictive features that are close to impossible to get out of any tabular data structure great and then what we can also just quickly take a look at is the the machine learning graph algorithms so you can see last year we debuted our graph algorithms library had all of you know all had had your standard graph algorithms areas and then this year we've also just about doubled inside and out pathfinding search all about finding optimal routes to things centrality all about looking at what's the most important function or important node within your network it could be a bridge it might be something with the most influence and then community detection is well often used in AI machine learning as well to try to think how do things clump together and how alike or dissimilar they are we've added quite a few of similarity algorithms as well we've got an e-book available online with those and then early next year we'll be coming out with a graph algorithms book with O'Reilly as well so the last thing I'll add on that is this is something that we're consistently looking to expand to improve whether that means you're looking at running an algorithm and you need help doing that or you're looking for a new algorithm that you believe should be supported in the graph we really need that customer and user feedback to understand what your priorities are so please reach out to us to both understand how to better leverage these algorithms but also to help us prioritize what algorithms come next as we continuously release new ones so Amy talked a bit about one type of context in AI but there's there's many types of context right we were looking at what are the contextual predictive relationships you have that might help you classify or predict a next action and I want to talk about the contextual way in which you make decisions in which people actually think right Hilary this morning talked about how neural networks are an incorrect approximation of how the human brain thinks but it's not always really understanding exactly how neuroscience works it really it's using them as a pointer to how can we speed up and optimize our own workflows so one example that I was thinking about for this is I don't know there's many comedy fans in the in the audience but when I think of Steve Carell I think of lamps and if you're a fan of anchorman if you're chuckling it's you know brick loves lamp and if you haven't seen any man you think I'm insane I promise it's that you should go check that out but that's a that's a relationship but if someone asked me is something a bit more intensive and I said who does Steve Carell remind you of I'm not going to run that it's the PEI nearest neighbour classifier of Steve Carell against lamps and against chairs and against abstract concepts right I'm gonna go through a process of filtration to think of okay let me think of what are the attributes that they shared an arrow down arrow down arrow down and then I'll run the more computationally intensive aspect there so I'm gonna obviously start with people right I'm not gonna move into he's a male I might look at complexion I might look at non physical features he's an actor he's a comedian and we're I'm probably going to get is if you've never noticed this Steve Carell looks a lot like Ben Stiller but I didn't just do this in a brute force algorithm of let me look at every image I've ever seen in every other image and that's actually fairly close to how modern neural networks do computer vision algorithms they're they're going through and those each layer is looking at even though it's not a generally a human understandable concept it's looking at filtering down the machine learning algorithms that a lot of you are putting in practice in your enterprises though are not always as intelligent I mean sometimes the filtration process that you're going through is really inefficient so we want to talk about ways in which the graph actually helps you speed some of these up and get into production faster and speaking of getting into production faster the data bricks and I eg had a study come out earlier this year about what the biggest roadblocks to scaling AI and the enterprise are and one of the things really surprised me well what's one of things I've seen Gartner had another study that said 55 percent of enterprises are piloting AI this year only five percent have something in production data bricks came out and said only one of three AI pocs are making it under production and when they looked at what the biggest complaints were tied for first was the iterative time the time it takes to iteratively train a model and data science is an iterative process right the more times you can try and tweak something the better it's going to get that's that's basically a rule what really shocks me about this is it was tied with not enough data if you either are a data scientist or work with one you got we all we complain way too much about the data not being good enough right it's the consistent complaint but one of the actual biggest problems as well is just how much time it's taking to work through these iterations to train well a lot of that is because you're not taking really efficient process he's at heart we're doing what Hilary did when she held up the tiny chip right if to Amy's point neural networks have been around for 60 years why have they taken off in the last 10 years mostly because GPUs have started to be leveraged for highly parallel applications you're waiting for more hardware before neo4j I worked at Intel we loved everyone waiting for more hardware but there's more to optimization than what's the next CUDA release right and there's definitely more to optimizing your machine learning than renting more CUDA clusters on AWS right start thinking about where are the inefficiencies in my model when I think about why neo4j exists how we got started I think the general problem statement was we we exist to solve the computational inefficiency of connected data right of relationships and what's the most common way relationships manifests themselves it's in table joins so if you get nothing else for me and every other talk here I just wanted to be table joins table joins table joins table joins their terrible stop doing them right they're great if you're just exploring data but if you're building an application you should be looking at a graph if table joins your bottleneck but they're not the only bottleneck so obviously if you're seeing them as you're bringing in data but what are some of the ways that that inefficiency manifests itself in data science one of them is a what Emily talked about this morning sparse matrices if you look at some fairly common and well understood machine learning practices we talk about collaborative filtering a decent amount that's really a giant sparse matrix right I'm a power user of Amazon maybe I've bought a million products well they have a billion so at a high level the matrix that represents Jake is 999 million null values and a million positive values and obviously there's there's ways that you can chop that down but no matter what you do those zeros have cost and compressing them because there are definitely sparse matrix compression methods those are just more tables they're more indices they're more lookups they're more complexity in your model graphs don't have no values we just have the dot values we just have the relationship values so where you see yourself spending a lot of time compressing matrices that's another place you can start thinking I might have a graph problem when you start seeing directionality write multiple actions multiple hops but not just one linear direction not just that tensor or you have multiple pads there's a reason so many logistics companies use us write graphs are able we have directional relationships when you start seeing you're getting bogged down with so many different trees so many different basically multi-dimensional matrices to track these directions you might have a graph problem and then finally what I want to talk about a little bit more closer to the brick loves lamp example is brute force there are lots so brute force is just the concept of I am going to run this calculation against all of the data that I have or I'm gonna do some manual subsetting or I'm going to put in some kind of Bayesian inference to try and subset it what those generally those processes generally have in common is they're fairly manual they generally require a data scientist to put in practice and they're slowing down your iterations why why why why is it so hard when in reality what you can look at is clustering based on relationships right if I'm looking at a KN n if I'm looking at classifying something via similarity I can probably just look at anyone that has any shared attribute or how about anyone that has a shared attribute of someone I know or someone they know write it in an in a graph the query to say return me a subgraph of only people that share any attribute that I have is trivial its milliseconds and what you can do if you take this visual PowerPoint example of a recommendation graph and you can imagine that this graph might expand out into infinity instead of running whatever a class if I am running against all of those nodes I can close to immediately project a sub graph I can do it in an automated fashion I can do it in a and sometimes it's better to just do it in the in the dumbest fashion possible I can look for specific features I can use graph algorithm to tell me what features to run against but what I can do is you in real time consistently and then from there I can run my classifier to set up those weighted relationships of whatever I'm looking at right whether it's a classifier or whether it's a regression but only compare things against things that are likely to have anything to do with them right the graph helps you speed up that pipeline so I want to talk move on to Oh so yeah just coming back through just to think through when what you see that you might have a graph problem in an inefficient data science workload table joins I'm always tempted to just get up and just like try to lead a table joins chant stop doing them but also sparse matrices directional problems and brute force is it's really as inelegant as it sounds right you should stop doing it so moving on to how we're creating the context for decisions that amy was talking about um one of the one of the AI spaces and data sight spaces is moving into production that fast this is decision support right it's where you're taking a human decision that requires that person to have the right contextual relevant information and trying to automate it in some way now there's lots of things that you can automate out that are mundane tasks but that still leaves you with the hard tasks and generally hard tasks are solved today by domain knowledge right you just train more experts you just build up more experience so that someone can say I've seen this problem before I remember it there's really no reason for that to be the case anymore and and really what we're talking about is knowledge graphs and I want to at least try to explain it define knowledge graphs how I define them it's a term that gets thrown around a lot I think of knowledge graphs as a connected dynamic and understandable repository of different data types so pulling that apart quickly it needs to be connected it is not a data Lake right it needs to be connected around the attributes that are actually relevant to this knowledge knowledge is not data is not knowledge right not all data is knowledge you're looking for relevant information that's connected on contextual lines it's dynamic in that the graph itself understands what connects these things right it's not looking at you need to go through and manually program every new knowledge piece that comes in it's able to make those associations across the attributes that are important to you because you've already programmed that in um and it's understandable sometimes you say it's semantics the knowledge tells you what it is there is intelligent associated with it so that you can traverse this graph to find what you need to answer this specific problem even if you don't know exactly how to ask for it in general and different data types I generally tend to define knowledge graphs as it needs to be heterogeneous it's not just you have all of the same data type in here in general there's there's ways that graphs can help you with that but I wouldn't call that a knowledge graph we need to be looking at multiple different types of data in the last part of the definition a knowledge base is not a knowledge graph right having everything in a data Lake is a fantastic thing to you need it to get started but if you can't figure out what something is and where it is and how it's connected to other things that that is not a graph so instead of talking about one example since I think we've really are the de facto leader in this space most of the most common knowledge guys people talk about other than the Google knowledge graph are built-in neo instead of giving one example I want to talk about the three major types of knowledge graphs that we see in the market and these aren't neo4j can in terms these are these are just my terms so you might not you might see them slightly different but I tend to think of these three types of knowledge graphs of context rich search external insight sensing and enterprise NLP I like to think of them as basically a hierarchy the first one we're gonna talk about is the hierarchy of your data types what are the types of knowledge you're looking for what are the types of documents that we're trying to traverse the next level I tend to think of as what are the entities what are the things the specific objects that we know are important and what's all the information that's being generated around them and how can we generate insights from that in a non prescriptive way and then the last one is getting into the actual granular level of your language it's the the biggest problem in scaling NLP and most enterprises is that you're not talking about natural language right the natural language part there's a lot there's a long way to go but that's a that's a fairly well solved problem Google is not helping you to find your own ontology right your technical terms your product names your product families your industry acronyms your synonyms your common misspellings of those technical things your part numbers these are all technical terms that are custom to you they're enterprise language where we can step in is to help you map those together and build that ontology so it's give me a little more info on each context search search doesn't actually just have to be search but if you if you are looking at just doing tf-idf term frequency inverse document frequency word document search across large corpuses of heterogeneous knowledge you've probably found it doesn't work that well what a knowledge graph can allow you to do is understand the context and how things are related and traverse that much faster but there's non search applications as well seeing them used in customer support if you're in an enterprise where you get tens or hundreds of thousands or more complex technical support issues a year being able to show a technical support person what's the most similar problem we've ever seen how was it solved and what are the associated knowledge documents and technical documents to this problem can accelerate resolution enormous Lee where I tend to talk to people about looking for context rich search identify a process in your enterprise that you know is inefficient measure that inefficiency and build a knowledge graph and make sure that you're actually targeting moving that KPI up right building KPIs and around AI workloads is critical so look for that in efficient knowledge worker process and start asking the question what do you need to answer this question better and build that into your graph external insight sensing is generally where we see this is when you're combining internally known entities and the knowledge around them with external information so we see it in things like supply chain risk right the ability to look at all of your suppliers all of the places they have manufacturing all of your your supply lines and associate those with disruption risk being able to tell that your competitor is being tweeted about buying one of your suppliers and flagging that information being able to look at a natural disaster is hitting a specific place that you have a you have a supply chain and giving someone options or what are similar suppliers that you have or looking at market opportunities looking at financial information looking at investment management or excuse me investment banking being able to understand what are the types of companies who who's talking about buying who so that you know who she you should be talking to who you should be targeting in your CRM to go sell services but in general you are looking at there's an enormous amount of information in the market we need to be able to sense it determine what's contextually relevant and present it to the right person and then finally enterprise NLP I I already jumped ahead of myself and talked about how we can help build Hilary Mason this morning talked about word embeddings there are lots of fantastic word embeddings libraries all the CSPs are getting better they're probably not doing it for your business they are using grass to build them associate you and then the last one I want to talk about is a bit more on the research side that's another thing was talked about this morning explain ability and it's it was kind of funny at the Graf talk one of the way she didn't talk about adding explained ability to neural networks is actually there's a lot of research going around around graphs pacifically graph databases to start to bring that explained ability to complex neural networks and I do think it's important to recognize when we're talking about explained ability we are talking about deep learning in general even complex decision trees if you get into a large random forest their outputs are still individual layers you can still see feature and coefficient right it not be it might not be business analyst readable but you can get to an explanation complex neural networks are not explainable to the data scientist who wrote them so I want to give an example of that there was an interesting example last year in an academic setting of a professor who asked his his class to build a classifier for photos of wolves vers dogs and that's a very hard problem right those are actually pretty similar even though it's a snarling wolf and an adorable Vizsla puppy it's a pretty hard thing to be able to identify one student actually did a really good job though and he was getting really good values on labeling things as a wolf versus a dog and no one else had been able to do this I'm the professor which don't understand how did this happen so one of the methods that people use to bring explain ability is called perturbing the model you're gonna take in a known data set they're gonna feed it in and you're gonna see how your classifier reacts to that data set to try and abstract okay I change this specific feature I can see how where this is going it's a pretty cumbersome process but he was able to perturb it and find pretty quickly what was going on fed in not this exact image of an adorable puppy helping his owner clear the driveway but something similar and what they found was every time that came out it was coming out as a wolf give me my stamp there it is and then in whatever you know dystopian bureaucracy that you can imagine that the algorithm says to do this if this wolf would then happen to this adorable puppy and what do you realized it's really similar what hillary was talking about this morning so french fries near the water snow and a dog equals a wolf that's obviously not right but in order to understand that you had to feed through known datasets figure out where the problems were manually go through and look at these go back and piece it apart and this is a professor of computer vision right this isn't this is if you start thinking about we all talk about these all the time right health care credit risk scoring approvals crime detection sentencing like the list goes on and on for the things that people want to apply deep learning to if you can't tell that this puppy is not a wolf you probably shouldn't be recommending sentencing yet so what are the different types and it's important to break these problems down into you know the type of problem we're talking about right it's not just it's not just explained ability for all AI because not all AI is inexplainable is inexplicable right it's deep learning within deep learning what are the types of explained ability we're talking about well it's explainable data what data did we use right can we even understand how this was trained that can actually be a lot harder than you think if you start talking about some of the major cloud service providers telling you what data that what data Facebook is used exactly to get to all of its algorithms it's actually not that easy of a problem explainable predictions that's what we're talking about things like perturbing the model where you're looking at the algorithm itself isn't explainable but I want to understand specifically why it's giving me this value let me try to piece this apart and then the the panacea explainable algorithms so algorithms where you can actually get those coefficients out and the important part of this one is those already exist in deep learning they're just not performant and we already talked about how hard it is to get more GPU and how great and Vidya stock price is doing right now performance matters so limiting performance is probably going to be a non-starter as well so how are graphs starting to be looked at in this space explainable data is an easy one for us most we have 20 of the top 25 financial institutions using us in some way data lineage is a pretty common way right the ability to understand how data was changed where it was used why it was used who used it is something that we can do really well if you're pulling that data in from a knowledge graph it can also tell you what that data is right it's it's explainable it's got semantic understanding so that's that's low hanging fruit explainable predictions there's something cool that's come out very recently a partner of ours Fujitsu AI labs this is this photo is actually from some of their public information if you search for deep tensor and explainable AI they're using knowledge graphs to help get two explanations for specific predictions so this particular case is taking genetic mutations and looking at them to try to associate them to specific diseases in ways that haven't been done before um and when you're combining hundreds of thousands of mutations with equal amount of diseases and multiple layers of classifier that's very difficult to do except what they did is they built a knowledge graph that include you did each one of those mutations included those diseases and it also included all of the academic information that is that describe those diseases that talked about them and they set up that semantic connection around look for mutations look for diseases look for the things we're looking at and what happens is you can still see the neurons in its case the nodes that are used in a neural network what you can't tell us how they were used but if you can see which nodes we're actually using a prediction you can then traverse your knowledge graph to say oh well this makes sense I you know see that this dot this research paper is a its associated this research research paper and actually I can see that this genetic mutation is being talked about in something similar right so it's still not very elegant but it is more elegant than just trying to feed in more data to perturb your model because you don't have the photos of more dogs to feed in that are labeled for this right you're looking at unknown patterns if you don't have known patterns to pretend that model it can be close to impossible to do that so the knowledge graph is allowing researchers to say this is this mutations are associated with these diseases and here's all the documentation abstracts at why and then finally we get into explainable algorithms this is a bit further out we are starting to see research and who just is actually working in this as well of actually constructing your tensor in the graph making sure that linear relationship is actually weighted relationships between nodes which has is showing signs of being able to actually get at the explanations and the coefficients at each layer but I don't want to over promise that too much because I actually haven't seen it working yet but that that is that is some of the cool fun and exciting stuff that we're working on right now so with that I am gonna pass it back to Amy to close us out thank you so I think by now you've hopefully understood the the theme is context-free I and in particular to just remind you that aggregation aggregation of information is not intelligence nor is learning without context that's not intelligence and comprehension so think about you know think about what you're doing and if you're still asking yourself will grass help me will grass help my AI or my machine learning and they're pretty much you know for areas I think we covered them but just quickly to summarize is look at your model do you have predictive relationships or network components that are predictive that you can't reach you you've got to to hops but you're trying to get to three or four hops out and look for those indirect inferences and and connections are you you know dealing with a lot of joins I think we talked a lot about we think we've logged that yes/no joins joins are bad but also if you have a lot of sparse matrices or other highly indexed components in your model also directional we talked a little bit about that that is such a classic graph application is understanding paths and understanding direction and how things spread through a network and of course the knowledge sources if you've got a lot of heterogeneous knowledge sources and stories / or sources all over that are hard to integrate that would be another indication as well so with that if you guys have what we really need for you so we want to help and we want to work together on new solutions but what we really need from you guys is come tell us what algorithms aren't in the library that you want and come tell us how we should integrate with the workflows that you have so thank you everybody come find us afterwards for questions [Applause] 