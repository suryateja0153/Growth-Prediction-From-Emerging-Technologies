 hello there everyone I'm Kareem Wallach I am the community manager for developer relations team at neo4j and I am here with my colleague Mark Needham who is also one of our developer relations engineers and another one of my colleagues Andy Jefferson who is our presenter today who will be talking about deep learning on graphs so Andy I will let you introduce yourself if you would like alright hi everyone and yeah I'm a software engineer at near PJ and I also work as a researcher at an organization called Octavian and so I'm going to talk about the research that I do at Octavian on knowledge brats yeah so let me let me just quickly and just do a little bit of housekeeping before you go you get started then Andy so if you have any questions while Andy's presenting feel free to ask those on the the YouTube chat that we've got on the right hand side sometimes YouTube doesn't print your resolution at the profiting proper proper resolution so on your bottom right hand side of your control panel there's a little cog with HD icon on it set that to at least 720p and you will be able to see everything well clearly I do what I also add if any of one out there if you're interested in sharing on one of our online meetups in the future you can go to our community site and post your project under projects or content if it's like a blog post and that's what we're going to be selecting our future talks so you can also vote there for things that you want to see in future listen okay I hope you guys can see the screen yeah real good what's cool so I need to talk about graph AI and I've already introduced myself a bit but just to make this super clear I'm an engineer on the cloud team at neo4j web building near 4j software service in the cloud that's a pretty exciting thing that's coming up but today I'm talking about the research that I do on artificial intelligence at Octavian Octavian is an open-source researcher so but if you're not graph is that we have these nodes we have these nodes which connected by edges and and in near pj we have this property graph model so in the property graph model both nodes and edges can have properties so no like employee can have properties like name date of birth ID number maybe country residents and then relationships can have properties as well as having a relationship type so here we've got two different types of relationship like location and CEO so the image below is the graph model that deepmind use in a recent research paper and the reason I pulled that out is to show that these two models are the same thing did in a more mathematical formalism but you've got the same information there are nodes connected by relationships and both the nodes and the relationships have properties in deepmind model these are called attributes and they're often thought of as vectors but the models are the same and we can transport them between it to them so that's that's all we can send one of these graphs and graphs are really powerful expressing all kinds of different knowledge information and data so why are we interested in these graphs well like I said they're really powerful and we can use them through search has been around of the concepts since Leonard Euler who's the guy in the top left here and who's famous for the seven what he's famous with in graph theory further seven bridges of königsberg burg problem which is the first kind of recorded mathematical graph problem or graph proof but here I've got some other examples of graphs that we think about in the top right there's a semantic graph where we're mapping out a kind of idealized set of relationships that we can use to describe the world and in the bottom left I've got more of a kind of database graph and so we've got lots of information about different instances of customers product for interacting with each other and to keep track of things like just how many customers do we have planned in the bottom right I've got a transit network graph and anyone who's lived in a city is probably familiar with that and so each of these representations are useful to us in a different way but they're all graphs and they're all made up of nodes and edges so we're interested in graphs because graphs are everywhere we can represent lots of knowledge of graphs and if we know how to manipulate graphs cleverly then we can understand and interact with all of that knowledge the question is what is AI I'm not going to try and answer that because I don't know the answer and what I'm going to talk about is what is deep learning the deep learning has this is its building block if this was interactive I'd ask how many people have seen this before but typically when I ask that question it's about 50% of people in the audience I present to you and and what this is rep this is is the densely connected layers in a neural network so in this neural network each layer each note each item in each layer is fully connected to all the items in the previous layer we've got two hidden layers and that's where the deep comes from so we talk about deep neural networks the depth is a number of layers in the neural network and and these fully connected layers where the three machine learning and gradient descent we learn the parameters of how nodes in one layer interact with nodes in the next layer is the building block of a whole range of AI innovations that have come around recently but the the fundamental building block of these neural networks usually using densely connected layers so well they've been useful and this alphago is probably the most famous application of deep learning and alphago use deep reinforcement learning to train computer to learn to play go better and some of the best go players in the world and so this is a great example of AI outperforming humans using deep learning and but there are a few others that we can look at as well so here are some from the world of image recognition and on the left there's something there's a neural network called magnet which I'm a massive fan of and it this shows multi step reasoning from a neural network at the same time as doing image processing and on the Left we've got the question that was asked so this this is combining image recognition at natural language processing and multi-step reasoning which is really awesome so the question this was asked was what color is the map thing to the right of the sphere in front of the tiny blue block and in the central column we can see which of the words the Machine the neural network was focusing on in each stage of reasoning towards getting to the answer and on the right we can see which parts of the image the AI was focusing on any each of those reasoning steps as it got towards the answer so its start to find the sphere and then finally it picks out the cylinder which is the answer to the question and so this is really awesome and that sign has inspired a lot of our work on the right you've got some really complex problems and the top right one is image classification problem but it's classifying multiple parts in the same image and in the bottom right we've got a problem where the AI is predicting where a car is most likely to go does doing path prediction but from a still image so this has learned about cars the cars tend to go forward it's trees and they tend to drive along roads and an answer like really sophisticated piece of AI and all it is the AI is performing as well as or better than humans on the task so we want to bring this superhuman performing AI and we want to apply it to the world of graphs which we use to represent all kinds of information from the stock in our store to how we travel around big cities before we get to that let's take a step back and think what is machine learning what is the machine learning process and interface look like so in machine learning what we generate is a trained model can learn how to understand some data and then we train it by giving it lots and lots of examples of data and from that it learns a set of parameters which allow it to predict from the data some outcomes when we've trained the model there's basically two things that we can do with that model we can look at the parameters or the weights that it has learned or we can just use it to make predictions sometimes we do one or the other sometimes we do both to make this a bit more concrete we can just think about linear regression within this framework the linear regression our model is y is MX plus C and hopefully most of you familiar with that model in this model the parameters or weights that we learn are M and C and the training data that we use to train the model is a list of x and y values so X is the input value and Y is the output that we're predicting and we train it with loads of those values and then if we want to figure out what Y is for a given X we can plug it into the model with the promises that we've learned sometimes in something like v fix you're actually is and less about the predictive nature of the model other times we care much more about the predictive nature of the model and not at all about parameters the same approach applies to classifiers like neural network classifiers like the image classifiers that I showed before we train them in exactly the same way we have a model which is a neural network that model take the whole load of practice so here W which is the parameter our model is actually a matrix of maybe thousands or hundreds of thousands of values is and the category of those images so the training data could be imagenet for example which is list which is a whole collection of pictures and for each picture a classification that says this is a picture of a dog this is a picture of a cat this is a picture of a car and so on and we take those hundreds of thousands of images and we train the model and it learns a whole set of weights and those weights enable the neural network to classify the image another example and of machine learning is on natural language and this is an example where we use the weights rather than the predictive power is when we generate a word embedding so that would be something like would Tyvek word Tyvek is a model which converts essentially words a whole dictionary of words into vectors those vectors are then used for further steps in different kind of machine learning or predictive processes and the way that's trained is by predicting a probability that certain words will be at will appear in the context that surrounds a given word and we train in your network to predict that but with this training we don't actually care about the ability to predict what words will appear in the same sentence as a particular word what we care about is the resulting word in bedding which is a train weight that we get out and the fact that that word embedding has this property that similar words have similar embeddings so here's what is deep learning and what deep learning is is machine learning training models with data and this is gradient descent don't worry if you're not entirely following all of that and so long as you understand this is the high level view thing we're trying to do that's fine so how do we take this deep learning and apply it to graphs this is probably the scariest slide in the whole presentation and but what we tried to do at octavian was having taken that step back to understand like what does what is the pattern of machine learning we then said how would we like machine learning on graphs to work so we thought about image classifiers in the way they work is they learn to predict the probability that an image belongs to a category right is it a horse a dog a cat a car so it lets predict the probability for each category given an input image and it does that by applying a convolutional neural network to the image so that's great what we want to do with the graph is perhaps take a section of the graph a sub graph and predicts some category that belongs to or perhaps we want to predict class belonging to a node based on the sub graph around that node and so an example prediction up we might want to make is that are you going what way you're going to vote are you going to vote Republican or are you going to vote Democrat and we might be the hop or two hops okay now we don't want to do that because we don't want to subvert democracy but that's an example of a predictive graph algorithm so we went through with that we want to make we want to research how we can make machine learning applied to graphs and applied to sub graphs and we want to be training it on sub graphs or nodes from graphs and predicting outcomes you know regression classification or embedding based on those so that was our goal now there are some existing mechanisms that you can use on a graph such as node Tyvek which we didn't like because they didn't fit into this pattern and so the way that node Tyvek works for example is it takes random paths random walks from within the graph and then it uses those with the neural network as the training data and the reason we didn't like that was because each item of training data is just a random walk in the graph thing as you're throwing away graph structure and graph information in order to convert from a complicated sub graph into a simple sequence a random walk and you're throwing away data so that you can fit into the training model that you have so the training model can only cope with sequences because it's adapted from natural language processing and so we're throwing away graph data in order to sue our neural network model and we didn't want to do that we wanted to keep all the graph information and find some way of adapting neural network to be able to deal with that graph information so we had a kind of goal but obviously we're not the only people doing that so we took a look and and we took a look at what's already going on in the world of deep learning with graphs and there's a reason and a stuff out there so these are a couple of examples of results from research on using networks on graphs one of the things I want to pull out from here is just the fact that these aren't achieving with superhuman performance and that we can get with deep learning on things like images I think or planning go so there are results on the left here and looking at deep GL which is a graph embedding approach and no Tyvek which is another graph and approached and deep gel is best performing better than no Tyvek but you can see that the success rate is getting is 87% best and really there's a lot here that have below 80% and that means below 80% means getting it wrong more than 1 in 5 times and we're really looking for stuff that's going to achieve a higher success rate than that and similarly on the right hand side with classification tasks you can see there are things that are achieving 50 or 60 percent and on the bottom right there's a task which is through predicting chemical reactions and the chemical reaction prediction task is really interesting because it deals with lots and lots of small sub graphs compared with looking at things like reddit which is analyzing one really massive growth and but still on the chemical prediction when it's saying we're looking at the first prediction like the top prediction from the neural network the success rates are 70 percent 78 percent so we thought that there was some some scope to improve this so the challenge but a lot of these agency mechanisms take on is how we take a complicated and variable structure like a graph and fit it into a fixed size matrix in order to apply existing your network techniques so most existing your matrix pretty signs of matrix and and from that it's able to make process it and make a prediction and but with a graph if you say I want to look at a person and I want to look at their friends and or if I'm doing a transit station transit network and I say I've got a graph and I want to look at all of the places that I can get to that are within five stops of the station that I'm at depending on where you are this the size and shape of your data could be radically different you might only have ten friends or you might have a thousand friends so going from the graph into a fixed size matrix is the technical mathematical challenge and a lot of these approaches try to solve that and one example of how they solve that is by taking random walks in the graph but this after doing more research this turns out to be a bit of a red herring so thinking about how neural networks work and a lot of people say that neural networks are good for unstructured data but actually the way they work and the things they're successful on are tied to very specific data structures so your networks are great at classifying images they talk about and doing all kinds of image processing tasks and they're really good at dealing with sequences so natural language tasks are almost always approached as sequences they take a sequence of words sometimes even a sequence of letters in a word and sometimes they process the sequence bi-directionally but they're always dealing with sequences of our natural language and these image grids for images and the property that these have is that we already know in advance how individual data points are related to each other or individual positions within the input matrix so in a sequence the item that comes before something and the item that comes after a more relevant than item that's far away in the sequence with an image we know that pixels that are adjacent to each other or close to each other are more relevant than pixels that are far away to each other and the net the structures your networks use to be successful with these represent that data so taking a taking a view of the NGO board the NGO model is able to use against your own network because every location on the board is equally relevant to every other location because of the way that go is played you can play a tile in one corner of the board and that can have an effect on a position at another corner of the board so using a dense network makes sense for that because there's a equal likelihood that any position on the board might have an important impact on any other position on the board but like I was saying that's not true for images and sequences and as it happens the most accessible models for images and sequences don't use dense neural networks so the images we actually use convolutional neural networks and a convolutional network builds in this property that adjacent pixels are more important than far away pixels so instead of connecting every single pixel in the input and adding it or mixing it in some way with every other pixel in the input to produce an output or and we combine those together to make a pixel in the intermediate layer and you cut out a little bit can you repeat what you just said I think you cut out so that's trying to make sure everybody can hear yeah sure cool I'm just recapping that for Karen and in convolutional neural network what we do is we use a convolution kernel and that convolution kernel only looks at image pixels that are close to the particular pixel we're concerned with and then connects those together it doesn't take into account the values of pixels that are far away and so we don't actually use these dense neural networks with images we use these convolutional neural networks and they've built in that expectation pixels that are adjacent to biology we do a similar thing with recurrent neural networks for dealing with sequences in a recurrent neural network makes the item that comes before me in the list much more relevant than items that are far apart so based on this how should we be thinking about graphs if we look at the graph and the dense neural network it should be obvious that the dense network isn't encompassing the properties of the graph what we like would like is the things that are close to each other in the graph to have more of an effect on one other than things that are far apart in the graph but that's not something was built in to a dense Network so we don't expect what we shouldn't they should work out with a model and your own Network model and that can maintains the priors of the graphs maintains this property that things that are close to each other have more influence on each other than things that are far apart the challenge with graphs is that they're really variable in structure alright so images and sequences have the exact same structure every time but a graph is going to have a different structure or a variable structure even if it's got a fixed schema so the challenge with AI is not just how do we turn a graph into a vector so that we can put it through dense neural networks it's how do we come up with and your network architecture that can understand grass more effectively to mount semantics or knowledge graph as it is - and more database graphs so biases and and to put it in a phrase is how can we structure the neural network to retain graph structural priors so I'm not the only person and we obtain not any people thinking about this my ideas of and it's a recent paper a lot of this we were working on before this paper came out and then when we read this paper we're just like wow these guys and they're they've had all the same thoughts as rest it might better than we ever could so your I think our inductive biases deep learning a craft network from deep mind here in the University of Edinburgh so that paper is a lot of work to read I think I've read it about five or six times I'm still getting stuff out of it but I wanted to pull something out of it for you guys so one of the things that that paper expresses is this model the graph processing and these are the only equations by the way in this presentation other than y equals MX plus C so it's it doesn't get any harder than this and it's gonna get easier they proposed this algorithm for producing a graph the first step the edge update is that every edge is updated based on the nodes that it's connected to and the global state the second step in algorithm is we update every node based on the values of the edges that are connected to it and then the final state is we take the whole graph all the nodes and all the edges and we update the global state and to do this we define six functions and one function for transforming the edge and one function for aggregating all of the things attached to the edges one function for transforming the node and function for aggregating the nodes and a function that aggregating all the last step and with these six functions and this algorithm that paper proposes that we can implement lots of different existing graph algorithms and that they don't have to be anything involving your own networks they can be algorithms like PageRank or you breadth-first search and they just propose this as a framework for doing a graph computation but they go on to say if we were to make these functions neural networks or just some of them can be neural networks and some of them can be a kind of identity function and then we can train those functions using gradient descent tool [Music] but fully trainable the mechanism and that learns the functions necessary for transform the graph towards some goals to put this hopefully and the idea that posing is that we load the specific graph off that we're dealing with into memory and then we use some collection of neural network functions that are able to learn to transform that graph in memory and then eventually we get some output that is reading information from a transformer does the approach think we should we might have to have you repeat that one set piece again because I think the internet was cutting out again okay so right that um a simpler way to look at this is what we're proposing that you do is you take a graph and you load it into memory inside some application where we can then transform that graph in a series of steps and each of those transformations is being carried out by some neural network or combination of neural network functions and that are end-to-end trainable and then eventually and and this can be thought of in this graph memory Network setting so we're not transforming the graph into a vector and put it through some process but we're actually interactively updating the graph and with a collection of neural network functions so that's the approach that we think is suited to the structural priors of graphs and it's the approach that deepmind has proposed in that paper and question is you know from our experience does it work so with which to train and test whether it works so we build this dataset called clever graph it's inspired by the dataset that's used for the image training and reasoning process that I showed earlier on where we were looking at you know what is the color of the sphere to the left of the gray cube or whatever was that task comes from a dataset called clever inspired by that to create it set there was graph based rather than image based and we called it clever graph whatever graph consists of is 10,000 and data points and each of those is a question answer and a graph so and each of these graphs is unique and we modeled it on transport networks and roughly based on the London Underground transport network but each graph is different and synthetically generated and and each of those synthetic directed graphs is effectively unique it has different station different lines different organization of those stations and lines and each the questions are sampled there are about 20 and it's seven for each question type there are some different wordings which we generated where the computer and then there are these different answers so on this data set we hope to be able to take these unique graphs and articles it's really important to understand that we're not training the graph to memorize we're not training in your own network to memorize answers on a specific graph we're actually training it to deal with dynamic graphs that it's never seen before and figure out answer given that the schema is fixed so here's an example of one of the graph network that's generated by the clever graft so you can see this is the kind of section from a transport graph I'll pull up the questions that are included particularly interesting questions for me in here and are for example Chris oh I'm sure that Andrew cut out of it when you're explaining which questions were interesting um yes so there's within these questions that I find interesting and the station adjacency there are that of kind of architecture station is adjacent to station they require potentially multiple steps of reasoning and to see up to if to find which stage in an adjacent to the station given a particular property requires looking at multiple nodes attached to a given node and looking at properties and whether there is a station exists at all was also quite an interesting question so is there a station called Oxford Circus and because it might not exist in a particular one and that that question is very interesting because a lot of neural networks don't a neural problems don't deal with the case where their answer doesn't exist and so for example when you do image classification typically every image contains something that you can classify a train with images and just white noise or empty space and very these these questions require a mixture of different skills from our graph reasoning engine those skills include counting nodes counting ages reading properties from nodes and these multi-step reasoning that require to transferred traverse a graph find the shortest path between two nodes or to combine facts combine or compare data in the graph so this is how we're doing with our architecture training on and testing against clever graph you can see we're able to get because it's synthetic data were able to get virtually 100% and on these questions once we have achieved the right architecture so here what we done is we've separated them out into questions that require different skills the first set and skills just require looking at properties on nodes then we move on to things that are more complex so the next one station adjacency requires looking at both nodes and edges then we need to look at nodes their properties and edges then we go on to things like and how many stations are between one station and another station that requires the network to learn how to do Dijkstra's algorithm right finding the shortest path between two points and and you can see we're still getting 90% accuracy on those with diced algorithm we're getting 92 and accuracy on stations that are up to nine hops apart the there's also the existence questions that I talked about and they're challenging for different reasons and then you can see there are these questions that we haven't yet tested the network on so this is a work in progress but each time we look at this and we work on mentor questions we're maintaining the performance on the previous questions and adding more abilities to our graph okay that's right so does it work I hope I've shown that we're able to achieve a pretty good level of success my range of different problems and imagining this end to end so the neural network gets as its input the english-language text and that u unique graph and these training results are on graphs that the network hasn't been trained on so it's never seen those graphs before it just has been trained on the schema of those graphs the fact there are stations they have a particular set of properties there are edges that have particular properties and those connect those stations so we're really confident that this approach is showing much better results than a lot of the previous deep learning on graph approaches but we don't have the full set yet to show you so does it work and promising but I can't say for sure but let's talk a little bit about how it works so I reckon I've got about eight minutes um and that should be about right so the questions we've got to recap we've got a graph we've got an english-language question and then we've got an answer which is either a number or it's one of the stations or lines in the graph the graph networks algorithm gave us a method to propagating information through and transforming a graft which is great but we have to also bring into a bring our question into the equation we have to prime the graph to answer our specific queries if because we could have the same graph but different question all right so I might instead of saying how many stations are between Bank and temple I might say is there a station called Oxford circuit or I might ask what is you know is there a rail connection and at Bank so we have to take the graph and we have to prime it or somehow Prime in your own network to answer that question so deepmind gave us a a big like boost in how we construct your neural network to retain graph priors but it hasn't helped us and figuring out how to prime graphs to answer questions which is the task we've set ourselves so we took a look at other research and this paper really stands out and really is the foundation of a lot of work in and deep learning at the moment and this introduced this new cell called the attention cell and that has a big impact for example of network used for the Mac image at research of the hood the Mac nets image reasoning that I showed earlier so this introduced this attention cell which is a fundamentally different building block for a neural network from the deep layers and we've we can use this to solve that problem how do we prime the graph and also to solve problem of how do we read out from a graph so where the attention cell works is it allows us to attack to take a query and then to take a list a potentially variable size list of elements and score each of those elements against that query so there's embedded question tokens in this case it's the tokens of the words of the question and but it can also be a list of nodes it could be a list of edges or it can be a list of edges and nodes and we take the query and we score it against each of those items and then we use the softmax to transform that those scores into a probability distribution and then we wait the input items by the the score that they get after that normalization would softmax and then we aggregate them all together and output a fixed sized control signal so what one benefit of this is it allows us to take a variable length list like a list of nodes or a list of edges and convert it into a fixed sized signal and attention has really been used to make some groundbreaking improvements in things like a natural language processing and word embedding as well as multi-step reasoning and understanding for example what the graph is looking at by looking at these scores were able to generate those images that I showed earlier that show you what the neural network appeared to be looking at so we're gonna use these attention cells and the way that we use them with a graph is that we use them to write a signal into the graph based off of the query that we were given so the query is like what are how many stops are there between temple and Bank and that query then is used with an attention cell which is looking at every single node in the graph and this could be every single node and every single edge or it can be every edge but in our implementation we just look at the notes and basing based on the score for each node with respect to the query we wait we give a weighted input signal on to that node so if we were asking a question about temple and by clearly those stations would be more relevant to that question than stations that didn't have those names and we wouldn't expect them to get the same input from this attention so we use this approach to get a signal into the graph to prime it and make it care about the question that we're dealing with and then what we can do is we can propagate the information through the graph with message passing and then the challenge we have is how do we read the answer out of the graph so the node that is the answer to the question or pull out the number and that is the ask the question or the name of a station whatever it is then we can use attention for that as well and so we use attention to read on the graph by taking the query again and trying to figure out which node and the graph contains the information that's relevant to answering the queries so we put all that together and this is arktech be calm a cross and there's Andrew you cut out just right when you were saying that this is called Matt graphs you can repeat yourself again yeah say we call this Matt graph and this is the really high-level view of the architecture and there's a lot more information about one and I'll get a repository like I said all of our work is it can throw us and but from 20,000 feet we're using attention to send a signal in the graph she's passing to let the graph using functions neural network functions that learn over time how they're supposed to operate and then we're using attention to read information out of the graph again and that's the architecture that we're using to get those results on that range of question-answer tasks on unseen graphs so what we've been able to achieve is a model that we can train neural network model we can train with gradient descent that learns how to navigate a particular graph schema and with that graph schema learn multiple algorithms to answer multiple different questions and we're doing it with end-to-end training so we're training it on what we want to be able to do the exact task and that training looks like you know question answer and the graph that we want you to deal with and we've shown that we can take a single model and architecture and it's able to learn a range of different graph algorithms to solve different problems right we've got Dijkstra's algorithm we've got reading properties of nodes and we've got looking breadth-first search at finding you know whose adjacent with a given property and we think that that's that's really promising and there's a lot more already we've shown that this approach can definitely learn graph specific algorithms and yeah we hope to continue researching this and using the approach that we've done which is building synthetic datasets and using roads synthetic data sets and to understand neural network and how it works and to improve the performance on parks that we should be able to win um so that's that's where we are today after octavian and I invite anyone who's interested to come and participate you know you can email us you can tweet us and you can check out our repos on github and we're always interested to get more people involved in this research and then to find new problems and that we think we might be able to apply it to so with that I'll hand it over to Marc I think oh yes we do have some questions that came in Marc Christophe asked if there's any recommended learning resources to help someone better understand the material and we did David Mack did already respond in the chat saying about that saving a blog and a deep learning book but if you any other suggestions there may be unlike one of some of your journey of exploration yeah and so reading the reading some those articles is really great particularly you know I'm a big fan that deep mind paper and there's a thing called graphs age and network group at Stanford here you have some really good articles and papers and that's that's really a howl of my journey has come through as well as reading that material giving yourself really simple problems to do can you know can you just train something that can take a list of numbers for example and pick a number out of that list using attention can build very simple problems and apply these same techniques to them nice okay we also have another question from Victor Lee he asked he said so in general in the general computational model in each iteration every edge in every vertex is getting updated question mark is that really practical on the o4j so that's that's how the model works is that we transform the whole brass multiple times and we're not using neo4j to do that we're doing that in maritime using tensor flow and whether or not it's practical to use Nia today if we were running inside the JVM then I think it would be practical Oh out so if we're running inside new today my belief is it would be from outside of near PJ it may be a little harder and but it depends on the speed and parallelism that we can support so potentially these are super parallel algorithms but you cut out a little bit towards the end and potentially these are really parallelizable so you're just sending a lot of updates and you know concurrent updates into the graph and concurrent reads and that's not a like that's a workload that it's that's happening earlier today so if anybody has any other questions after this hangouts is over you can ask in our neo4j community site there is a link to it in the description of the YouTube you are also able to post there if you have an idea of something that you want to talk about for the next online meetups under projects or content categories you know if it's projects and you can put it under a project category and also is there anything else Andrew that you want to mention before if though people can you know aside from the octavian blog if you have anything else that might be good or no I mean aside from the near-field a community and the octavian blog and there's a great place if they're asked by students we do have another question that came in that's interesting Robert Shimizu I'm sorry if I mispronounced that he asked if you're using neo4j to train tomorrow and train the model and a tool a tool asked how multiplying apples are yummy do you actually see the chat because you might actually see these questions too so maybe there's something in here that you have interesting I didn't see the chat all right okay but yeah so we're not using therefore J to Train and this the other stuff we're doing we are experimenting with altering and adjusting neural networks and and changing all kinds of aspects of that and so we're working in in memory intensive flow but we do use Nearpod a for storing and sharing and transforming graphs and and we hope you know to be able to bring a synergy with you when we're not working on synthetic problems so if we're working on a real-world data that's transactional and being stored and has some importance than I expect that we gonna need to deal with databases much more a lot of people are saying that they really like the talk if everybody watching if you really enjoyed Andy's talk please put a thumbs up on the talk and we do have some other questions somebody Santiago gonzález asks have you applied the techniques you explained here to compute subgraph similarities okay again con he said have you applied the techniques you explained here to compute subgraph similarities okay I think the I think the sound cut out a little bit so it is my job uh I'm from the chat and and reply in text way I think the internet god yeah I mean if anybody has again if anybody has any other questions that you want to ask you can definitely go to our community site and II actually posted a thread and you can talk directly to him you can ask questions there and it'll be good and valuable for other people and even after the fact we also have a talk next week on Wednesday November 21st at the same time as this one that's going to be on similarity graph algorithms so that should be an interesting one yeah so I think that that I think we can probably call it a end of show thank you so much Andy for taking the time out and showing us this I hope everybody enjoyed it and hopefully we'll see you guys next week yeah welcome yeah 