 all right thank you Andy so the work I'd like to present today is called heat space so my name is Andres fender and the co-authors of that work yeah Andy listed them already David in the borough Philip airholes mark Alexa from tu Berlin and my supervisor York Mueller so the problem we're trying to explore with this is the fact that information displays are often placed by intuition so if we're looking at that image for example of a cafeterias entrance area we can see a display in the upper right corner which is visible in principle like you can see it but the people in that space are mostly busy with picking up their trays and cutlery and so on so they're not necessarily looking at that display and so it's important that the installer actually knows how the space is used in the end and that's often not the case and that's also true for other environments like like in a manufacturing domain or in office spaces so the idea we have is that we are that we let uninstrumented users interact in the physical environment during the normal everyday routine and while they're doing so we measure their viewing behavior and in the end our system automatically proposes display placement based on these measurements so and this is what it looks like so first we capture the environment and we do some 3d reconstruction and then we let users interact in that environment and we measure the visibility while they're doing so and we reconstructing the physical environment based on static surfaces and and moving objects and we also build an application where installers can try out potential display positions and get real-time feedback and finally we optimize the placement of these displays so what we're contributing with is an perk analysis of user viewing behavior and an optimizer for position and size of this place and the overall goal is to replace guesswork with quantitative measurements so these are the components of heat space and I'd like to go through each of these in detail so let's start with a capture so before getting into the details I'd like to show the example setup that we used so we were using three depth cameras connect two devices and they were all positioned so that they were seeing the scene from an orthogonal angle the area of the space is four four meters by four meters and we use various different furniture arrangements of the different examples so the capturing is pretty straightforward so we're using the the feet of the depth cameras to to reconstruct the physical surfaces and then we generate in real-time meshes meshes based on these depth cameras and they serve as a point cloud and also for as colliders for rate castes which is going to be important later on and we also using the built in skeletal tracking the Kinect SDK is built and skeleton tracking for user viewpoints so the second and one of the core components of the system is of course the generation of the heat maps and first of all I want to mention how we actually store the data so we're using walk cigarettes which are just like uniformly distributed in space and each of these voxels are storing local data about different about different aspects which I'm going to explain in detail right now so each voxel is storing first of all the persistence the distance and the visibility so let's start with persistence so persistence means how static an object is so we can see for example in that little video that the white part is pretty static in the table and the reconstruction we can see the reconstructed static surfaces and which have a high persistence and then we have medium persistence objects which are like moving objects and these are indicated with these white glows and that rendering and then we have a distance this one is pretty straightforward so we're simply measuring for each voxel in space we're measuring the distance from the users view point to the voxel and in that image we can see that the table close to that stylized a user is very like orange which means that it's close and the wall and and the background is a bit farther away and therefore it's green so with ability is a bit more as sophisticated and like kind of the most important measurement we do so with the visibility it's just a measure of the quality of that walk so like how well visible is it for a user and we were splitting it into three components the first one being occlusion then we have the field of view and the viewing angle so occlusion simply means for that instance in time is the voxel actually occluded or not is visible from the users point of view and if there's a physical surface in the way in the line of sight then the occlusion is zero or the the value is zero and otherwise it's one so the field of view is decreasing whenever the angle between the users view direction and the line of sight between the users hat and the voxel which is depicted as al is increasing so we're getting the users view direction simply by using the face tracking capabilities of the Kinect SDK and so that just means the further in the peripheral the voxel is the worse the quality gets for that voxel and viewing angle is taking the surface itself into account so let's say there's some geometry within that voxel and that geometry also has a surface normal and then we are simply measuring the angle between the line of sight and that surface normal to get the viewing angle so what that means is that when you're looking at a table for example with a very narrow angle then the quality of that measure is really bad and if we looking straight at a wall for example then the value for for the viewing angle is very good so in the end we're multiplying all these things to get our visibility all right so all these measurements are calculated at each moment in time for each frame and what we do in the end just weird time accumulating them so the persistence and the distance and the visibility each our time accumulated so for example this is the visibility for a for the first instance in time where we can see like pretty much like one focus point but over time it's getting a bit more distributed and over time we getting we are finding out which areas are getting more intentional which are getting less attention and I'd like to talk about like one of the implications in particular about occlusion so for example here we see a user interacting at the table and for that instance in time we can see on the table there's the occlusion is either a 0 1 depending on if it's secured by the arms or not but over time we can see that that the whole area is kind of bad quality even though they are not occluded in that instance in time so that means that this place would be positioned further away from the user even though the viewing angle everything is nice but still is since the hands are always like waving in front of them of the user it's still a bad surface area for example all right so the last big component is at the display placement so the way our application is structured is like that so first the installer needs to specify some display so there are some parameters some of them are optional some are required and so the first parameter is the number of this place so that's a very that that all has to has to put in and also for each display the resolution has to be has to be set and optionally the Installer can kind of try out some initial positions for or for the displays and optionally the size can be fixed or it can also be delegated to the optimizer to optimize the size of the display if if that's a degree of freedom for for the installation and in the end based on these values who want to find an optimal display surfaces so and this is what the application looks like so we're we can explore the reconstruction and the Installer can just freely positions and put potential displays anywhere in the room and then set some parameters and move to displace around and why the install is doing so here she's getting real-time feedback so within the displace we can see different shadings indicating how good that portion of the display can be seen and also the qualities is being output so in this place can be positional any static flat surface which can be vertical or horizontal and if it's horizontal them and the display can also be rotated so and finally after the the Installer has set up some initial positions the optimization can be triggered simply by clicking optimize all right and I want to briefly talk about the optimization of wooden and I don't have time to really go into detail but I just want to provide you an idea about how we are actually doing the optimization so the first step is to find planner and generate typical a persistent candidate regions and the way we do that is we're creating a connected components graph and that graph is based on persistence and planarity so that means for example if we're having a flat table and a white board which are like close to each other than these two are two different connected components even though they are physically attached they still have a different surface Norman therefore they have two different connected components so the second step is to sample potential display positions and then for each display to be optimized we do the following so we placing the display resilience on the sample with the highest score where there is no display already and now that that step is kind of important so if this place end up on the same connected component we're using an alternating gradient decent approach so so this means for example if there's like a big wall or a big whiteboard whatever and two displays or position on that big connected component then it would be kind of unfair for for one of the displace if we first optimize the first display and then the second one and then the second one is to figure out some other space somewhere close what we're doing instead as putting them both at the same time on that big connected component and then gradually locally optimizing both of them at the same time so that's the alternating gradient descent approach but of course way more details in the paper so also I'd like to show some example results so in that first example we are having two users sitting in a big table and what we can see here is that the second user is blocking the view to the ball for the first user which is also reflected in the real-time feedback and of course the optimizer is taking that into account and is moving the display so that it's not so that the occlusion is kind of minimized we can also see two smaller displays to the right and even though they're kinda in the peripheral of the first user the the resolution requirement and the size requirements Forsythe's this place to be really close to the user so the second example are a two users sitting opposite each other and what we can see here is that the first user has a pretty good overview of of the room behind the other user however if we are looking at the comb heatmaps of both users at the same time then we can see that only in the peripheral for both users which is like the wall to the right can actually be used for this place and then the optimizes of course taking that into account and positions the displaced so that they are visible for for both users not only for one of the users so of course we're having some limitations some of which we are also trying to take on the future the first limitation is that this place have to be attached to static surfaces so what we mean by that is that we for example we cannot use ceiling suspend at this place or this place which was just standing in the middle of the room because if we would use these displace then they would change the whole physical layer of the of the room and all the occlusion patterns and everything would just look different and therefore we cannot optimize these and also of course there there might be a possible and there might be a behavioral change after installation and this is especially true for interactive touch displays so which means this place which where the intention is to actually change the interaction that workflow within the space and therefore we are more focusing on this place where the intention is to not change the workflow too much to seamlessly integrate these displays into the workflow so primarily information this place or otherwise we can just keep measuring so after after an initial optimization we can just measuring it maybe refine or verify the display placement so to conclude so the system I just presented is called a heat space and we're letting uninstrumented users interact in the environment and while while they're doing so we are measuring persistence and distance and visibility and these vows are stored in boxer grids measurements are a time accumulated so they're they're measured over time and the display placement optimization is based on that empirical data and the oval objective is to replace gas gas Brook quantitative measurements alright and with that I'd like to end the talk and of course I'm open for questions thank you Steve founder Columbia University I love work that tries to automate the performance of certain things that people would normally do by hand I have a question and this is something I encounter I know many other people encounter as well you have a multivariate equation there that's going to determine the score for something so what made you decide on the particular weights of the individual things you were combining together and did you do anything to try to validate though doing the right way to do things so this is kind of briefly also discussed in the papers that's a very good question also something we thought about so these different weights of course right now they have to be set by the Installer and that requires some like knowledge and maybe some intuition about how to have two-way occlusion against like peripheral field of view and so on and depends on like for example if the content is like moving a lot then it's maybe not so important that's in the central field of view because if there's motion then we can kind of relax that condition about the propria review and then the Installer can kind of like adjust the exponent for that value but of course in the future would be kind of cool to also explore setting these parameters and automatically based on the content to be displayed so for example if we know that the content is animated then we can can adjust the weight for the peripheral view or if we know that the display has a really bad like a viewing angle range I don't know the exact work for that right now but like these altars place which are like really hard to see from the side it's not such a big problem nowadays anymore but if we know that and of course the viewing angle would have like more weight so to say then that display has to be seen from the front for example so there's a lot to explore in that direction also and also doing studies you see with a particular layout how people actually perform you know to get a sense of which things really matter one thing I noticed that didn't seem to be in there unless I missed it was if you and I are at opposite sides of the table there's a display on the surface of the table and it's got stacks that we have to read one of us may end up reading it upside down unless you sort of make both of us read it from the side so did you try to take that into account you know the orientation of the content actually not yet but that's a pretty cool idea actually yeah so I guess we only have that problem with a horizontal displace so maybe that's something where we need to investigate a bit more like in detail how how it works differently when we're kind of focusing in horizontal this place when all these additional challenges are have to be taking into account thank you 