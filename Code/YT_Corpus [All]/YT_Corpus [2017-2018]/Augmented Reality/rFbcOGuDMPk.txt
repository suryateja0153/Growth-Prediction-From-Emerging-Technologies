 [Music] hi everyone I'm Tom I'm one of the technical leads over on our VR and AR efforts at Google my team specializes in developer productivity tools I'm really excited just one week after announcing a our core to stand here today and talk to you all about the AR K or Core SDK preview and how you can really easily add a our capabilities to your Android apps so just a quick run-through of what we're going to talk about today so we're going to talk about AR and it's really closely related cousin VR we're going to talk about the concepts that underpin AR core we're gonna have a walk through the API we're gonna talk through all the options that we have to build our AR content and we're gonna talk about all the options for building your code as well so before we get into the technical details I really want to start with what immersive computing means where we've come from and what can it do for you so I'm always asked like you know why AR and VR you know we have these mobile devices and it's all great why should we go and do these other things so with cardboard daydream and tango we've been investing in this space for quite some time and we really believe that interacting with your data in a more natural way as the future and most of computing removes a lot of those abstractions between you and your computing so I want to take a minute just discussing these technologies Before we jump in some code so we think they are in VR is being points on a spectrum of immersive computing on the far left you have reality which is the world as we know it and we're sat in today if we start to add digital content into our reality then we start to augment it where those hence augmented reality and if we completely replace that reality with the virtual we have virtual reality and so AR can bring any of your digital content to you and in your world such as this guy posing with the dog so some of the benefits of AR you can see objects at real size and scale in your world imagine being able to buy furniture and being able to see if actually fits in your house before you buy it you can also see things in context again imagine buying that furniture and making sure that it matches with all of your other decor in your house and being able to imagine being able to annotate the real world with like post-it notes without actually causing a real physical mess and it also adds the ability for natural input for 3d scenes so if ever you've used digital content creation and modeling tools it's actually really hard to control that camera in your scene with augmented reality you just hold your phone and you just look at it and anyone can do it and then if you replace everything in your world with the virtual with digital content you've got virtual reality it allows you to go to places and visit worlds in an instant and some benefits of this are you get complete immersion you really feel like you've been transported to another place and there's some really really huge innovations in input that really lets you work naturally in your world both VR and I are unable to us experience computing more likely experience of real world and both take advantage of a lot of the same technologies so back in 2014 we started with the idea that your devices should be able to understand more of the world so we dedicated Hardware tango allowed us to understand the depth and allowed us to build some really great applications from being able to measure your world being able to map and share your house and to be able to play some games on your tabletop with your friends we built to consumer devices with Asus and Lenovo and developers create more than 100 applications for tango we learned a lot and now we can do more with software than we could three years ago and we don't have to rely on custom sensors which leads us today and last week's announcement with a our core so last week we announced a our core it's a preview SDK that allows you to get open we're building a our applications right now Alcor takes everything we learned from tango and it makes a a are available on the phone you have today no depth cameras or custom special sensors required Alcor is currently improve you and we're looking to developers like you to give us feedback on how to make this sdk work for you and with two billion devices out there we have a huge potential audience for this technology with a our core so far running on the pixel pixel XL and the galaxy s 8 we currently run on millions of devices already and we're working with manufacturers like Samsung Iowa LG Asus and others so that the end of the preview we anticipate that we're going to run on 100 million devices and we're working with these hardware vendors to make this possible with a really consistent bar for quality and high performance in the same way that we did with daydream and so before we get into the code I think it's really really helpful to understand some of the fundamental concepts behind they are just so you know how AR core is working under the hood so there are three main concepts to think about one is motion tracking 2 is environmental understanding and 3 is light estimation so let's go through them in a little bit of detail now so to render a our content you need a virtual camera that matches your physical camera you render the virtual scene you composite it with your camera and you're done this sounds simple it's actually really really hard whilst your phone gyroscope is really really great for rotation it can drift over time and whilst your accelerometer is great for those instantaneous inputs it's not so great to figure out an app actual position so the really hard part of getting AR right is to figure out this translation and rotation of your device in real time so you can render digital objects with the same virtual camera as your physical camera if you get this wrong objects in the world will be misaligned with their virtual equivalents they will swim and jump and they won't appear properly rooted in the world and you can see how effective we've done this with a our core because of this scarecrow who looks just like everyone else queueing for tacos so to do this Alcor uses the device's camera and inertial measuring unit to track exactly where your devices in the world using a process called concurrent odometry and mapping also known as comm it looks for visually distinct features that can track over successive frames and builds up a point cloud so it can localize against that point cloud this combined with that high frequency I new data gives you rotation and translation in the world so you can render your virtual content in exactly the right place this is over an above or the Ray experiences you may have tried which only uses the gyroscope to get a rotation which has the problem of content sliding around your world and you can't move in closer for a better luck this is really really key for anchoring your digital content over the real world so this illustration is an example of the device tracking feature points in your world and create a point cloud but when it does it for real there's actually a lot more points than just for so on top of motion tracking which is really important environment understanding is also super important rendering content isn't actually that interesting by itself you need to be able to interact with your world as well so a our core is looking for clusters of those feature points that appear to lie on common horizontal surfaces and it makes these surfaces available to your app as planes since planes are mathematically infinite alcohol also provides the bounce of these surfaces as a polygon and you can use this information to place objects in your world like this Android guy here so planes are detected on horizontal surfaces such as the floor tables kitchen countertops benches chairs you name it however because a our core uses feature points to detect these surfaces flat surfaces without any texture or highly reflective surfaces might not be detected properly and then finally light estimation so Alcor is able to detect information about the current in slighting and gives you a value representing that average intensity of a given camera image this information lets you light your scene and your virtual objects under the same conditions as the environment which increases a sense of realism if you don't do this your digital objects will stand out and not appear to be a part of that world which is really really key to realistic rendering and let's use some really fun effects like have this lion who gets scared when you turn out the lights and it might seem frivolous but it's actually really important if you've ever taken your camera and just look to a light source or pointed out the window the auto exposure of your camera actually changes that range pretty hugely so it can be really not so great if you don't take this into account so now we've gone through the main concepts of aircore let's jump into some code and see what it takes to build an AR application in Java so this is how I think of the API you create a session which represents the owl session that you're running once you've got this you update the session and it gives you a frame once you have a frame that represents your camera and all the metadata that goes alongside that you get planes once you have planes you're able to create anchors with anchors you're able to place content in your real world and our hello AR app exercises all of those parts on the the API and is on github and is great to get up and running with this so going through the code it's really really easy to start a nail call session you just start a session we provide a default configuration file which basically turns on every part of the API motion tracking plane finding light estimation simple way to check if your diet device does not support AR core and when we come to render our application first thing we do is clear the frame and then we simply call session update one of the key concepts to understand here is when you created your session one of the things you might want to change within your config is whether it's using blocking or latest camera image if it uses blocking it basically gives you a frame at the rate at which your camera runs which kind of makes sense you don't want to render any faster than you have to you want to make sure that every pose aligns with every frame but for some of your applications where you want a really smooth update you have animations you might want to just render as fast as you possibly can and at the expense of power and performance so once you have a frame we have this helper function hit test which helps us to cast array into the world based on a touch location and see if you have touched one of these planes if it collided and the tap was within the bounds of that detected plane then we create an anchor and we'll go into anchors and a little bit more detail later so we also have to see if the frame was actually tracking you know if you have put your hand in front of the camera or some and you're not tracking then you want to make sure that you your intersections are correct and then you query the frame for all the data that you need to render your objects and don't worry if you haven't used projection matrices before and you don't know what a view matrix is that's okay as mentioned earlier to render this a our content you have to match the field of view of your virtual camera with the field of view of the real camera the projection matrix contains all of those properties that you need and you just query a or quartic the Alcor session to get access to those and in this example we set the range of objects that your render from 10 centimeters to 100 meters and then the view matrix is what contains all of the information from motion tracking that actually contains the pose where the camera is in the world and then finally just getting access to the lighting estimation it's just a simple access a call once you've got that value you can either use it for some logic or you can use it to affect your rendering or your lighting and then finally we loop over all of our anchors that we've placed in the world and if they're being tracked we render those objects so if the road projection matrix contain the camera properties and the few match matrix contained the camera location the model matrix contains the location of that anchor within the world and with that combined Model View projection matrix you have everything you need to put the pixels on the right place in the screen properly overlaid on top of your camera feed so talking about anchors why do we need anchors what is this concept of an anchor so you might think well it's three dimensional space why don't we just call where you start the application the origin and place your objects relative to that so it turns out there's actually some error in the poses you get back from motion tracking motion tracking is constantly updating its understanding of the world and if you use anchors as this understanding of the world updates the pose of your models will update as well a good example where you want to use anchors is imagine walking around a building in a loop when you get back to where you started that drift will have accumulated over time and you really want to use anchors to make sure that they're all stays correct and so if you place an anchor in the world you should also make sure that you place the digital object on top of that anchor as well so anytime there is error in that rotation then the further away that your object is from that anchor you'll end up with this lever arm effect where like you're rotating around a pivot there's in the center of the object and it translates off and where it wants to be and then really think about these anchors are there to route your digital objects your physical objects in the world if I was going to place an object on a chair I want to create one anchor for the chair and place the objects on the chair if I want to correct put ten objects on my desk I don't need to create ten anchors I should just create one and place them all relative to it and then just again avoid using those global coordinate systems you'll you'll have a bad time if you use those so we kind of skipped over this apart from cleaning the screen we didn't really talk about rendering 3d rendering 3d is actually a really big topic that I couldn't possibly cover in a 30-minute talk here today but to give you something to look at there are a few options you can use OpenGL ES directly and we actually have some great tutorials on how to get started with this on our developer.android.com or you can use frameworks such as Rajawali which does a lot of that heavy lifting for you or you can take a look at us a hella way our sample which actually contains some model loading code and model rendering code as well if you just want to play some objects in your world so not only does a our core work with Java it also works with unity and unreal and we've done a lot of work to make sure that it integrates really well common game engines like these they remove a lot of the complexity from managing a complex 3d scene so that you can focus on actually building your application so we've ported hello AR into unity you can see it there it exercises all of those same api's and we've designed it so that you can easily get up to speed use those scripts and prefabs in your own application and it comes as part of the SDK when you download it and similarly we have the same for unreal as well we've done everything to make building your AR applications really easy using the tools that you're most familiar with so another option is WebEx are so real web standards for AR don't exist yet but these prototypes allow web developers to start building augmented reality web experiences today the experiments will teach us all what AR on the web could look like which will hopefully make the real web standards arrive faster and be better designed these capabilities are built on top of web VR so if you're familiar with that it's really easy to get up and running with WebEx AR and this demo that you can see here also works in experimental versions of Chrome for AR kit as well and so we've talked a lot about building a our applications but the really hard part is how do I get content we all know how to create content for 2d we know how to get images we know how to get text fonts videos we know how to create them and we know how to use them in our applications so Google have been working in AR and VR for quite some time and we've made creating this content really really easy which is why we created blocks so blocks lets you build 3d content in VR really quickly and even if you don't have a VR headset take a look at our library any Content tags as remixable is available for download so let's take a quick look at some of the content that's that our creators have built for blocks so if you want this guy a ramen chef in your kitchen that power is now open to you and if you want this guy to watch over you at night on your bedside table you can live that dream it's really really impressive what people have made using blocks and it's a super easy way to get 3d content into your application so I encourage you all to head to our blocks website download it and give it a play it's really really fun building this content in VR so before we finish I'd love to give you a taster of some of the things that we built in AR core [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] so this has been a whirlwind tour of the capabilities of AR core and all the information you need to get started is there today on our website so we've talked about the fundamentals of AR we've talked about the options you have to build your AR applications and we've talked about how to build content for those applications we're actively seeking feedback from developers such as yourselves through this preview phase and we'll be monitoring our Gipper github issues not other channels closely so that we're building a platform that works for what you want to build so you can see the AAR mocks and expert and what's possible mobile devices get started with the AR core preview today we really can't wait to see what you build [Applause] [Music] you 