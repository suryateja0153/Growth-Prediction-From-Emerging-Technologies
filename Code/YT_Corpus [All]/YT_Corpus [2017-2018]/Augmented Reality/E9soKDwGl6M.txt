 and Bernie Peters so my talk is gonna be on smooth moves and you might feel a bit out of place because it was planned through another session but because the focus is on a selection mechanism the moves away from pointing hopefully you'll find it interesting and thanks again for accommodating my talkity in the viewer the end of your session so our work includes myself of course from Edinburgh Napier David from Eindhoven and Ian and his students from Ulsan in South Korea so I think we can all agree that augmented reality glasses are rapidly maturing technology and I would also say we can agree that it's quite still very unclear what's the best way for interacted a our contents and interfaces current and viable input modalities for head mounted displays includes on set touch and here use for text entry so the Google glass here is not exactly our but the example is still quite good you can also interact with midair gestures such as projector they are or using wearables such as gloves or your belt it's quite interesting approaches and finally you can also use clickers or proxy devices such as your mouse so this is a screenshot from the meta to a our a glass and while these solutions are really really interesting many smart glasses focus in augmenting in a primary physical task which can at times fully engage your hands or the users hands and many of these in many of these examples you see here the head mounted display ends up serving just as playback device not really as an interactive device as such a more practical and appealing approach to interaction when they are is by tracking the users eyes or heads movements not only do they the hands ruin free but all sensing can be integrated and into the head mounted display with the problem this approach is that it requires explicit confirmation mechanism to trigger a selection that is you know where the user is looking but not when the user actively wants to interact with what's in front of them a common approach approach to this problem is to dwell which enables you to select the target by simply focusing your gaze on its and for a short amount of time Google calls it fuse I believe the drawback of this approach is is sort of a lack of precision and a fixed time costs and these quotes are from the developer websites for both the meta to VR headsets and the Google cardboard headsets another approach to trading selections is just using speech you can do the whole lens but again the drawbacks of this approach is it's a bit of true Civ and especially in shared work environments and can be hard to capture in noisy environments such as in a factory floor so this is not mine you can get the reference there in the bottom so someone else is doing this this is from someone else's work to address these selection selection limitations our paper follows a recent body of work that looks at gaze input systems based on super suits eye movements to trigger selections and suits are distinctive continuous slowly low latency adjustments the keys that are naturally produced only when visually tracking a moving object the smooth pursuit systems operate by showing your user by showing your user set of moving targets while tracking gaze and statistical matching between gaze and the target trajectories is used to infer which target the user is attending to and there are two main advantages to this type of input mechanism first instead of relying on a set of predefined gestures or speech commands that the user needs to learn and memorize each control encodes necessary input information in their singular movements the second advantage is that because they operate on movements not position the smooth pursuits interfaces are relatively immune to target size and if the user can see a target and track its they can actually interact with it so you can imagine this the same interface here if the red green and purple were quite small as long as their trajectory is quite distinct the user could still interact with them so the sizes becomes less relevant and we use this in a previous work on Orbitz that was presented here two years ago and basically we use this advantage of target size to create an answer interaction technique that enables you to interact very tiny targets Smart Watch interface button AR interfaces it could also be used to interact with faraway targets like the sort of lamp dates in the Far which could be hard to select with pointing to sort of just sort of following this Fitz law tradition and because you only look at movements we can also have controls that overlap in 2d space such as these two hanging lamps again which could be hard selected pointing because they're overlapping each other finally smooth pursuits input is robust against tracking inaccuracies capturing changes in gaze is much simpler than accurately determining where someone is looking at and also we don't need to go through this sort of the tedious calibration process to map the users gaze through to a display but they're also several limitations to smooth pursuits input first the wearable eye trackers are rather expensive you can see here two examples one is the pupil Pro and these are already the cheapest sort of versions of these so this is already the pupil Pro that you can add on to all ends and on the bottom you have Yoji glasses which compute eye movements and using pairs of electrodes instead of cameras in the second limitation is a form factor so I tractors require cameras or electrodes to be placed at three specific locations or the users face and with the former also requiring a clear line of sight to the users eyes and of course optical systems are also susceptible to changing light conditions such as those that occur when using air outdoors and at the same time when we were doing the this user studies for orbits we noticed that at times the system would stop working and this was because the user was actually started following the targets with their heads instead of their eyes and this is because gaze is the inseparable product of I and and head movements of a head than eye movement sorry and the relationship between head and eye is quite sophisticated so at the most basic level the view are continuously established stabilizes gaze by adjusting basically inverting eye position in response the head position or changes in the response to changes in head position in contrast during smooth pursuits tracking of moving objects the iron head moves together to keep the object of optic optimally in view so if although we can actually track the head instead of the eyes during smooth it's type movements we could address many of the limitations of wearable eye trackers for AR so ed movements can be tracked with I'm use which measure rotations along three axes you see here quite straightforward and there are several advantages as you can imagine to these types of sensors and comparing in comparison to optical tracking first is a form factor so the IMU is can be placed anywhere in the heads or smart glasses they're quite small and light so you can embed them into many head borne items such as jewelry or headphones and furthermore these sensors already embedded in this AR and VR headsets already so in this talk we just I'm just gonna go over the our four contributions which are smooth moves which is an emphatic need for AR that relies on rotation data from head mounted motion sensors that enables users to select targets by smooth pursuits type head movements we also present three studies comparison of head movements to an eye tracking baseline across a range of target moving conditions and performance differences between handhelds and head mounted base AR using smooth moves and a qualitative user study of AR of an AR system for smartphone control and more specifically as smart lights control so our work is heavily influenced by prior Smith pursuits interaction techniques but it replaces the use of eye coordinates on a plane or display by yaw and pitch data from the IMU in quite quite straightforward if the Pearson's correlation between a targets X&Y and the user's yawn pitch are both higher than 0.8 the target is selected very again very simple in straightforward visually smooth moves closely mimics our early work on orbits and a graphical control is comprised of a trajectory around the center points in case of ER the center point could be a physical object that you want to controllers as smart device you under control and then you have one or more targets that continuously Traverse this trajectory as you can see here in the video and target this evacuation is achieved in two ways first targets move in different phases as you see in the video and you can also have them move clockwise and counter clockwise let's go over first study quite quickly and so we captured both eye and head movement data in three conditions that you see here so this is basically what you told participate to track things naturally to track things with their eyes and track things with their heads idatr was captured using the pupil Pro you see here the participant is using and as with other suits input systems no calibration was was performed head data was captured using a 9 axis IMU do you see position there on the the edge of the the pair of glasses and we use Mahoney's filters to extract the on pitch and pitch values participants always started in natural condition but then we counterbalance the other two conditions and they would see that the moving targets on an external display the position around 60 centimeters from them we recruited 18 participants who completed 432 trials each and each possible try a combination occurred once for each of the three main experimental conditions and the targets varied size the varied trajectory visibility so we would show targets with the line or without line that shows a trajectory speed type Direction types P type targets with speed up halfway to the trial a direction type targets would move in opposite direction halfway through the trial we run correlations between the all eye and head data in the three experimental conditions using different window sizes these results indicate participants follow the target more naturally their eyes and their heads which is not particularly a sort of it's quite expected I guess but the data from the head condition however shows that Ed based tracking can be readily achievable participants would even better performance and that the ED correlation coefficients were higher than for the eyes in any of the target conditions as you see in the table here again you can see all this in the paper more detail of course the results were also uniform across eyes and head conditions basically small to Jack series were harder and target targets that change directions halfway to the trial so there basically there were the hardest one to select and finally we also looked at the scale of head movements which is quite different from the from eye movements and which can impact a range of factors such as such as of through sickness and social acceptability and long-term comfort with a technique so you see here some heat maps for the trials that are highlighted and in black and participants you can see in line one exaggerated the head movement for smaller trajectories and modestly reduce them for larger trajectories these are the means if you look at medians the head movement can be very subtle for the smallest objector ease median head rotations who were just six point seven degrees and so you believe these movements are sufficiently small to ensure the technique is discreet and not very very fatiguing to use so I'll just study too quite quickly we calculated also error rates and acquisition times in two conditions the head-mounted display condition which in which users were using the epsilon mu vary or AR glasses and hand held the display which used a nexus 6p which is a more common AR system if you think of sort of pokemon go and all these new AR applications you can get on your phone and so these were again contrabands course participants and in both cases participants were the same hand mapped in the IMU that we showed in the first study we recruited 16 participants who completed one hundred and ninety few trials each we re-examine trajectory size as we've shown to impact performance in the first study and added target cardinality as a condition shape was equally distributed among starting carnality and the remaining variables were constant so we didn't change the speed type and direction type anymore and participants have simply asked to track the target in red which isin at random so onto the results error rates increase the target cardinality which is not particularly surprising so the more targets you have the harder they are to select if we exclude targets with very small trajectories and interfaces with over six targets we report a mini rate of two point six for the glasses and four point nine for the phone and if you compare this with similar hands-free approaches it's quite quite good results and the acquisition times around two seconds and and will only negatively effected by smaller trajectories these two seconds also include the half a second at the starts where no correlation is populated to minimize false positives doing visual search and again these results also quite positive you compare it to similar approaches so in summary there is also the study confirm the smooth moves targeting works well in two different AR scenarios and in fact may be might be particularly suitable for head mounted displays this is useful as the systems already have all the required sensors to support the technique so we conclude with the design and an evaluation of a prototype they use they are for displaying moving controls around the Philips you smart lights and smooth moves for inputs and captured using a normal hololens with no modification we use smart lights we could have used any other smart device that could benefit from an AR interface so the motivation for the the the prototype you can see on the right and the idea of the prototype is really really simple so you see to the moving controls displayed in space in proximity to Deline stick control or in person me to the device a control and these positions are certain using predefined images or real-world objects so you can tag your lights and then these these controls our show and the controls enable the user to turn the lights on or off as you've seen in the video playing or they can access preset light schemes like a cool energy ting lights or like a warm relaxing relaxing nights and these controls also enable the user to change the intensity of the lights and to scroll through different hue colors using continuous head movement so the plus minus and the RGB logo if you just follow them you adjust the intensity or the color and we accrued 10 participants and we we had this lab study that took over at 30 minutes for with each participants and was based on a participatory design technique to elicits in-depth user feedback in general persons responded positively to the technique try it is clever minimalist there were some concerns over selection times or the technique initially required some concentration but some of these participants then got got better at it has quite quite quickly as LOM participants also appreciated simplicity of the interface they can see themselves using it in home or work-related tasks that occupy both hands they also mention accessibility for as a use case it could be quite quite useful as well and it can also some persons also felt that it could be easily integrated with other input modalities such as using course gestures to bring up the movement so you don't have if you don't have room that has all of these movement movements always on so you could just use the course gestures to start turn off the system or using speech to provide more detailed instructions when needed so this is definitely interesting future work that we want to look into and again participants prefer discrete over continuous input reflecting this general idea the smooth moves is more suited for quick and direct interaction and finally the most interesting quotes are this relationship between eyes and head is sort of what makes gaze and these the quotes you see here it's sort of reinforced the idea that gaze the combination of eyes and head motion and for several participants even with the instruction some of their heads these modalities were hard to separate and distinguish as some of the feedbacks that she's doing it with my mind is something it's something we find often when people are describing eye tracking experiences for example so I was wondering if the results from this first study where the data had data was so poor in the natural condition were simply due to the protocol we chose to use the the fact that users know they were using an eye tracker even if we told them just to follow targets without mentioning the eyes or the head the fact that were using an eye tracker could of condition and their behavior or somehow so in conclusion so this we can all have coffee smooth moves by participants and it was viewed as convenient relaxing and well-suited for quick interactions in hands three situations and was also the amande obtrusive in contrast with smooth pursuits input systems based on eye tracking smooth moves can be immediately implemented on a wide range of devices that feature embedded motion sensing such as most AR and VR headsets I'm going to open for questions but I understand if you guys want to rush for coffee I'll I'm gonna say behind a little bit so if anyone wants to chat I will just here while the video plays in the background thank you a yellow fig from MSR how do you avoid false positives because those motion attracts the eye and you tend to follow them even without thinking of selection yeah that's a very good point so this is something we were very careful when we design a purely eye based system in which the the the target information would be at the core and then the controls would be sort of orbiting around the control so that there is this two-step of actually reading the center piece that has no sort of actionable information and then opting to look at the at the at the motion but that's one of the the challenges here is movement attracts the eyes that's how we evolved and how to train people and I think that's reflected in one of the comments where people say it would be really great if I could use course gestures just to bring all this movement up and then I can actually bring it down because I really can't cope with these things even though some people find it calming at some point it is it is it is one of the challenges definitely and I think it's it's these studies always reflect a very narrow experience with you come in lab and the experience this for the first time over an hour and so will be definitely interesting to see if people can cope to to Lauren would sort of learn up to cope with movements a bit better and to avoid triggering things when when they know that it could trigger something quick question so how could you you know I will you can short this you can reduce the selection time so you say that the selection time is around to two seconds right it's a way to make it smaller so this is why when we did the first study we had we looked at different windows sizes to see all sort of a little data you can use to and again one second is quite standard across other similar approaches and it seemed to work well so it's a compromise again 