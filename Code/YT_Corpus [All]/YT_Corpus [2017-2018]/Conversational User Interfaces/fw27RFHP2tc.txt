 [Music] welcome to our fishermen in work mode of interaction I would like to start introducing my colleague jarrod prodeman he's part of the conversational design team and he's also a lead for a big center for the Google assistant and this is my esteemed colleague Adrianna almost she is also product designer on Google assistants user experience team where she is in charge of the design effort for third party multimodal experiences before we dive right in I wanted to share a personal extort a story a story that I feel really captures a perfect real-world example the multimodal experience I was traveling to Asheville North Carolina in October to attend a wedding for two of my friends and I'm originally from West Virginia so North Carolina's not that far away and I had been to ask you one or two times but didn't know the town very well and didn't have a good idea where to get dinner luckily my hotel had a concierge desk and so I talked to the concierge about what I liked what I didn't like we went back and forth he was asking me questions about how far I was willing to go whether I had a car and after he talked to me for a while he did something very interesting he narrowed it down to a few restaurants and then he handed me over the menus to those restaurants I took a moment to peruse those menus looked at the things that we were looking to to kind of eat and I made a decision and I told the the concierge that we had decided to go to the southern restaurant on at which time he actually resumed the dialogue with between me and him and he said well when you want to go how many people are going and he made the reservation for me so in this multimodal interaction the voice part is represented by the dialogue between me and the concierge and the visual part is represented by the menus and to me this kind of epitomizes what a multimodal experience looks like in the wild designing multimodal experiences is an interdisciplinary effort my background is in voice design so I've been designing conversational interfaces for several years now so anything that involves spoken inputs and the things that you hear that's my area of expertise Adriana comes from a more traditional visual interaction design background websites mobile applications and so forth but it takes combining these two efforts along with things like motion design UX writing user research visual design to create a really kokis cohesive monolithic user experience for multi modality but before we go any further let's take a quick step back and talk about how we as human beings experience things in the real world so imagine the last thing you we're watching on the beach you can literally hear the ocean smells - coming to you the water and touches and it's all these senses triggered at you simultaneously it makes a compelling experience by contrast today our technology is mapping to multi screen main senses sight hearing and touch and actually the way that we consume content is basically based on sight and hearing so for efficiency stop we're going to be focusing in these two so in order to have any chance at all of designing a compelling multimodal experience it's an absolute prerequisite for you to know what each modality does well and what it does poorly so for example voice does some things really really extremely well and other things it's just objectable at so we're going to talk a little bit about those and it's really important for you to recognize what those are so you can leverage those liabilities and avoid leverage the strings rather and avoid the liability so let's start with voice so voice is very very good at flattening a menu structure in providing direct access to what you need before I started using Google home I used to think that the mobile my mobile device was the epitome of convenience it's always right here in my pocket and I have really quick access to information and every facet of my life is on this beautiful device but then when I started interacting with Google home across the room in kind of a four field way I was able to ask it across the room what's the score of the West Virginia Gonzaga basketball game it would give me that score immediately compare that to pulling the phone out of my pocket unlocking it going to the sports app going to the college basketball section looking up the score and there's the score when you win you do that with a far-field interaction it makes pulling the phone out of your pocket seem too inconvenient and in many ways that's changed the game so as compelling and exciting as the benefits of voice are there are many there it doesn't come without its drawbacks and one of the biggest drawbacks of using voices and interfaces it's ephemeral nature so users can only retain a certain amount of content a finite amount of content in their short-term memories it's always designers you have to manage that if they have any hope at giving through the interaction at all so for those of you who may have a visual design background or for those of you who have developed websites and mobile applications imagine if the only visual affordance that you've had to present content to your users was a scrolling ticker that goes across the screen and the user can only look remember that text that just evaporated on the edge of the screen to help them get through the interaction this is what voice designers grapple with every day and this is what we as designers have to manage and I'm not even sure if it had a chance to cycle all the way through but that's what the ticker said so obviously content big chunks of content are better presented and it's a lot easier for users to absorb on a static screen like this let me give a really quick example let's say I want to ask for the hours for a restaurant in San Jose I would just ask Google assistant when it's black sheet grass reopen here's one approach to answering that question black sheet Brasserie is open today from 5 o'clock to 9:30 p.m. and tomorrow from 5 o'clock to 10 p.m. on Saturday from 5 o'clock to 10 p.m. on Sunday from 10 a.m. to 2 p.m. and again from 5 o'clock to 9 p.m. and they're closed on Monday all right so while it's usually a good idea to anticipate users needs and over-deliver when you present so much content that the user can't grok any of it it becomes meaningless here's another approach to that black sheep raster is open today from 5 o'clock to 9:30 p.m. here are the hours for the rest of the week so you give the salient the most salient pieces of information up front ie 2 hours for today and then you defer to the screen for more dense content that the user can peruse at their leisure and then finally one of the drawbacks of using voices and interface is when I talk to Google assistant people can hear me that's kind of how talking works right so imagine if the neck the person next to you right here in the middle of Adriana about to say something really profound and amazing decides to pull out their pixel phone and ask Google assistant what to score the Warriors game was on Tuesday that would be really obnoxious right and of course it is but there are some social taboos associated with using voice interactions in a public setting like this and even though the device is capable of it it really limits the range and access of voice interactions in public settings like this by the way if you're really interested in this topic there's an incredible session happening at 3:30 today on stage 5 given by Daniel Padgett he's going to be talking about the right use cases the right voice interactions for your application I highly recommend it by contrast visual interfaces can be as permanent or complex and dynamic as we wanted to be and in fact we just a matter of milliseconds there's a lot of information that can be delivered take for instance it's as simple as a traffic light and we can control the timing of the red screen in order to convey some meaning or it can be as sophisticated as the YouTube app where you can consume content also pull up what is happening what is trending and helpfully where what did you like in not that video there's a lot of things that you can do just in one screen now the custom is how we're going to be combining voice and visual information in a way that is beyond passive consumption there is actually for building into interactive experiences and one approach is to conversation in a natural conversation that we have with people we point things we describe things and this is nice back and forth among human beings they just type in natural and the things like when we start thinking about applications and happening on mobile they take a lot of information lives from day right away but we have to be careful for instance let's take a piece of mobile app for ordering food from here there's a lot of things that I can quickly do but if you remember the last time you call or you walk into the store you actually having a conversation with the person and there's information or just information that deliver back report so it should be suit together they look really different and in fact probably it's more closer where we're saying to be today with chat box and where we're going to experience with the assistance so one important piece of information to take into account is that not because you have an app on your phone that should be your starting point it should be more a conversation where a human where you should be inspired in order to start building your applications that's going to leave in the Google assistance so multimodal interactions is not something new at all and in fact you have experienced in the past you probably tried a karaoke machine there's sound in the background and then you have them use the visual and then you are singing along so there's a lot of things happening and that's the multimodal experience as well we also experience asking Google for questions and then Google will present the search results in the list but where we have to think is like how we're going to be from two modalities in that way that is not overwhelming to the users so Jared and I are very near with the strengths and weaknesses of these modalities oh boy I think a voice and visual but the problem is is that it's easy to fall into the trap that because we depend for one of the surface we know how to design and for all and in fact that was the equation of the sauce it's not that simple and there's a lot of factors that we need to take into consideration so we're going to be talking about these factors that we wanted to take it's a cohesion when we think of all these platforms that we're going to be designing for so we wanted to talk a little bit about these factors that we've identified to help you think about how your experiences will manifest as Google assistant comes up on two more surfaces and these factors are is your user in motion is the device designed to be used if the user is walking running driving or some other situation makes this that makes the screen otherwise inaccessible think of your phone as opposed to your TV the next is the environment is the device designed to be used in private or anywhere and is there a one-to-one relationship between the user and the device like your phone or is it designed to be shared among a group of users like a Google home the next is proximity so are you close enough to tap to quickly tap on the device to interact with it think of a wearable as opposed to something like Google home or your TV that's optimized for far-field interactions the next is audio capability does your device just have a very small mic that can capture speech from a few feet away or how does it have an entire array of mics that is designed to capture speech from across the room and of course visual capability do you have a full economic ly compatible 4d keyboard like a laptop or is it a smaller one like on your phone or are you dealing with something need something even more primitive like a d-pad on it be remote and then finally visual output and that basically just really boils down to screen size so these are the factors that we want you to consider and keep in mind and we're going to go through a few of these on some surfaces where Google assistant has already been deployed and where we anticipate that it may show up so keep in mind that this is kind of a future looking forward looking presentation to give you some guidelines by which to anticipate how to deploy your actions on other surfaces so let's talk about Google home first but since it was such prominently presented in yesterday's keynote and it's on getting a lot of traction in the marketplace so I mentioned before about how voice is really incredible at flattening the menu structure and it's really really convenient but there's another binet up side of speech that I think is really important to present in the context of Google home and it's that speech what I'm doing right now that is the interface you're able to interact with Google home in a way that you've been doing since you were two years old by just speaking to it so there aren't any manuals there's no tutorials there's no learning curve all you have to do is know what kinds of features Google assistant generally supports like whether in in sports and you just ask for those things the way you would ask another person so if we look at some of the factors that I outlined earlier if we looked about whether the users in motion they're not right you put your Google home on a kitchen counter or your nightstand and it's usually planted there for quite a while Google home is deployed in a private setting your home but it's shared among a group of users so it's used somewhat private and you don't have to be close enough to interact with it it's optimized for far-field interactions and if we look at input/output capabilities if you've got really really strong capabilities for audio both input and output but very little on the visual side so we've distilled three really kind of overarching guidelines for how to for assistant in actions on Google home and the first one is don't read listen and what I mean by that is as you build actions and as you develop experiences on Google home you may be pulling content from some online source or something and a lot of these sources are optimized for written content right there they're designed to be read with the eye not the ear so you may think it may be tempting to think that you can just take these sources run them through a text-to-speech engine and voila you have your voice interface it's not that simple let's take an example so in the context of a weather forecast for example this string of text makes perfect sense to you right if I were to translate this string of text into something that's appropriate for spoken language I would say in Mountain View it's sunny with a high of 77 degrees with winds out of the North to Northwest at 10 to 15 miles an hour but listen to how this this is listen to how ridiculous this is when you run it through a text-to-speech engine sun-hi 77s winds NNW at 10 to 15 mph like totally incomprehensible right so just when you get your data source and you run it through just make sure that it's appropriate for spoken content don't we just take a few samples run them through your text-to-speech engine and don't look at the text listen to it to see if you can understand it the next one is avoid information overload I've already kind of lamented about the ephemeral nature of speech but I think it bears repeating just be careful how much content you present to the user so if I asked Google assistant on home what movies are out right now or you want to develop an experience like this this is one approach here's what's playing at your favorite theater alien covenants diary of the wimpy kids to come you champions Wakefield's guardians of the galaxy volume to snatch the fate of the Furious debauch baby Smurfs The Lost village kisses and the circles do any of these sound so not only does it over does it bombard you with 12 movies it forces you to make a decision afterwards so it's kind of a stressful experience compare that to this there are 12 movies playing at your favorite theater here's what's new this week alien covenants Diary of a Wimpy Kid and the communes should I tell you about any of them or keep going so that presents it in much more manageable chunks that's easier for the user to kind of manage and then finally answer the question that sounds like a pretty vague statement so here's what I mean if you have something like a map or the users asking for directions or navigation a map really is efficient at communicating information in this really nice compact rectangular shape right it conveys how far you are from your destination what the fruit the preferred route is what some alternate routes are how what traffic looks like all that kind of stuff so it may be tempting to just punt to the screen on interactions like this so when Google home was first deployed and people were asking for questions about directions and navigation this is how we handled it sorry I don't have a screen so I can't do that for you so pretty disappointing and unhelpful right so we learned our lesson we thought about why people are asking Google home these questions and we kind of distilled a few salient points about what people are asking for right in bay area the quintessential question is to 80 or 101 what's traffic look like how long is it going to take me to get there so we took this pros approach instead the best way to get to work by car is through 87 and 101 north and we'll take about 90 minutes in light traffic so even though an image of a map is much more efficient and can convey a lot more information to the user there's no reason why you can't distill the most salient pieces of information into a neat little verbal summary now let's talk about smart phones the muscle mass is wonderful machines every we bring everywhere in our life and the things like there's no one way in which we use our phones volume would be up one minute and next time is going to be down probably you will be running through a session or thank you will be pairing with a headphone take four dishes for instance this example I used to live in Canada and I was up there the ski slope and we knew that that day was going to be a snowstorm and we needed to know what time we needed to get that or the heels because otherwise we were going to get caught and it was at the time where we wish we could just ask before and and say like me when it snows are going to start and because the last thing we wanted to do is take out our meters when we went up there and then I were born flying down since this time where we wanted to be we could wish that we could use our phone as a little comb they were carrying our pocket there were other class where we're running and navigating pedestrian traffic and you wish you could speak to your phone and quickly get an answer to your questions but in that example the last thing you want to do is have all these information be blurts at you and just want a glass of the screen so the needs to the capacities of these devices are really vast and very broad we can be studying one position and then all the times we are running to places we just zoom in a very private or public context and what is wonderful about them is that we can have like rich interactions to them when you see them more compelling as well is that there's a lot of outputting v4 capabilities at the kid knows we still is exciting them or where we can even use at the camera for only a form of input so based on these we came up with three guidelines that we have been observing while we are drafting our experiences at Google if one mode goes away the other one should take over weekly another example let's say I'm planning a third-party application called genome and what keeps on belfies tells me fact about number is very simple I pull up your phone and then geek mom treats me and therefore excuses excuse me mr. impressin and answer the conversation and also it gives me suggestions of what I could say or I can say my own number but here's kind of look like if I was not really paying attention to the strings howdy this is signal I can tell you faster trivia about almost any numbers like 42 what number would you like to know about so this is when you learn their response when in fact if you pay attention it's a little bit different of it that what you see in the chat bubble we introduce little thin sliced pieces genome as I going to introduce himself one into the conversation because we didn't have enough visual cues in order to you from that we also add little suggestions as far as dialogue to prompt the user of what is what they could say and give them ideas but we didn't put that in the chat bubble in order to not to make it super comfortable in visually complex because you already have the suggestions cheap therapy love the chat bubble so it's very interesting that we optimized for the strongest mode that allowed for both so let's see another example it's festival time let's think that Samuel wanted to go at the beginning of this month you see the warrior and you can ask Ticketmaster through the Google Adsense for for the Knicks places he could go and buy tickets for and it's very simple and normal to see a list you see that in every website and that's the way in which we can consume these types of country quickly because Nellie's we can quickly stand things then the questions like how we're going to be presenting these are to auditory content okay the Golden State Warriors have a few games coming up the next one is against the Jazz on May 2nd which one do you want to be sport city look if you look at their response we say small things like ok so in order to acknowledge that we're having this conversation and making it sound more conversational also we included the first plate that is the next one with the Jazz team that is clapping on may 2nd and this is acknowledged to give a bit of information to the user also to editorial content notices well we didn't listed every single case because it will be overwhelming and we didn't include that in the chat bubble and what is very interesting is that this this type of approach lets you consume the content when you have it insightful modalities or also it's a graceful callback when you when the one is the optional the other so it's very important that we leverage the strength of each mode and avoid redundancy across all these modalities so we've talked about the surfaces that Google assistant already is deployed on right Google home and Android phones but we wanted to talk a little bit about where it's going to show up in just a little bit and I want to start by a surface that was prominently mentioned in yesterday's keynote the TV so if we quickly look at some of the conditions around how the DB TV is deployed it is a static device it's like literally anchored to your wall it's not moving and even though the user may move it's not going to move with it it is designed to be used in kind of a private setting among a group of users much like Google home and it's too far away to touch to interact with it now this is the interesting thing about TV look at where this is on the output scale compared to the input scale very very rich audio and visual output capabilities but quite moderate or limited input capabilities what does that tell us that tells us that the TV is mainly a consumption device and not really an intense interaction device and I think that the the reason that we're kind of bringing this up is as you anticipate your actions coming to life through Google assisted on TV just know that we have to kind of minimize it onto a BAM around on about the bottom third of the screen to not disrupt after active programming going on while the TV is it on being played right so we don't because it's primarily a consumption device people watching programs we want to kind of leverage just that bottom third real estate for that so know that when your experiences come to life on TV that's how they will manifest and because we have a few car people in the audience today wanted to touch a little bit on cars and Google assistant on cars Android auto is an existing solution which does a beautiful job of moving audio audio rich content by way of projection from your phone onto your car's interface but let's talk a little bit about where the car shows up in our multimodal matrix so it is not static right it is moving around it's carrying you around at 80 miles an hour so drivers really shouldn't be looking at the screen a lot and even though cars are out and about in the public setting the cabinet the interior of the car is ostensibly a kind of a private setting and users generally have access to quick touch interactions on our car to adjust their temperature or change a station or something like that but you don't want cart on the driver interacting with things a lot they should be driving in terms of input output capability cars have these really robust stereo systems so there's nothing wrong with just presenting a lot of spoken output and they have pretty decent mics although sometimes recognition is a challenge with all that ambient noise um but here's what's interesting if we overlay Google home on top of the car they they're strikingly similar in terms of where they fall on this matrix so what does that tell us that tells us that Google home is a very voice forward almost voiced only device and that tells us that the par probably should be somewhere in that range as well so a few takeaways as we kind of summarize here if you don't take anything away from today's talk please take this away know the strengths and weaknesses of voice as opposed to visuals leverage what speech does well leverage what the screen does really well and avoid the liabilities that we've talked about optimize for the strongest mode but allow both if the screen does something better point the user to the screen that still let them do it in voice and even though these interactions involve both visual and auditory modes they're usually invoked by a spoken input so make sure that the first bit of content that you're hearing is appropriate for spoken language and one of the overarching principles of using Google assistant the thing that we strive for at Google when it comes to bringing these solutions to life for you is we want them to be efficient um and each turn in a conversational dialogue should be really short and sweet and easy for the user to absorb and consume and help them get through their interaction so we talked a lot today about how to package their response and making sure that all these two modalities are cohesively may a compelling response and also they're they're there to serve in case one is active and we also talked about conversational design and how important is to get inspired from having a conversation with a human before we jump into diving into the design of the application but the work is not over there's a lot at Google we need to get moving and one of them is like once we have other nasty response huh that we're going to be presenting this response automatically that is take into account the context in which the user is a weather if they're running how we represent that information or if they're just athletes sitting in a couch also how do we more and more can do these custom between one type of surfacing to another in a matter that is almost magical and the use of the thing has to ask and even more further like now they're launching our multimodal interactions on the phone how we can make these interactions like leasing carts and all these things that the people can interact with more dynamics to allow for more complex interactions and in a way that is more reaching easy to use so we're very excited that you guys are joining joining this journey and start building your applications with us there's going to be a lot of other talk happening today and tomorrow you're more than welcome to join in case you're interested in the related stuff for the Google assistance and the power you Hawkeye as well that there is a challenge and we can't wait to see all the things that you will be submitting to it thank you very much [Music] [Music] 