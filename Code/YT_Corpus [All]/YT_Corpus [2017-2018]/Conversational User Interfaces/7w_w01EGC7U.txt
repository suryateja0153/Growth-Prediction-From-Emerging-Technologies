 [Music] I'm excited to be here at Google cloud next talking to you about a topic that I'm very excited about speech recognition I hope you're having a great time here at Google Google cloud next so you know why do we even have speech recognition at at Google cloud platform so you know we started developing speech recognition at Google more than a decade ago I think even more than 15 years ago and we did it for our internal uses for for web search and then later for our other apps because we figured that voice will be you know used very prominently in our apps but you know we never intended to offer it to third parties as a commercial service and that changed last year and the reason why that changed was you know we went through a big revolution at Google with machine learning I'm sure you've heard a lot about it in our you know keynotes this year and last year and you know we really wanted to help other companies take advantage of machine learning and benefit the same way that we did and we're doing that in two different ways one way is well helping companies build their own custom machine learning models and you know anyone can use tensorflow it's open source you know don't just download it for free and start using it right away sorry or you know we we started offering in beta in September and yesterday we announced general availability for a cloud machine learning engine which lets anyone use machine learning on the cloud to develop their own models so that's very exciting and we think over time that's gonna enable a lot of new opportunities different use cases for businesses but the reality is that most businesses don't have sort of the expertise or the resources or the data that is needed to develop their own custom and you know in our aspiration to still enable people to take advantage of machine learning we also offer what we call our machine learning API so ready to use models that anyone can can just use as an API as a service that we've already pre trained and in that set we have cloud vision cloud speech cloud jobs cloud translation natural language api and then yesterday we added one more to this list which is our new video intelligence API so so that's how speech sort of fits into the picture we're very very excited about speech it's currently in beta it's about to go VGA very very soon so you know what is so what is so exciting about speech speech recognition people have been doing it for a very long period of time but it's actually an inflection point right now so you know other analysts have said yeah it's it's it's now the biggest revolution in human-computer interactions since the iPhone introduced touch so 10 years ago and you know it's it's dramatically changing the way applications are being developed and and you know will be developed in the future voice is you know faster easier more convenient in many ways than typing or navigating through menus and so we're seeing you know just just in the last year an explosion of voice apps but in the future we're going to see even more of it you know today more than 20% of our Android app searches are already voice you know and we're very excited about that we you know we we want to make it as easy as possible to use all of our services as you know which you're all app developers are out there so in terms of the kind of broader landscape for speech recognition you know what is it good for we kind of see it in in two broad themes one is around human-computer interaction we just talked about so that as things like voice search for its actions messaging dictation you know this is fueling different surfaces like mobile apps web apps connected devices and to a lesser extent also phone directed apps and basically enabling a new way for humans to interact with systems the other kind of big domain is speech analytics and this is fairly new we're just starting to see this come out in a big way now because the technology is starting to be big enough and that could be you know call center analytics video analytics messaging meaning analytics basically taking speech that is already happening in our world today and generating new insights from it so I'm gonna show you later a bunch of different demos more centered around the domain on the left on human-computer interaction and I'm gonna invite our guest speaker Gary who's gonna show you how they implemented a very interesting use case on the right hand side on call center analytics so you know it's gonna be it's gonna be a fun session we're also going to do a kind of a crazy code lab I'm gonna try and you know write an app in less than 10 minutes that will do speech recognition sentiment analysis and translation using Google's API so hopefully it will work we'll see but we're gonna do a little bit of that later okay so you know what is cloud speech AP API and why are we so excited about it so you know it's it's our neural network based speech recognition we've I already mentioned before we were making it available for the first time now and it's universally available across 80 plus languages in variants basically you know I think more more than half of the people that use smartphones today around the world are already covered and and we're aiming for more than that you can use it in two different modes real-time streaming and and batch and I'm going to show you a little bit of both later today and it's accurate in noisy environments and with different axes so when we built our speech recognition we needed it to work for you know people that are walking around in Times Square walking on with their phones and asking their phone what's the weather like in New York while there's a hundred other people right next to them and so it knows how to filter background voices but there's also people in India that are walking the screen and there's a bus passing right next to them and and so it knows how to filter kind of all different kinds of background noises and focus on the on the primary speaker and the last thing here I wanted to emphasize we have a really easy way of customizing speech to your domain so you know if you have a name of a company or a name of a product that's not actually a word in English or in the language that you're trying to work in we make it really easy all you need to do is just tell us what that word is and we automatically add it to the dictionary for you and we can start recognizing it if you're in a specific mode in your application where you're expecting a certain command so if you're building speech recognition for a player and you're playing a movie so you might want to detect stop and pause but then when you're not playing a movie you might want to tech play or other words you can use word and phrase hands to detect exactly the words you need in the right context so we have three different methods for using speech API streaming sync and async streaming allows you to process speech in real time while the user is saying it and so it's it's it's the fastest way of using speech API it works with something called G RPC which is our sort of improved binary version of rest that your PC people are gonna kill me for saying that but it's a it's a it's a faster more efficient protocol and way of sort of accessing remote procedures and allows you to process any audio up to a minute that's included in line in the request sync is a much easier to use method any you know it's a way you can use it on REST API you can even do it with a curl command and it's it can process any audio up to a minute you can either embed the audio in line or use Google Cloud storage and and then we have async which allows you to process larger files it's basically not limited it says up to 80 minutes but you know that can be increased if you get a quota increase and it can also be accessed by rest and process files either inline or a Google Cloud storage so with with async because it's it's not blocking you can quickly process a large number of files and then just check back in to see if they're if they're done processing in terms of standards if you know when I wrote this the slide I was yeah even like I forgot how many different standards we support it's it's it's it's pretty Universal that's something we're very proud of you know there's a large list of programming languages you can use to access cloud speech API and a large list of audio encoding that we support now this is you know this is not an experimental project that someone at Google is running this you know is is a commercial service that we're fully backing and you know really developing investing a lot in so you know wanted to give you guys just a little bit of best practices if you are developing speech recognition apps you know what we recommend and and first first slide is about human-computer interaction so the left-hand side of the use cases that we spoke about earlier so for human-computer interaction you really want to use streaming whenever you can you know when you can't use streaming you can use sync but then there's gonna be a delay and sort of users are waiting for a response so it's it's not recommended you always want to use uncompressed audio if you can and if you have if you want to compress you should use lossless compression because lossy compression like mp3s or things like that they really degrade the accuracy of the recognition you know ideally want to use a high as high of a sampling rate as you can but at least above 16 kilohertz or 16 kilohertz or more you don't want to go below that if you if you want to use human-computer interaction and if you you know if you if you have a choice if you're not running on phone phone lines and whenever you can you want to use speech context to you know to get the best accuracy results that you can now using speech API requires that you specify the language in advance so what we see a lot of app developers do is they detect the language of the OS and that way they know what language the user is going to speak and they provide that to cloud speech API so then there's the other set of use cases which are speech analytics so they're you know you're typically dealing with longer form audio like you know a call could be 5 minutes 10 minutes 15 minutes a movie could be you know one hour two hours three hours so you want to use a sink most probably you you know you want to store those files on Google Cloud storage and then process them in batch and you know if there's if there's multiple speakers you may want to separate the the streams so that you can process speakers separately if you're processing a video you you you might want to use a tool like ffmpeg to strip out the audio and just send the audio to to class speech API and it's very easy to combine it with natural language API and translation soup API which you're going to see in a second okay wanted to give you guys you know a few examples of human-computer interaction because when we're going to spend more time later about the about speech analytic so a czar is a video chat discovery app does anyone here in this room know about a sir okay a few so you know apparently they have 15 billion discovering matches already so what what they do is they they connect people with new people that they don't know and they've done at 15 billion times already yeah every time I hear about this I'm just shocked and most of the time or many times it's people in different countries that speak different languages and you know that it's it's there's a little bit of a language barrier like you can imagine when two people from different countries want to talk to each other so what they do is they use called speech API to transcribe the speech into text and then they use translation API to convert that from one language to another and they allow two people in different countries to talk to each other and they can as they're talking they see on the screen what the other person is saying and they can react to that so it's pretty cool it's really sort of enabling use cases that we're not really of possible maybe even like you know one or two years ago very very exciting you know another example is tribe which is a messaging app they use this video and it allows people you know that don't want to type a message they just want to you know say something to their friends they can record a quick video say whatever they want to say and tribe will show the video Oh enable adding text to that video by transcribing the text and all stand and defining some sort of keywords or alerts or things like that so if you you know if you mention a dinner or a restaurant or something it can sort of react to that and give the user more information about what what they're talking about pretty cool okay so now I want to show you guys a few demos can we switch to the to the demo computer yeah cool so this is always risky speech recognition demos in a large room with echo is like yeah not what it was intended for but what I'm going to show you now I'm just using a regular computer and using the mic from the computer for my computer nothing special no special equipment I'm just going to talk close to the computer and you know let's see let's see what it's able to do so okay that passed by okay I think it got a hundred percent I'm not sure you know usually it gets a hundred percent or close to it not it doesn't always get every single word but I think this time it might have and I don't know if you guys noticed but while I was talking you could see on the top the words were appearing in real time almost in the exact same moment that I said the word so you know it's pretty impressive it's very very fast you know the other thing I would say is interesting here is you can see that I said the word meet twice and even though you pronounced it in exactly the same way it knows how to differentiate and from the context of the sentence it knows when I said you know this mean and when I said that mean so I'm this near works I'm I'm always amazed it's pretty it's pretty cool let me show you something I really love Taylor Swift which is true by the way okay we have a bug in the demo here but you can see this demo uses a couple different elements so one is it combines speech API with natural language API so you can see the overall sentiment is positive so it detected that and it also extracted the entity Taylor Swift and it brought up a card it knows it's a singer-songwriter it has a little bit of a description from Wikipedia and you can click on the link but the other thing I wanted to show you is you know our speech recognition was built originally for for our search engine and so you know it's built to recognize named entities really really well you know we get searches for every single entity you could imagine and a lot of those searches are voice searches and so we developed our speech recognition in a way that it knows how to handle that and when you when you mention entities with speech we will detect them even if it's not a word in English and we will format them so you can see it's capitalized and it knows how to recognize the entity so the third and last demo so this is a demo that sort of tries to teach you how to speak a new language with with speech API and and with with translation API so you know it's going to detect that I'm in in an airport and about to fly to Tokyo and it's gonna try and teach you just nice did you guys hear that call meet snap okay it wants it wants me the second is your Scylla I'll try and get this wrong and let's see let's see what happens konnichiwa konnichiwa okay that way that wasn't wrong enough let me again give Eska let me try this one again key Sokka organ key so apparently what I said in Japanese is King motto he may soccer so actually recognized my language in Japanese and then translated it back into English so so pretty cool okay so so you know these are sort of pre-prepared demos and you'll say okay that's fine but like you know how easy is it really to use so what I'm gonna show you guys you know if you just go to cloud google.com slash speech you click on view documentation and you click on our samples you can see samples for all the languages that I mentioned earlier I'm gonna go to the Python sample and you know you can see there's basically a Python example here and you know I've loaded it onto onto my computer and I recorded this this audio file out I'll play it for you guys I'm really loving this session so far it's pretty awesome I especially like all the code examples so hopefully that's represented or some of you if not then maybe a little bit later and let's try and you know i pre downloaded this this demo let's try can you guys see my my terminal maybe make it bigger better now okay I'm gonna try and transcribe it and and let's see what's happening so it's it's taking this file that I recorded in a in a library called code and it's it's sending it to the cloud and getting a result back so you can see really loving the session so far it's pretty awesome I especially like all the core examples so it kind of got the the gist of it but now you know what happens let's say we wanted to add sentiment analysis to that how do we do that so I'm going to go to our natural language page and you know if you just click how to analyze sentiment and go to Python there's a code segment here I'm just going to copy it and add it here to our Python script and I can just put this up front here and that's also import from google doc cloud import language and let me fix the indentation and now we want to feed it this text alternative transcript which is what comes back from the speech API so let's feed that here and then we do the sentiment analysis and then let's print this cor I don't think we need the magnitude so you can just find that let me save let's run it and let's see if it actually works so this is um it's it's okay so it worked - got the transcript and I got the sentiment score so what do you guys think about that that was pretty easy [Applause] okay so let's make it more complicated now and let's also add translation and let's go back to my editor and let's paste it in here put the client up front and I'll import the library from Google the cloud import translate and this here and now let's feed it instead of text let's give it this alternative of transcript and instead of target language let's just give it a es for Spanish and now we don't really need the text because we already printed it let's print the translation okay now let's run it let's see what happens now when we run it with the combination of so what it's what it should be doing now is showing us the transcript then showing the score and then showing the translation so that's it we have it it worked in Spanish and and with sentiment analysis let's try one more thing so I wasn't sure if you guys would really like this so I also recorded a different option let's play it I really hate this session so far it's very boring I can't wait until it's over okay so let's try and transcribe that with speech recognition and also analyze sentiment and translate that and let's see if that's gonna work so that's it it worked [Applause] so it's it's pretty awesome and you know this is this is our beta version as I mentioned we're gonna have our GA version coming out soon and it's going to be it's going to be even better you know I can't go into too much details but we're very excited about it so with that let me welcome onto the stage Gary from interactive tell thank you Dan if we can go back to the slides please interactive tell provides services that capture and analyze business communications a lot of the data we collect is audio from the recording of phone calls we have clients that accumulate a quarter of a million minutes and one month of conversations to put that in perspective it would take one person a hundred and seventy three days of continuous listening to review every call now normally those recordings would just sit there until something happened that caused a particular conversation to come under scrutiny when that conversation went under scrutiny it may be because of a disgruntled customer or even a lawsuit services like the ones that I'll show you today transfer information from a phone call to CRM companies to lead and contact management systems and even to managers in the form of a text message when something goes wrong we're able to provide this value by translating speech into text using cloud speech API and provide value to our clients since the new services we were able to provide we've been able to add 75 new accounts in 30 days and over 2,100 new opportunities just from these new services let's talk a little bit about the architecture that we use from our telephony infrastructure we streamed speech in real time the Google's cloud speech API we take the results we got from Google and then pass them through our analysis engine which extracts sentiment keywords and key phrases all or which are used to construct an understanding of what's happening on a call we take our findings and then publish them to CRM companies our data visualization tools and other subscribers they take that information and then pass it along to managers who can intervene if necessary and analyze the data to see what's happening if we can go to the laptop now I'm going to share with you a video of exactly what happens in a dealership when this technology is in use thank you for calling ABC motors this is Jack how may I help you hi I'm calling about a car I saw advertised on your website it's the 2017 blue sedan that was fired by John Davis yes we've received several calls on that car it's really one of a kind let me take down your number in case we get disconnected I'll call you back I don't mean to be rude but I don't worried about my number I'm just interested in car can you tell me if it's still available let me go put my hands on to be sure please hold I also came up with a nice demonstration for you guys which is gonna require some participation sure is on your part so our specialty is new business communications capturing an analysis so we thought it would be pretty cool for this demo to have members of the audience call a number which represents the telephony portion and we tie in to Google Cloud speech API will ask you where you're from you mentioned your country city state or what have you and we'll plot that on the map so the number that's on the screen 201 340 800 for any willing participant out there if you pick up your cell phone make that call you'll be prompted for to say where you're from and hopefully we'll get enough participation that we'll start getting some blips up there on the map so if you will please to make some calls that's pretty cool so this is using G RPC to stream real-time while you guys are on the phone and you'll notice a lot of names are popping up there we're doing reverse phone to pen to also attach additional value what's also interesting about this particular mashup is if you don't say a city or state and you say a particular company if that company is you know has a presence on Google Maps that would also show a dot there we'll leave this up for probably for the rest of the day so if anybody was savvy enough to copy the URL the phone number this demo will be active for the remainder of the day I'm gonna transition into the code behind behind this particular demo it's very similar to what Dan showed earlier [Music] effectively you know this is the left-knee code where we're playing greetings recording the file locally and the transcribe function is very transcribed file function is very similar to the one that Dan showed earlier of course we've enabled the profanity filter to make the results less interesting if we can go back to the slides please we did have some technical challenges when implementing cloud speech API the first of which was timestamps and I don't know if any of you were here are part of the user groups but that's a topic that came up quite frequently many phone calls were 15 minutes or longer in length and an important part of our user experience is allowing a person to click on a key word or phrase and skip ahead to a phone conversation to the point at which that word or phrase was spoken currently cloud speech API does not provide timestamps when we did devise a way to implement them that works for us the way we did this is is as follows we pre process audio to detect silence in between words phrases we then use that detected silence to segment the file and submit those segments individually to cloud API the responses we get back we associate with the segment and the interval at which we detect it and it allows us to know that for instance segment a which comprised the first eight tenths of the of the conversation was the word Google we repeat that process for each word or phrase and the recording and enables us to construct a transcription of exactly what happened on the call and the time in which it occurred another issue we ran into a speaker Dyer ization as we're you know telephony company we have two sides of the conversation cloud speech wasn't really designed initially for this type of conversation so we record our files in Styria which gives us a caller on one channel and the colleague on the other now if we were to take that audio file and separate them into two channels we'd end up with one minute of audio one minute of phone call and two minutes of audio one minute for the left side one minute for the right effectively doubling our cost for processing a stereo file in order to mitigate that we expanded our pre-processing algorithm to detect large stands of silence we then remove that silence from the audio and submit the segment's individually keeping track of whether that was submitted for the left side or the right side of the converse of conversation we then reconstruct the transcription using the method that I showed you earlier associating segments with time intervals but also associating them with the caller or collie our experience with loud speech API has been pretty awesome I learned to turn yesterday which is dark data and for us recordings and probably a lot of you that may have audio that that happens today and recording for a Matt it's just dark data you don't know what's going on there well using cloud speech API we were able to shed light on our recordings and I like to thank you for allowing me to share that experience with you if you can stay on stage I actually have a question for you so could you talk a little bit about so once you transcribe these phone calls like for example that scenario that you showed in the video where customer is calling about a car that's not in stock sort of what do you guys do in terms of that analytics in terms of the workflow what how do you provide value to car dealerships that's a great question so example is you see a car on the website you call in and for whatever reason that car is not available this particular dealership is using our system so we know exactly what's happening on the call and the automotive space they have a notion of everybody that calls in you want to get their contact information their name their number product of interests so that if that product does become available because it's a very fluid market you have a point of contact to reach the reach back out to them so we identified key factors number one the call was for sales number two there was a product that they were interested in and number three there was no contact information that was collected in our space we call that a missed opportunity and which becomes actionable because we'll send that to a sales manager in the form of a text message and or an email based on his settings to let him know that this particular opportunity may have been missed what ends up happening in those cases large dealerships have this instance occurring you know dozens of times a week and it really impacts their bottom line to be able to have a second chance at selling that particular customer so what if what have been reactions for dealerships that have tried the new system well the reaction has been overwhelmingly positive in fact large dealer groups are that we've demoed and and had an opportunity to to show our system to end up pushing it out throughout their whole group I don't know if I'm allowed to mention customer names or I mean if you can just say you're like in terms of business value or like like are they actually making more sales or is this just like a nice-to-have well not only are they making more sales but it's causing a shift in the mentality at car dealerships because now everyone in the dealership is being held accountable it's one thing to have a recording or monitoring solution in place and people know it's there but that's reactive meaning the only time that that information is ever going to be leveraged is if there's a situation that calls that into question you know whereas using cloud speech we're able to mine these these conversations for actionable intelligence to allow us to empower dealers to be proactive and provide a higher level of customer service and it's really becoming disruptive in our industry great awesome you cannot you can stay here I just have two more slides and then we can do Q&A so I just want to give you guys a plug for two more sessions that are that are happening you know a little bit after this one in about 30 40 minutes one is sensitive data management so you get all this text coming back from speech API a lot of it is going to contain PII you know a lot of customers have asked us what can we do about that and so if you note if you saw the keynote today they spoke about a new product that we just announced this morning called data loss prevention we're going to talk more about that product in this session room 30 25 20 and then for those of you that are interested in building conversational applications and connecting to Google assistant you know that there's a lot of overlap there with speech recognition there's there's a session that's gonna dive deep into that right in this room actually at 5:20 so those are their two upcoming sessions with that let's go do a Q&A [Music] you 