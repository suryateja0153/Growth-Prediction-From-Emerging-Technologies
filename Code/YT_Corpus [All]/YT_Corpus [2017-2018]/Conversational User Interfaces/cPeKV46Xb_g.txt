 Okay, good morning everyone. So as part of the workshop, there's going to be these talks every day. Different levels of detail. Yesterday you learned how to use Azure, which is pretty important. Here what we're gonna do is, we're gonna talk over two lectures this morning about Spoken Dialog Systems and Dialog Systems. This is mostly tutorial stuff, there is nothing, it's not research numbers coming out of this. This is to give you a background about what actual dialog systems are like and the problems that we have with it. It is based on some of the lectures that I did at CMU, and I've tried to fit them neatly into two sets. They might not, so I might overlap a bit between them, but we'll do that. Really, initially, the first part is we're gonna talk about what we call task oriented dialogs that have got a purpose. And they're usually hand crafted, and they're usually pretty successful. The second part we're gonna talk about more data driven, more modern techniques. And the issues that they have, cuz it's far from being solved. We'll also be talking about some, the spoken personal assistants, so things like Cortana and Siri. And so you get to understand some of these. Please feel free to ask questions, okay? You've all got pretty diverse backgrounds, and that's what makes this workshop interesting. So I'm expecting that not all of you will know all the terms and all of the things I'm talking about. So ask questions about it, and I'll try to explain it to you. So, initially when people think about dialog systems they maybe think that what you do is you take a speech recognition system. And then you get the output of it, and you do something with it. And then you stuff the output, or whatever you do into a text-to-speech system, and there you have a dialog system. So you do that, and then you discover that doesn't work, okay? Because actually, human interaction is much more complex than just speech in speech out. There are things that are going on in a dialog system that are important to communication, that are not just you get some input. You work out what it is. Now sometimes that works, but in general that's not how human communication actually works, or even human machine communication. And so we want to be able to do that. So there's different styles of interaction. And what people have done in the computational field is they've found styles, which are good for particular problems and then they've built walls. And then they've, That too hard to do. So what you get, and this is a very common thing in research is that you find a bit you can do and you try to do that. And then you ignore all the other bits and that's definitely what we've been doing in spoken dialogue systems. And later on I'll talk about some of the issues of things that have been forgotten about. For the most part, spoken dialogue systems are systems that you speak to, okay? And that are used in the planet are very carefully handcrafted systems. There's training from data in them, but most of the time it's humans have actually constructed them. And there's usually a task design for that. So to get your bank account number, and find out how much money in your account or find out whether in a particular place or stocks or whatever is going to be, that is very targeted information. And the so called Mixed-Initiative Systems where either side can direct the conversation, are relatively rare. And they only really exist in the research space. Nobody's really deploying them, because they're hard to get right. And the problem is that the end user doesn't really care about the system at all, we'll talk about that. And they just want the solution, and they don't necessarily want it to be nice. Because for the most part, people will use the system once and rarely use it again. But there are a number of systems out there that are called classification systems. And they're often named after one of the first systems that did this, the so called how may I help you system that AT and T did. It's very common use of the technology, and so what you have is the as you imagine that you're calling up some big company and you wanna get connected to the right person in the company. And usually there's a receptionist that listens to the question to decide which group you're going to be connected to, and the idea is well we'll replace that with a machine. What we'll do is, we'll do speech recognition on what the person says and then we'll redirect them in the right place. So AT and the big US telephone company replaced their main number with this, so when people would call them up and say things like. My mother told me I had to go and talk to my aunt because it is our birthday tomorrow, and the machine doesn't care about your aunt actually. But the people were saying these things and so, my mother said I have to talk to my aunt and she gave me her number. But she only gave me seven digits and didn't tell me what the area code is. She lives in Miami, so I need to know what the area code is for Miami. So all this thing about your mother and her birthday, and aunt and stuff is irrelevant, okay, to the machine, okay? It's probably relevant to you and definitely relevant to your mother. But you just ignore that and you go, it's actually trying to get area code for Miami. And therefore, the system should actually do that. So the system looks at all of the words that are in there and tries to find out which things are actually relevant to things it can do and then try to put you in the right class. And it might be that it's sort of confusing and then it could ask you a question about, did you mean billing or did you mean information? And so it can actually sort out between which of these systems it would actually be. And through conversation on machine-human interaction, we'll come back to that at the end of today's talks because it's something that we're just so far away from. And people like to think that we're now beginning to use speech communicate with machines and we are more and more. But we're still far away from the source of communication that humans do to humans, and so we want to be able to look at that. So I'm gonna look at some simple dialogues. I'm gonna actually talk about vice XML, which is a standardized way of writing systems. It's not in itself, is may be a little archaic, but it gives you a good idea of what this is. We're not gonna look at Olympus Systems which were developed at CMU, we're not gonna do that, we'll do a little bit of real world deployment as well. Some of the things that we're doing are information giving. So the system is a database and its gonna present you with the information be that bus times, train times, weather, things where there is a database and they're trying to replace. Having to look it up. Now it might be searching on the web is easier, but it's not always the case that you can actually do that, and more and more people are using speech to be able to that. Historically, people didn't have computers, and so the telephone was the only way that they'd be able to get access to information, and so it was much more common to do that. And as little as five years ago. And driving directions is now a thing about how to get from A to B. We all know that in what you do is you get in the car and then you sit there for three hours, try to gradually move there in one block. And then you get out and walk cuz it's gonna be faster. But in other countries, they actually can drive between places without being stuck in traffic. And use information, so navigation of more complex pieces of information. The information navigator. So, in the first one, what's happening is the data that's given is sort of all known in advance. I mean it's gonna vary in some sense but in some sense, it's in a database and you know what it actually is, you know how it's gonna be spoken. You can actually tune your system for it. And the second one is a little bit more general. So the information is coming externally and being presented to the user by speech. So imagine that you actually had a mail reader that was talking to you all the time. And what you discover very quickly, is your email is full of useless information that you really don't want to hear. And cuz normally, what you do when you look at your email, you only look at the important line and you ignore everything else. And machines are very bad at doing that. And imagine that you go and search the web for something and you go and get the reply back from Bing or Google, okay? Imagine if you wanted a speech interface to that so you put in Justin Bieber, and you get 15,000, billion pages about Justin Bieber. And then it starts reading them to you. I mean, that would be so useless, okay? So how would you even present information if you were trying to do web search through speech? And that's an interesting problem, because ultimately we may want to do that. Was that a question? And it's gonna be an interesting problem about how to be able to get information, which normally you're very good at seeing and choosing which parts to actually click on and scan over, but speech is actually not a good way of doing that. And providing personalities, game characters in video games, video games are incredibly important to most of our lives nowadays. And having a character that you interact with, that you speak with and don't just have fixed responses. That's a surprisingly big area, and growing a whole form of entertainment. Disney already has a research group on the CNU campus. And one of the things that they're investigating is how to be able to have interactive characters so that when you go to Disneyland or when you're on the web. You can interact with your famous Disney character and have personal conversations with it. Toys and robots are always an important thing. One of my ex-students is now kidding about building a dialogue system for the Barbie doll. He probably never thought when he was doing research, of seeing that his ultimate goal was to be able to engage with 8 year old girls about whatever Barbie talks about, that's a good thing. What do 8 year old girls talk about to machines? But he knows now. Speech to speech translation is one of the other things that we've been doing. It's a good use of the technology. Microsoft has actually got deployment and Skype, so it allows you to actually interact with people with other languages. It does support Hindi, but I don't know anybody who's actually used it yet. Have you actually used it?  Not yet.  Yeah, right. I mean, I've seen it demoed and I've seen people actually use it for non-demo purposes for Chinese. But it's still there, so you gotta think about a situation. So for you guys it doesn't count, even if you don't speak Hindi, you know it better than most other people and probably than the machine. But imagine I'm trying to communicate with somebody, how often am I in a situation where I want to communicate with somebody in Hindi, where that person doesn't speak any English? So none of the students are gonna be in that situation, they are all gonna be able to speak English at some level. So that means it's probably talking to my students grandparents, they might not speak English, but I don't talk to their grandparents, maybe at their graduation, but I know enough to greet them and that's sufficient. But it's an interesting thing, what situations are you going to be in? Where you're really genuinely going to talk to somebody in a language you don't know. Yeah, when you travel the world and you go to China and you go to a hotel and you're going to be able to interact, then maybe you would actually want to do that. Although, most hotels have sufficient English around the world to be able to do that. Yeah, okay, if you're somewhere in Western China, it might mean that you want to have that, but they don't actually speak Mandarin in Western China, but. So I talked about these three basic types. System initiative is a form-filling paradigm, it's the most successful one. It's the thing that actually works. It's the thing that people do, everybody hates it. Everybody hates talking to a machine that's just trying to get the three pieces of information out of you, in order to be able to make some system work. But it's very successful. It has task completion, and therefore, if that's your goal, that's what you're gonna do. Nobody is gonna enjoy talking to it, but you might prefer talking to it than having to wait 45 minutes, before they can get a human online to actually speak to. But it depends what the problem is. I mean, If it's a simple problem, it might be reasonable. But if it's a complex problem, no, you need to talk to a human to be able to do that. Mix initiative, either side can initiate what's actually going on. These systems are still relatively rare, people get confused because they don't know what to say. It's one of the biggest issues in the dialogue system. People don't know what to say and it's very hard to teach them. Because, as you know, when you get a new computer, the first thing you do is read the manual? No, no, nobody ever does that, okay? The manual is this thing, it's sole purpose in life is maybe to hold up the monitor to a different level. But its not actually for reading, and so, how do you actually teach people to do things, if they don't know how to do it, especially with speech. How are they gonna do that? And I'll come back to that issue, cuz it's actually very important, if you offer these services, how can people learn about it? And it's really hard to find ways to educate people in better ways of doing things. They guess and they might get it wrong, okay? And there might be very a easy way to do it. But being able to tell them that is something that's, we talked about classification. And so, one of the system initiated systems that I'm gonna keep coming back to a number of a times, is a system we built at CMU, back in 2004, 2005. It gives you the time of the next bus in Pittsburgh, okay? Unlike most American cities, in Pittsburgh, we actually have a bus service. Many other American cities don't have a bus service, but we actually have a bus service. And there's a schedule, and sometimes they're on time, they're not always on time, but they're often on time. And so, we built the system that if you dial the local bus company, it would automatically come thru to CMU, and then you could find out the time for the next bus. Let's see if this actually works.  Welcome to the BMU Let's Go Bus Information System. To get help at any time just say help or press zero. What can I do for you?  61A.  The 61A, where do you want to leave from?  Dynamo Way.  Leaving from Dynamo Way.  What is your destination?  North Braddock.  Where are you going?  North Braddock.  Going to North Braddock, when do you want to travel? Next bus.  The next bus. Just a minute. I'll look that up. The next 61 A leaves Dynamo Way at beach, at 9:26 PM.  So we don't care about the actual bus. The male voice is a synthesizer. In fact that's Brian who knows how Barbie speaks, it says voice. And the human is random human in Pittsburgh and calling to be able to get the time of the next bus. And that works pretty well. I'm fairly sure she's called before. She knows how to use the system. She's doing things pretty well. There's one mistake, it doesn't understand and asks for a time, but now everybody knows how to use the system. And so I've been teaching people to do it. So here is a call that is not a successful but the system works out that it's not working very well. And therefore tries to help the person through and succeeds in the end.  Welcome to the CMU Let's Go Bus Information system. To get help at anytime, just say Help or press 0.  What can I do for you?  I'd like to find out the city 1C schedule route.  For example, you can say, when is the next 28X from downtown to the airport, or I'd like to go from the Keys Port to Homestead tomorrow at 10 AM.  When is the next 54C from Oakland?  The 54C, is this correct?  Yes.  Okay, leaving from Oakland, did I get that right?  Yes.  All right, I think you'd want the next bus, am I right?  Yes.  Okay, where do you wanna go?  To Center Avenue. Going to Center Avenue, did I get that right?  Yes.  All right, just a minute, let me check that for you.  So she didn't know what to say first. The system detected she didn't know what to say. The system gives explicit examples. You can say I wanna go from from X to Y this afternoon. I want to get the next best studio that gives very explicit examples. And the person then echoes those back, with the particular stops in it. It's very important to show people what to say rather than just say I don't understand cuz then people don't know how to speak with the system. So giving very explicit examples on how to be able to correct that. Also what the system does is because it know the person doesn't really know how to use the system it then does explicit confirmation. So when the person does actually get something misunderstood it then says so you're leaving from Center Avenue, and you're going Downtown. And it asks for explicit confirmation to be able to make sure that the call is actually correct. And in general, people are very happy about that. Externally when you listen to it, you think, my god that's just incredibly slow. How would anybody like it? But when you're in the conversation, you're getting feedback that know it's working, so you know that you're actually getting somewhere, and people are relatively happy with that. And that's an important thing, to design your system so that people are content that the system is actually working and moving forward. And giving you the answer that you expect and understanding you. Cuz if you don't, if the system says okay. Like okay for what? Okay you understood what I said? Or okay you're just gonna give me some random bus time that's got nothing to do with the actual busses? Which may or may not make you happy, or not. And so the system ran for ten years, from about 2005 to 2015, did about 100 calls a day approximately. And so there's 120,000 dialogs. When we started, we were about 40% accurate, which isn't very good. And in the end we were 92 point something percent accurate at the end, about how we would actually give bus times at the end which is pretty good, okay? We did a lot of research in the system and we used it as a research platform in order to be able to improve things. And so it was pretty good having real users to do that and I'll come back to one of the ways that we used it as a challenge, as a research challenge later on. And some of the little background about it, sometimes people called us for no reason at all, so there was no answer. They just call us and talked to us, and we still don't know why they did that. Maybe they had nothing else to do. And we also got drunk people in the middle of the night and we would occupy their time for 20 minutes, half an hour, or something talking about nothing. But maybe we stop them from driving home, so that's okay. But these will be candidates' failure, cuz we didn't give them a bus. But when you listen to the call, you think, I don't really know what the system's supposed to do. We would actually misrecognize what somebody said and give people the wrong bus, okay? It's relatively rare, but we would do it and even when you explicitly said, somebody said something and the system would say back, Do you mean x? And they said yes, and no, they didn't mean x. It's a different part of town and it's not exactly clear what happened there. But in general, we did pretty well in being able to get systems. Mixed initiative is much more like a human conversation. Human conversation goes back and forward, either side can initiate sub tasks when they're actually speaking. But for the most part, humans don't do this. It's a much more realistic dialog and if you can do it, and for a number of research systems, we could actually do this. And you can do much more complex tasks. But it gets really confusing for both the machine and the human. We don't know what's been confirmed, we don't know if they've succeeded in the sub task or not. And so unless they're very familiar with what's going on, it's really hard to actually have general people use it for the first time. One of the things that's quite interesting when you use a dialogue system is the first time you use it, it has to be successful, otherwise you'll never use it again. So imagine you get your brand new fancy iPhone, and it says we've got a new version of Siri, it's the best thing ever, you can talk to it. And so you talk to it and it fails to recognize what you say, you're never gonna use it again, okay. So it's really, really, really important that the first time people use these systems, they're successful. Therefore, you wanna be as conservative as possible when you're actually building the system. And make sure the first time actually works, cuz you can improve things afterwards. But if somebody decides this doesn't work, they're never gonna use it again. And that's something you gotta be careful with a mixed initiative system, it's just too open. People don't know what to say, they don't know how to communicate. They don't get a result, then they never use it again. It's fine if they sort of have to be trained to use it and they're using it a lot, okay. But most people are random first time users and they continue to use it afterwards. We'll skip that one, yeah?  So is mixed intiative a domain specific?  No, not really. So it might be something about booking lots of things like hotels, cars, and hotels. And you can then talk about in any way and jump about to do that, so it's not really a domain specific thing. It's still task-oriented. But it's that either side can initiate things. And possibly if you think about, for the most part, these systems are system, human, system, human, system, human. But if you imagine, the human can ask things or the system can give you a little ask from them. That's more like a mixed initiative system, yeah?  What type of?  What type of?  So you can still get the same thing. It can still be lots of little sub things. And you can sort of argue that Siri, Cortana are sort of towards mixed initiative. But they're not, cuz they almost never give you alerts, okay? But they're almost always, human has to start it with hey, Siri or okay, Google before they actually get it. And the system itself never initiates the conversation, okay? Maybe that's annoying, I don't know. We talked about classification things, the area code of Miami. This is surprisingly successful. And it solves a major problem that lots of people have. Cuz if they've gotta have a human to do it, it's very expensive. And often, it's a relatively simple task. And 90% of the queries that come in can actually be dealt with by machine and they can be faster. And the 10% go to the remaining human afterwards. For a long time, probably until the mid 2000s, almost everything that happened in spoken dialogue systems was for the telephone. This was partly because the big companies involved made sure that nobody else did anything else, cuz they didn't wanna do that. And because they had a big monopoly and they were making money out of it. And so there was very little work on actual robot interaction. Things with human faces, the non-player characters in video games, there were very little. And then as people began to get more smartphones, they realized there's a bigger market here, there's a bigger chance to do things. Then things started drifting away from a standard telephone about ten years ago. But it's still the case that telephone is important. There is still a large set of the population who think that the purpose of a smartphone is to actually make telephone calls, imagine that, okay? And they do that. And speech to speech translation also began. I mean, it helps that speech recognition got a lot better in the last ten years. And really a lot of the early work was on how to deal with bad recognition. While really speech recognition from about 2005 and definitely from about 2011 when deep neural nets started happening. And people started training on huge amounts on data. That just wasn't possible ten years ago. For the most part, speech recognition for a large set of the population, maybe not those with accents, is pretty good, okay? It works most of the time. Doesn't work all of the time, but it works most of the time. And therefore becomes much more practical to have these systems than before. Some talk about some of the other systems that we worked on. So we had a system called Team Talk. And you would have a hoard of robots and, well, actually about four robots, I don't know if that's big enough to be a hoard, it's not quite an army capable of taking over the world. If you want to have your robot army taking over the world, you'll probably need more than four robots. But anyway, we had four robots and you could control them. The scenario was a rescue operation. So you had a bunch of robots that could go into building and you could send them to people's offices or through the building and try to find out what's blocked, if there are any injured people, okay? Rather than sending humans into the building to do that. Now, what became important then is you're talking to multiple agents, okay? So you have to know which agent is responding, cuz if the agent says, okay, you have to know which one it is, robot a, b, c, or d that's actually speaking. And so, when we change of what they are so that we could actually identify. The robots would sometimes identify themselves. The robots also communicated with each other, and you would overhear what was happening so that you had an idea. So imagine you have a team of people, and they're doing things, and they're talking over a two-way radio. You can hear what's going on, people are identifying themselves, and you can actually follow what's going on. You can give commands to specific robots, okay? Or even give commands for every robot, okay? And so, for Team Talk, what we were doing which was funded by Boeing, actually. We built these systems with multiple robots in order to be able to communicate. I'm gonna skip that one. None of these really do real communication like humans. Humans are much better at communication. I'm gonna come back to this at the end. We make lots of verbal noises that are not to do with speech, not to do with actual words and speech. We laugh. We do hesitations. We do false starts. Part of how we communicate. If you remove it, it then becomes a little weird. It becomes unnatural. We do eye gaze, so we know what's happening by how the person is actually looking. And you know when the person is listening to whether they're looking back at you or not. They don't look back all the time, cuz if you have a robot that stares at you all time, it's gonna be uncomfortable. But it's got a, but if it never stares at you, then you think, this is a really weird robot. It's some sort of that's not willing to engage in conversation. You've gotta get timing right, which is incredibly hard. Human's timing is really quite subtle. We actually know when to speak, when the other person's gonna stop. We're probably doing things, definitely sub second. We're often speaking before the other person stops, even as much as a few hundred milliseconds. That's incredibly hard for robots to do. Back-channeling, you have to say, okay, gotcha at the right point. If you don't, people get confused. I lived in Japan for years, I speak Japanese. And in Japanese, back-channeling is a necessary part of communication. So in English, it's sort of you're supposed to do it, but you don't have to do it all the time. But in Japanese you must do it, otherwise you're not communicating. And probably, even if you don't know Japanese, you're probably aware of what the Japanese back channels are because they get into the international thing. So back-channels in Japanese are mostly things like hi, okay? Which sorta means yeah. It means and so when somebody is speaking Japanese, the other person is replying hi, hi, hi at appropriate pause. Now if you don't speak Japanese very well, okay, you can still learn to do the back-channeling. So you can go in a whole conversation where your professor comes up to you and tells you to do something, and you as a poor Scottish research student at a Japanese university replies back, hi, hi, at all the right places cuz that's quite easy to do. But I don't have a clue what the person actually asked me to do, okay? But at least I put the intonation all in the right place. Eventually you get better. Movement, okay? Something that's actually still all the time a little frightening. It should move a little bit, but not too much, okay? So that's something that we do, we expect a certain amount of animation, but not too much animation. So these things are there. And we'll talk more about in the next lecture, in the second part on talking about nothing, yeah. At CMU, the entrance to one of the buildings, we have a roboreceptionist. This is actually an older one. It doesn't currently have speech recognition as the keyboard, you can type it, you can talk to it about anything come back to you. And has facial movement, has a whole significant background character, it mostly talks about nothing but if you look at the logs of what people say for the most part people do talk about nothing. You can ask specific things about the way people are, about all kinds of things and events, but most people don't actually do that. The character actually is a background and has been written there and if you ask her what she really wants to do, she'll eventually tell you that she wants to be a singer. And if you ask her about singing, you've got to ask her three times and she'll then actually sing for you. And then you discover why she's a receptionist and not a singer, cuz she's not [INAUDIBLE]. Don't tell her, because she gets upset if you know that, but if you're ever there, you should probably try to do that. Okay so, we're now gonna move into a little bit more detail about spoken dialog systems. I'm gonna talk about the parts of the system, okay, and what's actually going on. It's not just speech recognition in text and speech. You actually have to deal with a number of stages in order to be able to understand how dialogues gonna happen and we're gonna talk a little bit of that. There is speech recognition, okay, and they're certain issues that happened in dialog system that's different than when you're dealing with just speech recognition and isolation. We talk about parsing, so this is natural language understanding, it's often called NLU, where you get the output of the ASR engine and you've gotta do something with it. You've gotta understand what it is. It's just a string of arbitrary talk. How do you do know that this is referring to this particular place or this particular person when there's multiple ways to refer to the same person or place? Then once you have that information, you start doing a dialog system proper about interpreting what's going on. Knowing what you're gonna do next, deciding whether you're gonna confirm things, deciding whether you're gonna introduce new things, deciding whether you're gonna ignore them or not. And then once you get any information, I don't know say it's some bus information. Look up some SQL database, you're gonna get some table back and finds it, and somehow you have to be able to get that information back to the user. You can't just read out the SQL output of the query, because you're gonna get some funny thing with brackets and backslashes. And stuff that's just going to be nonsense and so what you have to do is you have to convert it into something that's speakable. Into some string of words that are reasonable to actually be spoken out. So that is an actual generation program problem. It's got to be done at an appropriate level. Appropriate level of formality, friendliness, the right language, the right choice of words that you are actually doing it. And then you got the text to speech engine again where the synthesis we are actually making the way form out of it and playing it which should be understandable. Okay, so we have a model like this. We have a recognizer, we have a synthesizer with the human standing at this end. We have the parcel. We're going into a dialogue manager which may interact with the web or the database, whatever is they're actually going to do. And then we've got the language generation that comes back from that, goes back to the synthesizer. The reason that the recognizer and the synthesizer needs to be linked is that actually, people interrupt machines all the time. Now that makes life hard. Now if it is actually a robot and it does have a laser weapon, it can probably get rid of the humans that interrupt them. But we don't usually have laser weapons on our robots because we get accidents, and we're not allowed to do that anymore. And so what you want to do is you want to allow your interruptions to happen. And then you discover that's incredibly hard, because somebody speaks, you get the answer, they start hearing the answer. And then they interrupt and say, no, that's not right. I wanted to go downtown, and so you now have to wait until the end of the synthesizer telling you about how to get to some shopping mall you don't want to go to. No, it should be able to interrupt it. But much more importantly, you should know when it's being interrupted. What word just got interrupted? Okay, so maybe the system's offering a choice and says, we have lots of different ice cream. We have vanilla, we have chocolate, we have strawberry. And you wanna know when the person goes, yeah, I'll have that. What word aligns with them saying yes and you might go, that's easy. No, it isn't, because the system gets sent a synthesizer and it goes away and doesn't speak it, and then the audio device's speaking on those have delay and the time of. And you don't really know what word is being said or what time. And you don't need perfect accuracy, but you probably need it less than a second accuracy to know what's actually being spoken. And so you need that alignment, so what you find is some sort of audio server who's listening to what the human is doing and making sure that it fits well with the synthesizer. You've also got to make sure that the human back channels But you don't interrupt the system, okay? So if the human action goes mm-hm, the system doesn't stop and says, I'm sorry, you wanted to go to mm-hm? I don't know of a bus stop called mm-hm. And no, just don't say anything. You have to know what's actually going on, enough so there has to be a link between those things. So, what the parser does is basically take the words that come out of these and get the structure, okay? So if it's gonna do something as simple as bus information, it's gonna know what the bus stop is that's actually being talked about. Okay, in Pittsburgh, there are 15,000 different bus stops. Okay, and each of these bus stops has multiple names, okay? They have an official name, okay, but the official name is pretty stupid and no human would ever use it. The official bus stop name of the bus stop just outside CMU campus Is Carnegie Mellon University at Morewood Far Side. Nobody's gonna say Far Side, okay? It means it's at the other side when you're driving past the road. But nobody does that, okay? And so you're probably just gonna say CMU, you're probably gonna say Carnegie Melon, you're gonna say Carnegie Mellon University, you might see Morewood and Forbes, cuz that's what is actually is as well. And so there's a bunch of names that all refer to the same physical stop. And ultimately you want the name of that stop to be able to look up the database. And so you have to find out what the person said and be able to map it to the right thing. And that's the job of the NLU. You also have to know whether that's the destination stop or the departure stop, okay, which could be confusable as well. You could confirm it, but you might know what people are likely do. Maybe even you know where they are you can do that. The dialog manager has to care about the state of the dialog, what's going on, has it got all the information, has it got some missing information, how sure is that information? Does it need to confirm it or can it just go ahead, is it reasonable? Did somebody just ask for a bus stop to go from x to itself and back again? One of the most amusing things you could do with our earlier flight information system, which would book flights for you, would say I would like to fly from Pittsburgh, Pennsylvania to Pittsburgh, Pennsylvania, okay. Then the system would disappear into a loop to try and find the most optimal way for you to fly from the same airport. So it would try and to look for a close airport and then fly you there and then fly you back again. No, I don't think I want to do that. The dialog manager might also care about Preferences, user profile for the person who's speaking. It might know particular things about where they travel, when they travel. One of our earlier bus information systems, which was a little bit too successful, I have to admit. So we had this bus information system, one of the first ones you could dial up and ask going from A to B. It got your caller ID, okay? So it knew who you were. So it knew what you asked for before. And one of the students who worked on it realized that she now had all of the numbers of all of the students and faculty who worked on this system, okay? And she knew it all pretty well, and so it ended up, instead of when you call up the system, I didn't have to say, can you tell the time of the next bus from CMU to Squirrel Hill, I live in Squirrel Hill. What happened was, she just basically said, well if it's early in the morning, Alan's probably going to work. If it's late in the evening, he's probably going home. So you would just call up and the system would say hi, Alan, your next bus home leaves in 20 minutes. And you think, I didn't say anything. How does it know who I am? Which is a perfectly reasonable and useful thing to do, and Google does this all the time. Always frighteningly when you look at your Android phone and it tells you the answer to the question that you haven't asked yet. That's a little bit too much, but maybe it isn't, maybe that is the right thing to do. Language generation from structured words, we have some representation that comes out of your database. You have to make it into reasonable words. Note that humans do variation when they do generation, they never say the same thing, okay. So if somebody tells you, you can get a bus to the airport at the corner and they name the corner, and you say to them, what did you say? The next thing they say wont be exactly the same. They'll modify it in some little way. Okay, and we do that all the time. And if they didn't do that, it's a little weird and you notice it. Now, sometimes when you're learning a language, okay, and you don't understand somebody, you want somebody to say exactly the same thing again so you have a better chance of understanding it. But in general, people do not do that. They always just give a little bit of variation. They don't even think about it. They probably don't even notice that they do some sort of variation, but when a machine generates things, it should give a little bit of variation as well, otherwise, it seems Too robotic, okay? So we can't just say, tomorrow the flight to Boston leaves at 9 AM. What you have to say, tomorrow there is a flight to Boston at 9 AM, or at 9 AM tomorrow, there's a flight to Boston. Or, the flight tomorrow US Air has a flight to Boston, and so you just want a little bit of variation when you're actually doing these, otherwise it sounds weird. And a little bit more detail about parsing. So what happens when people speak is they really say random stuff. They're not very syntactic when they're actually speaking. Everybody likes to think they are. But no, and so it's mostly just a bunch of words. There's some structure to them. Okay, I wanna go to Boston tomorrow. Or we have, the British speaker, the English speaker says, if it's not too much trouble, I'd greatly appreciate the possibility of considering arranging my flight to Boston tomorrow morning, cuz they're always super polite. Okay, and while of course the American is not gonna do that, he's just gonna say Boston tomorrow because I'm American, you don't need to do that. But this variation happens all the time. This is something that's part of how humans communicate and it's necessary to be able to deal with very wide differences of having the same meaning but depends what they're. And you have to collect all this data cuz it's really hard to know what people are gonna to say to begin with. And so, for the most case, you build a system, you deploy it, you look at the answers that you're actually getting. And then you improve your system from that. In fact, there is no dialogue system that works perfectly straight off. They always deploy early, they always collect data, they always start doing modifications. So I wanna go to Boston, tomorrow, you maybe get some slot filling form that says exactly what it is, and there's maybe extra information about no prop plane, or something like that. Let's skip that. For those of you do speech recognition, you're probably very familiar that in a speech recognizer, you have an acoustic model that tries to model how people do acoustic phonetic and pronunciation. In addition to that and speech recognition, you also have the language model about the expected form of the words that people are going to say. So you train that on a lot of words. Parsing is not the same as a language model, It's a different thing. Parsing is trying to find which words are important that refer to things in your domain that you're going to actually have to manipulate, okay? And usually what happens is, you'll have a language model that you're using in your speech recognizer that sort of constrains the things that you're actually expecting a human to say. But then you actually have to pause that to be able to get the information. There's some relationship between these because the words that are in your domain, the bus stop names, the place names that we're doing, which have to be in your language model as well. But the parser can ignore some things. So one of the things that we often have cuz it's speech, is we have to deal with swear words and swear words appear all over the place. Actually in our telephone dialogue system, we would refer to them as polite words, because what you discover is actual polite words like please and thank you are not important. Okay, as is the case of swear words are not important and you can think of swear words as being negative polite words. But they're still polite words. And so you can sort of ignore those. They don't really make any difference to the communication. Cuz we don't really care whether the person is polite. If they're polite we tell them the correct time of the bus. If they're impolite we don't tell them, in fact we tell them some other time. We don't do that, well, maybe you could do that. No, no, no, we're not gonna do it. So there's all these extra words that aren't really very important. And then there's the keywords that actually are. And that's where the parser system actually has to. Maintaining stages, dialog management gets harder and harder and harder, the more complex it is. And we'll talk about this in a little bit, about how to deal with this. And dealing with barge-in when the user interrupts is surprisingly hard and absolutely necessary. If you don't do this, people are extremely frustrated in the system. Language generation. So you get some query. You can have some template filling and often it's template filling. Nowadays it might be some sort of DNN trained model that does it but it's surprisingly hard to get that good. And it's much easier to come up with templates and fill them, but you have to come up with the templates and discover them and how people are actually talking to them. Yeah, yeah?  [INAUDIBLE]  Mm-hm.  [INAUDIBLE]  Yep.  [INAUDIBLE] [INAUDIBLE]  Yeah.  [INAUDIBLE]  Yeah.  [INAUDIBLE]  So that's sort of true, but we have actually done that. So we call it lexical entrainment. It's very specific about if somebody actually uses a term we try to use that back, if we have multiple choices. So the specific thing we actually even did in the bus system was you imagine of whether you're leaving from versus where are you departing from. So you have leaving and departing. And if the person uses the word departing, we used that back at them, and with the word leaving. And we find the people who don't notice it but they prefer it if you do the right thing. There's a mentionable effect where people think that their conversation went better, when you actually do this lexical entrainment. And either if you so you do what they do. But you also discover that if you use terms to them, they're more likely to use them back, especially if they're slightly unusual. And what you often try to do in a system is you try to direct people to words that are easier to recognize, okay? And so instead of yes, the word yes is incredibly hard to understand in a speech recognition system, which is really bad. Why do we have a word that's really important to conversation and it's really confusable. And what you discover is if you actually go and talk to people who talk over radios, they never use the word yes because they know it's really confusable across noisy lines. And so, they use a word like affirmative or negative instead of yes and no. And they deliberately do that. People who go to the moon, they don't say yes and no they say affirmative and negative. Now that's a little weird in a conversation on a telephone. But getting them to use things which are not confusable is something, but also echoing back what they're doing. It's a good idea to do that. Let's skip that. Help should be standardized, people get lost all the time. They're never, ever, ever going to read the manual. They're never even gonna listen to examples and we did some interesting research here to find the best way to try to correct from errors. Because usually what happens on these systems is you get an error and you can't recover from it. People say some things system I wouldn't understand and or I don't understand, did you say the airport and the person says, yes. And the system says I didn't understand, did you say yes? And the person says yes. And it says I am sorry, I didn't understand said there, did you say yes? It gets lost. Okay, and it's really hard to get out of that death struggle. And there are tricks to do that you change what you are talking about. And you can the best known. The problem is that when a human misunderstands and they say, I don't understand what you're saying and you give the second reply, they change the way they speak. And speech recognition system is particularly bad at second mention things, when people are saying, really bad. And the more frustrated that the speaker is, the harder it is to understand them and so you fail to recognize them. So it just death, okay. A little story about this. We had an automatic method for trying to detect whether people were frustrated on our system, and we wanted to find ways to calm them down. And I volunteered that we should build a synthesize that got angrier and angrier with the customer until they eventually hung up, and then they called back again, things would be better. I was voted down. I was not allowed within our project, picky I'd love to build an angry synthesizer. And so we tried a bunch of other things. One of them was to change the subject so you no longer ask that question, you're going to do something else. You're trying to get a bus stop name out of them and you're not succeeding and so what you do is say, I'm sorry, I don't understand the bus stop name that you're talking about. Can you tell me the neighborhood that you're going? And that's enough to sort of reset the person, to stop them trying to say something that's failing, okay? Maybe it's our pronunciation, maybe it's our ASR, maybe it's cuz it's too much noise in the background. One of the other things that we didn't try but we felt should work, if we change the synthesizer. So if our system said, I'm really sorry, I'm not understanding what you're saying, it's our speech recognition system, and we're gonna change it to our newer system and you get a chance to talk to a newer system about that. And then we change the synthesizer from a male to a female, we don't change the speech recognizer at all. But the person would think that we changed the recognizer.  So I have a story about that so we had this clustered [INAUDIBLE] and we had this [INAUDIBLE] small dialogs [INAUDIBLE] and one of them actually [INAUDIBLE].  It's all it actually did.  [INAUDIBLE]  Yes, so that's it. It's an interesting way, one of the other things that we did was we discovered that if you changed the volume of the synthesizer and made it quieter, it actually calmed people down. Okay, not that it was so quiet, but he changed it and they perceived something was different. They concentrated more and they changed the way they spoke and made it easier to do. So there's lots of little things like that that are actually very important to success. These make a big difference to success of the overall conversation, okay? And when you communicate, when humans are communicating, they will often do various techniques to confirm that everything is going the right way. And machines have to do this too. So you can do explicit confirmations, like, where are you traveling to? The person replies to Boston and so you can reply Boston. Did I get that right? Now when I played the bus information one, the one that was failing earlier, it dropped into explicit confirmation mode, because it knew the person wasn't very familiar and, therefore, that sort of, it makes it more likely to be successful. So this is what we call explicit confirmation, but it's a real pain to deal with every time. If you say when's next time for the bus from CMU to the airport, after 2 PM, if it then confirms everything? It makes it unacceptably slow. But if it's not very sure, this is quite a good thing, cuz it'll give you more confidence. So explicit confirmation is a good way for success, but the cost that the user who's actually talking to it is going to be a little frustrated of all of these explicit questions. And there's another form of confirmation that's called implicit confirmation which humans use all the time okay. So where are you traveling to and the person says Boston and then the system can say, Boston, when do you want to travel. And so, it's confirmed that it got the word Boston, okay, but it's not explicitly asking you whether it's correct or not. But if it was wrong, you could barge in and correct it, okay, and so in other words if you're sorta confident that this is right, but not very sure that it really is. This is quite a good way of actually trying to confirm but without doing any explicit confirmation. People who are using systems are relatively happy with this, because one of the things that they're never convinced that the system gets it right. Cuz the system says where are you traveling to and you say, Boston and it goes, okay, you think well did it get that or is it gonna like send me to somewhere else in the country instead? So it's nice to know that it's actually doing it. So implicit confirmation is now the, but you have to have good margins, you have to know when you're wrong and think about the cost of that. Cuz it's slightly more expensive to fix in a margins, but you have to say, well, most of the time I'm gonna get this right, but sometimes I'll get it wrong. And explicit is safe, but slow implicit is natural, but requires really good support for margin. And you got to have a good confidence about whether you think you're correct or not. And humans do this all the time, it's called grinding when you are talking about things, you echo back the phrases that the other person uses, but there is also a way to confirm that you are talking about the same thing. We do this all the time, so when you are talking about giving directions. You want to know how much the other person knows, so that you know that they're referring to the same thing, and if not, you want to be more explicit. So if you say something like, again, directions around to see a new campus, if you say, well, you go down Forbes to Craig, and the person goes, past the Starbucks? And you go, yeah, past the Starbucks. You now know which actual intersection you're talking about. Both sides actually know it, okay. If they didn't know about Forbes and Craig, they might say something about, so how far is that? And it's like, well, it's about half a mile. So being this grounding technique to know that you're talking is actually very important for humans and very important for machine as well, otherwise the conversation doesn't go forward. When you're designing these systems or when you're having a system that generates these, you have to care about how you ask the question to the user. And one of the things that we teach our students when they're building these systems is to build your system you put in your prompts. And the first time you use your prompts on somebody other than yourself, what you discover is, the prompts are stupid. The prompts, actually don't ask for the piece of information you thought may be asked for And it's surprisingly hard to actually craft prompts to really ask the right questions, okay? We just don't think about this when we're speaking. Maybe we do when we're actually speaking, but humans are so good at recovering from things that are unclear. That we're actually not very good at writing down what prompt would be most appropriate to get the answer that you actually want. Okay, what bus would you like schedules for? And there's either bus. How may I help you is a nice way to start a conversation, but you could get almost anything from them. Don't do that, cuz you'll never understand it, so be very specific, then you're much more likely to have that success. And I'm looking for time.  [INAUDIBLE].  I have 30 minutes, I'm gonna skip this part. This is about a programming language for building systems but I'm gonna skip that for the time being. The thing I'm gonna talk about now is evaluation. Almost everything in language technologies that's interesting is really hard to evaluate. Many people think this is a fault of their language technologies, and therefore it's something that we shouldn't work on this because it's too hard to evaluate. And why can you work on something that doesn't have a good evaluation metric. I've always thought that the opposite is like, I don't really want to work on something that has a strong evaluation metric, because it probably means it's too easy. Okay, it's hard to evaluate systems and language technology, because it's hard to evaluate human interaction. It's just fundamentally hard. We don't have an external metric for what these things actually are. So almost anything that we do, like language innovation, speech synthesis, machine translation, dialogue systems, there just isn't a good way to evaluate it. And that's not something to say, we won't do that, we'll going to something where there's a really clear objective function, and we'll just work on that. The point is, no, we actually have to do this as part of the research, we actually have to find a good way to be able to evaluate things. And so, experimenting on different evaluation techniques is one of the things that we do in language technology. And in spoken dialogue systems, that's what we really have to do. So we have to come up with ways of finding a way to measure our systems, which are a valid way of telling us whether they're good or not. And of course actually it's not just this external thing about, is it good enough? We typically are doing constant development of the system and improving it and changing it. You want to know whether it actually got better, and say, I now got this wonderful new speech recognition system. Its word error rate is significantly less, does it make my system better? Okay, how do you measure whether it actually makes the system better or not, rather than just saying, well, it should be better. You wanna be able to do that. And there's lots of ways in the task-oriented form to do this. But in the non-task-oriented one, it's particularly hard to find good ways to know whether you're getting better. But it's an important thing to do, and this usually means that we have to end up with a number of measures. And you look at them, and you try to decide which ones are reliable or not. So what are good metrics, so task success. If you're gonna give a bunch of bus timetables to people you want to know whether you did do that, okay? Every call that you don't give some bus information for, in the bus domain, probably means it's a failure. Actually it doesn't, because we used to do that and then we discovered people would ask for buses that don't exist. I mean, buses that genuinely didn't exist, and we would say, but there isn't a bus, and they would go, okay. And that's actually success, but we didn't notice that people were actually asking about buses. They are often asking for buses that used to exist that don't exist anymore. But past success is a good way of doing it, and maybe that's what you want to do. But you wanna be a little bit careful about that, because that might not be the only measure that's actually valid. If it takes two hours to tell people the time for the next bus, that's pretty cool and you're successful with it, but two hours is an awful long call. And there's something wrong with the person, that they're willing to spend two hours on the telephone trying to get the time on the next bus, cuz they've probably missed the bus by then. And in systems where there are humans to fall back on, you could talk about call escalation. So if the person doesn't use the automated system and actually falls back to the human, you could talk about the trade off of that. Presumably the human is more expensive than the machine, and so you can talk about trying to minimize that. Most commercial systems actually do that, and they've usually got some window of what they have to do. Otherwise, they lose money in building systems. Minimum call time is another thing that might be a reasonable way of doing it, because you probably wanna keep a call short. You don't wanna waste people's time, you wanna get to the answer as soon as possible. But if you just take minimum call time, and if you take minimum call time and task you can get some really weird things starting to happen when you do the optimization. So what you then do is you then have a system that detects the accent of the person in the first utterance. Works out that they have a funny accent, like a Scottish accent like mine, realizing that they're not gonna be recognized and then hangs up on them, cuz they know the call's never gonna be successful, and it's gonna take a lot of time. That's not a good solution, but it matches the metric. So you have to be careful, and there's all of these indirect things that happen with metrics, and you've always got to be very careful when you choose metrics and know whether to trust them or not. Error recovery versus WER error rate, so the number of times you go into an error state in your system, how often do you recover from it? And that's a pretty good measure of whether you're actually causing people to fail, cuz if they just fail when you go into an error thing and never get back, there's something wrong with your system. And this was one of our earlier things to actually do to see how often we could recover from error. And Dan Bohos, whose now at Microsoft Research in Redmond and his PhD did a lot of work on error recovery, within our bus information system, [LAUGH] and made it much, much better, and so actually we could recover from errors we weren't really doing it. But this is an important point about how can we measure whether a system is doing well. When you go to come up with measures, and you probably have to come up with multiple measures, and try them, and see how they're affected over time, and to make sure that they're actually helping your system. So one of the things we did, because we are really interested in how to evaluate systems. And we had this platform, we had these bunch of random Pittsburgh people calling us. And so what we decided to do is we decided to introduce this challenge to the research community. And what we would do is we'd find common task, in this case it was gonna be bus information system, we're gonna ask bunch of sites to be able to build a telephone dialog system fo getting information and about Pittsburgh buses. And then, we'd run their system on the live real humans in Pittsburgh to see how well these systems work, and then be able to have a good comparison where it's all in the same task, it's got the same community of users, and that allows us to be able to do a better comparison. One of the biggest issues, especially in research, is somebody in some lab does some task, and somebody in some lab does another task, and it's really hard to evaluate, cuz there's too many different variables that you're playing with. But here we've got the same user group, well, it's not quite the same user group, cuz it's different days, but it's gonna be about the same user group, and we've got the same task, giving bus information, and how well can we actually do that. So way back in 2010 we find the bus schedules for Pittsburgh, we're gonna use the Let's Go System. We actually handed out our core system, and the Cambridge UK group built on top of it, but eventually it replaced the lot. Jason Williams, who's now at Microsoft, was still at AT&T at the time, and he did it, at Heriot-Watt University in Edinburgh, did it as well. So the standards system answers calls by the Port Authority, even though we're way in the center of the continent, we still have a Port Authority, cuz we have some big rivers for weekdays from 7 PM to 6 AM and weekends from 4:30 to 7 AM. And we provided the full source and all the data for training for about six or seven years before. Actually, we did it two years, cuz the system changes quite a bit, and so we wanted to be similar to what the current colors actually are. You could build a complete system or you could say replace speech recognizer within our system, but all of the four groups ended up building their own system. You could also do evaluation and get all the data to do evaluation, and actually the Hong Kong University Science Technology did that. Also, some people build a simulator and what ended up was, we had human callers every day, but during the day time, we actually had machine callers that were actually collecting data from the system, which was a little weird especially, as used random synthesizers. And I remember listening into one of the calls, my voice was being used to be able to speak to the system, which is just a little weird. You think I'd been replaced by a robot. So we announced the thing in September 2009. We give some seminars on how to be able to do it. We released the data, and we gave people about three, four months for the development. Then what we did was we wanted to make sure that we didn't piss off our users by putting systems that weren't reliable. And so we had a bunch of control tests in May. What we did was we set things up to have external people call the systems and make sure the systems were reliable, they didn't fall over. And the infrastructure as well, it wasn't just the systems, cuz we were connecting phones all over the planet. We weren't very sure how well that would actually work. And then during July and August of 2010, we actually switched the systems every day and we had an even set. There was about 14 days for each system, I think it was, and we tried to vary which day of the week it is cuz there's slightly different coverage depending on the day. So there was four systems, systems 1 to 4. They got about 100 calls in the control tests. I'm gonna say how we did this cuz it's actually quite important. The people who were calling were all experts in telephone dialog systems. They were basically the developers. They're different types of people, so if you're a researcher in language technologies and dialog systems, you talk to a system in a different way. Also, apart from the CMU people, you're not in Pittsburgh, you don't actually care about the bus, you don't even know where these places are. So you don't care about the answer. A number of years ago we had a flight information system. And one of the things that we noticed in it when we had people call and test the system, somebody called and said they wanted to fly to San Francisco. And the system says, okay, flying to San Diego. And the passenger said, yes. Okay, now, San Francisco and San Diego are about 300 miles apart. They're two hours by plane. They're in the same state, but no, they're not the same place. So nobody who's going to San Francisco is going to accept San Diego instead. And the point is the person who was calling wasn't going to San Francisco. They just had to fulfill the scenario by pretending to go to San Francisco. And so real users care about the real information. But if you pay people to do it, they don't care. I mean, no matter how much you try to get them to care, they're not getting on the bus. So they don't care. And so this is something that worries us. The expert callers were not local, they were not getting the bus. They were very good at completing the task, because that's their job. They're gonna complete the task. Well, local people in Pittsburgh hang up if it's not working properly. They hang up because it doesn't matter. The live tests, 14 days each, evenings and weekends, real callers who actually want it, and only three systems were stable enough to do it. And actually, to be fair, the third system that wasn't stable enough, it was partly to do with the infrastructure of transferring the call from Pittsburgh to Edinburgh just wasn't reliable enough and it would break a lot of the time, and therefore we couldn't do that. Their system was also not very stable. But even still, we decided not to do that. And one of the things we did when we're giving the control test, and this is important, how do you instruct people about what they're going to do? If you give them text, they're going to say the text. And we wanted not to try to feed them words, we wanted them to come up with their own ways of saying it. So what we decided to do is we decided that what we'd do is we'd actually give them as pictorial as possible. So here we've got Pittsburgh, and we've marked at the airport, and so we hope they're gonna use whatever the term is for the airport. The bus stop name at the airport is Pittsburgh International Airport at arrivals level two. But nobody says that, okay? And we don't expect them to do that. We give them a time, and you can say any bus, and you want to go to Bigelow Boulevard, this is relatively near CMU. And we hope that people would only use the street names and not do any more than that. We use web-based instructions to be able to do that, to be able to call the systems. We had to call random numbers all around the planet, two in the UK, two in the US. It required international calls. Also have to remember time zones, cuz what would happen is the Brits would go and call, and they would go and call the system and it was 2 AM in Pittsburgh. And the system would say there's no bus, cuz it's 2 AM and there is no buses at 2 AM. And the person goes, well, I tried to get from the airport and they give me the time, but there's a mismatch on that. So it was hard to do that. Also, one of the Cambridge systems had the problem when they were giving the system, they gave it in UK time, okay, rather that Pittsburgh time. And so if you said I wanna leave at 7 AM, it would look 7 AM Cambridge time, UK, which is 2 AM in Pittsburgh. And so it all failed miserably. So they ended having to flip the machine onto New York time. We used a number of different people throughout the world to do this, mostly the research People from all the different groups, many unfamiliar with Pittsburgh, many non-native speech, it seems that nobody in the UK is actually a native speaker of English. At least all the students there, although that might be true in CMU as well, all of their students are non-native. Anyway, they don't care about an XBox, but they're much more forgiving about the system, and they try much harder. And so, sort of cheating at some level. Finding callers was really hard. I ended up having to go in the corridor at CMU and trap people and push them into a room and not let them out until they actually called the system a number of times. A whole bunch of people would avoid me in the corridor after that. Measuring success, non-empty calls. So sometimes people just hang up or it doesn't go so it's like that if the calls actually actually succeed. No information, call ends before there is any information and I don't have that. Though sometimes that's the right thing cuz there were some of the scenarios that didn't have. And positive output actually gets it. We hand labeled all of this, we have somebody to do all of that. So total number of calls across the different systems, we don't identify which system's which deliberately. Though I can almost remember, but I only tell about the third slide, I don't remember whose who. And there was quite different success rates from all of these. System two was an eight watt system, so they were relatively low but both the Cambridge and the AT&T system are kind of my modus operandi. We were doing better than the CMU system. Which was actually quite interesting, their speech recognizer was probably better than ours. So this is the control and this is the live user. Live users we had much more callers. What we did was every day we came through to my office and we switched the call to a bunch of different systems elsewhere. We'd set up a thing at AT&T that could forward to the two UK systems or to the AT&T system. Rather interestingly, we've idea who paid for the calls, Jason Williams, who worked at AT&T set it up. And then he left before he had to pay the bill, so we don't know. At some point, when I go and visit AT&T the next time, they're probably gonna say, here's your telephone bill, please pay this. And with charges for not paying or something for two years. Length of calls were very different, which is quite interesting, but the systems were very different. And the systems were very different because some were much more wordy than others. We also discovered rather interestingly, all of the systems used American English and speech synthesizer apart from the Cambridge UK system. Cambridge U system used the British English. And what we discovered was, our poor users in Pittsburgh were somewhat surprised of this system that many of them called regularly, all of a sudden stopped having an American accent and started sounding like some British person from TV. And what happened was, they started being polite to the system. So they started saying please and thank you. Well they never said that to our system. They only used the other polite words at the other end of the spectrum. And people used a lot more polite terms when they were talking to a British English system which we just didn't expect at all. And so, they had more words than some of the other systems, but maybe people were happier.  [INAUDIBLE]  Yeah?  So in the last line, what's [INAUDIBLE] quantitive output [INAUDIBLE]?  So if you say the next bus leaves at 8:00 PM, okay? So there's a positive output, you're given a bus time. But that could be the correct time, or it could be wrong, okay? So there's two types of things. So there's no output, which could be correct, it's like there is no bus that goes from X to Y. Or no output and that's wrong, cuz there is a bus that goes from X to Y. And so most of the time when it gives a positive one, it's probably correct, but it might not be. And most of the time when it gives a negative one, it's probably wrong and misunderstood. So there's different ways of giving the information, okay? And this really required somebody who understood the bus system and knew where the buses go to know whether genuinely, this was a valid thing or not.  So the live activity had [INAUDIBLE] how [INAUDIBLE].  Well, it's probably got something to do within the live one. People were actually asking for buses that actually exist that people travel on. So the distribution of buses they were asking for followed the distribution of the most popular buses. While in the control test that wasn't the case cuz we made them up, we deliberately made some hard ones. So there's probably something like that. And I'd say about on an average span maybe about half of the users are repeat callers. And they're pretty good at using the system. It's hard to really know cuz we don't have that information, but some of them really know how to use the system. [INAUDIBLE]  Yeah.  So even though there's so many issues with performing the controller test, you think it's still important how?  So, this is an important point about, I think it's critical to do it on real users. Your control test is gonna tell you how stable your system is. It's not gonna tell you how good your system is cuz the control people are not real users, okay? They have a different goal, they have a different way of speaking to the system. They're just not the same as the real users. But if you fail the control test, it's not gonna be good for the other. But being good, the control test and there is a difference of the best system three and four which is the AT&T and Cambridge Academy, which we random goes if you look at the word data rate. And the word data rate is different depending on the control compared to the live test. And so in other words, if you optimize in the control test, you're not gonna get your best system for the live test, okay? So you need to do the control in order to make sure you're stable. But it's not really gonna tell you how well your system is. And the live system is something that you really have to care about. Gonna skip some of these. Controlling live is very different. And real user's different. You have to care about that. Control callers speak for longer and they don't give up. Their job is to finish the call, okay? Other people just hang up. In fact, on another project later on when we're looking at repeat callers to see whether they're better or not, we discovered that repeat colors were not better. And then what we discovered was, well, that's not completely true cuz there's a style that some of the users do, is that if you find two calls in a row that's the same speaker, what it actually is is many speakers, many callers, will come in. If they're failing to get the right thing, they hang up and call back, okay? And then the second one is more successful. So there's a usage path, and the users discovered were successful was that if they're failing to be recognized, that they hang up and come back again, they're gonna get a better success. So there is a lots of ones where you get two people the same person, the second call successful and the first call is a failure. But they're just deliberately just recalling it in. If you look at people who were calling it in over multiple days, they're usually very good. And also rather interestingly, if you looked at the very high frequency people, and even though they had interesting accents, many non-native English and eastern European accents. We have quite a significant population of Russians and Poles in Pittsburgh. And they were definitely accented in quite a serious way. No problem, they'd learned how to use the system, and were very successful in doing it. So people who use the system a lot get much better at it.  You just said if we don't have a very good [INAUDIBLE] introduction to the system, generally we don't go back to it.  Yeah.  So why would people call the second time?  Cuz they knew that that worked. So they probably had a bunch of successful calls. So the people who called the first time, they failed. So when they called the first time and it succeeded, they called again. But then when it failed on the fifth or sixth time they were using it, it got, but this worked before. So they hang up and called again.  [INAUDIBLE]  Probably not. I mean, we don't know cuz we're not Google, we can't track everybody. So we have no idea how many people we lost by not successfully doing the first call, okay? You can't tell, okay? But we were failing, so one of the other things is we only had one system running, okay? So if you called and it was busy, you had to wait for that call to end. And we were a little worried about that, about how many calls we have. People always say, but there's thousands of calls, you're gonna have to have thousands of machines to answer it. And that's not the case if you look at the math. Actually, during this particular test, the AT&T system could have up to eight parallel calls. And so we tested it to see that it worked, and the only time we ever had more than two parallel calls was when we tested it. So even though there's continuous calls coming in, especially between 7 and 9 PM, they're almost always back to back, and there'd be little gaps between them. And so, even though you have 100 calls a day, you don't overlap. You only sometimes [INAUDIBLE]. And so in this test that we did, we could hand label everything, okay? So after we only had 1,000 calls for each at most for each of the systems, and we had somebody who went and individually labeled them all and whether they're right. But you couldn't do that with a real system, okay? There's 100 calls a day, okay? And every month you're getting 3,000 calls. And we just couldn't label them. And it's very interesting, because if you talk to the big companies that do these things, they think that the number of calls that you get was trivial, okay? But the point is that Once you get above about 20 calls a day, there's no way you can actually look at them, you've got to have automatic metrics. And what's really interesting is you get in these positions where your system's running, and then the task success goes down. And this actually happened the year after we ran this test, we started getting worse. We had no idea why, okay, and how do you know why you're getting worse? It wasn't much, we were maybe at about 80% and then went down to about 70. It's like, yeah, we don't wanna do that, so how can we actually do? We began to investigate it, we're not even still sure why it is. One of the things we think it was was an app was released by a third party that allowed you the time to the next bus. The app, of course, only ran on iPhone and Android, and we maybe lost our best callers at that point. People who were good with technology then moved to the app, and no longer were willing to talk to the machine because it's probably quicker than an app. What we did was we flipped the system from being open, saying, tell us where you want to go, to actually individually asking questions. And this meant that we got a lot better. There are other possible things that might have happened, it might have been they changed how the cellphone system worked in Pittsburgh, okay? Also we were aware that when we started, most of the calls were landlines. And over time, you get more and more cellphones cuz there are much less landlines than what there used to be, and so there's some external thing. And this was, we still don't really know the answer, we ended up getting better again, but we had to re-tune the system. But we still don't know what actually happened apart from, listening to the calls didn't really help. Okay, so that's the first part, so we're now going to take a break for half an hour. Okay, and then after that I am gonna talk a little bit more about personal digital assistants like Siri and Cortana. Okay, and about large systems which are basically building systems from data, rather than hand constructing them. Okay, are there any questions at this point? Okay, so let's start again at 11:30, okay?  [APPLAUSE] 