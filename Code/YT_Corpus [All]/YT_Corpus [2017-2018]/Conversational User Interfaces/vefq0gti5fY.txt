 And Google is pretty much killing the race right now, like in a big way… My name is Johannes Start and I work for Vectorform, an invention company from Detroit, as a solution engineer for mobile and emerging technologies. today, I want to talk to you about how to communicate with machines, – so vastly different to what we heard before from Ogenio. Communicating with machines has a lot of different requirements than communicating with humans. So let's take a second first to talk about how humans communicate with each other and what makes human communication quite special. So, basically, there are two different types of communication say like very simple and basic physical communication and more advanced and complex communication. Physical communication includes what we do with our bodies in the physical space. That includes gestures, facial expressions, emotions, and sound. f we look at how animals communicate it’s pretty similar and you can compare the two with each other quite well. Now, comparing that to the more complex types, like language, either written or spoken, that’s vastly different because with language we can transfer abstract concepts and ideas. We can give very complex commands and exchange a lot of different information which we just can’t with basic or simple communication. So putting that in the context of user interfaces is quite interesting because on the one hand, we have humans who have a large range of different ways to express thought and complex information and to communicate all that and on the other hand, we have computers that are very data-driven, are very simple and are very logical. They’re not emotional, right? So, the question of user interfaces emerges. How can we bridge the one world into the other world. How can we have a machine understand a human? So, what we started with back in the day – this isn’t the first one of the type, but one of the first - - is the Commodore 64. TThat’s one of the first interactive computer systems. Interactive because you can type things. Press a button and the computer will respond. You don't have to reset the whole machine to do something, you can basically just enter commands and you get stuff back. Going back in time, this was the oldest computer that I ever touched. what was quite interesting for me as a kid was to actually use this because it’s so vastly different to the way I got to know modern computers and to the way humans communicate. This accepts language. You can type things into it. It uses characters and symbols and, as a matter of fact, it uses the same characters and symbols and language that we use as humans. It uses English and it uses the alphabet. It has a key board, it’s semi-easy to learn the concept of a keyboard. It has keys on it with letters, and the letters are the ones that we know. So I start typing on it, right? The symbols appear on the screen. That’s all good. The computer is responding, so I think it gets me. I hit enter and it does not. It didn’t understand anything I’ve told it because the system is built in a way that I can enter anything I want. It’s pretty much an infinite canvass of information that I can throw into this machine, but the machine expects a very, very small subset. And the machine doesn’t tell me that it expects a very, very small subset. I have to know that. And, not only do I have to know that it accepts a very small subset, but what exactly I have to enter. What command I wanted to do, with what parameters and I even have to know the right order. I have a typo, it will not accept it. So if we look at how natural the user interface is, it's not very natural at all. If we look at how it matches user expectation, it does not match user expectation at all, because I think I can put anything into it. I have a keyboard right I can put my language into it, and it doesn't understand me unless I read a book about how to use this thing. If I stick a floppy to the drive and want to execute a program off of it, then I first have to connect to the floppy drive, load the program into memory, jump to the start position and run it. That’s not great. But the freedom of input which is infinite is quite important because we're going to come back to that later. A little bit forward in time, not much actually, just two years later than the commodore 64 was released - - in 1984, was this user interface – one of the first graphical ones, if not the first one – that has a vastly different concept of interacting. What they did was, they thought, “Hey. How can we take a physical concept that people get quite easily and transfer that onto a screen?" Typing text is something we get as a concept, but it's not great for a user interface, at least back then. This is a bit different because we all can imagine what a desktop is, and what things on the desktop are. If I have a note laying on my desktop - if I take it and read it and take it away and put it in a drawer, I know physically where I'm putting things. It's basically taking a concept that we know from real life and putting it in a two-dimensional screen. If we look at how natural it is, therefore it's kind of decent, right? It's a concept that we can understand, that we can relate to. We don't have to really know how the machine behind it works, except for maybe knowing what a file system is and where to like put files and stuff. Does it match the users expectation? Kind of. I mean, it shows you pretty much what you can do with it. If I click a system folder it'll probably open up the system folder, but do I have to click once? or twice? Or is it right-Click? is it a middle click? I mean, we all know the scenario from our parents when they say "Okay, I double-clicked this thing and it was a link and all these windows opened up and nothing works like it should." This is way better than before but it’s not optimal yet at all, because it shows you certain paths that you can take and it shows you clickable areas, Bbut it doesn’t tell you how exactly you can do it, but the way we interact with it is not natural at that level. If we look at the freedom of input, it's starting to get limited and that's what makes it usable. First we had this infinite space that we could just type anything into, and now we're starting to get limited because it's showing clickable areas. We have menus and options for the user to choose from. Jumping forward in time a long way - because in between there wasn't too much of a big change in the base core concept - 2007, the iPhone was introduced. The new thing about the iPhone was not a touchscreen. There were devices before that with the touchscreen, even like capacitor ones, right? The great thing about the iPhone was that Apple really sat down and thought about the concept of interaction with a finger or with natural body parts. And Steve jobs said this back then when he introduced the iPhone - why would you have to carry a pen around with you to type on a small screen? You have a finger with you all the time, right? And it's natural for us to touch things that we want to do things with. It's natural for us to grab the things and to make things bigger and all these things, right? So if we look at how natural the user interface is, we can say it's pretty natural. We can see things, we can touch them. It's pretty clear. There’s visual cues about what you touch and what you can’t, and things happen that feel natural to us. As a matter of fact, it’s so natural that kids learn how to use these devices before they even learn how to read and write. If you have a one or two year old, it's really interesting. Give them an iPad and see how they use it. It's crazy. They can't read, they can't write, but they can navigate that thing like a pro, probably better than my parents. [That's super super interesting. Now, does it match user expectation when using it? Yeah, it kind of does, because we know what we can click and what we can't. What's the freedom of the user input? Can we put anything we want into that device now? No, we can just touch the things that it’s showing to us. If I have a travel app open, there are three buttons I can press, then I can press the three buttons and the open button and that’s it. That’s what makes it very intuitive, because we can choose very, very, very clearly from a very limited set of options in a very natural way. That’s what made this device explode. Now, going a step further, Apple said, “Hey. Why touch the phone if you can just talk to it?” Isn’t that a great way of adding a new user interface to the device? And, in theory it is, because it’s very natural to talk. We do it all the time. That’s our main means of communication, next to writing and reading. It allows us to exchange very complex concepts and ideas and information in a very short amount of time. It also allows us to give or tell the machine what we want in a very short amount of time as well. If we look at the freedom of user input: Now what we saw before.. we had 'infinite', then we had ‘semi-limited’ to ‘very limited’ to ‘super-limited’. And now again saying things, using our language, is infinite again. It opens up the space to say anything you want. And that makes it difficult because matching user expectation again while being able to say whatever you want is very hard. That's something that Apple noticed really quickly after launching this product - basic features like setting up a calendar invite, like texting a message or calling somebody - they were great. That was totally fine. But how can you tell the user what he can do and what he can't if you allow them to basically speak with their voice? With a human, you expect them to understand pretty much everything they say, unless it’s utterly complex or you’re too drunk to speak or something. But a machine doesn't work that way. People have to implement the functions and the features that are behind this interface. And if you don't tell people how and what's possible, it's hard for them to know that. I guess that's pretty natural. So how does the user know what they can do with this? Going a bit forward in time again, the Amazon Echo was released about two years ago. And this is quite interesting because they just took the graphical user interface away completely. They said, okay, here's a device, you put it in your home. You can talk to it. There's a cloud platform called Alexa behind it that does all the like gritty work and makes sure that the user is understood and will enable it to be a good voice user interface. Now what's really interesting about the Alexa platform and the Amazon Echo is that they limited what you can say per application to a very limited context. So what Amazon did was that they set up different hierarchies of, say personalities, or they'd say, skills you can talk. The highest personality or the highest level is Alexa herself. So if you start talking to the device, it's usually Alexa you're talking to. Alexa can do very simple things like Tell me the president of the United States? What is 2+2? What's the weather going to be tomorrow? Very very simple things things that we expect a digital system to do, right? But things that go past that – users expect Alexa to do, they expect the skills to do it. And the way they built the voice user interface is quite interesting because you don’t ask Alexa directly anymore. You tell Alexa to ask somebody else. So, take the Deutsche Bahn, for example, if we’re looking at transit, you would basically ask Alexa to ask Deutsche Bahn when the train from the Hautbanhof is leaving for Frankfurt. So, what they did is quite interesting because the first level of Alexa now knows that she’s not responsible for what the user is asking, it’s Deutsche Bahn. And Deutsche Bahn, from the context that’s it’s providing, is very limited in itself because it does transportation. I’m not going to ask Deutsche Bahn when the ice cream shop next door is going to close. That's kind of smart what they did there. It does have a problem though. If we look at what Siri is on the phone and what Google does with the Google System, for example, when you ask it something you get audio Feedback and you get a visual feedback. Whereas with Alexa you don’t get a visual feedback with this device. This means you can only ask for very simple information. If you ask for the history of train departures from the Hautbanhof over the last two weeks for a specific train and she starts reading those to you, you’re going to be like, “Arrghhh! I’m not listening any more.” But if you see it in a graphical user interface, it's a graph. It looks nice. It's the correct way of displaying information. And that was something that.. Alexa has pros and cons and it’s specifically the Echo device. In regards to that, but to tackle that they introduced a new one, the Echo Show. It’s basically a tablet with a stand and a better speaker in it. I wouldn’t say that this solves all the problems. Having the Echo Show as a stationary unit introduces a whole bunch of other questions that you can bring up, but it’s an interesting take that Amazon is bringing to the market. Essentially what they did with voice technology was similar to what Apple did with touch technology back in the day. There were other voices systems out there when Amazon launched its Echo, but what Amazon did was build a voice only and voice first device. They took the whole concept of user interaction and just focused on voice first and now they're adding other features. Just like Apple focused on touchscreens and touch interactions first and then added other features. So it's going to be interesting to see where this goes. I'm not sure if this device is going to really hit the market, especially outside of the US, but we're going to see. A question that comes up a lot is, Oare voice user interfaces a complete game changer? Are they going to completely revolutionize how we interact with technology? Are they going to be the next big thing and replace all the computers and stuff that we have? I think the answer is very simple. No. Voice use interfaces are great for what they are, but they're basically a step towards natural user interfaces and right now they are a good try towards natural user interfaces. So, natural user interfaces are all kinds of user interfaces that interact with humans in a very natural way. We've talked about the different forms of how humans communicate before and that may include our facial expressions, that may include our gestures, it may include tone and voice and language. It may include our context and how we physically move in it and that's basically this whole space of natural use interfaces where computers have to start learning how we communicate and we have to stop learning how computers communicate. Because the more complex our computer systems get and the more connected they get, it's going to get even harder and harder for us to understand how they work and what they do and what's behind them and orchestrate 300 devices in our home. Even our water tap is going to be connected at some point, right? So, the systems have to start communicating with each another and learn what the user is doing. That's the very great thing about natural user interfaces and voice and one of the first steps that’s mainstreamly being taken right now with Amazon Echo and Google Assistant. So, getting back to voice. I’d like to talk shortly about the intelligence behind voice assistants where we are at right now Aand what’s missing to really make them great and to really make them intelligent. The first part is speech recognition. Speech recognition has pretty much been solved. If we look at the rate of the quality improvements of speech recognition over the past eighteen years, we see a 30% improvement solely over the last three years. Basically we had a long period with a lot of research where not much happened and then we have three years where it totally went through the roof. And in those last three years, two years ago, the Amazon Echo got launched. We got to a point where the we got to a point where the voice and natural language of humans can be translated with very high accuracy from speech to text, and this is necessary to be able to process it further and make a computer understand what we're trying to say. The second point is being able to analyze the syntax, right? so that means looking at how we construct sentences – where the specific parts of sentences are. It’s arguable whether this is completely solved or not. II wouldn’t say it’s 100% solved but we’re at a point now, and it only happened recently, where we can syntactically analyse and basically understand pretty complex sentences already. There are APIs, like the cloud natural language API from Google, does a really great job. They just released a new version and they talked about it at Google I believe, just recently. And what's really great about that is that pretty much any English input we throw into it, we're able to analyze the complete syntax of that sentence. But what it does not give us is the semantics. So we know where things are, but we don't know what it means. We have systems that allow us to approximate what it could mean and in specific cases we can even know what it probably means, but the problem with language is that it's so complex, And you can build , I don't know.. take the English language, or German language, even better. You can build sentences that are so long you probably couldn't even like print them on that screen, right? Now, make a computer that's able to understand that. That's really hard. So semantics specifically for voice interfaces are especially hard because the way we speak is vastly different to the very structured way we write. So often when we speak we don't have the correct syntax. We don't use the correct words and usually we leave away all the important information. Humans are smart and can infer a lot out of the context, so if we're sitting in this room, and I say it's a bit dark here, what I'm trying to say is "Somebody turn up the lights, in this room, in this tower, right now". But a computer doesn't know that if I say "It's a bit dark in here". If I tell that to a human, it's pretty clear to us. What computer systems would have to do to be able to really understand the meaning of sentences – they would have to be able to infer a lot of context and be able to think somewhat on their own. And then the last part, and that's the crown discipline, there is the So, if we have all that before, that’s going to be tricky enough to get the semantics correct, like really correct, so conversation is the next big thing. The computer understands what I’m trying to say and it understands my intent and the broad range of conversational aspects maybe, but being able to have a coherent conversation with a human is really hard. There have been algorithms that seemingly, in specific scenarios, are able to fake a human conversation to a pretty high degree – Bbut to take a specific theme that humans are well-versed in and put them in a machine, they’re probably going to find out that it’s a computer within one or two sentences max – if it’s programmed really well, it’s four. Now, let’s take a short look at where we see assistants and voice assistants. Desktop and mobile, naturally when Siri came out, Apple had pushed it throughout their whole ecosystem. They're on Macs now, they're on the tablets and they're on mobile, obviously. Google's rolling out their Google assistant throughout all their devices, throughout Android, throughout even iOS now. That means Google Assistant is right now the most cross-Platform assistant around. Amazon is catching up really quickly too, but more about that later. The smart speaker market is really interesting as well, and I'm going to get into a bit more details around that later. We have two key players right now - one is Amazon, one is Google. We have the Amazon echo and a bunch of smaller devices around it, like the Echo Dot and Echo Tap and a few other devices. And then we have the Google Home. What's interesting about Google here is that Google doesn't only offer this device and a very minimal integration of other Echo systems, but Google offers full integration into their Android platform and now even iOS, so that's interesting. Automotive is super important for voice assistants and digital assistants because when driving I don't want to touch buttons or I don't want to look at a screen somewhere. I just want to tell my car what I want it to do and it will do it. In the future, when mobility is going to become more of a service and cars are driving on their own, what am I going to do in the car when all my friends are busy doing something else? Yes, I could probably work, but if I could have a conversation with my car or talk about work topics and discuss a few items that I have to get done for the next meeting with my car, that would be quite interesting as well. And, last but not least, it’s very important – Smart Home. If we look at voice assistants, and we're going to see that in a second, the main use for them is the smart home and controlling Smart devices. It's way more convenient to say "Alexa, turn on the lights in the living room!" than to get up and press the switch. Pretty much all of the other use cases that Alexa and Google Home offer do provide value, but not at the level of what they could provide for Smart Home. So, looking at the platforms. We have four main platforms, as I said: Amazon Alexa, Apple Siri, Microsoft Cortana - they're starting to build that out really big now, but it's not all published yet - and the Google Assistant. Just to point out a few highlights, because it's a lot of information here, to run through all of it. Advantage wise, I think that these two here are super-important, so Ecommerce and the Advantage part here. Amazon Alexa, right now in the US, has around 70% market share, [ so that’s quite interesting if you look at how many devices are out there and they're pretty much all.. most of them are on Amazon’s side. That’s probably going to change with Google pushing their offering. Amazon is pretty much owning the market right now, because they were first. Ecommerce is super important because Amazon obviously has some goals around the Echo, and one of them is definitely selling products through it. So, the Echo is, or the whole Alexa platform as a matter of fact, is Ecommerce enabled by default. You can buy an Echo and tell your Echo that you ‘d like to buy tomatoes and it will get you tomatoes. Apple Siri was one of the first platforms out, especially one of the first mainstream platforms out, so it's really interesting. We're expecting them to release a voice assistant, a digital home voice assistant, at WWEDC – or at least announce one, so we’ll see how that goes. What's important about Apple is that they have a lot of devices out on the market, so there is a huge market penetration by Siri, but it's not very open to third parties. So, there are a few minor things that you could do through it as a third-party developer, like payments and messages and stuff like that. You can for example build you own Smart Home integration, or add your own e-Commerce platform, and then buy through voice or something. Microsoft Cortana is not out in the market or in the state that Amazon Alexa and Google Assistant are, offering a third party STK to build your own full applications. We’re expecting that to come out soon. It’s closed Beta, but we’ll see. What's important about Microsoft, and that's definitely their advantage, is that they built Office Outlook and they have a deep access into the gaming industry with Xbox. So Microsoft's Cortana take I think will be bit different than the other platforms and way more focused on productivity. How can you use voice when you're in an excel sheet or even a powerpoint document? Your presenter notes, for example, are you just going to be speaking them? That's starting to be done right now, but what's the next level of it? That's going to be super interesting and Google is pretty much killing the race right now, in a big way. They just brought out the Google assistant and of last year and within, I'd say about half a year, they have a full integration into their Android system rolled out over all the modern Android devices. They just brought out an iOS application that offers full Google Assistant support. They have the Google homThey have the Google Home device that is estimated to capture around 20-25% market share this year. And they're offering a payment platform , and they have an open platform, which has always been key to most Google products. If we look at Android, it was so successful because it was an open platform, and they're doing the same thing for Google Assistant. So it's very interesting to see where this is going to go. Market share - we talked about this shortly already. Amazon Echo: 70% Google Home: 24% by end of this year, and everyone else is going to be at 6%. This is for the digital home assistant, so this does not include Siri or mobile-only digital assistants. It wasn't personal The Cortana’s connector on Microsoft bot platform was released, so you can sort of create bots already? TThey did it a week ago as far as I remember. The next question is - do you have something on Viv? No, we don’t have anything on Viv. Viv is interesting but not rolled out to the market and… Is that the full Viv platform or is that parts of the Viv platform? With bill last week, that's kind of interesting. I believe they announced it, I’m not 100% sure on that – the Microsoft Bot framework integration But I'm not sure if it's actually released yet. We haven't been working yet with that so I can't really say too much about that. But definitely interesting to look into. I’m going to do that after the presentation. Having the central device standing there, what about the energy consumption? Why do you have Alexa separately from Google Home? I understand if you take it one step further like Jibo, that’s a little social robot that interacts and shows you his reactions and looks at you. And it's kind of okay.. but Alexa could also do that. So, what’s the reason? So, the main reason for Alexa is that, or Amazon Echo for a matter of fact, are sole, alone, standing devices, Dso you don’t have to grab your phone. You can just like blurt at it or start talking to it at any given time. You could be laying on your couch with a phone in another room, and you could be like, “Alexa, turn on the lights.”. So, the key element here is that you don’t have to do anything but activate your voice. WWhereas with a mobile you have to get it out of your pocket, usually, or right now because the Microsoft phones aren’t good enough Or the speaker isn't good enough to respond to you. But then the next step would be a social robot? It depends on from which angle you see it. I feel like a lot of people would be anxious about having moving robots in their house that follow them around. Wouldn’t they be more anxious with Amazon Alexa listening to everything they say? That's definitely true. think the important part to think about here is how can technology become invisible and more natural to us. Thinking that someone is following you around, like a robot and listening to you is thought in a very human way, right? We’re used to, when we speak to people, they have to be close to us and they have to follow us around and they physically have to be there. Otherwise it doesn’t work, we have to call them or use some device, right? Now, if we think this a step further, we see technology really becoming transparent and invisible at some point. We want to have a context and a system context that knows where the user is, that’s able to understand the user wherever he is and is able to interact with him in whatever situation he is in his home, without having to be followed. We built this scenario for natural user interfaces for one of our clients that we presented at CS this year, where you can walk up to a kitchen counter and you can interact with it in your natural way and it will automatically detect what you want it to do. Take a sink, for example. You walk up to the sink and you have a cup of water in your hand. You put the cup in the sink and it fills it with water, at drinking temperature, or it’s just going to be a bit colder. On the other hand, if you go up to the sink and put your hands in it, the action you want is vastly different. You want it to run water, warm water probably, until your hands are clean and you can take them out again. If you want to wash an apple it’s different again. We don't see the future in a robot following you but having a smart context around the user that's able to understand what he's doing in each situation and in each context that he might be in. Which could be camera based, which could be something following the user, but not preferably. which could be Alexa-microphone based; which could be radar based and a bunch of other solutions that can be integrated into a system like that.  GGoogle Glass? Okay. Google Glass is an interesting topic. The whole technology world freaked out when they saw Google Glass. It was like, “Oh my God this is so amazing!” Until they went out on the street with them, and somebody was like "What’s that? It looks creepy!" And people realised technology shouldn’t look like technology. It has to look like things that we’re a company to, and that are natural to us. Having this thing in our face that’s not a glass, I don’t know, something that’s completely different and it looks kind of creepy and has a camera looking at everything you look at, made people not really trust the device. Additionally to that, we believe Google didn't do a great job with how the whole user experience around it was around it was consummated. As a first version or a first prototype, it was great, but they sold the vision of it and made people go out with that vision and, by far, it was by not there. If you look at what Microsoft is doing with their hollow lens – they are doing it through a completely different angle. They don’t expect people to take a hollow lens, put it on and walk out Son the street and go to a cinema with it. They expect people to use it in their work place and in their homes, and so on. And they also brought the technology a few steps further to a point where you actually have three dimensional holograms that you can interact with, with your gestures and your voice and they stick to the room and it’s not just like a screen that's always visible to you. I think Google Glass was great for what it was but it was a product at the wrong time, with the wrong user experience, for the story they were telling. Do you think at some point that Apple needs to allow third parties to access their voice assistant series? Definitely. Yes. As soon as possible.  Yes, there are two options. Either Apple hires 20,000 developers and has all of them build series skills and run them through their internal testing and all of that, or they open it up and do what they did with the App Store. That’s what made Apple and the iPhone so successful with what it is. Even though the platform isn't completely open it still enabled them to use a bunch of development sources that they did not own, that they did not pay for, while still promoting themselves and while still taking 30% of any revenue that goes over the platform. So, yes, it should definitely be open. Okay, so then why build a voice user interface, after all that we heard before? There are three simple reasons. First to drive innovation: in your market you want to show that you're an innovator. You want to bring to market a first case in using this new technology. You want to increase empathy, which is interesting because what you're doing is giving your brand or your product a voice that you can talk to. You're making it semi-human. You're giving it a persona. And you’re giving that away in a very convenient way to access it. Basically you just start talking to it, you don’t have to call anybody or wait or open an App or get your phone. And you can reach more customers. There are estimated to be over ten million digital home assistants in the US market right now, so that’s quite a number already. Looking at what people are doing with digital home assistants, it’s quite interesting, so two out of three people that were asked were using it to listen to music. So, as you can see, it’s basically transforming the way media streaming is working, and how we consume it. Another important point is news, weather and traffic and anybody providing content should be thinking about how can content be provided by a voice? And what's very important to us is lights, thermostats and fans, so everything in the smart home sector. We see it’s not on the top of the list but 75% of consumers that use this technology say that they are super interested in Smart Home, and it is very appealing to them. It might be a bit too expensive right now. So, how do you build for this? We have a very simple process that we run through. It’s three steps, if we look at Alexa for example: design, develop and certify and what’s important to us is that we understand how users interact and how they naturally interact. We do a lot with interviews, we sit together with users. We do dialogues and play them through. We have them act out Alexa and have them act out the user, and write down everything they hear. We take that and formalise that into basically conversation flows. We build diagrams that show how the conversation can flow, what’s connected to what, and then we develop a persona for the voice. And that’s very important if you speak about how people communicate, especially via language. There's a lot of emotion and there is a lot of like persona in the way we communicate Every human has this naturally, but machines don’t have it, so we have to give it to them. And it has to be designed to fit the brand or the product that is being represented through. In development, what’s quite interesting is that if you use the Amazon Alexa platform, for example, they take a lot of the natural language processing away from you. They give you a very structured format of data that you can process but there are a lot of quirks and a lot of things you have to be aware of. One of the challenges is that you have to generate a long list of things a user could say and you could just have per feature that you want to present or per intention that a user wants to say, like two or three or four sentences, but theoretically there are probably… for a lot of things, an infinite amount of things that you could say to imply that intention. So, the more sentences you have around it, the better. And we basically have a tool that generates a bunch of sentences. If we give it a certain grammar and we built that on top of other conversations that we did with users before. Running through that real quick, at the end at the end we have to hand in to certification so that’s pretty much similar to what the App Store does, not to what the Play Store does and there’s quite an extensive list of requirements that has to be fulfilled for Amazon and for Google as well. If they are all fulfilled then you are launched and in store and everybody’s happy. If there’s not then you have to talk to them and maybe do a few little changes and then it’s handed in. After we go through all of that we start from the beginning After we go through all of that we start from the beginning and think about other cases and other ways of looking at user problems and how to basically iterate on the product that we have built. One question that always emerges is, what about privacy? And I it was picked up before. Privacy in this whole space is super interesting, and a whole and a whole separate conversation and a whole separate presentation in itself, so let's discuss that over beers or I’ll do a separate presentation on that, otherwise we are going to be here until midnight. Do you have any questions? Yes, and no so Alexa does not record everything per se. Alexa only starts recording if it understands the Alexa wake word. Then what it does is take the language that is spoken and streams it to the cloud, until it believes that the spoken sentences over a specific time threshold is reached, so that means it does not record everything around it, just records when you say the wake word. The data - depending on which location you're in - is streamed to the European Data center or the American Data Center. What amazon does with the data afterwards is up to Amazon and they probably copy it to the United States. You work for a digital agency, as I understand, so you build these voice assistants for clients? I work for an invention company and we also build voice assistants for clients. We work with a lot of emerging technologies, voice is one of them, to help our clients – mainly world leading brands to help them solve complex problems and reinvent themselves in the age of digital disruption. What kind of data do you have? I can imagine you have the question that they ask you, but do you have also the entire voice recording of the user? No. We don’t get what the user said. Depending on the platform we get what the user said in text representation. With Amazon Alexa, for example, we just get what was processed by Amazon and states the user’s intention and potential parameters. Whereas with Google, for example, we do get the full sentence that the user said. Great presentation, first of all. Do you think that to some extent that the rise of this voice assistant will change the way that we communicate in order to be more precise, referring to your example of, “It’s a bit dark in here”. You could have said: “Could someone turn on the lights?” That’s probably something that Siri and everybody else would understand. So do you think that somehow it will change how we communicate, at least to some extent? I think it might make us aware of how we communicate more often, but as humans, especially when we talk to each other we don’t put too much thought into how we build our sentences and structures. It's way more It’s way more subconscious than we sometimes think: the way we communicate and the words we use and the way we formulate and the amount of preciseness we put into our sentences. It's interesting. If you see people, especially kids, talking to Alexa the first thing they try and do with the device is talk completely natural. They’re going to say thank you and please if they were raised to be friendly. Even though you don’t have to say thank you and please to a device like Alexa, but they do it anyway. And what comes to their mind after not a very long time of interacting with it is that it works differently than humans, and they start changing the way they talk to it. They start rephrasing and sometimes even have to think for a second how they have to say things so the device will understand it. For grown-ups it’s easier to adapt that language, but I think that right now we are strongly differentiating if we're talking to a machine or for talking to a human. And as long as the machine interfaces or voice interfaces are not going to get way more natural, I don't think that it's really going to change anything with the way we talk to computers. because I should I say the type of interaction, or the way, we sort it in our brains, is completely different. I do believe this will actually change the way we communicate – maybe not us, not our generation, and not even the kids that are born now or five years ago, but the kids that will be born in 10 years or something. The interfaces will be quite adopted, but maybe still not at human level. When they will be more widely spread, basically. They will be existent in every or nearly every family. This will mean that when the kids will hear their parents talk to the device this way more often, from the very early childhood. And what kids do - they just repeat stuff, and that's it. So I believe it will have some impact. I don't know which kind but but I believe so. All right, then to finish off I'd like to show you a real short video about Vectorform, the company I work for, and some things that we've done. It's just a minute long. We're hiring so if you're interested in building the future with voice or anything else with new technologies, holograms, or if you are just interested in building mobile applications, we're a great company to work for. Here are some pictures of our office.   So if you are interested check out vectorform.com. Feel free to drop us a message in the contact form or just reach out to us after the event. Thank you very much. 