 Here's Jakob Nielsen's keynote speech from the Nielsen Norman Group UX Conference in Copenhagen, Denmark. Thank you, Sarah. Thank you, everybody. It's great to be back in Copenhagen. And its actually really nice to have this conference at the Royal Hotel, because it is a masterpiece of modernist architecture. But beyond that, it is also an early example of total design, as we're advocating. The architect didn't just design the building but he also designed the drapery, the carpeting, even the lamps, even the chairs, even  the cutlery, silverware for the restaurants. All this was designed to form a total user experience. Unfortunately they [later] changed the carpets so I have to say "This is not the design." But back in the 1960's, it was early example  of total design, which is what we advocate. So that's a really interesting thing, for a user  experience conference to be in this type of environment. In fact, many of the chairs that were designed for this hotel have since gone on to become classics. You're sitting on some of these famous  chairs; they named the conference rooms after the famous chairs. The "Egg Chair"  is one of the most famous ones. It's actually even found in San Francisco  Airport today. Even though, I have to say it looks better in the lobby of this hotel, which is where it was designed for. But it is definitely a famous site. I want to talk about two different main themes  today which relate to user experience: It is something that is not magic,  it's something systematic. Good user experience comes from  following methodology, from knowing about concepts  and theories and principles, not from wishing for it or throwing things at the wall, any of those types of ideas. At the same time, user experience is not really driven by technology, it's driven by focus on humans. I think it's quite often,  in at least the trade press and trade shows, and other conferences than ours, there's  a lot of focus on the technology. I really want to drive back to the focus on the people, on the humans, because that's what user experience is about.  But there has been a lot of technology, which I'll get to a little later, some examples. There have been a lot of generations of technology — of user interface technology — that we have been through; certainly, in my career, many, many different ones. What usually happens when we get a new user interface technology, is that it's actually used really poorly. The first designs to come out are confusing and terrible. It's really often that a new fantasy means a step backwards in usability. But then, on the other hand, if it, in fact, is a good technology, then eventually we learn how to use it and we get to come two steps forward. So, a new technology usually equates to two  steps forward and one step back. If you do the math, it is one step forward,  in net outcome for the users. I just really detest — why do we always have to have that step back every time we do something new? I think we ought to be better by now, but it's like the lesson from so many years that every time there's something new, we have a step back.  That's one of the things I want to campaign against. I get easily annoyed and it's  something that really does annoy me. I think we need to reliably know things  about what makes things easy. By way of analogy, I want to talk about another  field, which is astronomy, and talk about what causes a solar  eclipse. This is the guy who discovered that. But before him, the thought was that a solar eclipse was a punishment from the gods. Your king had angered Zeus, and as a result, Zeus darkened the sun over your country. This honestly was a really plausible explanation. If you think about it, those ancient Kings  had always — given their lifestyle — had always done something to anger the gods. So, any time there was a solar eclipse we could think about, "Oh yeah, last week the king did  this terrible thing and so that's why." It was a plausible explanation,  but, of course, the wrong explanation. It's a little bit of the same thing with UX. Often,  we end up being given the wrong explanations. Thales of Miletus, he was the first person to predict  a solar eclipse based on actually understanding the mechanics and the principles of what happens, which is that the moon goes in front of the sun, and casts a shadow on the earth. Once you know that,  you can predict the next one, which is what he did. And it happened, right. Similarly, with user experience, once we know the principles of what causes things to be easy or difficult, we have a better time predicting  or driving our design in an appropriate direction. Actually, there is another analogy that matches  with the solar eclipses. Today, the astronomers can predict, to the second, when the next solar eclipse will be, many, many years in advance. And of course, Thales of Miletus he didn't have those precise, big telescopes — no telescope, in fact. His prediction was more like, "On this day there will be a solar eclipse," which happened. Very similar to that, with user experience today, we don't have the ability to predict to several decimal points exactly how much your conversion rate will go up if you make a certain design change. We don't have that accuracy that astronomers do,  that's a more advanced field, since they started 2,500 years ago,  and we only started maybe 50 years ago. I certainly wasn't the first, so about 50 years ago  maybe, people started studying in this field. That's still very new in comparison to astronomy. Today, we have a rough, coarse understanding of UX so we can't predict it that accurately. But that doesn't mean we know nothing. We're  beyond the state of thinking Zeus is angry at us if our design fails. That's not why. I want to talk about some of the things  that we know about user experience, and the type of things we know. Many people when they give these types of keynote speeches, they like to illustrate their examples with photographs they download from Flickr;  that's a common thing these days. I just don't like to be like all other speakers, so instead I'm going to illustrate my examples with old 19th century paintings from Denmark. Here's my first painting, and what does this show? Rhetorical question, it obviously shows a bunch of  grapes. This is partly because before photography became common, it was the number one job of an artist to paint things that look like what they were. If you're going to hire Rembrandt to paint your uncle, it'd better look like your uncle. Similarly, if you have a painter painting grapes, they should look like grapes and look delicious, like, "Ah, I want to eat that juicy grape!" and that's what he did. But beyond that, one of the other reasons you immediately see what this is, is simplicity. He didn't do a lot of other stuff. I think a lot of  websites in particular, but also some other interfaces today are not like this painting.  They're like an angry fruit salad. All kinds of things bunched together, mushed up; not  necessarily as appetizing as one single juicy grape. That's one of the types of things we know about user  experience: a really broad, broad principle: simplicity. If we do fewer things people are more likely to understand the things that we do. That's what I mean when I say, "We know things." We do know that simplicity works. Let me show another example of a painting.  Here we have a nice group of farmyard animals. by Theodor Philipsen. This is a naturalist painting which means that if we look at the duck, it looks like a duck, or if we look at one of the cows, it looks like a cow. So far, so good. But what if we put the duck next to the cow? Man, that is a big duck! It's like Attack of the Killer Ducks! This movie hasn't actually been made yet, but that's like the poster for that movie.  But no it's not actually an attack of the killer ducks. The duck is just physically, graphically bigger because it's in the foreground. This is an example of a painting technique called, repoussoir, which means put an object in the foreground that pushes to the back the rest of the picture.  That creates a sense of depth, even though, obviously, a painting is a two-dimensional object, we all know that. But it creates a sense of depth in the painting. This is similar to a more specific design guideline as opposed to this broad principle of simplicity. The equivalent for web design would be things like: if you're going to do search, you make it a type-in box, a type-in field. You put that search box in the upper right hand corner. You put the logo in the upper left-hand corner. If you follow those specific design guidelines, people will be better able to use your site: they will focus more on your content, your products, your offerings, your message, and not be confused about how to operate this website. Now we know about this idea of putting  something in the foreground, but not just "put a duck in the foreground," here's a city painting now. What is this broom doing in the lower left hand of this picture? Man, that's repoussoir! It's up in the foreground  and pushes the rest of the picture to the back, and creates a sense of depth. So you see this looks like it's actually from downtown  Copenhagen even though it's just a flat picture. These guidelines do work and they  can be used in a lot of different contexts. Let's look at a very different one, these are naturalist paintings where they try to give it a three dimensional sense by this repoussoir technique. This painting, on the other hand, doesn't have that — doesn't have that duck in the foreground. If you blur it a little bit, you'll notice that there's three stripes of color. That's this painting, just three stripes of color, no three-dimensional effect at all. It's the sky, the land, and the water.  And yet, we can interpret this immediately, even though it's not as naturalist as the other paintings. We can tell the sky is above the land, and  we can tell the water is in front of the land. But how can we tell that? Because honestly, if you look at the graphics The water could be below the land, and the sky could be next to the land or something. The land could next to the sky. The reason we can interpret this painting despite lack of these 3-D effects is because of another UX principle, which is consistency with the user expectations in the real world. You guys have all been out in nature, you've seen lakes and fields and the sky above the land, and things like that, many times. So, you know that when you see something like this, the sky is above the land, you know that, and the water is in front of the land, this way. Because of our expectations, as long as they  follow our expectations — unlike, a very modern painting that's weird —  we can immediately understand what it shows. Here we'll go back and we'll look at an  older painting style, again, a very realistic style. This is a painting from London.  You can see St. Paul's Cathedral here, the monument to the Fire of London; and you can see,  smack in the middle of the painting, the London Bridge. Those of you here who are from London, may say now,  "Well that's not how the London Bridge looks." And it's true, that's not how the  London Bridge looks today. The bridge in this painting is, today,  in Arizona, United States. It's a tourist attraction; it was moved brick-for-brick  to the US and built back up in Arizona. But, back when Holger Drachmann was painting this painting in 1874, the bridge was still there. There's been  a London Bridge in that exact spot since the days of the Roman Empire,  so almost two thousand years, there's been a London Bridge  that's been there at the same spot. In those 2,000 years, many, many different designs of  the bridge, and many, many different engineering technologies were used to construct  ever bigger and more solid bridges, for 2,000 years but always the same place. The reason for that is that's the place you'd want to cross the river, and that's a place where different roads go to, so it's convenient to have a bridge there. This is my example of a more broad principle of user  experience, which is the persistence of infrastructure and social organization. If you're designing something  for big system or big organization, like maybe a big city or a big company,  something like that, you have to really figure out the way that  that organization works, its existing systems and infrastructures, and way of working and way of thinking. Because if you try to disrupt too much  of that, you may quite likely fail. Or conversely if you're designing something completely  new, we have to be aware of our responsibility. We are often creating some of that infrastructure  and thoughts that could be there long after our specific designs or individual  screen designs are long gone. But the infrastructure, the systems,  all the processes that we build up, they can remain for very long time. Maybe not 2000 years, in the case of your  next project, but you never know. So, think about it. That's why we've got to study these  things more carefully when we do this type of design. I want to talk about a number of different user interface  technologies that we've been through. They mostly follow this life cycle that I'm depicting here. So we start from the very top: that circle is called the Magic Bullet. With a new technology, people are initially very enthusiastic and thrilled, and happy about this new technology being the solution to all our problems. If only we could speak to the computer, there would be no usability problems, right?  Or if only — think back 20 or 30 years — If only we could have windows, a graphical user interface, there would be no usability problems. Of course, we know this is not true.  In fact, the opposite is true, because when we move to a new user interface technology usually the first designs to launch are technology-driven designs by people who are so enthusiastic about this new technology, that they forget about the humans.  The iPad was one of the worst examples of this. Almost all of the first launch apps for the iPad were completely wacky and weird. When we tested it, people couldn't use them;  but this is true for many other technologies as well. We get this problem with technology-driven bad design and then what later happens is what I call a UX panic, which is, we go, "This was supposed to be so good, and it's so bad! What's wrong?" Now people like us come in and say,  "Okay, research why it doesn't work. Let's do some studies, right? Figure out." And after that we can move on to the next stage. So, now we know what not to do.  Let's now figure out what works and what to do. We research additional or useful design options.  All that will then crystalize into best practice guidelines. Once we know how to design for this technology,  by now we're up here because this takes a few years to go through the cycle. But by now it'll be like,  "Now it's old technology." But at least we know how to do it,  how to use it, how to design for it, how to make good products with high  usability and great user experience with that generation of user interface technology.  By then it's usually time to go through the cycle again because there's going to be some other new thing that's popped up. What I would really like is if we could bypass this right  hand side of this chart and not have all the bad design. Why do we have to suffer that? Let's talk about some examples of some specific user interface technologies. First of all, personal computer, graphical user interfaces: they're in that last circle stage of being old technology but we really know how to do it. We have a lot of insight on how to make a good graphical user interface, how to make a good PC application, all that. They've been through that, there were definitely a lot of bad applications in the old days for very confusing PC systems, but now we know how to do it. For the web, we are  almost there. I don't think we're quite there yet, but we're at that stage, where the best practice  guidelines have definitely been defined, we have thousands, literally thousands of web design guidelines to tell us how to make a good website. Unfortunately, not everybody actually follows those. They haven't accepted that the web is an established design medium. We still get some some weirdo sites that come out from time to time. Mainly, we do have the best practices known, so pretty  soon it'll click over into an established technology. Agile development, on the other hand, we're not there yet. We are at this stage where we know that if we just do Agile the way it  was envisioned for programmers, we will have bad user experience as a result. The original idea was great for programming things,  for implementing things, but not for designing things. That, we know, doesn't work, and we've found out why.  The bad things are well-documented. We have some insights already now  into how to do better. In fact, we had an entire one-day seminar today about how to do better Agile user experience, with a lot of good advice on how to do right, but that said, I don't think we have it 100% nailed yet. I don't think we can claim to say it's a "solved" problem, but we know a lot about how to make it work. The same is actually true for designing for mobile devices. We've been through that phase of having a lot of bad design, finding out why those designs are bad, finding out why they don't work. We have a number of good apps, good mobile designs to look at; we're getting to the point where we're sort of getting ready to say, "We know how to make mobile work." But I don't think we're quite there yet. One really striking example of that is the entire area of responsive design, which quite often means bad design; because you can't really have exactly the same on a big screen and a small screen. So, we have to figure out how to make the design work on all different screen sizes. We can make it *display* on all different screen sizes, that's a technology issue, we know how to do that. But to make a good user experience on different  screen sizes is a much harder question. We're in a stage where we're trying to figure out:  what are the things that trip us up when doing responsive design.  That's a bit of an earlier stage. So, by way of illustrating that, let me return to my example with the farmyard animal painting. This guy really loved to paint cows. Here's another painting by the same guy. What's the difference between these two paintings? One thing you might notice is this painting has very long shadows. That might lead you to think that  it was probably painted sometime in the evening maybe, like getting closer to sunset,  which seems reasonable. But that's not really the big difference between these two paintings. You will never guess, because I'm only showing you photographs of these, and when you only see photographs, you don't get what's really the true striking difference between these two paintings.  You're going to see them for real. One is huge, and one is small, and size matters. They're really different experience-wise, to see the big painting and to see the small painting. The last thing  I want to talk about is artificial intelligence. That's in that early stage, it's being very hyped up right now, and people are really thinking that it's going to be be our salvation for user experience. I think it could be like this painting of Mount Vesuvius. One possible future is a beautiful view, that you can enjoy while you're on vacation, like this woman is doing. Or it could erupt and blow up and bury our city,  like it did with Pompeii. Mount Vesuvius has those two aspects to it, and I think AI is the same. It could definitely have some good things to it, and it could just as well create a lot of nasty trouble for us. What is the artificial intelligence user interface? What is going to be the user experience of that?  I think there are two different ways of looking at it. One is, as I've been saying, that it's the same as the other things I've been talking about. With every other technology, people are very thrilled about it, but it's not going to be as good, and it's going to be a lot of trouble, early on.  I do believe that. I do believe that AI is like everything else, because we  see this so many times, history will repeat itself one more time also. But there's another way of looking at AI as well: that it is, in certain qualitative ways, different than the  other user interface technologies that we've had. I'd like to discuss those differences. But let's remember the first point: that people who are driven by technology enthusiasm are going to over-hype things like this. It's our job to be the reality check and say, "Well, remember the people." So let's talk about AI. I'm going to start from the lowest level of the interaction design which is the lexical level of the words that we are  communicating to the computer. The big difference between AI and every previous user interface technology is that most of these AI systems are recognition-based user interfaces.  By contrast you could say, let's say a graphical user interface, if I click an icon, I click that icon. There's no real debate about it. I click the icon, that's the icon I click.  End of the story. But in AI, if I speak a word, or if I handwrite something,  or if I make a gesture, one of these things, there is a debate about what I did. Which word did I say,  what word did I write, which of the different command gestures did I make? It's going to interpret or recognize what I did, but it's going to do it wrong some of the time. Whereas, if I click an icon, I click that icon,  and there's no discussion. The fact that there's going to be mis-recognition, we  have to have a way of recovering from mis-recognitions. That adds a layer of complexity to the interaction  design, which is, right there, a disadvantage. This will always be with us, but it's certainly getting better and better, and it's getting better at an astonishingly rapid pace. If I refer to my example, "Google Now," it got five times better in the last four years. This is unheard of.  Four years ago, they mis-recognized 25%, one quarter of all words were mis-recognized  when you try to say something to it. Just a few weeks ago, they announced they were down to about a 5% mis-recognition rate. They cut their error rate by a factor of five in four years; that's hugely impressive. I think it's kind of easy to predict that improvement  will continue, not exactly that fast, but it will definitely continue, down to one percent wrong, and then, maybe some years later, a tenth of a percent wrong, So, one out of every thousand words wrong. By then I think we can say, "Okay, it's still not perfect, but it's pretty good to perfect." Beyond that,  computers can be better than reality. It's not just a matter of the computer being as good as  a person at recognizing speech. But it can also do things like accommodate accents, which can be difficult for people. If you're used to hearing words pronounced in a certain  way and somebody pronounce them in a different way, that could definitely cause a problem for a person, but the computer has been taught to recognize all these different accents. One day, they'll even teach it  how to recognize a Danish accent. Definitely, there are possibilities for improvements here, and things can be better than reality. I think it's not all that bad. Let's look at the next level up, which is the syntax levels. I can combine my words into a command or instructions to the computer, and these are the questions: what can I say, when I say it, and here we have certainly already seen some issues. Most of these current systems have a very awkward way that you have to phrase things.  You have to work on its rules as opposed to human rules. You have to say things like "OK Google," or "Hey Siri," which annoys me to no end. That's not a normal way of acting. I don't go around and say, "Hey wife!" as a way to talk to  my wife. Let's not do that, that's very impolite. Even with we get over those types of annoyances, there's also the deeper problem of reformulation, which is that if I state my problem in a certain way to the computer, it's not always the right way. Sometimes I may want to get alternatives or variants of what I said. That's something quite well-supported in graphical user interfaces because a computer can show me not just, "You asked for this, but here's some related items that you might be  interested in as well." And this is really well-suited for scanning over a two-dimensional — particularly if you have a big screen,  you can really do a lot of great work there. It's hugely improved conversion rates  and sales, by the way. So that's good. When it's in speech or any kind of linear user interface,  it becomes very awkward to present these kinds of alternatives.  That's one more thing that's an issue here. People have to adapt to how the computer needs to work, which is contrary to our basic ideology. If there's any one ideology we have in user-centered design, it's that technology should adapt to the people, and the people should be like they are and left to live their lives. So that's very, very unfortunate. There's also, right now, much initiative to put a lot of these speech recognition systems into cars. and put apps into cars, so you can manage your calendar and things like that while you're driving, until you smack right into something. There's a huge risk of driver distraction with those types of interfaces. I think it could they could easily end up  killing large numbers of people. One of the biggest causes of traffic accidents these  days is people being on their mobile phones. When I say, "being on it" I don't mean holding the phone.  Hands-free is just as dangerous. It's not a problem of whether your hand is on the phone or the steering wheel,  you have two hands. But what the problem is is the cognitive load, the cognitive distraction.  If I'm talking to somebody who's not in the car, my mind is kind of halfway out of the car. So  your reaction time goes up, your accident rate goes up. It's about as dangerous to have to a phone call as it is  to be drunk when you're driving. If it's that dangerous to talk to a person, just imagine how dangerous to try to manage your calendar. The cognitive load on that has got to be really nasty.  Those are the issues of that. Generally speaking, there's a lot of talk right now about conversational interfaces. It will be dangerous with cars, but even with other  systems I don't think they'll necessarily be that good. One of the reasons I want to talk about that is WeChat. WeChat is the most successful system in China these days. China is the world's number one online market. Of course everybody else is very excited about what works in China, and they want to learn from the Chinese. That makes all sense. But they're learning the wrong lesson from WeChat. WeChat is called chat for historical reasons because a long time ago that's what they did. But today, that's not why people in China use WeChat.  So we did some research in China with Chinese users on how they use WeChat,  what they think about WeChat and so forth. So, this is this one example of a mental model that we had the users draw. One person's mental model. You can kind of see the chat as a peripheral part of this mental model. What was more central not just for this person but for a lot of users — What's more central was things like the ability to scan QR codes, a version of bar codes, integration with the physical world — which is a really good thing that WeChat has achieved — or also other integration abilities that they have in WeChat like very simple payment across both the physical and online world and person-to-person payment. All these things are very  smooth and nicely integrated in WeChat. Integration of useful facilities, integration with the physical world. Those are reasons people use WeChat. The way they use it is quite often by using menus or graphical user interfaces. Very rarely actually by using chat.  There's a misconception that WeChat is a conversational user interface. Moving up one level to the task level which is, I am trying  to get something done with the computer. There is a truly big difference here between AI and traditional systems. Because traditional computer systems... they do as they're told.  This is both good and bad. It's good that the computer does as it's told but it's bad  because what I tell it to do is not always what I want it to do. That's a problem if people are not exactly understanding computer facilities and so forth. People will ask for the wrong thing quite often.  But that said, it's still a comforting feeling if a computer does what it's told. In contrast, with AI,  the computers does as it pleases, or as it thinks will be good. The computer will get agency in these AI-driven systems. That raises a new aspect of user experience that we otherwise have not looked into. Some of the simple examples of this that we always see now is these examples of — let's say somebody was on  Amazon.com searching for things like pregnancy books, baby books and stuff like that. And a  year later, they still get pregnancy books recommended. I think they actually fixed that one by now, but that was, for a while anyway, the situation. The thing is, the AI has to understand the bigger context of use, not just the specific simple things. Things like, if I buy one book, I will buy more similar books. But maybe not generalized for certain types of problems, certain types of issues. Let me show you another painting here. This one looks a ship in the middle of a city. Very odd, doesn't make sense. Now that's not actually what it is,  but if we recognize each element in the painting, we can recognize the ship, we can recognize  buildings, so our first thought is, "Man, a ship in the middle of downtown." No, what happened was in the good old days they used to have these docks where they would build ships, and around those docks were warehouses. So that's what's happening. You can see it's actually a ship that's  just being built. It's almost finished, but in the process of being built, and there are warehouses around it. So understanding context is crucial, for interpreting this picture correctly. Not just recognizing each element;  that will lead you very much astray. Your first thought would be wrong.  Finally, with AI the society level. A really broad based question here. So I have here a list  of what today are the most famous AI products. What characterizes this list is they're all from huge, truly huge companies. That's because it has turned out that the best way to do artificial intelligence is by machine learning from immense data sets. It's so huge and immense and they've merely collected it by various privacy violations. So, then these big companies  have what other people don't have. I think that is one of the really biggest  challenges and problems with AI. We're going to be restricted to being driven by a  handful of huge corporations. If we think back to the previous revolutions in user interface technology, they really had the opposite effect. The PC revolution meant that now, thousands of companies could could create software and could make applications and sell applications. In the past with the mainframe era, it was only a few companies, like big iron type organizations, that would make applications. The software back then was terrible as a result. With PC software, thousands of companies made PC software, they were able to do that with this new technology. Sure enough, many, many PC applications had bad design, definitely. But also many had good design, and had a lot of  interesting innovations in user interfaces. And did new things and empowered a lot more  average, normal users to be able to use computers. The PC revolution was a huge advance in usability and similarly for the web revolution, even more so. Now it's not just thousands of companies that could  create PC applications, but millions of companies could create websites. Basically, any company in the world could just put a website out any time if they wanted to. They could create new design  ideas, new features, new services, new products; they could do a lot of different things. And again, I want  you to understand, so many bad websites. Particularly, in the dot-com bubble days. So many bad  websites but also so many good websites. When you have millions being attempted even if many are bad, there's gonna be a lot left over that are good. That was the beauty of those two revolutions in user interface technology. That broadening up the base and broadening up the number of things that were done and services offered, and, generally speaking, diversity. So that it would cater to whatever you wanted. It wasn't just like "This is what you do." It's really a Big Brother society we may very easily get into if this AI system turns out to be the the only way it's done. However, in concluding, I'd like to remind you that today is Wednesday, And Wednesday is Woden's day, or Odin's day. And Odin was the king of the gods in the religion of my bloodthirsty ancestors, the Vikings. Just as tomorrow is Thursday, which is Thor's Day, or the thunder-god day. Anyway, let's go back to Odin. So today is Odin's Day.  Odin was the king of the gods, but he had many names. He was also called the Allfather. Odin was also called Old One Eye, because he only had one eye. Why would the king of the gods only have one eye? Because he had sacrificed his other eye to gain wisdom. This story tells you, that the Vikings are not as  bloodthirsty as the modern television shows like to make them out. Look at television "The Vikings" and  they're just like killers and they rape and they pillage, and are really nasty guys. But they actually really respected wisdom. They said the king of the gods had sacrificed his eye to gain wisdom. So wisdom is something that we, in our field, gain by observing the users. That I think is the really core thing in user experience: the user. Go back into people, go back to the humans, observe the people, observe what they do: let them use different designs, see which work, which don't work. That's the way we gain our principles, both specific design principles, and our broad interaction principles as well. Both of those two, which we can be nice and call wisdom — our UX wisdom comes from observing users. I think it's kind of very happy that we can gain our wisdom by observing users, by looking, and you do that, you don't even have to sacrifice your  eye, and you'll be all good. Thank you very much. 