 Good afternoon. Welcome to this talk. Today we are very happy to have Yuhuai Wu from Toronto to give a talk on some recent work on Deep Reinforcement Learning. Yuhuai is an intern at Open AI in the summer. So before he goes back to Toronto to start his third year PhD, we distracted him a little bit and invited him to come to Redmond to visit for a few days. And Yuhuai has a very amazing record of publication in his two years PhD, has five NIPS papers, and some other ICR papers already. And this work is one of the papers he's going to present at NIPS this year, so I hope you enjoy it.  All right, and thanks for the introduction. All right, so today I'm going to talk about some recent work that me and my collaborators have developed for Deep Reinforcement Learning using some previous developed method called K-fact. So it's a joined work Elmon, Shawn, Roger Gross, and Jimmy Ba. All right, so, I guess most of you have some experience with training deep neural networks. Well, but most of times on these days, they are still trained using the first-order method. Some variant of stochastic gradient descent such as Adam, Arms prop [INAUDIBLE]. But these first-order method has suffered from this well known problem which is you usually just go bouncing around, a lot in the high curvature directions, but making very slow progress on the low curvature directions. And so this is caused by the curvature is very badly conditioned. So before I talk about my work, I would just give you some background. So for the background material, I think, the very important notion is the notion of a norm in optimization. So basically gradient ascent is just a special case of steepest ascent. So the steepest ascent is trying to find a direction that give you a maximum increase in your objective functions, sorry, steepest ascent, subject to the constraint that the norm of the change is less or equal to one. So there comes this notion of the norm, and this term here is very important because of different methods could be derived from using different norm, using the same framework, which is steepest ascent. So for example, gradient ascent is just a special case, where the norm is chosen to be the Euclidean norm. But this norm, it has problems, because it depends on the underlying parameterization of the model. So basically what we are trying to do is that we want to optimize inter spacial functions, but we could not do that directly. We have to rely on some parameterization of the models. So basically the weights and biases of the neural networks. So for example, but this has some problems, because for example, if we want to use parameterization for the sphere then we create some distortion in the distance. And what we actually want is to do the optimization on the space of functions. So basically compute the gradients, and do optimization on the globe, but not on a flat map. And natural gradient provides a way of doing this. So natural gradient provides a way of doing the optimization directly on the globe. So, basically, the analog of the globe and the map is, for the neural networks, is that we want to define some sort of distance metric on the spatial functions. And the natural definition is basically you first try to define a disseminating metric on a output of the neural networks. So basically, if I want to measure the distance of two functions, I look at for a given input, what's the difference of the output for these two functions, and then I integrate over the input domain, according to it's input distribution. So basically, for example, here, I have two, one variable function, FMG, and if I want to measure their distance basically I try to calculate the square. So, for example, if I use the Euclidean L2 distance, then I will just calculate the difference between the two, and integrate over the input distribution, oops, and this is just this D, f and g here. But this distance is usually, this nonlinear function D here is very hard to deal with. So in most of time we just use a second order Taylor approximation to this function, to this metric D here, and what we get is this PST matrix G which defines a norm on a space of functions. So this function G here is called generalized Gauss Newton matrix. And so if the dissimilarity matrix that we use for the output space is the L2 norm, then this G here, voiced onto the Gauss Newton matrix, as we all know about it. Any questions at this point? All right, so basically we define a metric on output space, and then we take expectation over the input, and then we take the second order approximation to that metric, and we got this generalized Gauss Newton matrix. So here I'm showing one example of this generalized Gauss Newton matrix, which is using the L2 norm. But for the function of which we really care about which is the neural networks, we actually have a more natural metric on a output space. So the neural networks, most of time they actually define some sort of distribution. So in the case of classification it's a distribution over the possible labels, but for in the case of like RL, Reinforce Learning, it's a policy distribution over the possible actions given the current state. So in many cases it's some sort of distribution. So for distribution it's very normal to use diversions to define a distance between two distributions. And so basically, if we choose KL-divergence in the previous formula, so for the row here. Then, again, we do the sigma order approximation to that, and what we get, eventually, is just a Fisher matrix. So the Fisher matrix is just a matrix that's, it's a second-order approximation to the KL-divergence. It can be also defined by taking the covariance matrix of the log-likelihood derivative. So this is an equivalent definition. Okay, so basically these Fisher matrix define a norm, a distance, on the space of distribution. So why do we like this? Well, basically Fisher matrix, or Fisher norm, is independent of parameterization. And as we can see, usually the Euclidean matrix, or other matrix that we use, is dependent on the parameterization. So here is an example, so if I have two normal distribution, p and q. And I write it in terms of two famous parameterization. One is the mean and variance parameterization. The other is using the natural prime interest which is also called information form. Then if I use, if I just calculate the Euclidean distance between these two distribution then we actually got different distance. So the Euclidean distance between p and q is not the same as the Euclidean distance between p and q, it parameterize by information form. However, if we calculate the KL-divergence, which also corresponds to the Fisher, it's actually the same, because it defines in a probabilistic sense. So you always take the integral of p of x times log of p of x divide by q of x. So this would be always the same. So yes, so we do prefer some metric that's independent of the parameterization we use because we want to move on the space. We want to optimize the function on a space of functions. We don't want to rely on any oppetry of parameterization we use to. And basically, we want to optimize on a globe, not on a flat map, to avoid any distortion of the distance. Okay, so yes, so as a illustration here, basically this is the Euclidean gradient we got, and so for optimizing the h data here, the green line which is the underlying parameterization. So if we use the Euclidean distance then this is the direction we get, which is perpendicular to the online contour. But then if we use natural gradient, then you will point to a direction which is more meaningful but is independent of underlying distribution. So it give you a more meaningful directions. Okay so, but of course, there is many problems with natural gradient. The biggest problem is about how to calculate this Fisher matrix. Well, the Fisher matrix is, have dimension number of parameters times the number of time parameters. And usually for the modern neural networks, it could have tens or millions of parameters. For example, if I have just two layer neural networks, both of them have 1,000 units, then I would have one million parameters. And then the Fisher matrix is just one million times one million, which is intractable to calculate. As well as it's inverse is also intractable. Okay, so the previous methods uses, there have many previous methods to solve this problem. So algorithms like Adam, RMSProp, they're basically trying to approximate the diagonals of the Fisher matrix. They indeed have very little overhead, but most of times we don't see much benefit we can get from them, compared to standard SGD with momentum. So another method is like, iterative methods like Hessian-free optimization. What they did is that they, at each update, they perform many steps of conjugal gradient. But that also takes a very long time. And also, they have this problem that you can only use the current batch to calculate the approximation to the Fisher matrix. You can never aggregate the history information to calculate the Fisher matrix. And this point, I will talk about this later on, but this is a very huge point for doing optimization in RL because we care about simply efficiency. And this in particular would cost you many samples to approximately convert your matrix. And also some subspace methods. They also can be very memory intensive. So, what we want, what we actually, so this is a previous work called K fact, which he published in 2015. And what they did is that they characterized the problem of calculating K fact, I'm sorry, to calculate the Fisher matrix as some sort of probabilistic modeling problem. So essentially Fisher matrix is the covariance matrix of the log-likelihood derivatives. And we can use some tools in, for example graphical models, to model this probabilistic model of log-likelihood derivatives. And basically sample some gradients here. And try to model what's the covariance matrix of this distribution. So instead of trying to model the data, we want to model the computation here. So the gradients is, we characterize approximating the Fisher matrix as some probabilistic model of modeling some model of the computation. Right, okay. So basically, we want a distribution that can be compactly represented. And also it's inverses can be efficiently computed. So we make some assumptions here. Basically, we assume that the log-likelihood derivatives we tried to different layers are independent. And also, the activation and its activation gradients are independent. So I will show you what I mean by this. So, basically what we do, what K-FAC does is that you approximate the Fisher matrix using a block-wise approximation. So, basically instead approximating the entire matrix, I approximate the Fisher block for each of the layer. So, yeah, so this is very specific to the neural network architecture. So how do we do this? Okay, let's say if we have a matrix W for layer L, and it's derivative it will be just the outer product of the backpropagated derivative which went to the output layer and deactivation. So this is just the backpropagation equation. And the way of calculating the Fisher block is that we, again, do this expectation of the other product to log derivatives. And if you substitute this guy into here, then It will just be some other products of this term, and this term. And then if you group them, if you switch the other, which is allowed for the terms of products. Then you will get the coding of product of this matrix and this matrix here. So another approximation we do is that we approximate the expectation of two Kronecker products using the Kronecker products of two expectations. So this is how we do it from here to here. And the reason we do this is because we can use some identities to calculate the update equation, which I will show later. But basically, from here to here we are using the assumption that the activation and its propagated derivatives are independent. And these two matrices are here I'm showing you just how we calculate the block feature matrix, yes?  What is the FAC operations?  The FAC operations is basically just, so matrix here. So the grid in here is actually a matrix, right, refers to a matrix. And then the FAC means I'm just flattening it out as a huge, yeah. Not it's just flattening it out.  Yes, yne long vector.  Yeah, one long vector, yes. So because I'm calculating the future matrix, that's parameter times parameter size, yes?  In what sense is that an approximation expectation of a Kronecker, instead as a Kronecker curve expectations?  Yes, yes.  But then it goes to [INAUDIBLE]  So it's not something that, I mean the underlying parameters of my problem have to satisfy some some [INAUDIBLE]  Okay, so this is just an approximation, which is wrong, that they make, but actually works in practice, yeah.  In practice, how you calculate those expectations, samples are input of-  Yes, yes.  Do empirical-  Yes exactly, yes, yes, so this is all Monte Carlo [INAUDIBLE].  So every state in order to approximate the feature matrix you need to run, say, [INAUDIBLE] a few times.  So in practice, we actually just use one sample, and that does suffice, yeah, yeah.  So you use one sample for average or not?  Sorry, average over the input.  Average over the input?  Yes. So basically-  [INAUDIBLE]  Input distribution.  Batch, you add it to the batch.  Yes, over the batch, yes the current batch, so that is a good question. So basically when we do the expectation over the input distribution, basically we sort of trying to aggregate historical information into this. So basically what we do is we keep a moving average of the visual statistics and to make it more accurate.  But the problem, if you just took one or two samples, the problems. That's your feature metrics will be just rank one or rank two. It is not [INAUDIBLE] vertical anymore.  No, no, so yes, it's rank one for one sample.  What's that called?  But then if you aggregate the history information. So basically if you do a moving average then that's not the right one.  So every time you copied one sample but in fact when you estimate a feature matrix, make sure you do average over the history.  Yeah, yes, so we keep a moving average of that, okay, besides that we also sum over the inputs, right? So that makes it also not known by quantity, yeah? Yeah?  I have a quick question. I'm a bit confused about the subsequent L. The different Ls, are they the same L, the normal L and also the-  Okay, so this L, I'm sorry, yes. So these are the same L, yes. And also the W should be WL. So the Ls W, so yes, the W for the Ls later. The matrix W for the Ls later, right. So I mean, I think the reason we want to do this approximation is given by the next slides. Again, so the current feature matrix is using this kind of approximation. It's a 0 on the off diagonal, but the block is approximated by this else block feature matrix. And the reason we do this approximation from the expectation to the products of expectation is because we can use these two identities to write the Fisher inverse times a vector gradient to be this form. So basically, first of all, P and Q is just the side on gamma. And instead of taking the inverse of this huge matrix. So even this block, we do the block approximation. But each block is still huge, right? Because you have the number of parameters of the weights, times the, of the weights at alt-layer times that, so it could be still huge. But then, if you use this Kronecker approximation to it, it could just be the number of activations times the number of outputs. So this dimension, the dimension of the site and the gamma is very small. So instead of taking the entire inverse, we take the inverses of these two small matrixes. And we can just calculate the inverse, multiply a vector by using this identity here. So this is the mean computation efficiency gain that we get by making this assumption here. By making the assumption of expectation of the Kronecker products, equal to Kronecker products of the expectation. Is that clear? So again so this guy here, each block is number of parameters at layer L times the number of parameters at layer L. But these two small matrices P, for example, on the side here, is actually number of activations times number of activations. And the lambda here is number of outputs times the number of outputs. And number of outputs times the number of outputs is much smaller than the entire feature block matrix. And taking the inverse could be very computational inefficient in this way. Yes.  I have a question about another slide. So why did you assume fully connected network? How did you use that assumption?  So you are saying, so this is only for fully connected networks.  Yes.  And yes, okay so there are ongoing work of developing it for there are-  Are these revisions of these formulations? Can they, are they-  So I'm presenting to you the first original paper which only applies to the fully connected network. But then there are a full author works, that basically address Are the same for colvolutional networks,recording new networks.  Okay.  Yes, or you were asking other questions or?  Yeah, I'm asking,so you use the [INAUDIBLE] assumption in the [INAUDIBLE].  Yes, yes, yes.so three fact is meaning for new netwoirks.  So this approximation here only works for new networks, so basically the flow of a deep learning tool.  I think he's asking if you change to CNS will the equation still hold?  So for CNS, you basically have sharing parameters, that's the only difference between here.  So is that just a special cases equation or a different equation?  This  The CNS version of that statement.  No, so the CNS would have a completely different form. It will just be a sharing parameters, but then you have to aggregate the derivatives all together. But here I am presenting only for free forward okay, so yeah. So basically-  So quick question, so if you used a special version of CNS with that wave sharing, then this formulation still holds?  Yes, yes, yes, yes, yes. Right, I mean so essentially this is, okay, so this approximation still holds in CNS as well. So the computational thing we got is also code for CNS. CNS, the only question is how do we do samples? How do we approximate different blocks, essentially it's a sharing parameter, and then you have lots of different copies of the same derivatives, yeah.  Can you just treat those different copies as samples, different samples?  Yeah, so basically that's what it did in the CNS papers, so, but, yeah, you should check it, I would give you pointers later on. Okay, so this is the approximation that they make and this is the K-FAC updates given off at the final K-FAC updates. Basically it's just the products of three matrices. Although, it's still expressive than the first order method, but I will show later that how it compares to first other in walkout clock time.  It's only 22, 25% of increase in walkout time.  Yeah.  So here you are explicitly computing the inverse times the vector, right?  Yes.  I understand you have made it very efficient by the factorization as options.  Uh-huh.  But then there's this like you had it in your related work section like the [INAUDIBLE] all that you can do. [INAUDIBLE] and then you just utilize the fact that you have an efficient way of computing a Fischer vector-  Products.  Products, right? So how does this compare visually [CROSSTALK] experience in the net optimization?  Right, right, right. So yes, as I said, there are two ways of doing this. So one is using the hashing free, right. What the hashing free method at each time stamp they still needs to do a lot of computation. So they have an inner loop of computing the and that is very slow as well. So for example, if you use TRPO, you can never scale up to like a modern neural networks.  [INAUDIBLE]  Yes, but one integration is like a for problem and backward problem, it's. It's still very expensive to do it, but for here we do this small matrices computation, inverse computation, and then you can do it asynchronously, so that wouldn't give you too much over. And also another reason that we prefer this is because I'm also doing a moving average of computing these statistics. But what the appeal is doing is they are not explicit constructing the Fischer matrix. So they can't not aggregate the history information. So what they can do is, given the current batch, I approximate the Fischer vs vector update. And usually, they require like a ten thousand or even two thousand, batch size which hurts the simple efficiency basically. I will show you the results later on, how it compares to TRPO. Any other questions? So right, so I guess this is all some backgrounds about K-FAC and also, so K-FAC can be also applied to CNNs, or RNNs, so, this has published and ICML 2016 and Kronecker factors for RNNs are also explored by Jamie and James. And I think they will release it soon, but I will not discuss them here.  Can I ask a question?  Yeah.  Just making sure, in the previous slide, you're still specifically inverting the small matrices.  Yes.  Okay.  Yeah, we're doing the inverses of the small matrices.  Okay.  The matrices again, it's just number of activations times number of activations and it's usually just very small so.  Sure.  And yeah, right, yeah.  So that you invoke the matrix, so that you can multiple that inverted matrix to another vector  Yes.  Can you approximate that? The matrix vector multiplication with the inverted metrixs or even computing matrix.  So you're saying, can we do direct fisuring with vector product?  Yeah, indirect matrix with the product, the matrix.  Right, so the future of matrix product doesn't do that. The Fischer vector for that which uses four prompt differentiation, and then back to more differentiation, but that requires explicit gradient computation graph. But if we made these assumptions that we could not utilize these computation graph. Cuz essentially we are changing the computation graph. And another thing is, as I said, we actually prefer explicitly to constructing this Fischer matrix in order to aggregating the historic information over time, yeah. Okay, again so, even if we made these assumptions, the K-FAC approximation are still invariant to reparameterization, so these are some, so basically, for example, if I have some reparameterization like pointwise, I find transformation, then the K-FAC would give you the same update. Whereas the SGD updates would be different, compute different functions. So these are also some theory that's presented in the first K-FAC paper.  Pointwise like every court, so pointwise the same as quarterwise so, it's like buying a matrix to each weight matrix? So you're applying, yes, to each matrix.  So each weight matrix, you conjugate by some-  No, no, no, okay.  So, it's not, the reparameterization we were talking about here is not that complicated, it's just a transformation.  So, [CROSSTALK] So take each weight in a weight matrix and just do a fine transfer for each of them.  Yeah, yeah.  So you multiple by constant and add constant.  Yeah, yeah.  So it's coordinate-wise.  No, because coordinate wise is complicated.  Okay, yeah. We should modify the freezing. [LAUGH] Okay, so All right, yeah, I am a bit late but okay, so let's talk about the the main stuff here, which is about Reinforcement Learning. So basically, in Reinforcement Learning, we care about simple efficiency. We actually care about simple efficiency more than workout time, because in practice, in real world interactions, it's a very hard problem of collecting datas for [INAUDIBLE]. So, one way to do this is to use a better optimizer, which can extract more information from a batch of samples. So, previous work such as TRPO approximate the natural way of using conjugate gradient, which is also known as the Hessian-free optimization, but they have several problems as I previously mentioned. They still require a very expensive iterative updates at each time stamp, each update. And also they only use the current batch for approximating the current information.  Expensive procedure to line search to figure out the constraint?  No, the conjugate gradient itself is also very expensive.  So when they do a conjugate gradient, do they require more samples?  Yes, they require more samples for approximating.  If your CG requires ten steps, then basically, you have to roll out ten samples?  No, that's not the same. So for doing conjugate gradient, and it's just how well you approximate the Fisher inverse factor product. So the more amount of steps you do, it's basically a better approximation to the Fisher inverse factor product.  Yeah.  But it's not the same as how well the Fisher metrics is. So that's two different problems. One, because Fisher matrix is a expectation of something, so you have to use samples to approximate it. And in the TRPO setting, you can only use the current batch to approximate the Fisher. And that's problematic because you have to use a large batch to do this approximation. And also it takes several iteration of conjugate gradient to calculate the Fisher inverse factor product. So these are two constraints. But in our case, we can aggregate the historical information to do a better approximation for the Fisher matrix. Is that clear, or?  Yeah, it's fine.  Okay, okay, so, right. So what we propose is ACKTR, which stands for Actor-critic Kronecker-factored Trust Region method. So basically, let me review a little bit or RL. So we basically apply K-fac to the Actor-critic method. So in RL or in Actor-critic method, the goal is to optimize the expected sum of discounted rewards. And the policy is parameterized by some parameter theta. And in policy gradients, which basically takes the derivative we feed to the data directly and gets this policy gradient for doing optimization. And it consists of two elements. One is the advantage function, a phi, and the other is some policy term. So, the A here which is, as the advance function, which is usually consists of some approximation, some values, some estimates of the values of the current states. So, yeah. So basically, what I want to say is that there are two components. One is the policy, the other is the value function. Okay, I guess this is familiar to most of you so maybe I just go quicker. So what we do is that we apply K-FAC to advantage actor-critic with is also known as the A2C. And A2C is a deterministic and synchronic batch version of A3C. A3C is better known because it's just the Asynchronous Methods for Deep Reinforcement Learning and publishing 2016. Okay, so why do we use A2C, but not A3C? So there are two reasons, one is that we actually don't see too much gain from doing asynchronous updates. And the other thing is that because we want to approximate the Fisher matrix and it's better to use a large batch size. So if we aggregate all the gradient information together and that could give you a better approximation to the approximated Fisher matrix. Right, okay, but the other way is to do something like aggregate the Fisher, but then doing asynchronous grid updates, and that could also be done. Yeah?  What's the deterministic part?  Yeah, so basically here, deterministic part, okay. Deterministic basically means that it's not asynchronous, so it's deterministic.  So it's the same as synchronous? Right, okay. [LAUGH]  It's not exactly the same, right?  I mean, synchronous method, it's always true.  Yes, true, true, true, true, true, true, true.  [LAUGH]  But when you select true, right?  Right, right, so yes.  So the deterministic word doesn't apply to the policy.  Right, right, right. Yeah, it's a bit confusing here. But here, yeah, mainly what I want to say is that deterministic means that the algorithm itself doesn't give you any asynchronous, I'm sorry, undeterminism.  So when you say there's not too much gain-  Yeah.  By gain you mean the training time, or?  The training time or efficiency.  I see.  Both, both.  Okay, so you're saying with A3C, you would take as much time as before, but just using k, as many by workers.  Yes, so basically, we need to say is what we do is like previously, you have like, for example, 32 that computes each of them collecting like 5 or 20 steps of. So right now, we basically aggregate all these gradients together, and then do one update instead of doing updates by each of the agent. So we don't see too much difference between A2C and A3C. And actually for some reason, for some of the games, we actually see that A2C can give you the better performance in the end, so basically it converged to a better policy in the end. And also A3C is just asynchronous and so some problems with the reproducity problems, like you cannot reproduce it easily. Yeah, so we basically apply K-FAC to A2C. So for actor, for constructing the Fisher Matrix is just usual natural policy gradient way. Like so this is a very old work, 2003. So we basically just used the policy distribution to take away the Fisher Matrix, and over the trajectory distribution. For the critic, so this is some new part of, we proposed, which is usually people just use like standard for the critic. But we also propose to use K-FAC, so the way we do it is by assuming a normal output distribution of the variable function, and use K-FAC to approximate it by this function. Starting to get lost, what was p of tau again?  Okay, so tau is the trajectory, so it refers to a state action and rework, or actually just state action here.  Where we take a sample from?  Yeah, so yes, it's just basically, you have three threads, and 20 steps of rows at each thread. And then you aggregate all of these together, so that's how you approximate a fission matrix. In the photocritic, it's all the same, but we are using a normal output distribution. Basically we make such assumption, and this is equivalent to a Gauss-Newton metric, yeah, yeah. And also we do have some other parts which, one is that we adopted the distributed implementation of K-FAC. And so this work is done by Jimmy Ba, et al, in 2017 I clear. And what they did is they basically asynchronously computes the matrix inverses. So we have these slow matrices, and we compute them asynchronously. So basically, in the main thread, you basically pull from the asynchronous route, the latest version of the inverse. So it could be that you are using a stored version of the matrix inverse, but it's okay, we don't see any performance difference. And also, we are using type of re-skilling for the learning rates. So, in they try to say that, I want my update, makes the policy before and after the update to be not different, by some radius which is measured by KO divergence metric. And we are also using the same, however, we are just using the [INAUDIBLE] approximated Fisher matrix to calculate the differences. Is that clear for most of the people, do I need to, okay. So basically up here, what they want to do is the delta theta, the change we make. Multiply by some matrix feature, and times the other feature, so this, in another form, is less or equal to some radius delta. And my learning rate is alpha, and basically given the delta, I can recalculate the alpha. So what we do is that we do the same thing but we use F hat, which is the K-FAC approximated Fisher matrix. Instead of the real Fisher matrix, to re-scale the linear rate alpha. And so we are using this, as well as a moving average, to accumulate the Fishers to six, as I explained before. So these are the three technical of how we use, okay, finally I can show some results.  So, question?  Yup?  I didn't see why in the traditional PR model, they can't use previous samples to use a sample average, to do a history average.  So, could you give any, how did they do it?  I do not know whether they do it or not, it's just an expectation. When you do that expectation, you just use previous samples to-  Okay, so basically, this is the thing, so If you do the Fisher inverse vector product, right, you have to calculate both basically together, it's a mix thing. So the vector here, you cannot use some previous samples to calculate the v, because TRPO is an on-policy method. So if you use the previous samples, then the v here, the gradient information here becomes biased. So you have to use the current batches to calculate both of them in TRPO. But in our case, the v, we all use the current data, the current gradient. But the F is some sort of approximation, although somehow average, only average.  So the essential the F, so they just do some fast way to handle, to calculate the F inversely.  Exactly, so they're not explicitly constructing the F, which is a disadvantage, but also an advantage, if you, yeah. But then another thing is that TRPO is very slow, and I will show you here, which is. So basically we're comparing ACKTR, which is our method, and A2C, and TRPO, TRPO is the green line. So here you see that TRPO doesn't quite learn on most of these Atari games. And I believe no one actually succeed in training the Atari games using TRPO. The reasoning is that it's just not easy to train convolution neural networks using TRPO. And so here, we're actually not using convolution neural networks because it takes too long for computing even one update. So what we did is using MOP, and that's probably the reason that it couldn't learn for these games. And also the ACKTR, compared to the baseline A2C, is like two to three speed up, in terms of simple efficiency. So the x-axis here is the number of time steps, which also refers to the number of frames, that you see for the agent. So remember, one time stamp refers to four frames. So this time stamp is not the time stamp for the updates, it's just some terminology we use. So basically, one time stamp refers to four frames, yes?  So for your method, do you use?  Yeah, for our method we use we just use the architecture, which is 32, 64, 64, 5112. It's a very standard architecture-  For the [INAUDIBLE] model to apply the pre-factor approximation, then, the formula will be different from the one that-  Yeah, so I presented the fully connected version. But then it's basically also developed in 2016, which is used for convolutional neural networks.  And you just, well, you mentioned you use moving average. [INAUDIBLE].  Fission statistics, yes.  Moving, I can't understand, you put more weight in the current sample?  Yes.  But it's not entirely clear to me why we have more weight in the recent samples, previously.  It's actually, so if we use past samples to act approximately, because we care about the current curvature. So that's why, so you're on the current, you're at a point on the current mouse surface. You want to approximate some current information at a current point, but not a past point. So, the Fisher matrix is actually biased, in some sense, if we use moving average. We do actually only want the current batch, to approximate the future matrix. But in practice, we actually find the historical information can help. To approximate better.  Yeah, you use historical information, but when you use just average.  Yeah, no. So this terms here, directory here, is basically using the current policy. So, this expectation is only, only means it is restricted to the current batch of data, its not the previous data. Moving average means you have an exponential average. Is that?  Yeah, so does that make sense? So like the momentum, yeah, yeah.  So it's like when we are calculating, when we are making our MS prof?  Yeah, MS prof or like momentum, momentum.  Not average of moving weight.  No, no, no. So it's just the momentum.  Yeah, similar to momentum.  Yeah, yeah, yeah. So yeah. Okay. Sorry I- [LAUGH] so right.  These results are  Yeah so this is so these are things. Here we have, so basically, we do see a lot of improvements, in terms of subway efficiency compared to A through C. And they are, in terms of okay, so I can show you a video, so the left hand side is the A through C, and this is [INAUDIBLE], on the same, while using the same number of samples to train them. And you can see, you can actually perform very well using the same number of samples. This is the score. You can actually see that the actor can get to, like, over 15,000 on the cuber game in less than 24 hours of training. Ok, and also we also did experiments on continuous control. Which is just the module benchmarks and usually the most sinkhole. Most standard cases, when we use the state space as end quotes. So for example, joins and very, low, dimensional inputs for doing a control. And we see that there are also many improvements so that the blue line is the actor. Compared to the other two methods. So in TRPO, actually in this case it's actually performing better. Because it's a low dimensional problem. So the policy could be even linear. So in this case TRPO can gets better. But it's still wings over.  Sorry, one question. So the two big differences here is with respect to actor is that is not using a CNN in.  Yeah.  And the other one is, in just kind of approaches you have a number of diverse universes, right? Like parallel universes where you're gaining experience and actuating ingredients, so you have something similar you have imposed on [INAUDIBLE]  You mean like parallel universes?  For case, yes. So, they do perform better, I think, if you use the environment. But we have never tried it. Yeah, so in terms, in the previous case, in case, there is just no way of doing any it's just very hard to compute.  I see.  Yeah. But for the original case, yes. So, I would say that, yeah. It's actually, sorry. So in this case, we actually used a single runner for all of the, for all of the stream single worker?  Yes, so in this case it's all single worker.  Okay, that makes it much more.  Yeah, so this is all single worker case.  Yeah, because otherwise it's hard to separate without much benefit.  Yeah, so this is all single worker, it's not parallel  Investor on the same model.  Yeah, this is all on the single models. The same model.  The same model.  Yeah, the majority of case, we're now using convalutionary levels, it's just fully connected.  But then for the previous work?  For the previous work, yes, but-  So.  I see.  So you,  I have never tried it, but I don't think that makes too much sense. It's just  I thought its not reading a baseline, but just to see how much gains from the modeling cells, versus how much improvements from the owners.  I see. Yeah, sure, sure, sure. Okay, yeah.  Do you have an idea of how this compares to PPO?  Yeah, I will show you later. Yeah.  Have you tried this on a toy domain where the actual is actually feasible to run?  Yes.  Do you understand how much of a gap you are losing because you are approximating with environmental methods?  Yeah, yeah, yeah. Sure, yeah, so you're right. So I think those comparison are done in previous papers, where it's not [INAUDIBLE], it's a supervised learning setting.  I'm not sure [CROSSTALK]  You're right, you're right. [CROSSTALK]  The other thing I'm thinking of also is the comparisons with DRPU, a more controlled comparison where I want to understand,  Whether the chronicle factor we have been the constraint. That there. Is allowing us to do better updates then like the expected right?  True true true.  But also like trying to pin point the benefit we are getting from a more  We're actually not enforcing that structure, we're just making that approximation. So, maybe that approximation, you're saying that approximation maybe a better [CROSSTALK]  Itself, it begins with very good motivations. But it finally just comes up with this quite hacky way to-  Approximate.  Approximate, yeah, and it could well be that yes, you're also sort of, coming up with a more heuristic approximation, but maybe that is more suited for our code.  Maybe, maybe, yeah, yeah.  No, is a good point, but you're saying that in the first?  Yeah, so they  Of those things.  But that's supervised.  Exactly, so the honorary, especially the modicum of here have a very different balance variance to them.  True, true.  If you're already telling me that using a moving average, which means it's a biased fisher of estimated variance, variance, and that's a good thing for that sort of changes the landscape of the problem.  I see. Yeah, that makes sense. Yes. Yes.  Okay, that's a good point. Right. Okay. So continuing to talk. So we also did those experiments from pixel inputs directly. So usually it's just from StaySpace. We also are able to train those agents directly from the pixel space for those continuous control problems. This is, I believe it's like, first or second work on this type of tasks. So the previous work, only one paper, DDPG, does something like this. But they only use some very easy tasks. But where we actually are succeed on some of the tasks. So for example here, like this is the inputs, we put it into the newer networks. It's also a networks. But we're able to transfer it using actor. Okay, so again, so this is some on PPO results. So actually, I'd like to say that PPO is not a competitor. Because actually it's just switching the loss. Basically switch another objective, but we can still use a key fact to optimize PPO. So this is what we do, okay, before that, if you just compared PPU and ACKTR, PPU using the normal optimization and ACKTR. Then they're about the same, at most of the terry games and the ledger hall tasks, right. So is actually a very good method as well, okay. But my point is that you can still use K-FAC to do approximately two optimizations on the PPO. This is some result that we obtained at OpenAI, as a side project doing my internship. With basically what we do is like switching the optimizer by using the same loss. So the green line is the ACKTR and the blue line is the SGD. So you can still see some of the improvements on some of the tasks, most of the tasks, I would say. So the one point here is if you use K-FAC, it's basically you run over the data twice, and the number of mini-batch, shoot, this is a typo. The number of mini-batches is actually just one, so you're not partitioning your batch of data into a lot of small batches. You're just sweeping all of them, and doing one update. But in terms, in a usual PPO setting, where they use SGD or other method, they actually, after they aggregate some of the flaws, they're sort of doing like a supervised [INAUDIBLE] training. So basically they run over the data ten times, and they partition your data into 32 small mini batches. So basically, they are doing 32 modification of the policy, 32 times 10, so basically 320 updates of the policy. But where we only did twice, so that's a difference, so it's like, we only use 160, x number, a small number of updates, two beat PPO, of two beat SGD, yeah. So if we use the same number of updates, then you actually can see there are a lot of gaps between K-FAC and SGD in most of the tasks. So K-FAC is very, very efficient in term of the performance you can get with one update. All right, so these are all good performance, then let's look at how about wall clock time. It only increases 10 to 25 computation time, so if you look at this, what I'm showing here is that, the number of timesteps per second. So number of timestep, per second, yes, so number of frames you do per second, so there is not too much differences between ACKTR and A2C. So, these numbers are all pretty much the same, like 10 to 25% of computation time. So one takes about like 16 hours, the other takes about 20 hours. That is the difference, which pretty much you don't really care about. Another thing that I want to emphasize here is we can get a lot of gain if we use large batch for doing those second order optimizations. So the second order optimization differs, basically, it extracts more information for a batch of data. Because in the same time I've calculated first order print, it also calculates the coverages. So I'm showing you here is that if you compare using a batch size of 160 and 640, of using ACKTR and A2C. For ACKTR, you can get this much of gain by using a large file size, but for SGD you can only get this much gain here. So this is another benefit we can get for doing stack order optimization. And this is something that I think current industry people really care about, which is using large batch size. So paper is like treating [INAUDIBLE] in one hour, or even 24 minutes, they all use some hacky tricks like some linear schedule, or warm up. But I believe if you use signal optimization, such as K-FAC, then you can just get rid of that. Just works out of box, because it can extract more information from the batches.  Question, so for each point on the x axis-  So for this part yes, so this is number of updates, so that means how many number of updates...  Each update has different sizes?  Yes, yes, yes  So it's not entirely fair because you're, one trajectory has a different-  So if you look at this curve here, so this means the number of samples, so this is, I guess what you want to see is this curve. So basically, if you use 160 or 640, the difference is not large in terms of number of samples. But for A2C, which is the first order like SGD, it actually, if you use a much larger batch size it just decreases, it just basically becomes crap, yeah.  I see, so what time is about the same?  The one part time is about the same, yeah.  Okay, so for large, so there's no increase in computation time for K-FAC if you have a large batch.  No, no. Okay, so another thing that I wanna compare with this that if we use only ACKTR for doing optimization versus, what we propose is that we also do K-FAC optimization for the query. So the only ACKTR means like the usual metric impulse [INAUDIBLE]. So we actually kind of also got some some improvements, we, yes?  So, I think the variant of is a very well motivated metric for the ACKTR, for the cryptic, did you actually try-  Yeah, we tried, we tried, we actually tried with different metric, but it turns out that the Gauss-Newton actually performs relatively the best. So we actually tried to estimate the variance, so the variance can be estimated from the variance of the bell and error. So it's just like a regression like the variance of the noise, but then that doesn't seem to work as good as just using like watts. Yes, yes, anyway, so yes, lastly, I want to show that because we're using this type of approximation for doing trust region. So, like 0.01 is our radius, and we calculate how the KL changes, do the exact change of KL, and we see that it's actually sort of bounding by this 0.01. So it's actually doing some reasonable trust region, although it's an approximation two feature. All right, so in conclusion, I presented to you a method called fact K-FAC which uses a probabilistic model to approximate natural gradients. And we applied it to a reinforcement learning setting, which comes up with this method called ACKTR. Which is two to three times improvement in sample efficiency, but also only takes 10 to 25 longer computation time. It can learn both continuous and discreet control tasks directly from pixel inputs. And is also able to use batch size more efficiently, it can scale up to more batch size more efficiently. And we found out that optimization [INAUDIBLE] using the K-FAC is also very useful. And lastly, the K-FAC [INAUDIBLE] helps us, helps transformation and also PPO, It's not competitory, it's something that you can also improve upon using K-FAC. All right, so coming soon is some side projects I did at OpenAI. Using K-FAC for Q-learning and DDPG and maybe PPO, and stay tuned. [LAUGH] All right, so thank you, yeah.  [APPLAUSE]  I think we have the room until 3:15, so we have a few minutes for one or two quick questions. So what's your main project at OpenAI, that's so many side projects?  Yeah, my main project doesn't work.  [LAUGH]  Yeah, my main project is trying to do meta learning, basically trying to learn an advantage function, but it doesn't work so. [LAUGH]  [INAUDIBLE]  So yeah, I basically spent most 90% of the time on the meta learning part of and it doesn't work. I tried so many things, it doesn't work, but then 10% of time, using kickbacks seems to get some results. So yeah, so K-FAC is really a good thing.  [CROSSTALK] [LAUGH]  K-FAC has been around for some time, but it's not widely-  Yeah, that's the reason why I'm standing here, to talk to you about this. Okay, so to be fair, I think before 2017, before last November I guess, there is not a good implementation of K-FAC. So only after Jimmy's paper, which is distributed implementation of K-FAC, it starts to become useful. So previously, there is only this sort of very general theory. But it doesn't seem to have very practical [INAUDIBLE]. The K-FAC does have a lot of implementation techniques that go with it. So I think the main disadvantage of this algorithm is that it's just complicated to implement. It's not as simple as like add-in or other first order methods that you can just, anyone can just increment, but yeah. But then if sorry, Google Brain is actually developing K-FAC from last November. So they should release it soon, and I think as long as it release it, I think K-FAC will eventually take over the world [LAUGH].  Essentially K-FAC is an approximate second order method. So could you kind give a comparison of different kind of second order methods other than K-FAC?  Right, so you're saying some methods like LBFGS, or those sort of like more approximated Newton methods?  That is kind of a stochastic one, but for stochastic, in the [CROSSTALK] K-FAC, do we have any other second order methods, and how K-FAC-  [CROSSTALK]  For deep learning, for neural networks, no. So this is the first that's scalable algorithm, scalable to some other-  Some people would argue that Adam is basically trying to approximate the diagonals of this.  Yes, yes, that's the case.  Simplest approximation, he could still have have two simply first-ordered, but gets under the-  [INAUDIBLE]  But I would argue that Adam, a method like Adam won't give you too much benefit over STT phase momentum.  [INAUDIBLE]  Yeah, well it's well tuned, actually, with momentum. I think the main benefit you can get from Adam is that you don't really need to tune it. But STT can perform as good as Adam in most of the cases to fit your.  Do you implement K-FAC in TensorFlow?  Yes, yes, it's in TensorFlow, yeah. So TensorFlow will give you this flexibility of constructing the computation graph. So you can see that you have to extract those activations, and if that propagator flips, and then constructing some sort of matrices. But TensorFlow, you can grab those operators directly and then doing some sort of automatic search of these elements in your computation graph. And it's a complicated code, but you can do it in TensorFlow.  You mean to call it, it's complicated, but mathematically, it's not so complicated?  No, I mean, mathematically, it's not complicated, yeah, yeah, yes?  Trying to see how I can apply this to some of the policies that Matthew has trained, right, where he uses LSTMs, or-  Yes, so LSTM is a bottleneck, so I think, right, you can modify the code to give support to LSTM. But I'm assuming layer-wise, timestep-wise, independent, so you can just basically, yes.  Also on the factor of cross time.  Yes, a factor of cross time, basically, you're treating each layer as a different sample, you just aggregate them out together. You can do that, but there are more complicated way to deal with the coordination across time. That work is done by Jimmy and James. Which they should release, but I don't know about the details yet.  I guess if you can have more questions, I could take questions offline. I will be here until Friday, okay so that's the main speaker here.  Thank you.  [APPLAUSE] 