 We present a model-based reinforcement learning algorithm that can safely learn and explore in uncertain environments, without encountering system failures. Autonomous robotic systems are starting to leave controlled environments in order to help humans commute, be more efficient at work, and complete their daily duties. However, these tasks require robots to interact with real-world environments, where they face large uncertainties. For example, different road- and weather conditions in self-driving cars. This means that robots must be able to safely learn and adapt online in order to solve their assigned tasks effectively and with high performance. We expect this automatic adaptation to happen safely. That is, if our robot takes an exploratory action we expect the current policy to be able to recover. However, current reinforcement learning methods do not consider safety constraints when exploring and when updating the policy. As a result, unsafe behavior occurs during the learning process, which can harm the robot or its environment. In this paper, we explicitly model and learn about uncertainties in the robot’s dynamics and its environment. This allows us to develop an algorithm that ensures safe exploration in the presence of uncertainties, as well as that an updated control policies continues to solve its assigned task safely. Our algorithm proceeds as follows. We estimate the safe region of attraction for the current control policy using Lyapunov stability theory. Then, we safely explore the state space on the real robot, making sure that we never leave the safe region with high probability. Lastly, based on the new information we update our statistical model of the dynamics and safely improve the policy. In the paper, we prove theoretical guarantees for the safety and exploration properties of the algorithm that also extend to neural network policies. Let’s see an example. Here we see the state-action space of the robot, with a policy that assigns actions for each state. Based on a Lyapunov comparison function, we encode a small region of the state space that is safe under the initial policy, here shown in red. Since we are uncertain about the dynamics initially, we need to collect data in order to improve our model. To this end, we safely explore the state-action space by collecting data within the white shaded region, which guarantees that we do not leave the current safe region with high probability. As we gather more data, the model uncertainty decreases, which allows us to simultaneously improve the policy and expand the region of attraction. As a result, we can safely explore and optimize the policy, without encountering instability. If you would like to learn more about how to encode provable safety guarantees into reinforcement learning algorithms, please visit poster number 203 on Tuesday between 6 and 10.30pm. 