 [Music] [Applause] [Music] [Applause] [Music] fantastic well so thank you all very much for testbed for its the tool for asking questions about how to develop the future of AI I'm sure one of the reasons that you're here today is that you are aware of the incredible progress that has been made in machine learning and artificial intelligence over the past couple of years we now have machines that can recognize thousands of objects that can recognize human language in real-time and translated to other languages so it's a really exhilarating period of time and the key question is how far we can we actually push this and how much progress can we make in developing future AI technology now trying to invent the future is of course very complicated so one of the key ingredients that is necessary this the tool that allows us to ask the right question and that is exactly how a secret in my I see it as the tool for asking questions about where to take artificial intelligence the first question of course is what is intelligence and what are we trying to solve here and this is very much a question that is still very hotly debated and that we have no consensus on in the research community my current working definition of intelligence is the one that legand had proposed in 2006 and they proposed the definition that intelligence measures an agent's ability to achieve goals in the wide range of environments now if we do want to develop artificial intelligence then I see two key components here in this definition the first key component is this ability to achieve goals so that implies having an agent that interacts with some environments that that can interactively taken in for me make decisions and act in the environment in order to achieve its goals there's also the second component about acting in a wide range of environments so this is really about flexibility it is about being able to develop an agent or an architecture that is able to learn to achieve a wide variety of course and that continuous to learn as it interacts with its environment now these two components flexibility and interactive position making put some key challenges in experimentation so if we want to go beyond for example supervised machine learning towards agents that can actually interactively learn to interact with the environment we need a new set of tools for measuring progress this idea of measuring progress is what I see as fundamental to scientific research in a wide variety of disciplines we have seen that once the right tools were invented for measuring progress or observing new phenomena we saw very rapid progress in those areas and that ranges from astronomy to physics to a wide range of other disciplines now in artificial intelligence is actually very hard to measure progress as you saw there is no agreed-upon definition even of what we are trying to achieve so it's even harder to come up with an idea of what to measure as a approach to tackling this very very hard conundrum we are proposing to use a platform that allows us to very rapidly ask questions test hypotheses and in such a way that it allows us to start measuring progress in this area now why is minecraft such a fantastic platform for doing AI experimentation for allowing this asking questions I don't know how many people here are familiar with Minecraft and because it's a little hard to ask this question remotely I just assume that maybe half of you have kids who have played minecraft and maybe the other half at least heard of the game but just for those who haven't been able to play minecraft before it's an open-ended game where players explore create socialized it's what's called a sandbox game that players enter there's no predefined goal so people can go in and play this game in whatever way they want for example one thing I like to do in there is just go travel to its emergence region and watch a sunset and I really like building treehouses in Minecraft so you can really come up with anything that you want to do this open-ended nature of the game makes it both very attractive for for human players it really allows people to be creative and explore new kind of roles and new kinds of games but it also allows us to build an AI X permutation platform on top of this game that presents an infinite variety of tasks that we can construct for AI agents as a result of this this allows us to take the platform we have developed implement simple tasks that we can address using current technology and then we can gradually build up the complexity towards environments that provide a huge amount of flexibility in terms of the tasks we want an AI agent to achieve in addition to that this world can be shared with other players so we can easily generate environments in which human anti-ice interact with each other both AI and humans tended can experience this world from a first-person perspective and this allows us to develop agent technology that can learn to interact with complex environment including environments where they share with people now what is needed on top of Minecraft so minecraft is already this fantastic game that provides this variety but what was needed in order to unlock its potential for AI experimentation was an ability to very easily connect AI agents into this environment and develop tasks that allow us to benchmark I have to benchmark algorithms and reproduce others results exactly this piece of connectivity is what project Momo provides the project memo is a platform for AI experimentation that is built on top of minecraft to make it as easy as possible to construct experiments and integrate agents into this environment in order to develop new AI technology in the next couple of slides I want to briefly show two use cases on how to get started using mono for AI experimentation and after that I will briefly touch on the research challenges or some of the research challenges that can be addressed and some of those that we are currently as well seen here in the team so let's look at the tool and the key use cases and design principle that guided us when developing project MO as I already mentioned we wanted to make it as easy as possible to connect AI agents into the game there was a an important balance to strike here between having an API that is powerful but also intuitive and I think our team has has done a fantastic job in achieving this we also needed to provide researchers with the tools for creating tasks building on existing minecraft capabilities and finally the platform is built for extensions and whoever uses it as I already mentioned the platform is available as open source online so people can go and change it or extend it and adjust it to the needs they have for their particular experiments that they want to run we made you to effort to make the platform as easily accessible as possible so we are providing a platform that is cross-language you can implement agents in java but nothing which has c c++ places etc and it's also completely cross-platform so it runs on Windows Linux and OS very briefly mamo consists of the following components when you start up by mo you start up the actual Minecraft game inside the Minecraft game there is and server architecture and each agent would connect to one of the main craft clients this is done via of mind that hopes into minecraft and the mod is really key to exposing the state of Minecraft world to the agent and getting back the commands to send to the Minecraft world so on top of this mod since the API that the user could actually interacts with and this API would be called to get observations about the environment etc so let's look at two simple examples of how to use project model so this is really the 101 tutorial of how to develop agents in the Minecraft world using minecraft the fundamental idea is that we want to have an agent that interacts with the environment and we can think of this interaction as an interactive hoop if you're familiar with reinforcement learning this must look very very familiar to you I'm very briefly an agent interacts with the world by taking actions and then observing observations and rewards about the environment so in this way we can abstract all situations where an agent takes an action that observes the consequences of his actions and has to learn from those now let me show you what this looks like in Python so here we have a very short setup where a very simple agent would enter a basic Minecraft world and as you can see down here it is interacting with the world right of the world is running it is collecting some rewards and observations but for now the agent will just ignore those rewards and observations and instead it will just move turn and jump and then you go very very easily you have an agent that moves turns and jumps in the plane basic Minecraft world now of course what the jumping is only so much fun when there's nothing to jump over so let's look at how to actually create a task to make this environment a bit more interesting there are two ways which researchers can create tasks in project mammal the first one that we is recommended is using a mission XML file and the key idea behind using this programming language independent XML file is that this should make it easy for research researchers to exchange their tasks set up so let's say someone runs experiments with a specific task that is defined in XML file then we can simply change this machine definition XML to enable others to run their agents on the same task and this is what I focus on in this talk here I've abbreviated the XML here a little bit to just cut to the key ingredients that make this that allow us to turn this into a for example it asked for a reinforcement learning agent the key of ingredients here are what's called the agent editors and for each agent that interacts with the environment this agent hunter allows us to specify what observation the agent should get from the environment so for example thus the agency purely the pixel frames from a first-person perspective or does it get a more symbolic view of the world or should it additionally get for example check messages so it can communicate with other agents so this is the observations what the agencies about the world in addition to that we can specify an action space so we can allow agents to move either and discrete or continuous space and this is important for connecting for example to certain parts of the reinforcement journey literature finally we need to specify the reward structure so in the setup that we have chosen the researcher can associate specific rewards with the right variety of conditions in the environment so for example there could be negative rewards for taking steps in the environment or for taking an action that could be positive rewards for collecting certain items or for finish so indeed something here we see a basic implementation of a canonical task in the reinforcement learning literature it's called cliff walking and it's based on the cliff walking task in the second important reinforcement learning book the task here is quite simple an agent starts on a starting block in a cliff that is surrounded by lava and the goal is to have the agent navigate to leap blue a gold block at the end so if the agent touches the coal block it gets a positive reward if it doesn't love it gets a negative reward and there is a small personal negative reward to encourage the agent to find a short path towards the goal now from these simple components this ability to construct tasks using the simple XML language combined with the power of the Minecraft game we can construct an infinite variety of different AI tasks and this opens up he research challenges in the next couple of slides I will give a very very brief overview of some of the starting points for doing for example reinforcement learning research using project mammal and then I will provide some pointers to some of the work that were currently doing right so we have just seen this very simple if walking task and I was mentioning that this is one of the canonical tasks in reinforcement learning there's also a very effective solution to this particular task and it's called tabular cue learning and this example is provided with the mango platform so if you were a check out the memo platform you can actually go in and try to run this and have an agent that learns to navigate to the to the goal block very quickly the idea here is that the observations that the agent receives are a tabular representation of the environment so in the view on the right and the cube you can see an illustration of what's called the Q table a representation of the states that an agent has explored so far and it's estimated value of taking an action in a particular state so you can see for the starting state and when the agent has just started to explore its environment it may know only a single state and so far it maybe has learned that walking forward from the start block as a bad idea it gives negative reward and walking of to the sides maybe it doesn't have it hasn't collected much information yet so the key challenges in reinforcement learning or that the agent needs to not only learn about how to you know what actions are useful in this environment it also needs to explore so only when it reaches a block next to the goal block can it actually explore which action is a promising action in that particular state so in key challenge is balancing this exploration of learning something new was exploitation of what the agent has already learned we can see here and in the example that we have implemented here we are using tabular cue learning was absolutely the exploration which means that some small portion of the time the agent would take an action at random and the remainder of the time it takes the action that it judges as the most beneficial so far and thus if the algorithm allows the agent to continuously learn about its environment you can see all the way on the right how the agent has already explored a larger part of this environment and it's slowly learning about the reward structure that is associated with taking actions in this environment and now if we run this algorithm for about 10 minutes we successfully a train an agent that navigates this environment all the way to the goal as you can see over here you can also see on the left there that the cue table the representation that the agent has learned has evolved or has been updated so that the most promising states have the highest reward estimates in accordance to the rewards that the agent has I hate perceived and it's pretty key to this algorithm to propagate information about rewards connected on future estates back to the earlier states so that the agent actually finds a path to to a promised single state now a key limitation of our lives with tabular representation is around generalization now on the one hand these algorithms are can be very effective so for example if I can give a suitable representation for solving a given task such as the tabular structure here I can get an agent that learns to achieve this task very very quickly in about ten minutes of running for about 100 episodes however as those tasks become bigger and maybe there are certain regularities that could be exploited for learning the agent wouldn't be able to generalize over that it would have to explore from scratch in every new state that it encounters this ability to generalize was what's key to the success of deep reinforcement learning that has been so impressively demonstrated over the past two or three years for example in solving Atari games or learning to play at her games and human ability and we can apply similar algorithms here in here in project model so here I'm showing an example of applying deep reinforcement learning to this task of navigating this room with some lava to successfully navigate to the be controlled in the top right top left corner of the screen what I'm showing here is a screen shot where a man agent has already learned to navigate this environment and you can see a representation of the Q values where it can judge that taking a forward action taking a forward move when it's facing lava is a bad idea so just this here's the nature that has learned to actually look around itself to collect the right visual information that allows it to navigate to the goal and with generalization using deep neural networks we can ensure that an agent can successfully find its weight even when the configuration of lava is changing something that wouldn't have been possible using for example tabular approaches now with deep reinforcement learning we have seen that agents can successfully generalize even in complex environments using only visual information a key limitation at the moment is that these approaches take a huge quantity a huge amount of data to learn effectively so for example in the example I just showed it requires about a Lillian - Lillian and to have steps in order to learn to successfully navigate to this incan if we were to look at a single individual task something like tabular clear learning could land is much much faster so a key open challenge is to understand how we can develop more data efficient approaches to deep reinforcement learning so that we combine the power of learning to generalize with this ability to learn quickly in a relatively new environment some preliminary work that we have done in the space was looking at so-called multi task reinforcement learning the intuition here was that in minecraft environment we are able to construct tasks that are very closely related to each other and this allows us to test hypotheses about how the similarity between tasks and that was an engine to learn tasks directly and to generalize what we found here was that with mighty task reinforcement learning on related tasks the agent is actually able to learn as fast or even faster than learning single husks from the same amount of data more interestingly we found that the agents learned shared representation in this multitask setting so in this figure here I'm showing data that was collected using two tasks that have been learned with mighty tasks or a set up in one task the agent had available actions where it could turn and move forward and in the other set up and it could walk north-south-east-west without turning the different tasks here are highlighted in red and blue and we can see that the clusters that emerge from projecting this representation in a 2d space are tightly intermingled and actually analyzing this representation we see that there's one just taped cluster that emerges when the agent faces danger danger when it's looking at lava and so we have a first first suggestion that with by the task reinforcement learning the agent can actually learn semantically meaningful representations for given tasks now this was only a brace learning point and I'm really excited about with project memo is that it gives rise to a variety of research directions it is really a sent box that allows us to ask new questions about AI and it allows us to push forward the stadium the state of the art a direction that I am particularly excited about is this ability to have AI agents and human agents in the same environment and this gives rise to new research challenges around learning to collaborate and in the long run we hope that this platform will give rise to agents that learn to successfully collaborate with humans this is something me and my team are currently hard at work on we will actually be looking at releasing a tasks and samples that were put research in this direction so stay tuned for updates on project mama and in the meantime I would like to encourage you to try this out for yourself project memo is freely available on github and there's a huge variety of examples if you look at the Python samples you can learn about reinforcement learning tabular approaches and you can also look at how to try to improve some of the current reinforcement algorithms that are already out there thank you so much for attending [Applause] [Music] [Applause] [Music] [Music] I want I don't know reinforcement learning just to just follow the agent what will be in a simple way what will be the reaction what will be the agent will be initialized the first at the beginning of the algorithm the agent is assumed to learn from tabula rasa so to not know anything about the environment different initializations of the queue tailors and I would give different would give rise to different learning speed typically we just initialize all the values to zero and then the agent has to learn and update those values according to its experience but learning could be sped up if we have prior information we could put that information into the initialization and billing Nicole no did mine and openly I came up with different words and especially especially video games a little bit like Minecraft what is the main difference between their approaches and what you're proposing here on the project man has a different view of and this is reflected in the kinds of tasks that people are selecting so for example a lot of the main tasks are very focused on physically accurate interaction with the world so it's really about learning learning physics for example and learning to interact with the physical environment something that I am particularly excited about is as I mentioned this ability to learn to collaborate with other agents including humans and this is what we focus on here so we take a middle ground between something that would be physically enter it and something that is very high in the Jex it in terms of interactions between agents okay and any word on the other environment you were mentioning essentially that deepmind one what do you think is the focus for one thing that is particular to the memo environment is that in man we have single coherent world with coherent physics and all the different tasks that can be implemented there underlie similar physics and similar regularities in this environment this allows us to do experiments that would allow agents to build up common-sense knowledge and then apply that common-sense knowledge to new tasks that come up in this environment according to the same coherent structure in universe this flexibility would be more generic we would try to push towards more generic urban serving rather than this ability to learn common sense knowledge great another question since somebody nobody is really raising their hands at some point in time you want a project member to learn about interactions with others learn the rules within a certain world and in terms of matrix how do you go about publishing a paper on doing passing things from project Marvel compared to another paper that has also beautiful things on project 9 what's I know for the classification you in those type of words and pushing in okay any other question one last one yes hello Katya to continue on the subject you are mainly saying that people are going to propose new tasks is Microsoft's gonna move in the direction of giving a set of tasks last not a long time ago Facebook just released a set of Yuriko so how English first said yah-yah-yah Rico set of tests for transfer learning and an environment a very small environment with it I think as you minecraft could be very very awesome for transfer learning on different set of tasks and so is Microsoft's gonna pushing also in this direction or are you gonna wait for the community to move on on tasks and complexity so at the moment my team is focusing on on collaboration so I think the first test that will come out will be in this space I am discussing with a lot of academic partners on how transfer learning tasks could be set up there are a couple of answers fundamental challenges in how to set these tasks up because if the experimenter knows that asks ahead of time we can put a lot of bias into our agents that allow them to learn very quickly on the particular set of tasks that we have defined but that doesn't necessarily allow us to test this very generic generic ability to transfer knowledge from one task to the other I don't have a very good solution to those challenges yet if you have any suggestions I'd be very curious to hear from you that will continue those collaborations and hopefully we'll be able to come up with a set of tasks in the future [Applause] 