 Welcome back everybody. Thanks for coming. I hope you had a good week and had a fun time playing with artistic style, I know I did. I tried a couple of things myself over the things with this artistic style, I just tried a couple simple little changes which I thought you might be interested in. One thing, before I talk about the artistic style, is I want to point out some of the really cool stuff that people have been contributing over the week. If you haven't come across it yet, be sure to check out the wiki (forums.fast.ai/lesson8-wiki-please-contribute/1639/1). There's a nice thing in Discourse where you can basically set any post as being a wiki, which means that anybody can edit it. So I created this post early on and by the end of the week, we now have all kinds of stuff with links to the stuff in the class, the summary of a paper, examples, a list of all the links, code snippets, a handy list of steps that are necessary when doing style transfer, lots of stuff about the TensorFlow Dev Summit, and so forth. So keep an eye out, we'll be having one of these wiki threads every week. One of the threads that popped out this afternoon from Xinxin, talked about trying to summarize what they've learnt from lots of other threads across the wiki. This is a great thing that we can all do, when you look at lots of different things and take notes, you can put them on the forum for everybody else. So if you haven't quite caught up on all the stuff going on on the forum, this post, Curating Lesson 8 Experiments, is a start. So a couple little changes I made. I tried thinking about what quite a few of you pointed out, depending on what your starting point is for the optimizer, you get to a very different place. Clearly our convex optimization is not necessarily finding a local minimum but at least saddle points it's not getting out of. So I tried something which is to take the random image and just add a Gaussian blur to it (filters.gaussian_filter). A Gaussian filter is just a blur, so that makes the random image into this kind of thing. I just found that even the plain style looked a lot smoother. So that was one change I made that worked quite well. Another change that I made just to play around with it was I added a different weight to each of the style layers, and so my zip now has a third thing in it which is the weights (style_wgts), and I just multiplied by the weight. I thought those two things made my bird look significantly better than my littler bird looked before, so I was happy with that. You could do a similar thing for content loss. You could maybe add more different layers of content loss and give them different weights as well. I'm not sure if anybody's tried that just yet. Question: In regards to style transfer for cartoons. With cartoons, when we think of transferring the style what we really mean is transferring the contours of the cartoon to redraw the content in that style. This is obviously not what style transferring is doing here. How might one implement this? Answer: I don't know that anybody's quite figured that out, but I'll show you a couple of directions that may be useful. Question: I've tried selecting activations that correspond with edges and such as indicated by one of the conv visualization papers, and comparing outputs from specifically those activations. Answer: I'll show you some things you can try. I haven't see anybody do a great job of this yet, but here's one example from the forum. Somebody pointed out that this cartoon worked very well with Dr. Seuss, but then when they changed their initial image not to be random, not the picture of the dog, it actually turned out a lot better. So that's one thing you can try. There's some very helpful diagrams that somebody posted summarizing what we learned, which is fantastic. I liked this summary of what happens if you add, versus remove each layer. So this is what happens if you remove Block 0, Block 1, Block 2, Block 3 and Block 4. You can get a sense of how they impact things. You can see for the style, the last layer is really important for making the image look good, at least for this image. There's some particularly nice examples. It seems that there's a certain taste for figuring out what photos go with what images. I thought this Einstein was terrific. I thought this was terrific as well. Brad came up with this really interesting insight that starting with this picture and adding a style to it creates this extraordinary shape here where (as he points out) you can tell it's a man sitting in the corner, but there's less than 10 brushstrokes. So sometimes this style transfer does things which are surprisingly fantastic. I have no idea what this is in the photos, so I don't know what this is in the painting either; I don't watch that kind of music enough. So there's lots of interesting ideas you can try. I've got a link here (you might have seen it in the PowerPoint) to a Keras implementation that has a whole list of things you can try (https://github.com/titu1994/Neural-Style-Transfer). Here are some particular examples, all of these examples you can get the details from this link. There's something called Chain Blurring. For some things (this might work well for cartoons), notice how the matrix doesn't do a good job with the cat when you use the Classic (this is our paper). But if you use this Chain Blurred approach, it does a fantastic job. I wonder if that might be a secret to the cartoons. Some of you (I saw on the forum) have already tried this, using color preservation and luminance matching. This means you're still taking the style, but you're not taking the color. I think it means in these particular examples this is really great results. It depends a lot on what things you tried with. You can go a lot further. For example, you can add a mask, and then say just do color preservation for one part of the photo. So here the top part of the photo has got color preservation, and the bottom half doesn't hasn't. That's beautiful. They even show in that code how you can use a mask to say one part of my image should not be stylized and here's the results. This is really crazy - use masks to decide which one of two style images to use. And then you can really generate some creative stuff. So there's a lot of stuff that you can play with and you can go beyond this to coming up with your own ideas. Now some of the best stuff you're going to learn a bit more today about, how to do some of these things better. Just to give you an idea, if you go to likemo.net, you can literally draw something using four colors, and then choose the style image and it will turn your drawing into an image. The idea is that blue is going to be water and green is going to be foliage and I guess red is going to be foreground. There's a lot of good examples of this kind of neural doodle (they call it) online Something else we're going to learn more about how to do better today is, if you go to affinelayer.com, there's a very recent paper called pix2pix. We're going to be learning quite a bit in this class about how to do segmentation, which is where you take a photo and turn it into a colored image. Basically saying, the horse is here, the bicycle is here, the person is here. This is basically doing the opposite. You start by drawing something, saying I want you to create something that has a window here, a windowsill here, a column there, and it generates a photo. Which is fairly remarkable. The stuff we've learnt so far won't get you quite through these two things, but by the end of today you should be able to. This is a nice example that some folks at Adobe built showing that you can basically draw something and it will try and generate an image that was close to your drawing where you just needed a small number of lines. We'll link to this paper from the resources. So we've got an in-class thread (as per usual) that you can post questions to. We also have the microphone and I love real person questions because then we can have more of a discussion. Please do put your hand up if you want to get my attention or Rachel's attention and feel free to, or chat on the forum. Rachel's going to be taking particular notice today of which questions or comments on the forum thread get Liked, so if you see a question or a comment thread that you would like, please add your Like to it and Rachel's more likely to mention it. Or just put up your hand. Welcome back everybody. Thanks for coming. I hope you had a good week and had a fun time playing with artistic style, I know I did. If there's anybody new who can't access the Part 2 groups in the forums, it probably might be easiest to private message Rachel and she will add you to the Part 2 category. I tried a couple of things myself over the things with this artistic style, I just tried a couple simple little changes which I thought you might be interested in. One thing, before I talk about the artistic style, is I want to point out some of the really cool stuff that people have been contributing over the week. If you haven't come across it yet, be sure to check out the wiki (forums.fast.ai/lesson8-wiki-please-contribute/1639/1). There's a nice thing in Discourse where you can basically set any post as being a wiki, which means that anybody can edit it. So I created this post early on and by the end of the week, we now have all kinds of stuff with links to the stuff in the class, the summary of a paper, examples, a list of all the links, code snippets, a handy list of steps that are necessary when doing style transfer, lots of stuff about the TensorFlow Dev Summit, and so forth. So keep an eye out, we'll be having one of these wiki threads every week. One of the threads that popped out this afternoon from Xinxin, talked about trying to summarize what they've learnt from lots of other threads across the wiki. This is a great thing that we can all do, when you look at lots of different things and take notes, you can put them on the forum for everybody else. So if you haven't quite caught up on all the stuff going on on the forum, this post, Curating Lesson 8 Experiments, is a start. So a couple little changes I made. I tried thinking about what quite a few of you pointed out, depending on what your starting point is for the optimizer, you get to a very different place. Clearly our convex optimization is not necessarily finding a local minimum but at least saddle points it's not getting out of. So I tried something which is to take the random image and just add a Gaussian blur to it (filters.gaussian_filter). A Gaussian filter is just a blur, so that makes the random image into this kind of thing. I just found that even the plain style looked a lot smoother. So that was one change I made that worked quite well. Another change that I made just to play around with it was I added a different weight to each of the style layers, and so my zip now has a third thing in it which is the weights (style_wgts), and I just multiplied by the weight. I thought those two things made my bird look significantly better than my littler bird looked before, so I was happy with that. You could do a similar thing for content loss. You could maybe add more different layers of content loss and give them different weights as well. I'm not sure if anybody's tried that just yet. Question: In regards to style transfer for cartoons. With cartoons, when we think of transferring the style what we really mean is transferring the contours of the cartoon to redraw the content in that style. This is obviously not what style transferring is doing here. How might one implement this? Answer: I don't know that anybody's quite figured that out, but I'll show you a couple of directions that may be useful. Question: I've tried selecting activations that correspond with edges and such as indicated by one of the conv visualization papers, and comparing outputs from specifically those activations. Answer: I'll show you some things you can try. I haven't see anybody do a great job of this yet, but here's one example from the forum. Somebody pointed out that this cartoon worked very well with Dr. Seuss, but then when they changed their initial image not to be random, not the picture of the dog, it actually turned out a lot better. So that's one thing you can try. There's some very helpful diagrams that somebody posted summarizing what we learned, which is fantastic. I liked this summary of what happens if you add, versus remove each layer. So this is what happens if you remove Block 0, Block 1, Block 2, Block 3 and Block 4. You can get a sense of how they impact things. You can see for the style, the last layer is really important for making the image look good, at least for this image. There's some particularly nice examples. It seems that there's a certain taste for figuring out what photos go with what images. I thought this Einstein was terrific. I thought this was terrific as well. Brad came up with this really interesting insight that starting with this picture and adding a style to it creates this extraordinary shape here where (as he points out) you can tell it's a man sitting in the corner, but there's less than 10 brushstrokes. So sometimes this style transfer does things which are surprisingly fantastic. I have no idea what this is in the photos, so I don't know what this is in the painting either; I don't watch that kind of music enough. So there's lots of interesting ideas you can try. I've got a link here (you might have seen it in the PowerPoint) to a Keras implementation that has a whole list of things you can try (https://github.com/titu1994/Neural-Style-Transfer). Here are some particular examples, all of these examples you can get the details from this link. There's something called Chain Blurring. For some things (this might work well for cartoons), notice how the matrix doesn't do a good job with the cat when you use the Classic (this is our paper). But if you use this Chain Blurred approach, it does a fantastic job. I wonder if that might be a secret to the cartoons. Some of you (I saw on the forum) have already tried this, using color preservation and luminance matching. This means you're still taking the style, but you're not taking the color. I think it means in these particular examples this is really great results. It depends a lot on what things you tried with. You can go a lot further. For example, you can add a mask, and then say just do color preservation for one part of the photo. So here the top part of the photo has got color preservation, and the bottom half doesn't hasn't. That's beautiful. They even show in that code how you can use a mask to say one part of my image should not be stylized and here's the results. This is really crazy - use masks to decide which one of two style images to use. And then you can really generate some creative stuff. So there's a lot of stuff that you can play with and you can go beyond this to coming up with your own ideas. Now some of the best stuff you're going to learn a bit more today about, how to do some of these things better. Just to give you an idea, if you go to likemo.net, you can literally draw something using four colors, and then choose the style image and it will turn your drawing into an image. The idea is that blue is going to be water and green is going to be foliage and I guess red is going to be foreground. There's a lot of good examples of this kind of neural doodle (they call it) online Something else we're going to learn more about how to do better today is, if you go to affinelayer.com, there's a very recent paper called pix2pix. We're going to be learning quite a bit in this class about how to do segmentation, which is where you take a photo and turn it into a colored image. Basically saying, the horse is here, the bicycle is here, the person is here. This is basically doing the opposite. You start by drawing something, saying I want you to create something that has a window here, a windowsill here, a column there, and it generates a photo. Which is fairly remarkable. The stuff we've learnt so far won't get you quite through these two things, but by the end of today you should be able to. This is a nice example that some folks at Adobe built showing that you can basically draw something and it will try and generate an image that was close to your drawing where you just needed a small number of lines. We'll link to this paper from the resources. This actually shows it to you in real time. You can see that there's some new way of doing art that's starting to appear where you don't necessarily need a whole lot of technique. I'm not promising it's going to turn you into a Van Gogh, but you can at least generate images that are maybe in your head in a style that's somewhat similar to somebody else's. I think it's really interesting. One thing I was thrilled to see is that at least two of you have written blog posts on medium. That was fantastic to see. I hope more of you might try to do that this week. Definitely doesn't need to be something that takes a long time. I know some of you are also planning on turning your forum posts into blog posts. So hopefully we'll see a lot more blog posts this week popping up. I know that people who've done that have found it a useful experience as well. One of the things I suggested doing, pretty high on the list of priorities for this week's assignment, was to go through the paper, knowing what it's going to say. I think this is really helpful, when you already know how to do something is to go back over that paper. This is a great way to learn how to read papers, because you already know what it's telling you. This is the way I learnt to read papers. So I've kind of gone through and I've highlighted a few key things that I thought were important when I went through. How many people went back and looked at this paper again (A Neural Algorithm of Artistic Style, Gatys,Ecker,Bethge)? A few of you, that's great. In the abstract of the paper, "an artificial system based on a Deep Neural Network that creates artistic images of a high perceptual quality." So we're going to read this paper, and hopefully at the end of it, we'll know how to do that. Then the first section, they tell us about the basic ideas. "When CNNs are trained on object recognition, they develop a representation of the image .. along the processing hierarchy of the network, the input image is transformed into representation that increasingly care about the actual content of the image compared to its detailed pixel values." So it describes the basic idea of content loss. Then they describe the basic idea of style loss which is looking at "the correlations between the different filter responses over the spatial extent of the feature maps." This is one of these sentences that when read on its own doesn't mean very much, but now that you know how to do it, you can read it and say, Oh, okay I think I see what that means. And then when you get to the methods section we learned more. The idea here is "by including the feature correlations of multiple layers" (this answers one of the questions that one of you had on the forum) "we obtain a stationary, multi-scale representation of the input image." This idea of multi-scale representation is something that we're going to be coming across a lot because a lot of this class is about generative models, and one of the tricky things with generative models is both to get the general idea of the thing you're trying to generate correct, but also get all the details correct. So the details generally require you to zoom into a small scale, and getting the big picture correct is about zooming into a large scale. So this was one of the key things they did in this paper, was show you how to create a style representation that included multiple resolutions. We now know the way they did that was to use multiple style layers. As we go through the layers of VGG, they gradually become lower and lower resolution, larger and larger representative fields. It's always great to look at the figures. I was thrilled to see that some of you were trying to recreate these figures, which actually turned out to be slightly non-trivial. So we can see exactly what that figure is, and if you haven't tried it for yourself yet, you might want to try it - see if you can recreate this figure. It's good to try and find in a paper the key finding, the key thing that they're showing. In this case they found that "representations of content and style in the Convolutional Neural Network are separable ... can manipulate both to create new" images. Hopefully now you can look at that and say, Oh yeah that makes sense. You can see that with papers, certainly with this paper, there's often quite a lot of introduction that often says the same thing a bunch of different ways. So it's often worth, the first time you read it, this one paragraph might night make sense. But later on they say it in a different way and it starts to make sense. It's worth looking through the introductory remarks maybe two or three times. You can see them talking about the different layers, how they behave. Again, showing the results of some experiments. You can see if you can recreate these experiments, make sure you understand how to do it. There's a lot of stuff I didn't find that interesting until we get to the section called Methods. The methods section is the section that hopefully you'll learn the most about after reading papers after you've implemented something, by reading the section called Methods. I want to show to show you a few little tricks of notation. You do need to be careful of little details that fly by, like here, "images shown were generated with average pooling." That's a sentence which if you weren't reading carefully you could skip over it. Now we know we need to use average pooling. So they will often have a section which often says, now I'm going to introduce the notation. This paper doesn't. This paper just kind of introduces the notation as part of the discussion. But at some point, you'll start getting Greek letters or things with subscripts, or notation starts appearing. At this point, you need to start looking very carefully. At least for me, I find I have to go back and read something many times to remember what's "L", what's "M", what's "N". This is the annoying thing with math notation, is they're single letters, they generally don't have any kind of mnemonic. Often though you'll find that across papers in a certain field, they'll reuse the same kinds of English and Greek letters for the same kinds of things. So "M" will generally be the number of rows, "N" will often be the number of columns, "K" will often be the index that you're summing over, and so on and so forth. So here we've got the first thing which is "x" with an arrow on top. "x" with an arrow on top means it's a vector. It's actually an input image, but they're going to turn it into a vector by flattening it out. So our image is called "x", and the CNN has a whole bunch of layers. Everytime you see something with a subscript or a superscript like this, you need to look at both of the 2 bits because they've both got a meaning. The big thing is the main object. So in this case, "N" is a filter, and the superscript is like an array (or a tensor). In Python, it's like a thing in square brackets. So each filter has a letter "L', which is which number of the filter is it. And so often when I read a paper, I'll actually try to write code as I go and put little comments. So I'll write like Layer[layer#] and then I have a comment after, just to remind myself. I'm creating the code and mapping it to the letter. So there are N.l filters. We know from the CNN that each filter creates a feature map (that's why there are N.l feature maps). Now remember any time you see the same letter, it means the same thing (within a paper, not necessarily across papers). Now each feature map is of size M and as I mentioned before, "M" tends to be rows, "N" tends to be columns. So here it says M is the height times the width of the feature map. So here we can see they've done .flatten to make it all 1 row. Now this is another piece of notation you'll see all the time, "a layer l can be stored in a matrix F.l", and now the "l" has gone to the top. Doesn't matter, same basic idea, it's just an index. So the matrix F is going to contain our activations. This thing here, where it says "R" with superscripts, this is a very special meaning. It's referring to what is the shape of this. "R" means that they're floats and this thing here (N.i x M.i) means it's a matrix, the "x" means it's rows by columns. There and N rows and M columns in this matrix. There's one matrix for each layer and there's a different number of rows and a different number of columns in each layer. So you can basically go through and map it to the code you've already written. I'm not going to read through the whole thing. There's not very much here, and it would be good to be sure you understand all of it. Perhaps with the exception of the derivative, because we don't care about derivatives because they get done for us, thanks to Theano and TensorFlow. So you can always skip the bits about derivatives. Then they do the same thing, basically describing the Gram matrix. They show here that the basic idea of the Gram matrix is that they create an "inner product between the vectorised feature map i and j". "Vectorised" here means turned into a vector, and the way you turn a matrix into a vector is flatten it. So this means the flattened inner product between the flattened feature map masks, those matrices we saw. So hopefully you'll find this helpful. You'll see there will be small little differences. Rather than taking the mean, they tend use the sum and they kind of divide back out the number of rows and columns to create the mean this way. In our code, we put the division inside the summation sign. So you see these little differences in how we implement things. Sometimes you may see actual meaningful differences. That's often a suggestion of, Oh that's something you should try, some differences you could try. So that describes the notation and the method, and that's it. But then very importantly, throughout this, anytime you come across some concept which you're not familiar with, it will pretty much always have a reference, a citation. You'll see there's little numbers all over the place, there's lots of different ways of doing these references. But anytime you come across something which has a citation (like it's a new piece of notation or a new concept, you don't know what it is), generally the first time I see it in a paper I ignore it. But if I keep reading and it turns out to be something that actually is important and I can't understand the basic idea at all, I generally then put this paper aside, I put it in my ToRead file and make the new paper I'm reading the thing that it's citing. Because very often a paper is entirely meaningless until you've read one or two of the key papers it's based on. Sometimes this can be like reading the dictionary if you don't yet know English. It can be like layer upon layer of citations and at some point you have to stop. I think you should find that the basic set of papers that things to refer to is pretty much all stuff you guys know at this point, so I don't think you're going to get stuck in any kind of a loop. But if you ever do, let us know on the forum and we'll try and help you get unstuck. Or if there's any notation you don't understand, let us know. One of the horrible things about math is that it's very hard to search for. It's not like you can take the function name and search for "Python functionName", it's some weird squiggly shape. Again, feel free to ask if you're not sure about that. There is a great Wikipedia page (I think it's called Math Notation or something) which lists pretty much every piece of notation. There are various places that you can look up notation as well. Okay, so that's the paper. Let's move to the next step. I think what I might do is draw the basic idea of what we did before so that I can draw the idea of what we're going to do differently this time. Previously we had a random image and we had a loss function. It doesn't matter what the loss function was, we know that it had to be a combination of style loss plus content loss. And what we did was we took our random image and we put it through this loss function and we got out two things, one was the loss and the other was the gradients. Then we used the gradients with respect to the original pixels to change the original pixels. So you basically repeat that loop again and again, and the pixels gradually changed to make the loss go down. So that's the basic approach that we just used. It's a perfectly fine approach for what it is. In fact, if you are wanting to do lots of different photos with lots of different styles (like if you created a web app where you said please upload any style image and any content image, here's your artistic style version) this is probably still the best, particularly with some of those tweaks in it that we talked about. But what if you wanted to create a web app that was a Van Gogh Irises generator, upload any image and I will give you that image in the style of Van Gogh's Irises. You can do better than this approach. The reason you can do better is that we can do something where you don't have to do a whole optimization run in order to create that output. Instead we can train a CNN to learn to output photos in the style of Van Gogh's Irises. The basic idea is very similar. What we're going to do this time is we're going to have lots of images. And we're going to take each image and we're going to feed it into the exact same loss function that we used before, with the style loss plus the content loss. But for the style loss we're going to use Van Gogh's Irises, and for the content loss we're going to use the image that we're currently looking at. We've got lots of images. What we're going to do is, rather than changing the pixels of the original photo, instead what we're going to do is we're going to train a CNN to take this. Let's put a CNN in the middle, here's the layers of the CNN. And we're going to try and get that CNN to spit out a new image. So there's an input image and there's an output image. This new CNN we created is going to spit out an output image that when you put it through this loss function, hopefully it's going to give a small number. If it gives a small number, it means that the content of this photo still looks like the original photo's content and the style of this new image looks like the style of Van Gogh's Irises. So if you think about it, when you have a CNN you can pick any loss function you like. We tend to use pretty simple loss functions so far, like mean-square-error or cross entropy. In this case, we're going to use a very different loss function which is going to be style plus content loss using the same approach that we used before. And because that was generated by a neural net, we know it's differentiable. And you can optimize any loss function, as long as the loss function is differentiable. So if we now take the gradients of this output, not with respect to the input image but with respect to the CNN weights, then we can take those gradients and use them to update the weights of the CNN so that the next iteration through the CNN will be slightly better at turning that image into a picture that has a good style match with Van Gogh's Irises. Does that make sense? At the end of this, we run this through lots of images, we're just training a regular CNN and the only thing we've done differently is to replace the loss function with the style loss plus content loss that we just used. At the end of it, we're going to have a CNN that has learnt to take any photo and will spit out that photo in the style of Van Gogh's Irises. So this is a win because it means you now don't have to run an optimization path on a new photo, you just do a single forward pass through a CNN, which is instant. Question: So this is going to limit the filters you use, right? Let's say you have PhotoShop and you want to change multiple styles. Answer: Yes. Each neural network is going to learn to do just one type of style. Question: Can you combine multiple styles? Answer: Yes, you can combine by adding multiple bits of style loss to multiple images. You're still going to have the problem that that network has only learned to create one kind of image. It may be possible to train it so that it takes both a style image and a content image, but I don't think I've seen that done yet, as far as I know. Having said that, there is sometime simpler and (in my opinion) more useful we can do. Rather than doing style loss plus content loss, let's think of another problem we can solve, which is called super-resolution. When Rachel and I started playing around with it a little while ago, nobody was that interested in it. But in the last year or so, it's become really hot. We were kind of playing around with it quite a lot, we thought it was really interesting, but suddenly it's gotten hot. The basic idea of super-resolution is that you start off with a low-res photo. The reason I started getting interested in this was I wanted to help my Mum take her family photos that were often pretty low quality and blow them up into something that was big and high-quality that you could print out. So that's what you do, you're trying to take something which starts with a small low-res photo and turns it into a big high-res photo. Perhaps you can see that we can use a very similar technique for this. What we could do is between the low-res photo and the high-res photo, we could introduce a CNN. That CNN could look a lot like the CNN from our last idea, but it's taking in as input a low-res image and then it's sticking it into a loss function. The loss function is only going to calculate content loss. The content loss it will calculate is between the input that it's got from the low-res after going through the CNN compared to the activations from the high-res. In other words, has this CNN successfully created a bigger photo that has the same activations that the high-res photo does. So if we pick the right layer for the high-res photo, then that ought to mean that we've constructed a new image. Question: Can you put a CNN between any two things and it will learn the relationship? Answer: Yes, absolutely. This is one of the things I wanted to talk about today. In fact, I think it's at the start of the next paper we're going to look at, they even kind of tack about this. So this is the paper we're going to look at today, Perceptual Losses for Real-Time Style Transfer and Super-Resolution. This is from 2016, so it took like about a year or so to go from the thing we just saw to this next stage. Maybe half a year. What they point out in the abstract here is that people had done super-resolution with CNNs before, but previously the loss function they used was simply the mean-square-error between the pixel outputs of the upscaling network and the actual high-res image. The problem is that it turns out that tends to create blurry images. It tends to create blurry images because the CNN has no reason not to create blurry images. Blurry images tend to look pretty good in the loss function because as long as you get the general, Oh this is somebody's face, I'll put like a face color here then it's going to be fine. Whereas if you take the second or third conv block of VGG, then it needs to know that this is an eyeball or it's not going to look good. It needs to know that this is a nose, or it's not going to look good. So if you do it not with pixel loss but with the content loss we just learnt about, you're probably going to get better results. Like many papers in deep learning, this paper introduces its own language. In the languge of this paper, perceptual loss is what they call the mean-square-errors between the activations of a network with two images. So the thing we've been calling content loss, they call perceptual loss. One of the nice things they do at the start of this (I really like it when papers do this) is say why is this paper important. This paper is important because many problems can be framed as image transformation tasks, where a system receives some input image and chucks out some other output. For example, denoising learns to take an input image which is full of noise and output a clean image. Take an input image which is low-res and spit out a high-res image. Colorization, take an input image which is black-and-white and spit out a color image. There are some other examples they mention which is turning an image into an image, which includes segmentation. We'll learn more about this in coming lessons, but segmentation refers to taking a photo of something and creating a new image that basically has a different color for every object. Horses are green, cars are blue, buildings are red - that kind of thing. That's called segmentation. As you know from things like the Fisheries Competition, segmentation can be really important as a part of solving other bigger problems. Another example they mention here is depth estimation. There's lots of important reasons you would want to use depth estimation. For example, maybe you want to create some fancy video effects where you start with a flat photo and you want to create some cool new Apple TV thing that moves around the photo with a parallax effect as if it was 3D. So if you were able to use a CNN to figure how far away every object was automatically, then you could turn a 2D photo into a 3D image automatically. Taking an image in and spitting an image out is kind of the idea in computer vision at least of generative models. So this is why I wanted to talk about generative models in this class. It's not just about artistic style. Artistic style is just my sneaky way of introducing you to the world of generative models. Let's look at how to create this super-resolution idea. Part of your homework this week will be to create the new approach to style transfer. I'm going to build the super-resolution version (which is a slightly simpler version) and then you're going to try to build on top of that to create the style transfer version. Make sure you let me know if you're not sure at any point. So I've already created a folder with a sample of 20,000 ImageNet images. I've created two sizes; one is 288x288 and one is 72x72, and they're available as bcolz arrays. I actually posted the link to these last week, it's on platform.ai [now files.fast.ai]. So we'll open up those bcolz arrays. One trick you might have (hopefully) learned in Part 1 is that you can turn a bcolz array into a Numpy array by slicing it with everything. Anytime you slice a bcolz array, you get back a Numpy array. So if your slice is everything, then this turns it into a Numpy array. This is just a convenient way of sharing Numpy arrays. We've now got an array of low-resolution images and an array of high-resolution images. Let me start by showing you the final network, this is the final network. We start off by taking in a batch of low-res images. And the very first thing we do is we stick them through a convolutional block with a stride of 1. This is not going to change its size at all. This convolutional block has a filter size of 9 and it generates 64 filters. So this is a very large filter size. Nowadays filter sizes tend to be 3. Actually in a lot of modern networks the very first layer is very often a large filter size. Just the one very first layer. The reason is that it basically allows us to immediately increase the receptive field of all of the layers from now on. By having 9x9 and we don't lose any information because we've gone from 3 channels to 64 filters. So each of these 9x9 convolutions can actually have quite a lot of information because we've got 64 filters. So you'll be seeing this quite a lot in modern CNN architectures. Just a single large filter conv layer, so this won't be unusual in the future. Question: Is stride 1 also pretty popular these days? Answer: Stride 1 is important for this first layer because you don't want to throw away any information yet. In the very first layer, we want to keep the full image size. So the stride 1 doesn't change, doesn't downsample at all. Question: There's also a lot of duplication, right? Answer: They overlap a lot, absolutely. But that's okay, a good implementation of a convolution is going to normalize some of that, or at least keep it in cache, so it hopefully won't slow it down much. Question: Have people used this idea for compression? One thing to mention is that if you look at the Lesson 9 thread, one of the things I put there was this is a github link, not to the iPython notebooks, which I don't think are great to have in github because they keep changing and they're just going to overwrite your changes, but the Python files are probably good to put in github. So this github repo (https://github.com/jph00/part2) just has the Python files and I'll try to remember every week to put in all of the currently updated Python files in there. So feel free to grab them. I'm not sure if there even are any changes this week, but just in case you might want to grab the most recent version, this way you can keep them up to date. One of the discussions I was just having during the break was how practical are the things we're learning at the moment, compared to Part 1 where everything was designed entirely to be the most practical things which we have best practices for. And the answer is a lot of the stuff that we're going to be learning, no one quite knows how practical it is because a lot of it just hasn't been around that long, and it's not really that well understood and maybe there aren't great libraries for it yet. One of the things that I'm hoping from this Part 2 is by learning the edge of research stuff (or beyond) amongst a diverse group is that some of you will look at it and think about whatever you do 9-5 or 8-6 or whatever and think, Oh I wonder if I could use that for this. If that ever pops into your head, please tell us. Please talk about it on the forum. That's what we're most interested in. Like, oh you could use super-resolution for blah, or depth-finding for this or generative models for this thing I do in pathology or architecture or satellite engineering, or whatever. So it's going to require some imagination on your part. So often that's why I do want to spend time looking at stuff like this where it's like, Okay what are the kinds of things that this can be done for. I'm sure you know in your own field one of the differences between an expert and a beginner is the way an expert can look at something from first principles and say, Okay I could use that for this totally different thing which has got nothing to do with the example that was originally given to me because I know that the basic steps are the same. And that's what I'm hoping you guys will be able to do. Not just say, Okay I know how to do this artistic style, but are there things in your field which have some similarities. We were going to talk about the super-resolution network, and we talked about the idea of the initial conv block. So after the initial conv block, we have the computation. And when I say "the computation", in any kind of generative network the key work it has to do in this case is starting with a low-res image, figure out what might that black dot be. Is it an eyeball? Is it a wheel? Basically if you want to do really good upscaling, you actually have to figure out what the objects are so that you know what do draw. That's the key computation this CNNs going to have to learn to do. In generative models, we generally like to do that computation at a low resolution. There's a couple of reasons why. The first is that at a low resolution there's less work to do so the computation is faster. But more importantly, at higher resolutions it generally means we have a smaller receptive field, it generally means we have less ability to capture larger amounts of the image at once. And if you want to do really great computations where you recognize, Oh this block here is a face and therefore the dot inside of it is an eyeball, then you're going to need enough of a receptive field to cover that whole area. I noticed a couple of you asked for information about receptive fields on the forum thread. There's quite a lot of information about this online so Google is your friend here. The basic idea is if you have a single convolutional filter for a 3x3, the receptive field is 3x3. So it's how much space can that convolutional filter impact. So here's our 3x3 filter. Now on the other hand, what if you had a 3x3 filter which had a 3x3 filter as its input. So that means that the center one took all of this, but what did this one take? This one would have taken (depending on the stride), probably these ones here. This one over here would have taken these ones here. So in other words, in the second layer (assuming a stride of 1), the receptive field is now 5x5, not 3x3. So the receptive field depends on 2 things: one is how many layers deep are you, and the second is how much did the previous layers either have a non-unit stride or maybe they had max pooling. So in some way, they were becoming downsampled. Those two things increase the receptive field. The reason it's great to be doing layer computations on a large receptive field is that it then allows you to look at the big picture. Look at the context. It's not just edges anymore but eyeballs and noses. So in this case, we have 4 blocks of computation where each block is a ResNet block. So for those of you that don't recall our ResNet work, it would be a good idea to go back to Part 1 and review. But to remind ourselves, let's look at the code. Here's a ResNet block. All a ResNet block does is it takes some input and it does 2 convolutional blocks on that input and then it adds the result of those convolutions back to the original input. So you might remember from Part 1 we actually kind of drew it. There's some input and it goes through 2 convolutional blocks and then it goes back and is added to the original. If you remember, we basically said in that case we've got y = (x + someFunctionOf_x), which means that the function equals y-x and this thing here is the residual. So a whole stack of residual blocks, ResNet blocks, on top of each other can learn to gradually hone in on whatever it's trying to do. In this case what it's trying to do is get the information it's going to need to upscale this in a smart way. We're going to be using a lot more of this idea of taking blocks that we know work well for something and just reuse them. So then, what's a conv block? All a conv block is in this case is a convolution, followed by a batchnorm, optionally followed by an activation. One of the things we now know about ResNet blocks is we generally don't want an activation at the end. That's one of the things that a more recent paper discovered. So you can see in my second conv block I have no activation. I'm sure you've noticed throughout this course that I refactor my network architectures a lot. My network architectures don't generally list every single layer, but they're generally functions which have a bunch of functions which have a bunch of layers in them. A lot of people don't do this. A lot of the network architectures you find online are hundreds of lines of layer definitions. I think that's crazy. It's so easy to make mistakes when you do it that way and so hard to really see what's going on. In general, I would strongly recommend that you try to refactor your architectures so that by the time you write the final thing it's half a page. You'll see plenty of examples of that, so hopefully that will be helpful. So we've increased the receptive field, we've done a bunch of computations, but we still haven't actually changed the size of the image which is not very helpful. So the next thing we'll do is change the size of the image. The first thing we're going to learn is to do that with something that goes by many names, one is deconvolution and another is transposed convolutions and it's also fractionally strided convolutions. In Keras they call them deconvolutions. Here I've got a spreadsheet to show you the basic idea. The basic idea is that you've got some kind of image, here's a 4x4 image and you put it through a 3x3 convolutional filter, and if you're doing valid convolutions, that's going to leave you with a 2x2 output because here's one 3x3, another 3x3 ... four of them. So each one is running the whole filter on the appropriate part of the data. It's just a standard 2D convolution. We've done that. Now let's say we want to undo that. We want something that can take this result and recreate this input. How would you do that? One way to do that would be to take this result and put back that implicit padding (surround it with all these 0's) such that if we use some convolutional filter and we're going to put it through this entire matrix, a bunch of 0's with our result matrix in the middle. Then we can calculate our result in exactly the same way, just the normal convolutional filter. So if we now use gradient descent we can look and see what is the error, how much does this pixel differ from this pixel and how much does this pixel differ from this pixel, and then we add them all together to get out mean-square-error. So we can now use gradient descent (remember from Part 1), in Excel it's called Solver. And we can say, Set this cell to a minimum by changing these cells. So this is basically the simplest possible optimization. Solve that and here's what it's come up with. So it's come up with a convolutional filter. You'll see that the result is not exactly the same data, and of course, how could it be? We don't have enough information. We only have four things that try and regenerate 16 things. But it's not terrible. In general, this is the challenge with upscaling. When you've got something that's blurred and downsampled, you've thrown away information. The only way you can get information back is to guess what was there. The important thing is that by using a convolution like this we can learn those filters. We can learn how to upsample it in a way that gives us the loss that we want. So this is what a deconvolution is. It's just a convolution on a padded input. Now in this case I've assumed that my convolutions had a unit stride, there was just one pixel between each convolution. If your convolutions are of stride 2, then it looks like this picture. And you can see that as well as putting the 2 pixels around the outside, we've also put a zero pixel in the middle. So these 4 cells are now our data cells and you can see it calculating the convolution through here. I strongly suggest looking at this link (http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html) which is where this picture comes from. In turn, this link comes from a fantastic paper, Convolution Arithmetic Tutorial which is a really great paper. So if you want to know more about convolutions and deconvolutions, you can look at this page (deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html), which it's got lots of beautiful animations, including animations on transposed convolutions. This is the one I just showed you, that's the one we saw in Excel. So that's a really great site. So that's what we're going to do first, we're going to do deconvolutions. In Keras, a deconvolution is exactly the same as convolution except with "De" on the front, but all the same stuff -- how many filters do you want, what's the size of your filter, what's your stride (or upsample, as they call it), border_mode, and so forth. Question: If TensorFlow is the backend shouldn't BatchNormalization axis = -1? Answer: It should be. axis = -1 is the default, so yes, thank you. Thank David Guttman, he is also responsible for some of the beautiful pictures we saw earlier. So just in case you weren't clear on that, you might remember from Part 1 that the reason we had axis=1 is because in Theano that was the channel axis. So we want it not to throw away the x-y information from BatchNorm across channels. In Theano, channel is now the last axis, and since -1 is the default, we don't need that anymore. So let's add deconvolution blocks. So we're using a stride of 2,2. That means that each times it goes through this convolution it's going to be doubling the size of the image. For some reason I don't fully understand and haven't really looked into, in Keras you actually have to tell it the shape of the output. So you can see here, you can actually see. It's gone from 72x72 to 144x144 to 288x288. So because these are convolutional filters, it's learning to upscale. But it's not upscaling with 3 channels, it's upscaling with 64 filters. That's why it's able to do more sophisticated stuff. Finally, we're kind of reversing things here. We have another 9x9 convolution in order to get back our 3 channels. The idea is we previously had something with 64 channels, and we now want to turn it into something with just 3 channels and 3 colors and to do that we want to use quite a bit of context. So we have a single 9x9 filter at the end to get our 3 channels out. So at the end we have a 288x288x3 tensor, in other words an image. So if we go ahead now and train this, then it's going to do basically what we want, but the thing we're going to have to do is to create our loss function. Creating a loss function is a little bit messy, I'll take you through it slowly and hopefully it will all make sense. Let's remember some of the symbols here. imp is the original low resolution input tensor, and the output of this is called outp. Let's call this whole network here the upsampling network. So this is the thing that is actually responsible for doing upsampling. So we're going to take the upsampling network and we're going to attach it to VGG. And VGG is going to be used only as a loss function, to get the content loss. So before we can take this output and stick it into VGG we need to stick it through our main subtraction preprocessing, this is just the same thing we did over and over again in Part 1. So let's now define this output as being this lambda function applied to the output of our upsampling network. So that's what this is, this is just our preprocessed upsampling network output. We can now create a VGG network. Let's go through every layer and make it not trainable. You can't every make your loss function be trainable. Your loss function is the fixed-in-stone thing that tells you how well you're doing. Clearly you have to make sure VGG is not trainable. Now which bit of the VGG network do we want? We can try a few things. I'm using block2_conv2 relatively early. The reason for this is if you remember when we did the content reconstruction last week, the very first thing we did. We found you could basically totally reconstruct the original image from early layer activations. Whereas by the time we got to block 4 we got pretty horrendous things. So we're going to use a somewhat early block as our content loss, or as the paper calls it, the perceptual loss. You can play around with this, see how it goes. So now we're going to create two versions of this VGG output. This is something which is very poorly understood or appreciated with Keras' functional API, which is any kind of layer, and a model is a layer as far as Keras is concerned, can be treated as if it was a function. So we can take this model and pretend it's a function, and we can pass it any tensor we like. And what that does is it creates a new model where those two pieces are joined together. So vgg2 is now equal to this model on the top (vgg_content) and this model on the bottom (outp_1), and remember this model was the result of our upsampling network followed by preprocessing. Question: In the upsampling network is the Lambda function to normalize the output image? Answer: Yes, that's a good point. We use a tanh activation, which can go from -1 to 1. So if you then go that plus 1 times 127.5, that gives you something between 0 and 255. Interestingly, this was suggested in the original paper and supplementary materials. More recently, on Reddit (I think it was) the author said they tried it without the final tanh activation, without the final deprocessing and it worked just as well. You can try doing that. If you wanted to try it, you could just remove the activation and you would just remove this last thing entirely. Obviously if you do have the tanh, then you need the output. This is something that I've been playing with with a lot of different models. Anytime I have some particular range that I want, one way to enforce that is by having a tanh or sigmoid followed by something that turns that into the range you want. It's not just for images. So we've got two versions of our VGG layer output. One which is based on the output of the upscaling network (vgg2). And the other which is based on just an input (vgg1). And this one, just an input, is using the high resolution shape as its input. That makes sense because this vgg network is something that we're going to be using at the high resolution scale, we're going to be taking the high-resolution target image and the high-resolution upsampling result. Now that we've done all that, we're nearly there. We've got the high-res perceptual activations and we've got the low-res upsampled perceptual activations. We now just need to take the mean sum of squares between them, and here it is here. In Keras, anytime you put something into a network it has to be a layer, so if you want to take a plain old function and turn it into a layer, you just chuck it inside a Lambda. So our final model is going to take our low-res input and our high-res input as our two inputs and return this loss function as an output. One last trick, when you fit things in Keras, it assumes you're trying to take some output and make it close to some target. In this case, our loss is the actual loss function we want, it's not that there's some target - we want to make it as low as possible. Since it's a mean-square-error, it can't go beneath 0, so what we can do is we can basically trick Keras and say that our target for the loss is 0. And you can't just use a scalar 0, remember every time we have a target set of labels in Keras, you need one for every row, for every input. So we're going to create an array of 0's, so that we can fit it into what Keras expects. I kind of find that increasingly as I start to move away from the well-trodden path of deep learning, more and more (particularly if you want to use Keras), you kind of have to do weird little hacks like this. So be it, it's a weird little hack. There's probably more elegant ways of doing this, but this works. So we've got our loss function that we're trying to get every row as close to 0 as possible. Question: If we're only using up to block2_conv2, could we pop of all the layers afterwards and save some computation? Answer: Sure, it wouldn't be a bad idea at all. So we compile it, we fit it. One thing you'll notice I've started doing is using this callback called TQDMNotebookCallback. TQDM is a really terrific library. Basically it does something very very simple which is to add a progress meter to your loops. You can use it in a console as you can see. Basically anywhere you've got a loop, you can have TQDM around it and that loop does just what it used to do but it just gets this progress bar. It even guesses how much time is left and so forth. You can also use it inside of Jupyter notebook and it creates a big little graph that gradually goes up and shows you how long is left and so forth. This is just a nice little trick. Here's some learning rate annealing and at the end of training it for a few epochs we can try out our model. Now the model we're interested in is just the upsampling model. We're going to be feeding the upsampling model low-res inputs and getting out the high-res outputs. We don't actually care about the value of the loss. So I now define a model which takes as input the low-res input and spits out as output our high-res output. With that model we can try it -- call predict. Here is our original low resolution mashed potatoes and here is our high resolution mashed potatoes. It's amazing what it's done. You can see in the original the shadow of the leaf was very unclear, the bits of the mashed potato were just kind of blobs. In this version we have clear shadows, hard edges and so forth. Question: Can you explain the size of the target, it's the first dimension of the high-res * 128. Why? Answer: This is basically the number of images that we have. It's 128 because that's the number of filters we have. So this ends up giving you the mean-square-error of 128 filter losses. Question: Would popping the unused layers really save anything? Aren't you only getting the layers you want when you do the vgg.get_layer on block2_conv2 [vgg_content = Model(vgg_inp, vgg.get_layer('block2_conv2').output)]. Answer: I'm not sure. I can't think quite quickly enough. You could try it. It might not help. Question: Intuitively, what features is this model actually learning? Answer: What it's learning is it's looking at 20,000 very very low resolution images like this and it's learning when there's a kind of a soft gray bit next to a hard bit; in certain situations that's probably a shadow, and when there's a shadow, this is what a shadow looks like for example. It's learning that when there's a curve it isn't actually meant to look like a jagged edge, but it's actually meant to look like something smooth. It's really learning what the world looks like. When you take that world and blur it and make it small, what does it then look like. It's just like when you look at a picture like this (and particularly if you blur your eyes and defocus your eyes you can often see what it originally looked like. Your brain basically is doing the same thing. It's like when you read really blurry text, you can still read it because your brain is thinking, I know, I got that. Question: So are you suggesting there is a similar universality or the other way around. You know when vgg is saying the first layer is a line and then a square and then a nose or an eye. Are you saying the same thing is true in this case? Answer: Yes, absolutely. It has to be. There's an infinite number of ways you can upsample, there's lots of information. So in order to do it in a way that decreases its loss function it actually has to figure out what's probably there based on its context. Question: But don't you agree, just intuitively thinking about it. Such as the example you were suggesting of the elbow, do you think it would be easier if we just feed it pictures of humans because the interaction of the circle, the eye, the nose is going to be a lot better. Answer: In the most extreme versions of super-resolution networks, they take 8x8 images. You'll see that all of them pretty much use the same dataset, CelebA. CelebA is a dataset of pictures of celebrities' faces. And all celebrities faces look pretty similar. And they show these fantastic amazing results. They take an 8x8 and turn it into a picture of a face. It looks pretty close. That's because they've taken advantage of this. In our case, we've got 20,000 images in 1,000 categories; it's not going to do nearly as well. If we wanted to do as well as the CelebA versions, we would need hundreds of millions of images. Question: It's just kind of hard for me to understand mashed potatoes and a face in the same category. Answer: The key thing to realize is there's nothing qualitatively different between what mashed potatoes look like and what faces look like. Once you can learn to recognize the unique features of mashed potatoes. A big enough network and enough examples can learn not just mashed potato but writing and pictures and whatever. For your examples, you're most likely to be doing stuff which is more domain-specific so you should use more domain-specific data taking advantage of exactly these kinds of issues. One thing I mentioned here is that I didn't use a test set. So another piece of the homework is to add in a test set. And tell us, Is this mashed potato overfit? Is this just matching the particular training set version of this mashed potato or not. And if it is overfitting can you create something that doesn't overfit. So there's another piece of homework. So it's very simple now to take this and turn it into our fast style transfer. So fast style transfer is going to do exactly the same thing but rather than turning something low-res into something high-res. It's going to take something that's a photo and turn it into Van Gogh's Irises. So we're going to do that in just the same way. Rather than go from low-res to a CNN to find the content loss against high-res, we're going to take a photo, go through a CNN and do both style loss and content loss against a single fixed style image. I've given you links here; I have not implemented this for you, this is for you to implement. I've given you links to the original paper and (very importantly) also to the supplementary material. It's a little hard to find because there's two different versions and only one of them is correct (and of course they don't tell you which one is correct). So the supplementary material goes through all of the exact details of what was their loss function, what was their processing, what was their exact architecture and so on and so forth. While I wait for that to load ... Question: Like we did a doodle regeneration using the model's photograph's weights, could we create a regular image to see how you would look if you were a model? Answer: I don't know. If you could come up with a loss function which is how much does somebody look like a model, you could. You have to come up with a loss function, and it would have to be something where you could generate labeled data. One of the things they mentioned in the paper is they found it very important to add quite a lot of padding and specifically they didn't add 0 padding (normally you just have a black border), they add reflection padding. Reflection padding literally means take the edge and reflect it. I've written that for you because there isn't one, but you might find it interesting to look at this because this is like one of the simplest examples of a custom layer. We're going to be using custom layers more and more, so I don't want you to be afraid of them. A custom layer in Keras is a Python class. As I mentioned before, if you haven't done OO programming in Python now's a good time to look at some tutorials because we're going to be doing a lot of it, particularly PyTorch. PyTorch absolutely relies on it. So we're going to create a class. It has to inherit from layer. In Python, this is how you create a constructor (Python's OO syntax is really gross), you have o use this special weird custom name thing which happens to be the constructor. Every single damn thing inside of it you have to type out manually. "self," is the first parameter. If you forget, you'll get stupid errors. Sorry, it's not my fault. And then in the constructor for a layer, this is basically where you just save away any of the information you are given. In this case, you've said that I want this much padding, so you just have to save that somewhere. And then you need to do two things in every Keras custom layer. One is you have to define something called "get_output_shape_for", that is going to pass in the shape of an input and you have to return what is the shape of the output that that would create. So in this case, if s is the shape of the input, then the output is going to be the same batch size and the same number of channels and then we're going to add in twice the amount of padding for both the rows and columns. Remember one of the cool things about Keras is you just chuck the layers on top of each other and it magically knows how big all the intermediate things are. It magically knows because every layer has this thing defined, that's how it works. The second thing you have to define is something called "call". And call is the thing which will get your layer data and you have to return whatever your layer does. In our case, we want to cause it to add reflection padding. In this case, it so happens that TensorFlow has something built in for that called TA.pad. Obviously generally it's nice to create Keras layers which would work with both Theano and TensorFlow backends by using that "K." notation, but in this case Theano didn't have anything that obviously did this easily. Since it was just for our class, I decided just to make it TensorFlow. So here is a complete layer. I can now use that layer in a network definition like this. I can call .predict which will take an input and turn it into this, you can see that the bird now has the left sides here reflected. That is there for you to use because in the supplementary material for the paper they add spatial reflection padding at the beginning of the network. And they add a lot, 40x40. The reason they add a lot is that they don't want to use same convolutions, they want to use valid convolutions in their computations. Because if you add any black borders during these computation steps, it creates weird artifacts on the edges of the images. You'll see that through this computation for the residual blocks, the size gets smaller by 4 each time, and that's because these are valid convolutions. So that's why they have to add padding to the start so that these steps don't cause the image to become too small. So this section here should look very familiar because it's the same as our upsampling network. A bunch of residual blocks, two deconvolutions and one 9x9 convolution. This is identical so you can copy it. This is the new bit. We've already talked about why we have this 9x9 conv. But why do we have these downsampling convolutions to start with? We start with an image up here of 336x336 and we halve its size, and then we halve its size again. Why do we do that? The reason we do that (as I mentioned earlier) we want to do our computation at a lower resolution because it allows us to have a larger receptive field, and it allows us to do less computation. So this pattern where it's like reflective - the last thing is the same as the top thing, the second-to-the-last thing is the same as the second thing, you can see it. It's like a reflection, symmetric. It's really really common in generative models to first of all take your object, downsample it increasing the number of channels at the same time. You're increasing the receptive field, you're creating more and more complex representation. You then do a bunch of computations on those representations and then at the end you upsample it again. So you're going to see this pattern all the time. That's why I wanted you guys to implement this yourself this week. That's the last major piece of your homework. Question: What is stride = 1/2 mean? Answer: That's exactly the same a deconvolution stride 2. So remember I mentioned earlier that another name for deconvolution is fractionally strided convolution. Remember that little picture we saw, this idea of like you put little columns and rows of 0's in between each row and column? You can kind of think of it as doing like a half stride at a time. So that's why this is exactly what we already have. I don't think you'll need to change it at all. Except you'll need to change my same convolutions to valid convolutions. It's well worth reading the whole supplementary material because it really has the details. It's so great when a paper has supplementary materials like this. You'll often find the majority of papers don't actually tell you the details on how to do what they did. And many don't even have code. These guys both have code and supplementary material which makes this an absolute A+ paper. Plus it works great. So that is super-resolution and perceptual losses and so on and so forth. I'm glad we got there so now we can go on to next week, just make sure I don't have any more slides. There's one more thing I'm going to show you. These deconvolutions can create some very ugly artifacts. I can show you some very ugly artifacts because I have some right here. You see these, you see this checkerboard, this is called a checkerboard pattern. The checkerboard pattern happens for a very specific reason. I've provided a link to this online paper. You guys might remember Chris Olah, he had a lot of best learning materials we looked at in Part 1. He's now got this cool thing called distill.pub. Some of his colleagues and he wrote this thing discovering why is it everybody gets these checkerboard patterns. What he shows is that it happens because you have stride 2 size 3 convolutions, which means that every pair of convolutions sees one pixel twice. So a checkerboard is just a natural thing that's going to come out. So they talk about this in some detail, and all the kind of things you can do. In the end, they point out two things: the first is that you can avoid this by making it that your stride divides nicely into your size. So if I change size to 4, they're gone. So one thing you could try if you're getting checkerboard patterns (which you will) is make your size 3 convolutions into size 4 convolutions. The second thing he suggests doing is not to use deconvolutions. Instead of using a deconvolution, he suggests first of all doing an upsampling. What happens when you do an upsampling is basically the opposite of max pooling? You take every pixel and you turn it into a 2x2 grid of that exact pixel. That's called upsampling. If you do an upsampling followed by a regular convolution that also gets rid of the checkerboard pattern. As it happens, Keras has something to do that which is called upsampling 2D. I'll find a link to it and I'll have to add it. All this does is the opposite of max pooling. It's going to double the size of your image, at which point you can use this standard normal unit strided convolution and avoid the artifacts. Extra credit, after you get your network working, change it to an upsampling and unit stride convolution network and see if the checkerboard artifacts go away. So that is that. At the very end here, I've got some more suggestions for things you can look at, although most of those were already in the PowerPoint, so I don't think there's anything else there. So let's move on. I want to talk about going big. Going big can mean two things. Of course, it does mean we get to say "Big Data", which is important, you have to do that. I'm very proud that even during the Big Data thing I never said Big Data without saying rude things about the stupid idea of Big Data. So who cares about how big it is? In deep learning sometimes we do need to use either large objects (like if you're doing diabetic retinopathy you have like 4000x4000 pictures of eyeballs or maybe you've got lots of objects, like if you're working with ImageNet. To handle this data that doesn't fit in RAM, we need some tricks. So I thought we would try some interesting project that involves looking at the whole ImageNet Competition dataset. The ImageNet competition dataset is 1.5 million images in 1,000 categories. As I mentioned (I think in the last class), if you try to download it, it will give you a little form saying you have to use it for research purposes and they're going to check it and blah blah blah. In practice, if you fill out the form, you'll get back an answer seconds later. So anybody who's got a terabyte of space (and since you're building your own boxes, you now have a terabyte of space), you can go ahead and download ImageNet and you can start working through this project. This project is about implementing a paper called DeVISE. DeVISE is a really, really interesting paper. I actually just chatted to the author about it quite recently. An amazing lady named Andrea Frome, who's now at Clarify, a computer vision startup. What she did with DeVISE was she created a really interesting multi-modal architecture. Multi-modal means we're going to be combining different types of objects. In her case, she was combining language with images. That's quite an early paper to look at this idea. And she did something which was really interesting. She said normally when we do an ImageNet network, our final layer is a one-hot encoding of a category. So that means that a Pug and a Golden Retriever are no more similar or different in terms of that encoding than a Pug and a jumbo jet. That seems kind of weird, right? Like if you had an encoding where similar things were similar in the encoding, you could do some pretty cool stuff. In particular, one of the key things that she was trying to do was to create something which went beyond the 1,000 ImageNet categories so that you could work types of images that were not in ImageNet at all. So the way she did that was to say, Let's throw away the one-hot encoded category and let's replace it with a word embedding of the thing. So Pug is no longer 00010000, but it's now the word2vec vector for Pug. And that's it. That's the entirety of the thing, train that and see what happens. I'll provide a link to the paper. One of the things I love about the paper is what she does is to show quite an interesting range of the kind of cool results, cool things you can do when you replace a one-hot encoded output with a vector output and embedding. Question: Just to clarify. So every one-hot encoded pixel suddenly becomes a vector? Answer: No, pixels are not one-hot-encoded ever. Pixels are encoded by their channels. Question: Sorry, I mean bit of one-hot-encoded result. Answer: Let's say this is an image of a Pug, a type of dog. So let's say Pug is the 300th class in ImageNet. It's going to get turned into a 1,000 long vector with 999 zeros and a 1 in position 300. That's normally what we use as our target when we're doing image classification. We're going to throw that 1,000 long thing away and replace it with a 300 long thing. The 300 long thing will be the word vector for Pug that we downloaded from word2vec. Normally our input image comes in, it goes through some kind of computation in our CNN, and it has to predict something. Normally the thing that it has to predict is a whole bunch of 0's and a 1 here. So the way we do that is that the last layer is a softmax layer, which encourages one of the things to be much higher than the others. So what we do is we throw that away and we replace it with the word vector for that thing, Pugs or park or jumbo jet. And since the word vectors are generally 300 dimensions, and that's dense, that's not lots of 0's so we can't use a softmax layer at the end anymore. We can probably now just use a regular linear layer. So that hard part about doing this really is processing ImageNet. There's nothing weird or tricky about the architecture. All we do is replace the last layer. So we're going to leverage big holes quite a lot. So we start off by inputting our usual stuff (and don't forget with TensorFlow to call this limit_mem() thing I created so you don't use up all of your memory. One thing which can be very helpful is to define actually two parts. Once you've got your own box, you've got a bunch of spinning hard disks that are big and slow and cheap. And maybe a couple of fast, expensive, small SSDs or NVMe drives. I generally think it's a good idea to define the path for both. One path to the mount point that has my big, slow, cheap, spinning disks, and this path happens to live somewhere which is fast SSDs. That way when I'm doing my code, anytime I've got something I'm going to be accessing a lot, particularly if it's in a random order I'm going to want to make sure that that thing (as long as it's not too big) sits in this path. Anytime I'm accessing something generally sequentially, or if it's really big, I can put it in this path. This is another good reason to have your own box is you get this kind of flexibility. Okay, so the first thing we need is some word vectors. So interestingly actually the paper built their own Wikipedia word vectors. I actually think the word2vec vectors you can download from Google are really a better choice here. So I've just gone ahead and showed how you can load that in. One of the very nice things about Google's word2vec word vectors is phrases. During Part 1 when we used word vectors we tended to use GloVe. GloVe would not have a word vector for Golden Retriever, they would have a word vector for golden, they don't have phrase things, whereas Google's word vectors have phrases like "Golden Retriever". So for our thing we really need to use Google's word2vec vectors, or anything like that which has multi-part concepts as things that we can look up. So you can download word2vec. I will make them available on our platform.ai [now files.fast.ai] site because the only way to get them otherwise is from the author's Google Drive directory and trying to get to a Google Drive directory from Linux is an absolute nightmare. I will save them for you so you don't have to get them. Once you've got them, you can load them in and they're in a weird proprietary binary. It's like if you're going to share data, why put it in a weird proprietary binary format in a Google Drive thing that you can't access from Linux. This guy did, so I then saved it as text so that it's easier to work with. The word vectors themselves are in a very simple format. They're just the word, followed by a space, followed by the vector space separated. I'm going to save them in a simple dictionary format, so what I'm going to share with you guys will be the dictionary. It's a dictionary from word or phrase to a Numpy array. I'm not sue I've used this idea of zip* before so I should talk about this a little bit. If I've got a dictionary which maps from word to vector, how do I get out of that a list of the words and a list of the vectors. The short answer is like this [words, vectors = zip(*w2v_list)]. But let's think about what this is doing. We've used zip quite a bit. We've used zip quite a bit. Normally with zip you go zip(list1, list2, whatever). And what that returns is an iterator which first of all gives you element1 of list1 and element1 of list2 and element1 of list3 and then element2 of list1 and so forth, that's what zip normally does. There's a nice idea in Python that you can put a star before any argument and if that argument is an iterator (something that you can rip through), it acts as if you had taken that whole list and actually put it inside those brackets. Let's say w2v_list contained ("fox:" arr..., "pug:" arr...) When you go zip star that it's the same as actually taking the contents of that list and putting it inside there. Question: You would want star-star (**) if it was a dictionary for list? Answer: Not quite. Star (*) just means you're treating it as an iterator. You're right, in this case we are using a list. But let's talk about star-star (**) another time. But you're right. In this case, we have a list which is actually ("fox", arr..., "pug", arr...) and then what's more So what this is going to do is when we zip this it's going to basically take all of these things ("fox","pug") and create one list for those, and all these things (arr..., arr...) and create one list for those and come back. So this idea of zip star is something we're going to use quite a lot. Honestly I don't normally think about what it's doing, I just know that anytime I've got a list of tuples and I want to turn it into a tuple of lists, you just do zip star. So that's all that is, it's just a little Python thing. So this gives us a list of words and a list of vectors. So anytime I start looking at some new data I always want to test it. So I just want to make sure this works the way I thought it ought to work. One thing I thought was okay, let's look at the correlation coefficient between small-j "jeremy" and big-J "Jeremy", and indeed there is some correlation (as you would expect). Whereas the correlation between "Jeremy" and "banana" - I hate bananas, so I was hoping this would be massively negative. Unfortunately it's not, but at least it's lower than the correlation between "jeremy" and "Jeremy". It's not always easy to exactly test data, but try and come up with things that ought to be true and make sure they are true. So in this case, this is giving me some comfort that these word vectors behave the way I expect them to. Now I don't really care about capitalization, so I'll just go ahead and create a lowercase word2vec dictionary, so I just do a lowercase version for everything. One trick here is I go through in reverse because word2vec is ordered where the most common words are first. So by going in reverse, that means if there is both a capital-J Jeremy and a small-j Jeremy, the one that's going to end up in my dictionary will be the more common one. So what I want for DeVISE is to now get this word vector so that each one of our 1,000 categories in ImageNet. And then, I'm going to go even further than that because I want to go beyond ImageNet, so I actually went and downloaded the original WordNet categories, and I filtered down to find all the nouns and I discovered that there are actually 82,000 nouns in WordNet. There's quite a few, it's quite fun looking through them. So I'm going to create a map of word vectors for every ImageNet category, that'll be one set, and every WordNet noun, that'll be another set. So now my goal in this project will be to try to create something that can do useful things with the full set of WordNet nouns. So we're going to go beyond ImageNet. We've already got the 1,000 ImageNet categories, we've used that before. So I grab those and plug it in. Then I do the same thing for the full set of WordNet IDs, which I will share with you. And so now I can go ahead and create a dictionary which goes through every one of my ImageNet 1,000 categories and converts it into the word vector. Notice I have a filter here and that's because some of the ImageNet categories won't be in wor2vec. That's because sometimes the ImageNet categories will say something like "Pug" bracket "dog". They won't be in the same format. If you wanted to you could probably get a better match than this, but I found even with this simple approach I managed to match 51,600 out of the 82,000 WordNet nouns, which I thought was pretty good. So what I did then was I created a list of all of the categories which didn't match. This commented out thing, as you can see, is something which literally just moved those folders out of the way so that they're not in my ImageNet path anymore. So the details aren't very important but hopefully you can see at the end of this process I've got something that maps every ImageNet category to a word vector (at least if I can find it) and every WordNet noun to a vector (at least if I can find it) and I've modified my ImageNet data so that the categories I couldn't find, I've moved those folders out of the way. Nothing particularly interesting there. And that's because WordNet's not that big. The images are a bit harder because we've got 1,000,000 or so images. So we're going to try everything we can to make this run as quickly as possible. To start with, even the process of getting a list of the filenames of everything in ImageNet takes a non-trivial amount of time. So everything that takes a non-trivial amount of time, I'm going to save its output. So the first thing I do is I use blob. I don't remember if we used blob in Part 1, I think we did. It's just the thing that's like "ls *.*". So we use blob to grab all of the ImageNet training set and then I just go ahead and pickle.dump that so I can pickle.load it later. For various reasons that you will see shortly it's actually a very good idea at this point to randomize the file names, put them in a random order. The basic idea is later on if we use chunks of file names that are next to each other they're not all going to be the same type of thing. So by randomizing the file names now it's going to save us time, so I can go ahead and save that randomized list, I give it a different name so I can always come back to the original. So I want to resize all of my images to a constant size. I'm being a bit lazy here, I'm going to resize them to 224x224. That's the input size for a lot of the models, obviously including the one that we're going to use. It would probably be better if we resized it to something bigger and then we randomly zoom and crop. Maybe if we have time, we'll try that later. But for now, we're just going to resize everything to 224x224. So we have nearly a million images, it turns out, to resize to 224x224. That could be pretty slow, so I've got some handy tricks to make it much faster. Generally speaking, there are three ways to make an algorithm significantly faster. The three ways are memory locality, the second is SIMD, also known as vectorization, and the third is parallel processing. Rachel's very familiar with these because she's currently creating a course for the Masters students here on numerical linear algebra, which is very happy about these things. So these are the three ways you can make numerical data processing faster. Memory locality simply means in your computer you have lots of different kinds of memory. You have for example, Level 1 cache, Level 2 cache, RAM, solid-state disk, regular old hard drives, whatever. The difference in speed as you go up from one to the other is generally like 10X or 100X or 1000X slower. Like you really really really don't want to go to the next level of the memory hierarchy if you can avoid it. Unfortunately, Level 1 cache might be more like 16K, Level 2 cache might be a few megabytes, RAM's going to be a few gigabytes, solid state drives are going to be a few hundreds of gigabytes and your hard drives are going to be a few terabytes. So in reality, you've got to be careful about how you manage these things. So you want to try and make sure that you're putting stuff in the right place, that you're not filling up the resources unnecessarily. And that if you're going to use a piece of data multiple times, try each time you use it to use it immediately again, so that's it's already in your cache. The second thing, which we're about to look at is SIMD, which stands for Single Instruction Multiple Data, something that a shockingly large number of people (even people who claim to be professional computer programmers don't know) is that every modern CPU is capable of: in a single operation, in a single thread, calculating multiple things at the same time. And the way that it does it is that you basically create a little vector of generally about 8 things, and you put all of the things you want to calculate, let's say you want to take the square root of something. You put 8 things into this little vector and then you call a particular CPU instruction, which is basically take the square root of 8 floating point numbers that are in this register. And it does it in a single clock cycle. So when you say clock cycle, you know your CPU might be running at say 2 or 3 gigahertz, so it's doing 2 or 3 billion things per second. Well it's not. It's doing 2 or 3 billion things times 8 things per second, if you're using SIMD. Because so few people are aware of SIMD and because a lot of programming environments don't make it easy to use it SIMD, a lot of stuff is not written to take advantage of SIMD. Including, for example, pretty much all of the image processing in Python. However, you can do this: you can go, pip install pillow-simd, and that will replace your pillow (remember pillow is like the main Python image processing library) with a new version that does use SIMD, for at least some of its things. Because SIMD only works on certain CPUs (any vaguely recently CPU it works, but because it's only some, you have to add some special directives to the compiler to tell it I have this kind of CPU, so please use these kind of instructions. And what pillow-simd does is it actually replaces your existing pillow. So that's why you have to say force-reinstall. Because it's going to be like, Oh you already have pillow, but this is like, No I want pillow-simd. So if you try this, you'll find that the speed of your resize literally goes up by 600%, you don't have to change any code. I'm like a huge fan of SIMD in general. It's one of the reasons I'm not particularly fond of Python because it doesn't make it at all easy to use SIMD. But luckily some people have written stuff in C that does use SIMD then provided these types of interfaces. So this is something to try to remember to get working when you go home. Before you do it, write a little benchmark that resizes 1000 images and time it, and make sure it get 600% faster after you install pillow-simd, that way you know it's actually working. We have two questions. I don't know if you want to finish the 3 ways to do things faster first, one is ... Question: How could you get the relation between a Pug and a dog in a photo of a Pug and its relation to the bigger category of dog. Answer: I'm not sure. Let me think about that. Question: Why do we want to randomize the filenames. Can't we use Shuffle=True on the Keras flow_from_directory. Answer: You'll see. The short answer has to do with locality. If you say Shuffle=True, you're jumping from here on the hard disk, to here on the hard disk, to here on the hard disk and hard disks hate that. Remember it is a spinning disk with a little needle and the thing's moving all over the place, so you want to be getting things that are all in a row. That's basically what it is. As you'll see, this was going to basically work for the concept of dog vs Pug, because the word vector for dog is very similar to the word vector for Pug. So at the end, we'll try it, we'll see if we can find in dogs, we'll see if it works. I'm sure it will. Finally there's parallel processing. Parallel processing refers to the fact (hopefully as you all know) any modern CPU has multiple cores, which literally means multiple CPUs in your CPU. And often boxes that you buy at home might have multiple CPUs in them. Again, Python's not great for parallel processing, Python3 is a lot better. But a lot of stuff in Python doesn't use parallel processing very effectively. A lot of modern CPUs have 10 cores or more, even for consumer CPUs, so if you're not using parallel processing you're missing out on a 10X speedup. If you're not using SIMD, you're missing out on a 6X to 8X speedup. If you can do both of these things, you'll get a +50X speedup, assuming your CPUs have enough cores. So we're going to do both. To get SIMD, we're just going to install it. To get parallel processing, we're probably not going to see all of it today, but we're going to be using parallel processing. I define a few things to do my resizing. One thing is I've actually changed how I do my resizing. In the past when I resized things to square, I've tended to add a black border to the bottom, or a black border to the right, because that's what Keras did. Now that I've looked into it, no best practice papers, Kaggle results, anything, use it that way. It makes perfect sense because our CNN's going to have to learn to deal with the black border. And you're throwing away all that information. What pretty much all the best practice approaches do is rather than rescale the longest side to be the size of your square and fill it in with black, is instead take the smallest side and make that the size of your square. The other side's now too big, so just chop off the top and bottom portion, or chop off the right and left. That's called center cropping. So resizing and center cropping. So what I've done here is I've got something which resizes to the size of the shortest side. And then over here I've got something which does the center cropping. You can look into the details when you get home if you like, it's not particularly exciting. So I've got something that does the resizing. This is something that you can improve. Currently I'm making sure that it's a 3-channel image, so I'm not dealing with black-and-white or something with an alpha channel. I just ignore them. What we're going to learn next time when we start is we're going to learn about parallel processing, so anybody who's interested, feel free to start reading and playing around with Python parallel processing. 