 [MUSIC] My name is Ryouta, I am sharing this session. And it's my great pleasure to introduce Zoubin. Zoubin doesn't need an introduction, but let me cover the basics. Zoubin completed his PhD 90, 95, at MIT with Mike Jordan. After that, he moved to Toronto to do postdoc with Jeff Hinton. Afterwards, he has been a founding member of Gatsby Computational Neuroscience Unit from 98 to 2005, and afterwards, he has been professor at Cambridge University here at the Engineering Department. He has been also the founding Board member of the Allen Theory Institute and most recently has been the chief scientist at UBA. So I'm very excited to hear about the latest exciting research happening at both the university and at UBA on probabilistic machine learning and AI.  Thanks. Thanks, Ryouta. Can you hear me? It's up. It's on. Yeah, great. It's really exciting to be here. This is the second talk I'm giving today. The first one was at 8:30 in the morning and it was being translated into Mandarin. So every time I said something I had to wait for somebody to translate and it made it very difficult to give a lively talk, especially because it was 8:30 in the morning. I've had a few coffees now, so I'm much more awake. It's a really exciting time to be working in Machine Learning. There's just so much going on. The field is exploding, it's crazy. I never imagined that it would be like this 20 years ago or so, and I was starting to get interested in this. And it's been really, really exciting because there's so many applications. 20 or 30 years ago there were a lot of people interesting in machine learning. We were interested in getting AI systems working. We wanted to get computers to learn from data. But it was all kind of an academic thing. It really didn't seem like it was anywhere close to being useful. People developed algorithms just out of scientific curiosity and test them out on small toy problems, on small computers and so on. And boy have things change, I mean they're just dramatically different now. We have just to put, just six examples that I could fit on one slide. We can put many, many more. We have speech language technologies like speech recognition, machine translation, question and answering, and dialogue systems that are really impacting people's lives. We have computer vision systems that are doing things that we really didn't imagine were possible. Like for example, this famous example from Stanford of an image captioning system, where, for example, we're the same age. That the computer has never seen before, it produces a caption which says [INAUDIBLE] black shirt is playing guitar. And that just seems like magic for computer vision systems of just a decade ago. We have machine learning that's really impacting the sciences in terms of areas like bio informatics, astronomy, a lot of sciences that are becoming very, very data intensive. We have of course the systems that we've grown very accustomed to in terms of recommender systems being all driven by machine learning technologies. A self-driving car is something that I've become much more involved in recently. Really look like they're not far from a reality that could affect millions of people around the world. And just as the sixth example, even the world of finance which has for many years been resistent to machine learning Is now seeing a huge number of start-ups and disruption by people trying to use machine learning methods. So there have been these tremendous advances, and some the most popular ones are these great examples from our colleagues down in London, DeepMind. This was obviously a breakthrough I'm sure all of you know about that really launched DeepMInd as a start up, and caught the interest of Google among others companies. Where they managed to get a machine learning system, that combined reinforcement learning with some deep learning to actually perform at Atari Games, about half of the Atari Games it could play at super human levels. Which is really pretty exciting, and people didn't think that that was gonna be that easy. Then, they moved on to Go, where now, they've actually managed to beat the world champion at Go. Another kind of landmark achievement for AI and machine learning, again, using heavily ideas from reinforcement learning and some deep learning. And how many people know what this is an example of? Have you seen this? Do you know what this is? Raise your hand, yeah? It's poker, so this was another landmark achievement. Maybe, it didn't get as much press. But I think it should have gotten as much press. From just a few months ago, this is Libratus system developed at Carnegie Mellon University. Libratus is a computer. This is how much it's winning. And brains are a bunch of human expert poker players and this is how much they are losing against the computer. The humans are not too happy here. The computer really wiped out the human experts. And poker is really interesting because unlike Atari and Go, the state is very hidden in poker. And most of us would think of poker as a psychological game that involves not just incomplete information, but bluffing is essential to poker. So you can't be a good poker player unless you're good at deceiving people. And so this was quite an interesting landmark achievement recently. And there was a team from Alberta that also similarly had a breakthrough in poker, just a little bit after this one. So for people coming into the field of machine learning, it seems to be a big complicated mess. There's so many different methods, and the methods are sort of rediscovered by many different fields. And there's sort of these relationships between the methods that are not necessarily clear to people. And I think it's useful to spend some time trying to organize the world of machine learning and think about, well, fundamentally, what's going on? How are these methods related to each other? And in my world view, everything is just examples of some fairly simple concepts. What I'm gonna do is I'm gonna spend just a little bit of time focusing on neural networks and deep learning. Just because it has been really so revolutionary in the last few years. But I'm also gonna spend a lot of my time talking about my passion, which is probabilistic modeling and Bayesian methods. Because again, I feel like there is at least the revolution in the making there. And it's not incompatible with what we've done in neural networks and deep learning. Any questions? We can hopefully take some questions at the end. But if you wanna interrupt me, feel free to interrupt as well. So let's talk a little bit about neural nets and deep learning, because as I said, they have been so revolutionary. The interesting thing is that as probably many of you know, it's not the first time that neural nets have captured people's attention. Neural nets were actually, back in the 1980s, when I was a teenager, that's what really got me excited about AI. Neural nets were going through a big wave of excitement in the middle to late 80s. When I was an undergraduate and that wave of excitement kinda died down and is come back up. So what are neural nets? Just to clarify, what we mean, they're many different kinds of neural net. But the main kind of neural network that people work with right now in the deep learning community are essentially tunable, nonlinear function approximators, okay? So a neural net is tunable, nonlinear functions with many parameters, but we could call these parameters theta. I like to use the symbol theta for parameters cuz that's often what's done in statistics. In a neural net, that would be the weights in your neural net. And generally, neural nets model some input-output relationship. That's what I mean by the nonlinear function. In this case, maybe the inputs are x and the outputs are y. And interestingly, neural nets tend to be nonlinear, both in terms of the parameters theta and in terms of the inputs x. The simplest kind of neural net is a very old idea. This is the equation for logistic regression That's basically a neural net that takes some inputs, does some linear projection of those inputs, and then maps that into a nonlinearity that turns into a probability of a class being one or zero. For example, if you're doing classification between, I don't know, images of cats versus dogs or something like that. So that's the simplest neural net I could think of. And if you think of most neural networks, essentially, what they are, are concatenations of these sorts of simple, primitive operations. So a multi-layer neural net, this one only has one layer. A multi-layer neural net models the overall function, from x to y as a composition of functions, each of those are the layers in the network. And here is an example of a two layer version of that, which might correspond to the network that looks like this. The first layer might have elements that look like this. And then you might linerally combine them in a second layer to produce the outputs, okay? In this case, maybe with some Gaussian noise, in this case, for classification. The details don't matter, you can mix and match these various forms of neural net. So these are nonlinear tunable functions, well what do you do with them? How do you train them? Generally, almost all training algorithms for neural networks can be seen as maximizing a likelihood. The likelihood is the probability of the outputs given the inputs, or maybe a penalized likelihood if you're trying to avoid over-fitting. Using some variation on stochastic gradient descent optimization. Maximizing likelihoods is an idea that's about 100 years old in statistics. Gradient descent methods are probably one of the simplest forms of optimization. And so boiled down to it's essence, a neutral network is simply a nonlinear function approximated combined with a little bit of very basic statistics and a little bit of very basic optimization. The interesting thing is that a lot of statisticians are very confused about what neural nets are. I've had statisticians come up to me and say, so what's deep learning, what's neural nets? And then I explain this to them, and they say, is that it? And then they're like, well, then I'm not interested, and they sort of move on. Whereas, we kind of know in machine learning that even though these are incredibly simple, boring things, if you throw enough data at them, they'll do really interesting stuff, okay? So that's what they are, and what's been happening? What is different between, say the late 80s when I was playing on using neural nets for parsing natural language and now? Well, there are a whole bunch of things that are different. So one of them is the major rebranding exercise, and I applaud the people who manage to pull this off against all odds. So now we call it deep learning. Cuz people though neural networks were kind of old and passe, but deep learning sounded like something new and deep. And so the rebranding exercise worked really well. But there are a whole bunch of new ideas as well that were developed. Some innovations in terms of architectures, in particular, going deeper, using more layers. Using different activation functions instead of sigmoid, rectified linears. New ways of regularizing them with things like drop out. Rediscoveries of some old and kind of interesting ideas like LSTMs for time series, etc. So there were some innovations, and the rate of innovation just keeps going. But importantly, there are just vastly larger data sets right now. Really web-scale data that you can throw at these things. And along with that, we have vastly larger compute resources. We can actually run stuff on the cloud, with GPUs. People now talk about like hundreds of cores on a single experiment and things like that, that were unimaginable a couple of decades ago. We also have much better software tools, things that have really managed to democratize people's ability to run neural networks and reproduce experiments. To the extent that you could be a high school student with good programming skills and not really even understand what a derivative is. And you can reproduce somebody's blog post on neural nets, and think, wow, cool, I'm doing AI. Right? And that's really what's transformed the inflow of excitement and interest. That's also translated into vastly increased industry investment, just because these systems have started to produce exciting results. And along with that, there's been massive media hype. Okay, and the media hype, the many ways of explaining it. One of them is when companies start spending hundreds of millions of dollars on something, the media pays attention. If it looks like it has something to do with the brain and Terminator and stuff like that, then the media go totally crazy. And you can imagine all the stories that come out of that. Of course it feeds on itself, and then the next company comes along and says, no, we're missing out. We need to spend a few hundred million on this as well, right? Okay so, this is what's happened since the 80s and 90s, and it really has been a revolution. I wanna be grounded in reality, but I don't wanna minimize from the fact that it has had a major, major impact. But it's important not to get caught up in the fantasy that AI is solved, or something like that. If you actually look at these nonlinear function approximators, they have a lot of limitations. And here are a few of their limitations. So, one of them is that neural nets tend to be, by and large, very very data hungry. Humans are pretty good at learning from a few examples. And neural nets, in most cases, need large numbers of examples, sometimes millions of examples. They're are also extremely compute-intensive. So if you think about, again, to use our poor primate brains as an example, we operate with the power consumption of a light bulb basically. And if you look at a cloud full of GPUs or something, that's using massive amounts of energy and compute to solve fairly simple problems. Neural nets and deep learning systems are actually quite poor at representing uncertainty. And this is something that really bothers me, and I'll talk about in some detail very soon. They're also easily fooled by adversarial examples. So, how many people have seen, here, adversarial examples in neural nets. Raise your hand if you've seen them. Raise your hand if you haven't seen them. Okay. I need to account for the fact that some people just don't like raising their hands. You always have to ask both questions. So as you've seen, adversarial examples are examples that are either perceptually indistinguishable from another example, but that give the neural network a wildly different output, or that the examples are wildly different from anything that you've seen. And the neural network is confidentially giving you some output, some response for that. So again, it has something to do with the neural net's inability to represent uncertainty. Neural nets are finicky to optimize. It's actually quite hard to do the optimization. They're generally uninterpretable black-boxes. They lack in transparency, and they're difficult to trust. And actually, when you look at examples, an industry where things get deployed, if you're just doing something fairly harmless, maybe you're just classifying images for fun, then it's okay to be wrong and not know why you're wrong. But if you're doing something actually rather important, like detecting pedestrians or something like that in a self-driving car, then the fact that you have a non-transparent black-box system that you can't then easily debug, becomes potentially a non-starter. You can't even think of using that, unless you have some reasonable guarantees, and some reasonable ways of understanding what it's doing and debugging it. So these are some of the limitations. And what I wanna do is, I want to actually talk a little bit about going beyond deep learning. And of course I have my own particular bias. There are many ways to go beyond deep learning. And in fact, to be fair, many people realize all of these limitations. And if you talked to some of the best research labs in the world, they're working on various aspects of trying to solve these things. But one way of thinking about going beyond deep learning is to take maybe a slightly different approach. To machine learning. So, a lot of people when they learn about machine learning, they take a course. And the course basically- I've looked at a lot of these syllabuses around the world, and the syllabus usually looks like- sorry, I found the back of this. It just looks like a toolkit of methods. So you'll learn about this, you'll learn about random forest, then you'll learn k-means, then you'll learn about back propagation, then you'll learn about Q-learning, and it's just a bag of stuff, right? There a little bit of an attempt at making some theory connections, like you probably want to have some theoretical analysis of empirical performance, like generalization error and so on of various different methods. The framework that I'm gonna talk about which I'm quite in love with, although it has its own limitations, is thinking about machine learning as kind of a science of learning models from data. And so most things can be put under that rubric, not everything. But the idea is that there is sort of like an overarching scientific framework for doing machine learning, and the details of the algorithms are just that, they are details. They matter, but you kinda look at the bigger picture and the bigger framework. So what do I mean by learning from data and what do I mean by modeling, in particular? So here is this sort of view of machine learning as probabilistic modelling. Well, let's step back and ask ourselves what is a model? Okay, the term model is use in many, many different fields, it means different things to different people.  But one way of thinking about models is, a model is a description of possible data you could observe from a system, okay?  So the way you tell, it might also be an understandable, interpretable or something like that, like a model in the sciences or something like that, or an economic model, or model in biology, and so on. All of these, the term model gets used a lot, but I think ultimately if the model can't make some claims about potential data, then it's very hard to know if it's a good model or not. At some point, you have to ground the model in data, and not the data that you had before because very easy to say, my model fits my old data perfectly. The model should say something about data you haven't observed yet. Now because of that, modeling is intimately tied to the concept of uncertainty. So to make any claims about data we haven't observed yet, we have to be honest about our uncertainty in terms of the data we haven't observed yet. You can't have, for example, a weather forecasting model that gives you a precise temperature for tomorrow without tell you what it's uncertainty is, it just doesn't make much sense. So we're gonna use the mathematics of probability theory to express all forms of uncertainty and noise associated with our model. And then, it turns out that, that same mathematical framework can be used to do inferences about unknown quantities, adapt our models, make predictions, and learn from data. And all of this used to be called probability. This part used to maybe sometimes we called inverse probability until early in the 20th century when Fisher, who's a classical, sort of one of the founders of classical statistics said, that probability, that inverse probability, you're just followers of Thomas Bayes, you're Bayesians. And he coined, actually he coined the term Bayesian as a derogatory term to distinguish himself because he was creating a new view on statistics. So we can call it inverse probability if we wanna go back in time, and the framework basically boils down to a very, very simple set of concepts. So in the world of probabilistic modeling and Bayesian machine learning and so on, there are only two kinds of things. There are stuff you observe or measure, that's what you call data, and then there's everything else. Everything else is stuff that you didn't measure. So necessarily, you have to be uncertain about the stuff you didn't measure. And we're going to use the mathematics of probability theory to represent all forms of uncertainty. Just like we use calculus to represent rates of change, probability theory is the mathematics of uncertainly. And actually don't like the term randomness because it connotes something weird. Actually, I don't believe much is random. For example, I can be uncertain about how many people are in this room, right? I'm certainly uncertain about that, but the number of people in this room is not random in a usual colloquial sense of the word. And when I look at this room, that's data that my brain gets about the number of people here. My brain is trying to process that. And I'm guessing maybe there are between, I don't know, 40 and 90 people in this room. Probably more than 40, I don't know, maybe between 50 and 90. All right, so how do I represent that uncertainty? Well, the idea in probabilistic modeling is use probability distributions for all forms of uncertainty, okay? So you use probability distributions for uncertainty. You get data, for example, I'm looking. And I might collect more data, let's say I start counting. That's just collecting more data. And at some point, my uncertainty collapses. Learning is the process of going from a state of knowledge before observing the data, that's the prior, to a state of knowledge after observing the data. And the process by which you go through that is a very simple process. You take your prior, which is the distribution over your uncertain quantities, your hypotheses. You multiply it by the likelihood, which is a probability of the observed data for any one of your hypotheses. Like the number of people in the room would be various hypotheses I could entertain. You multiply the prior by the likelihood. You renormalized summing over all the hypotheses you're willing to consider. And then you get the posterior distribution over hypotheses given data. It's as straightforward as that. And the beauty of this is that we can use this as a unifying principle for a lot of machine learning. Not necessarily for the parts of machine learning that involve decision-making, like reinforcement learning. That involves an additional thing that we need to add on top of this Bayesian framework. But for most of the rest of machine learning, we can actually use this to understand what's going on. So it turns out actually Bayes' rule isn't even a fundamental rule. Bayes' rule is a corollary of two even-simpler rules from probability theory, the sum rule and the product rule. And here are the sum rule and the product rule written out in a boring form, as you might find in a textbook. Sum rule says that the probability of some variable is the sum or integral if that other variable is continuous over some other variable y of the joint probability. And the product rule says that the joint probability of two variables, x and y, can be factored into this marginal probability of x times the conditional probability of y given x, or the other way around. The basic rules you might learn about in a textbook. If we take these x's and y's, and we replace them by things that we might actually might be interested in. Like D for data, theta for parameters of a model. And then condition everything on m, which may be like the model class like what we are currently considering. Then we get a single equation, this is now Bayes' rule, that defines learning from data for parameters. And it says, well, you have a model with some parameters. Before observing the data, you have a prior. Which represents your uncertainty about what those parameters could be. Then you observe the data, so you multiply the prior by this likelihood term. And then you renormalize it with this interesting quantity called the marginal likelihood. And that gives you the posterior over parameters given the data. It's very straightforward. If you had to make predictions, then probability theory tells you that to predict any unknown quantity, call that unknown quantity x, given the data that you've observed. Then the sum and product rule tell you there's only one and only one way of doing prediction. And that is average or integrate the predictions for different parameter values weighted by this posterior probability over parameter values that you've considered in the equation above. So prediction in the Bayesian framework is naturally an ensembling process where you're averaging, or voting together, many different predictors. And model comparison is essentially just the application of Bayes rule at a level suddenly higher up from parameters to a level of models. Like, for example, are there, I don't know, three or four clusters in the data or should I have 10 or 20 hidden units? These are all modeling questions, right, and you could answer them in principle using this framework. So any questions about that? [BLANK AUDIO]  Can you be more specific with why the enforcement learning doesn't fit into this framework?  So the framework I've talked about talks about inference from data. Think about that as knowledge acquisition or learning from data. Reinforcement learning falls into decision theory. So actually, it's not incompatible, it's just that it's a part. So there's Bayesian inference and there's Bayesian decision theory. Bayesian decision theory says one side inferred everything I'm interested in. Now maybe there is a loss function which tells me what the loss is for various different states of the world. And I should take actions that minimize my expected loss, and that's reinforcement learning. Loss, reward, it's the same thing. It's not immediate reward, you're looking at long-term reward. So it's Bayesian sequential decision theory would be how we would think about any sequential decision problem as in reinforcement learning, and it's nothing new. Been around for 60 years at least, yeah.  I've got a question about the learning behaviors. So what we're trying to learn is the posterior of the parameters given some observed data?  Yes.  Is it assumed that the other quantities in that equation are all given beforehand, so the likelihood of the parameters in the model m is something that we're given?  So usually, this term here, the likelihood is something that we assume is known. But the funny thing is that actually, we shouldn't really necessarily assume that the likelihood function is even known. Maybe the likelihood function has some parameters, but you have to infer. But if you consider a family of likelihood functions, you can just say that they're just extra parameters you have to infer from data that allow you to choose between different likelihood functions. It gets a little bit subtle. But just like in, basically, in almost all of statistics and also machine learning, people assume that the likelihood or the loss function is given to you, and then they try to optimize it. But I actually think your question is really insightful, because in most of those cases, that's the weak point in the model. That thing was an assumption. Everything is an assumption. The only thing that's not an assumption is the data. Everything else is an assumption. And so from the Bayesian point of view, what we really should be doing is questioning every assumption, and not just questioning the assumptions, but actually doing something about that. Trying to improve on those assumptions. So loss functions and likelihoods are also assumptions. Let me move on, so why should we care about this whole framework? I mean, deep learning has been so spectacularly successful that you would think, well, we can forget about all this probability theory and just run whatever Adam on my tensor flow model, and I'll be fine, or whatever. Well, here's at least one of the reasons I care, or two of the reasons I care, actually. So one of them is we really want, I think, we really want calibrated models, where the prediction uncertainties are something we can believe in. And it's not just kind of a nice thing. I think it's actually a necessary thing. We really need machine learning systems that know when they don't know. That if they're given a new input, they'll tell you, I don't know what the answer is. We don't want them to be confidently making stupid mistakes. If they're confidently making stupid mistakes, they'll be fine in the lab. They'll never get deployed in the real world. The other thing is that this framework actually gives us a really nice way of doing automatic model control and structure learning. So learning the structure of models automatically from data. And that's really beautiful, because of the things that annoys me the most about current machine learning practice is that people just come up with stuff out of thin air. Right, and it's just like a cottage industry of people randomly inventing stuff, testing it out a couple of times, and then saying, this works better than other things. It's just not a systematic science, it's a hodgepodge. It's really disappointing. And we would like to make it a systematic science. And actually, the way to make it a systematic science is get the human out of the loop.  [LAUGH]  I mean, maybe, make the human do other things or kind of oversee things. But the search over models should be something that's done by a computer systematically. And this framework gives us a nice way of doing that. So just to be concrete about what does it mean to be Bayesian, here it is in the context of a neural network. So in the context of a neural network, what are the sources of uncertainty? Well, the parameters are uncertain, right? Cuz you don't know what the parameters are. And actually, the structure is uncertain. People haven't really dealt with structure uncertainty. But in an ideal world, I would love to see both parameter and structure uncertainty dealt with in a neural network. It just might not be very practical. But that's why I said in an ideal world. And actually, their sorta world of neural networks and deep learning, and the world of Bayesian machine learning are not in any sorta conflict. They are actually quite compatible. The two kinda live in different spaces. So neural networks are a kind of model, some kind of function approximator. And Bayesian methods are just a framework for thinking about learning. So people, for a couple of decades at least, have thought about applying Bayesian methods to neural networks. And so here are some examples going back to the early 90s. People like David MacKay, Radford Neal, Geoff Hinton worked on this. Even Yann LeCun worked on this, even earlier, even though he didn't realize he was doing this. He will admit to that. He was sort of analyzed from a physics-y perspective, but it turned out, he was doing Bayes' rule on neural network rates in the late 80s. Who's sitting here has really nice paper on Stochastic Gradient Langevin Dynamics. It's a real beautiful idea where you can take stochastic gradient descent, SGD, add noise to it in a particular way and show that it approximates actually doing Bayesian Learning in neural network parameters. And some of our work in the machine learning group in Cambridge has also been kindadeveloping better ways of doing Bayesian neural networks, Bayesian deep learning. We had a workshop at NIPS, which was great fun last year. We are trying to have another one this year. And here is an example of why we might care. If you have some data and you're fitting a neural network to some input-output data, where you have data, you can afford to be confident. But where you don't have data, what you really want is something that shows a great deal of uncertainty. So that's sort of the ideal thing that we want. So when do we need probabilities? Just to kind of summarize some of the threads of thinking I've described. I think we need probabilities when we want to really carefully represent uncertainty. So this includes things like forecasting, decision making. Usually, decisions are made on the basis of future consequences of actions. So we need to have a representation of the consequences of our actions when we're learning from limited, noisy, or missing data. For example, in medical domains, this is very, very useful. Or if we're doing complex personalized modelling like in advertising, or again, in medicine. All data compression methods are basically probabilistic models, either implicitly or explicitly. And for automating this sort of scientific approach to modelling, this is quite useful. So now, as mentioned, I'm sort of leading Uber's AI research right now. And so why would a company like Uber care about something like probabilistic modeling? Well, it turns out that actually, it's everywhere that we look. So in the research lab that I'm trying to build at Uber, we have people with deep learning expertise. We have people with reinforcement learning and probabilistic modeling expertise, and so on. They're a bunch of different communities in there. But when we look around, there are actually a huge number of very Clear probabilistic modelling problems. So if you look at the kinds of data, the kinds of modelling that we have to do to optimize the service that we provide, this involves understanding the spatial temporal dynamics in being able to forecast that for both drivers which is the supply and riders which is the demand side. We have to be able to model ETA, how long it takes for food or person or package to get from A to B, we use models in all of our forecasting and planning. We're modeling cities, we're modeling traffic. It's all about modeling stuff. And these are complicated systems to model. And so we can use combinations of deep learning and sort of function approximators, and probabilistic modeling to build sophisticated forecasting models and then to optimize the system on top of that. So the service is just better for everybody. And that is, I said all of that and I'm not even talking about self-driving cars which is a whole, now there are several hundred people working on self-driving cars at Uber. And there are many modeling problems as well in that domain. And of course, we're also try to build personal aviation service which I'm sure will come with this modeling problem. This is by 2020 we have committed to doing trials in Dubai and Dallas of what you and I would all call flying cars. But the guys who run that at Uber hate the term flying cars. Cuz they don't actually drive on the road, they're sort of like giant drones that you get into and you fly from A to B. So watch out In the next few years, but this is part of what we're also trying to do. And again, although initially the vertical takeoff and landing vehicles will be piloted, eventually these will be autonomous electrical vehicles. So all the same sorts of inference perception and modeling problems that occur in the self driving domain will also occur in the flying domain. So that's it for the uber stuff. For the rest of the time that I have, a few more minutes, what I wanna do is I wanna talk about three areas that I think are really exciting at the cutting edge of machine learning and AI right now. And these are areas that my group and myself are working on, that we're very excited about. I've already talked a little bit about Bayesian deep learning. I'm not gonna talk much more about that. I'm basically gonna talk about probabilistic programming, the automatic statistician, and Bayesian optimization, very briefly on each of those. So let's start with probabilistic programming. The idea of probabilistic programming is all this probabilistic modeling stuff is nice, but it's really time consuming to define a probabilistic model and then derive an inference algorithm and implement the inference algorithm and so on. And it's not just time consuming, but it's incredibly error prone. The solution to that comes in the form of this field of probabilistic programming. And the idea is a two-part idea. One of them is, instead of defining your model as a bunch of equations, or something like that or a graphical model, a diagram in a paper, define your model as a computer program, okay. The computer program that can generate data. Now it could be a conditional data generator, like if you're trying to generate y as a function of x. The program could take an x and produce possible ys, that's a conditional program. Or it could be a generative model of like x, like if you're trying to build a generative model for language or speech or something like that it could generate the whole signal for you. In any case, it's a computer program. And that idea is obviously very old, like we have simulators. Many, many fields of science and engineering have relied on simulators, which are computer programs for their models. The really interesting bit of probabilistic programming is this second part, which is given your model, defined as a simulator, we can actually define a Universal Inference Engine, and that is a system that will basically do Bayes rule on computer programs. So what does that mean actually in practice? What that means is I have a simulator for potential data which has a bunch of random variables in it. But now, I have some real data, that's the data I've actually observed. And what I'd like to do is I would like to figure out, what are the plausible hidden variables or random variables in my simulator that are consistent with my real data? If I can do that, that's actually solving Bayes' rule on my computer program. Well, let's think about it this way. We're all used to running computer programs in the forward direction. You give them inputs and they produce outputs. This is like thinking about running them in the backward direction. Given that I know what the outputs are, that was the data. What are the calls for the random number generator that are consistent with those outputs? That's the basic concept. And this sounds kind of like magic, but it's actually possible. And there'd been a whole bunch of systems here around that have developed on this idea. And I wanna call out, in particular, a system that was developed at Microsoft research, in particular, in this lab here, which was really way ahead of its time. Over ten years ago, infer.NET was really like the most powerful policy programming language for scalable modeling. But by now, we have a whole bunch of different languages that have been developed including sharing which is a language developed in my group and I will show you some examples of that. And then under the hood, what's happening is that we have a whole bunch of inference algorithms. These are methods that can be used to figure out what those hidden variables are, giving the data. And these are various forms of Markov chain Monte Carlo, variation inference, or particle filter and etcetera. Here's an example from the programming language Turing. Maybe what I'll do is I'll play you the video. It's just easier to explain I think. Okay, so let me just make sure. Okay, I guess that's right. All right, so if you're familiar with, how many people know what a hidden Markov model is? Raise your hand if you know it. Okay, good, almost everybody. So here is a hidden Markov model defined in this probabilistic programming language, Turing. The main things that are here are this model construct. Turing is based on Julia, a kind of nice, modern scientific programming language which is meant to be a kind of a replacement for MathLab and r and everything else. And here's an h and m defined, and the main thing to notice is this little tilde character which is drawn from. So this define as it's drawn from the categorical distribution, either the states have it in Markov Model, and these are the outputs, like a normal, drawn from a normal distribution. This, I would argue, is about as natural and compact representation of an HMM, as I would imagine having in a programming language. So, the surface form is really nice. In, for instance like this, you say I'm gonna do sampling on my HMM, and I'm gonna use particle GIBs with some parameters. So this is a call to the inference algorithm, you don't have to implement that. And then what happens is you run this. And a few minutes later, it actually has done inference. This is just a visualization of the hidden states over time, which is a bit boring, they're jumping around a little bit. I'll just show another example maybe, and then we can move on. Okay, so here is another example of a Hidden Markov Model, but what this is, is it's a Bayesian Hidden Markov Model. Bayesian Hidden Markov Model, by this we mean, a Hidden Markov Model where we're actually representing the uncertainty in the transition matrix and in the parameters of the emission model. So we just add a couple four extra lines to this, to turn our previous code into a Bayes and HMM. And again we can do it and solve that In this case, it's a combination of Hamiltonian Monte Carlo and particle gibbs, all wrapped into an outer loop of inference, with gibbs. And this is just a visualization of inference on that. And now, it has some of uncertainty about where the means of the different observations are, and that's shown here. Okay, and maybe I'll show one last one. So this is just defining a mixture of categoricals. Let's jump ahead. This is defining Latent Dirichlet allocation. Again, the code for Latent Dirichlet allocation is really pretty straightforward, okay. So it's sort of nice and abstract that way, okay. So that was probabilistic programming. Let me move on, question? Yeah.  Is there any restriction on what effects you can do? Or are they all just MCMC, kind of-  So the restriction is on stuff that we've implemented, okay. And so, there are a few algorithms we've implemented under the hood. My team is implementing some other ones. Different probabilistic programming languages have different things that people have implemented. So we've done various versions of particle filtering, MCMC with Hamiltonian Monte Carlo, NUTS. We're trying to reproduce what Stan does, and benchmark against that. I mean, completely honestly, it's a very nice framework cuz it's all naturally embedded in Julia. It's not as fast as a language like Stan because that's actually optimized for finite models whereas we can actually deal with a broader class of models. And we're working on making it all much more efficient and scalable. And this is what's going on actually, interestingly, I have an effort like this going on at Cambridge. And the Uber AI labs know of good men who's one of the pioneers of probalistic programming is working at Uber now and we're developing a scalable kind of industry strength probalistic programming language at Uber as well.  Can you define how scalable it is? These frameworks not only take time, but are not very scalable.  In terms of data?  And in computation.  Yeah, I mean, the target that we have is as scalable as deep learning frameworks.  Okay.  Okay, so parallelizable, run on GPUs, etc. And in fact, we're piggybacking on top of deep learning kind of frameworks that exist, and adding a layer of probabilistic programming to them, yeah?  Related to scalability advantage, I don't know this question is in the right context, but what happens if you're learning parameters that are actually related to each other, being conflict with each other? Does that then make it that you need to learn covariance matrices, as well, or-  Yeah, in theory, if you have a model with many parameters, in the posterior, the parameters will be correlated, even if in the prior, they're not correlated. In practice, what people do is either they ignore that, which is an approximation, or they use a sampling method where you can infer the correlation from the samples. Let me move on cuz, I mean, I've been taking some questions along the way. But I wanna leave at least a couple of minutes for questions at the end as well. I'm gonna move on to very quickly talk about the two other ideas, Bayesian Optimization. The idea of Bayesian Optimization is that we wanna do global optimization of some black-box functions that are expensive to evaluate. So this is the basic idea, find the arg max of f(x) as a function of x. Very simple idea but the key thing is, what if evaluating f(x) costs quite a lot of time or money or energy or whatever it is? What do you do then? Well, the right thing to do then is you treat this as a problem in sequential decision-making and you model the uncertainty that you have in the function f to figure out what the optimal actions are. And this has just a huge number of applications. And it's important to think beyond the little world of machine learning to think about all of the other aspects of science and engineering that might involve optimizing something. Anywhere we can optimize something, we can actually do a Bayesian Optimization, I think. If certain criteria hold. This is just Bayesian optimization in a nutshell. The thing that drives it is, you're making decisions about where to evaluate the function. This is the function, let's say, you're uncertain about the function, that's why it's sort of shown with the shaded region. You've evaluated some data. This is why you're being Bayesian. You're actually trying to represent the uncertainty that you have in your function. And then, based on that uncertainty, what you'll have is something called an acquisition function which is telling you where it's best to evaluate your function next. It's sort of trading off exploration and exploitation cuz it's a decision making problem. So where is this useful? I've already said I actually think it's useful to think about any optimization problem. In particular, if evaluating the function is expensive, if evaluating derivatives is hard or impossible. If there is noise in the function evaluations, or there may be noisy constraints. Or there is prior information about the function that you might wanna use in optimization or if you need to optimize many similar functions. So in all of these contexts actually helps to think about things in terms of Bayesian optimisation. So the last thing I wanna talk about is very, very quickly, this idea of the Automatic Statistician. And the idea here is that we have a lot of data, but we don't have enough data scientists. So what can we do? We can super power the data scientists or we can replace the data scientists. And that's really the idea of building systems that automate model discovery from data. And I see Riota standing up. That's what happens when you take questions in an already too long a talk. So no, no, no, it's fine. We can sort of run through it. Maybe I'll just show you the video which only takes a couple of minutes. So this system basically takes raw data in the form of a table. It searches over good models. It finds a good model. And then it turns that model into a report that describes patterns in the data to a human user. And the key thing is that this is really meant to be the opposite of a black box. You really want a transparent box. You want the computer to explain in English what is found in the data to you. So here is just a demo of the Automatic Statistician running on a classic, tiny data set called Iris. So you load up the data. And then, it turns away and thinks about lots of different models. And then what happens is you wait. It says your report is being prepared. This is the report that it prepares. It's actually found, it's evaluated a few different models. Actually, what happened there? What happened was that the report was actually a dynamic object. So it keep thinking about models and it keeps updating the report which is a web page. Now the report is sort of finished and this is a very basic version where it's doing some visualizations. What it decided was that the best model for explaining what's in the data is to assume that there are just three clusters in the data. Which actually, if you know Iris, is not a bad assumption, sort of discovered that on its own. And then it produces a whole bunch of visualizations that show you that there's hypothesis of three clusters in the data Okay, so, I think I'm now basically ready to move, jump to the conclusions. That's just the Automatic Statistician. We've done it for classification as well. And I wanna conclude by saying a lot of the things I've described in this talk are in this review paper that I'm happy, if you can get it online, to share with people. And I wanted to thank a whole bunch of collaborators that I've had. Thank you.  [APPLAUSE] 