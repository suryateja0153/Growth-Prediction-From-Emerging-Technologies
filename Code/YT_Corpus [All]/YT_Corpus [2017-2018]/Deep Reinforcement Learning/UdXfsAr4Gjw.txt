 so whereas in transfer learning you have a sequential process we learn from toss a and then transfer then to toss B in multi toddler nning you start off simultaneously trying to have one your network do several things at the same time and then each of these tasks calls hopefully all of the other tasks let's look at an example let's say you're building an autonomous vehicle building a self-driving car then your self-driving car will need to detect several different things such as pedestrians detect other cars detect stop signs and also detect traffic lights and also other things so for example in this example on the left there is a stop sign in this summation there is a car in this image but there aren't any pedestrians or traffic lights so if this image is an input for an example X I then instead of having one label Y I you would actually have four labels in this example the no pedestrians there is the car there is a stop sign and there's no traffic lights and if you try to check other things then maybe Y I have even more dimensions but so now let's take of these four so my eye is a 4 by 1 vector and if you look at the training set labels as a whole then you know similar to before we'll stack the training data labels horizontally as follows y1 up to Y M except that now Y is a 4 by 1 vector so you should be z' is a tall column vector and so Y this matrix capital Y is now a 4 by M matrix whereas previously when y was a single real number this would have been a 1 by n matrix so what you can do is now train a new network to predict these values of Y so you can have a neural network equal x and output now a 4 dimensional value for y notice zero for the output layer I've drawn four nodes and so the first node when we try to predict is there a pedestrian in this picture the second output will predict as there a car here predictors there a stop sign and this will predict maybe is there a traffic light so why hats here is four-dimensional so to train this neural network you now need to define the loss of the neural network and so given a predictive output Y hat I which is you know four by one dimensional good loss average over your entire training set would be one over m sum from I equals 1 through m sum from J equals 1 through 4 of the losses of the individual predictions so it's just summing over the four components of pedestrian car stop sign traffic light and this script L is the usual logistic loss all right so just to write this out this is negative Y J I log Y hat J I minus 1 minus y log 1 minus y hat and the main difference compared to the earlier binary classification examples is that you're now summing over J equals 1 through 4 and the main difference between this and softmax regression is that unlike softmax regression which assigns a single label to a single example this one image can have multiple labels so you're not saying that each image is either a picture of a pedestrian or picture of a car features also and pictures often quite traffic light you're asking for each picture because they're the pedestrian or car stops on a traffic light and multiple objects will appear in the same image in fact in the example on the previous slide we had both a car and a stop find that image but no pedestrians and traffic lights so you're not assigning a single label to an image you're going through the different classes in asking you putting each of the classes does that class does that type of object appear in the image so that's I'm saying that with this setting one image can have multiple labels if you train a neural network to minimize this cost function you are carrying out multi task learning because what you're doing is building a single neural network that is looking at initial image and basically solving for problems that's trying to tell you that each image have each of these four objects in it and one other thing you could have done this just train four separate neural networks instead of train one network to do four things but if some of the earlier features in your network can be shared between these different types of objects then you find that training one neural network to do four things results in better performance then training for completely separate neural network to do the four tasks separately so that's the power of multitask learning and one other detail you know so far described as algorithm as if every image has every single label it turns out that multitasking also works even if some of the images we label on your some of the objects so the first training example let's say someone your neighbor had told you there's a pedestrian there's no car but they didn't bother to label whether or not there's a stop sign over a LAN this is traffic right and me for the second example there is a pedestrian there is a car but again the laborer when they looked at that image they just enable it with whether it has a stop sign over the traffic light and so on it may be some examples are fully labeled and maybe some example they were just labeling for the presence and absence of cause so there's some you know question marks and so on so with data set like this you can still train your learning algorithm to do four tasks at the same time even when some images have over your subset of the labels and others are so a question mark so don't cares and the way you train your algorithm even when some of these labels are question marks are really unlabeled is that in this sum over j from 1 to 4 you with some only over values or j with a zero or one label so whenever you know there's a question mark you just omit that term from summation but just sum over only the values where there is a label and so that allows you to use data sets like this as well so when does multitask learning make sense so when there's multitask learning make sense I think it makes sense usually when three things are true one is if you are training on set of tasks that could benefit from having shared low-level features so for the autonomous driving example it makes sense that recognizing traffic lights and cars and pedestrians those should have similar features that could also help you recognize stop signs because these are all features of roads second this is less of a hard and fast rule so this isn't always true but what I see for a lot of successful multitask learning settings is that the amount of data you have the each size is quite similar so if you recall from transfer learning you learn from some toss a and transfer it to some times B so if you have a million examples for toss a then and 1000 examples so toss B then all the knowledge you learn from that million examples could really help augment the much smaller data set you have a times B well how about multi tailoring it multitasking you usually have a lot more tasks than just two so maybe you have previously we had four tasks but let's say you have a hundred tasks and you're going to do multi table burning to try to recognize a hundred different types of objects at the same time so what you will find is that you might have a thousand examples Pro tasks and so if you focus on the performance of just one task let's focus in the performance on the hundred stars you can call a 100 if you had trained to do this final toss in isolation you would have had just a thousand examples to Train on this one toss this one over a hundred times but by trading on these 99 other tasks these in aggregate have ninety nine thousand training examples which could be a big boost could give them all the knowledge to augment this otherwise relatively small one thousand example training set that you have for the so toss a 100 and symmetrically every one of the other 99 tasks can provide some data or provide some knowledge to help every one of the other tosses in this list of 100 toss so the second bullet isn't a hard and fast rule but what I tend to look at is if you focus on any one task for that to get a big boost for multi toss learning the other task in aggregate need to have quite a lot more data then for that one toss and so one way to satisfy that is you can while the toss like we have in this example on the right and if the amount of data you have a huge house is quite similar but the key really is that if you already have a thousand examples for one toss then for all of the other tasks you'd better have you know a lot more than 1,000 examples if those other tasks are meant to help you do better on this final tools and finally multitask learning tend to make more sense when you can train and pick it up near a network to do well on all the toss so the alternative the multi toddler name would be to train a separate neural network with each toss so rather than training one your net for pedestrian car stop sign and traffic light detection you could have trained one year a network for pedestrian detection wondering that were for college detection one year network response and detection and one year network for traffic light detection so what a researcher rich Khurana found many years ago was that the only time that multitasking hurts performance compared to creating separate neural networks is if your new network isn't big enough but if you can train a big enough neural network then multitask learning certainly should not wash in very very hurt performance but and hopefully it will actually hope performance compared to if you're training new networks to do these different tools in isolation so that's it so multitask learning in practice multitask learning is use much less often than transfer learning I see a lot of applications of transfer learning where you have a problem we want to solve of a small amount of data so you finally relate the problem we have a model data to learn something and transfer that to this new problem but not eat our learning is just more rare that you have a huge set of thoughts you want to use you want to do well on using trim than all of those thoughts at the same time maybe the one example is computer vision detection I see more applications multi-thousand learning where one neural network traded to take whole bunch of objects at the same time works better than different neural networks trained separately detecting objects but I would say that on average transfer learning is used much more today than multitask learning but both are useful tools to have in your arsenal so to summarize multicolor nning enables you to Train one neural network to do many tasks and this can give you better performance then eager to do the task in isolation now one note of caution in practice I see that transfer learning is used much more often than multitask learning so if you see a lot of tasks where if you want to solve a machine learning problem but you have a relatively small data set then transfer learning can really help where if you find a related problem but you have a much bigger data set you can train in your network from there and transfer it to the problem we have very little data so transfer learning is used a lot today there are some applications of transfer of multi Tod learning as well but multi table learning I think is use much less often than transfer learning and maybe the one exception is computer vision object detection where I do see a lot of applications of training the network to detect lots of different objects and that works better than training separate neural networks to detect individual objects but on average I think that even though transfer learning and multi-touch learning often you are presented in a similar way in practice I've seen a lot more applications of transfer and learning than a multitasking I think because often is just difficult to set up what defines so many different tasks that you would actually want to train a single neural network for again with some sort of computer vision object detection example of being the most notable exception so that's it for multitasking or much not learning and transfer learning are both important tool to have in your tool bag and finally I'd like to move on to discuss end-to-end deep learning so let's go onto the next video to discuss end-to-end learning 