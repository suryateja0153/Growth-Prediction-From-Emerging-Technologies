 recent advances in neural networks have enabled computers to better see and understand the world they can recognize school buses and zebras and can tell the difference between Maltese terriers and Yorkshire Terriers we now know what it takes the train these neural networks well but we don't know so much about how they're actually computing their final answers we developed this interactive deep visualization tool box to shine light into these black boxes show you what happens inside of neural nets in the top left corner we show the input to the network which can be a still image or video from a webcam these black squares in the middle show the activations on a single layer of a network in this case the popular deep neural network called Alex net running in cafe by interacting with the network we can see what some of the neurons are doing for example on this first layer a unit in the center responds strongly to light to dark edges its neighbor one neuron over responds to edges in the opposite direction dark to light using optimization we can synthetically produce images that light up each neuron on this layer to see what each neuron is looking for you can scroll through every layer in the network to see what it does including convolution pooling and normalization layers we can switch back and forth between showing the actual activations and showing images synthesized to produce high activation by the time we get to the fifth convolutional layer the features being computed represent abstract concepts for example this neuron seems to respond to faces we can further investigate this neuron by showing a few different types of information first we can artificially create optimized images using new regularization techniques that are described in our paper these synthetic images show that this neuron fires in response to a face on children's we can also plot the images from the training set that activate this there on the most as well as pixels from those images most responsible for the high activations computer via the deconvolution technique this feature responds to multiple faces in different locations and by looking at the decom we can see that it would respond more strongly if we had even darker eyes and Rose your lips we can also confirm that it cares about the head and shoulders that ignores the arms and torso we can even see that it fires to some extent for cat faces using backdrop or decom we can see that this unit depends most strongly on a couple units in the previous Laird khana for and on about a dozen or so in Khan three now let's look at another neuron on this later so what's this unit doing from the top nine images we might conclude that it fires for different types of clothing but examining the synthetic images shows that it may be detecting not clothing per se but wrinkles in the live plot we can see that it's activated by my shirt and smoothing out half of my shirt causes that hack of the activations to decrease finally here's another interesting note on this one has learned to look for printed text in a variety of sizes colors and fonts this is pretty cool because we never asked the network to look for wrinkles or text or faces the only things we provided were at the very last layer so the only reason the network learned features like texts and faces in the middle was to support final decisions at that last layer for example the text detector may provide good evidence that a rectangle is in fact a book seen on edge and detecting many books next to each other might be a good way of detecting a bookcase which was one of the categories we train the net to recognize in this video we've shown some of the features of the deep v's toolbox and a few of the things we've learned by using it you can download the tool box of this URL and explore for yourself if you'd like to share what you find you can use the hashtag deep this thanks for listening and we look forward to seeing what you discover 