 So if you've done any previous research in Deep Learning you've probably heard of the TensorFlow library, it's a very popular framework developed by the folks at Google and they've been kind enough to make it open source and freely available to the world, so let's talk about what TensorFlow is all about and how it can help you construct artificial neural networks. Now, the thing that kind of took me by surprise when I first encountered TensorFlow was that it's not really purpose built for Deep Learning or even neural networks in general, it's a much more general purpose tool that Google developed that just happens to be useful for developing Deep Learning/neural networks. So more generally it's an architecture for executing a graph of numerical operations, it's not just about neural networks, you can have any sequence of operations and define a graph of how those operations fit together, and what testorflow actually does is figure out how to distribute that processing across the various GPU cores on your PC or across various machines on a network and make sure that you can do massive computing problems in a distributed manner, and in that respect it sounds a lot like Apache Spark. If you've taken any other courses from me, you've probably heard me talk about Spark, it's a very exciting technology, and Spark is also developing Machine Learning and AI and Deep Learning capabilities of its own, so in some ways TensorFlow is a competitor to Apache Spark, but there are some key differences that we should talk about. It's not just about distributing graphs of computation across a cluster or across your GPU, you can also run on just about anything. So one thing that's special about TensorFlow is that I can even run it on my phone if I want to, it's not limited to running on computers in a cluster in some data center, and that's important because in real world Deep Learning you might want to push that processing down to the end user's device. Let's take the example of a self-driving car: You wouldn't want your car to suddenly crash into a wall just because it lost its network connection to the cloud now, would you? The way that it actually works is that you might push the actual trained neural network down to the car itself and actually execute that neural network on the computer that's running embedded within your car, because the heavy lifting of Deep Learning is training that network, right? So you can do that offline, push the weights of that network down to your car, which is relatively small, and then run that neural network complete within your car itself. So, by being able to run TensorFlow on a variety of devices, it opens up a lot of possibilities that you wouldn't have otherwise of actually doing Deep Learning on the edge, you know, on the actual devices where you're trying to use it on. So that's pretty cool too. It's also written in C++ under the hood, which is also good, whereas, you know, Spark is written in Scala, which ultimately runs on top of a JVM, so by going down to the C++ level, that's going to give you greater efficiency with TensorFlow; but at the same time it has a python interface, so you can talk to it just like you would any other Python library, that can be very easy to program and easy to use as a developer, but very efficient and very fast under the hood. And the other key difference between TensorFlow and something like Spark is that it can work on GPU's GPU's again just being your video card, the same video card that you're using to play Call of Duty on or whatever it is you play, you can actually distribute the work across the GPU cores on your PC, and it's a very common configuration even have multiple video cards on a single computer and actually use that to gain more performance on clusters that are purpose built for Deep Learning. Plus it's free and it's made by Google, so it has that going for, too. Just the fact that it's made by Google has led to a lot of adoption. There are competing libraries out there to TensorFlow, but testorflow as of right now is still by far the most popular. Installing is really easy, all you have to do is use the pip command in your Python environment to install TensorFlow. There's also a tensorflow-gpu package you can install instead if you do want to take advantage of GPU acceleration, if you're running this on Windows, so, I wouldn't quite go there yet, I have had some trouble getting that to work on my own Windows system, really you'll find that a lot of these technologies are developed primarily for Linux systems running on a cluster, so if you're running on a purpose built computer in a cluster on a C2, or what have you, that's made for Deep Learning, go ahead and install tensorflow-gpu, although it's probably going to be installed for you already. So let's talk about what TensorFlow is all about, what is a tensor anyway? Well, unfortunately this is another example of sort of fancy pretentious terminology that people use to make themselves look smart, Yes! I'm editorializing a little bit there, but the end of the day a tensor is just a fancy name for an array or a matrix of values, is just a structured collection of numbers, that's it, that's all a tensor is. So to use TensorFlow it's a little bit counterintuitive, but you know, it's similar to how something like Apache Spark would work, too. You don't actually execute things right away, instead you build up a graph of how you want things to execute, and then when you're ready to execute it you say "OK TensorFlow, go do this," and it will go and figure out the optimal way to distribute and parallelize that work across your entire set of GPU's and computers in your cluster. So let's take a look at the world's simplest TensorFlow application here. This is, all this is going to do is add one plus two together, OK? But it's a good Illustrated example of what's actually going on under the hood, so let's see, we start by importing the TensorFlow library, we're going to refer to it as "tf" as a shorthand, and this is all straight up Python code, if you're new to Python. We'll start off by saying a = tf.variable 1 name = "a", and all that is doing is setting up a variable in TensorFlow, a variable object which contains a single value, 1, and which it's going by the name of "a," and the name is what will appear in visualisation tools for your graph if you're using something like that. Internally we're going to assign that to a variable in Python called "a" as well. So then we set up a "b" variable as well that is assign the value 2, and given the name "b," and then here's where the magic starts to happen, We say f = a + b, and you might think that that would put the number 3 into the variable f, but it doesn't, what f actually is, is your graph, it's the actual connection that you're building up between the A and B tensors, OK? So f = a + b does not do anything except establish that relationship between a and b and their dependency together on that f graph that you're creating, OK? Now! The next thing we need to do is actually initialize those global variables, so you explicitly need to say, "OK, now is the time that I want to go and put those initial values into my variables," we create an object called "init," that is our global variables initializer, and then we define a session within TensorFlow, we're going to call that session "s," although we're not actually going to refer to it explicitly within this block; we will take our global variables initializer object and run it, what that will do is actually stuff the values 1 and 2 inside a and b variables, so that just initializes them to the initial variables so we defined when we created them, and then we call f.eval, and this is where computation will actually finally happen, so once we call a eval on our f object, it will say: "OK I need to create a graph that takes the a variable, which contains 1, and the b variable, which contains 2, and add them together," and it will figure out how to distribute that incredibly complicated operation, I'm being sarcastic, across your entire cluster and that will ultimately print the result 3. So we have just created the most complicated way imaginable of adding 1 plus 2 together; but, you know, if these were larger tensors, you know, dealing with more large data or, for example, a huge array or a matrix of weights in a neural network, for example, that distribution becomes important, Right? So although this isn't a useful exercise to do with TensorFlow, once you scale this up to the many, many connections in a big neural network, it becomes very important to be able to distribute these things effectively. So, how do we extend this idea to neural networks? Well, the thing with TensorFlow, again, it's not really made explicitly for neural networks, but it can do things like matrix multiplication and it turns out that if you think about applying all the different weights and sums that happen within a single layer of a perceptron, which we talked about earlier, you can model that just as a matrix multiplication, where you can do that just by taking the output of the previous layer in your multilayer perceptron and doing a matrix multiplication with a matrix that describes the weights between each neuron of the two layers that you're computing, OK? And then you can add in a vector that contains the bias terms as well. So we talked about having bias neurons, those can just be added in after the fact. So at the end of the day you can modify this fancy diagram here of what a perceptron looks like and just model that is a matrix multiplication, so, you know, go back and read up on your linear algebra if you want to know more about how that works mathematically but, end of the day, this is just a straightforward matrix multiplication operation with a vector addition at the end for the bias terms. So again, you know, by using TensorFlow we're kind of doing this the hard way, there are libraries built on top of TensorFlow that make it much simpler and more intuitive to define deep neural networks, but as we're dealing with TensorFlow at a low level, it's purpose in life is really just to distribute mathematical operations on groups of numbers, on tensors, and it's up to us to sort of describe what we're trying to do in mathematical terms, but it turns out it's really not that hard to do with a neural network. Now, for us to actually create a complete Deep Learning network from end to end there's more to it than just computing the weights between different layers of neurons, right? Like, there's a... We have to actually train this thing somehow and actually run it when we're done, so the first thing we need to do is load up any training data that we have that we're going to use to train our neural network, by that I mean: data that contains the feature data that we want to train on and the target labels. So in order to train a neural network you need to have a set of known inputs with a set of known correct answers, so you can use to actually descend or converge upon the correct solution of weights that lead to the behavior that you want. We're going to construct a graph just like we did before, so kind of like we, before we had a graphic that just added a and b variables together. We're gonna introduce a new concept called a "placeholder" as well, and a placeholder is just a way of, well, having a placeholder within your graph for various data, and by doing this with a placeholder we can use the same graph structure for both our training and for our testing and we're actually applying our neural network in the future, so we can use the same placeholder for input values and feed in our trading data and, you know, do gradient descent on the resulting network. After we're done we can use a different set of data and feed it into those same input input placeholders and actually use that to test the results of our trained neural network on data that's never seen before. We will use variables, like we saw before, to keep track of the learned weights for each connection, so as we iterate through different training steps on our neural network, we're going to use variables to make sure that we have some memory in-between runs of what those weights were between these connections and between each artificial neuron, and tweak those using gradient descent as we iterate through more and more training steps. After that we need to associate some sort of an optimizer to the network because the whole TensorFlow does make that very easy to do, can be gradient descent or some variation thereof, such as Adam, we will then run our optimizer using our training data, and again, TensorFlow makes that pretty easy to do as well, and finally, we'll evaluate the results of our train network using our test datasets, so again, to set a high level we're going to create a network, a network, the network, a given network topology, fit a training data, I'll use gradient descent to actually converge on the optimal weights between each neuron in our network, and then when we're done we can actually evaluate the performance of this network using a test dataset that's never seen before and see if it can correctly classify that data that it was not trained on. One other gotcha here, if when you're using neural networks, it's very important to make sure that your feature data, your input data is normalized, meaning that's all scaled into the same range. Generally speaking you want to make sure that your input data has a mean value of 0 and unit variants, that's just the best way to make the various activation functions work out mathematically. Really what's important though, is that your input features are comparable in terms of magnitude, otherwise it's hard to combine those weights together in a meaningful way, Right? I mean, your inputs are all kind of at the same level at the bottom of your neural network and fitting into that bottom layer, it's important that they're comparable in terms of magnitude. So, for example, if I already created a neural network that tries to classify people based on their age and their income, age might range from zero to 100, but income might range from zero to a million, and those are wildly different ranges, so, you know, those are going to lead to real mathematical problems if not scaled down to the correct range at first. Fortunately, Python's scikit_learn library has a StandardScaler package that you can use to do that automatically with just one line of code, so all you have to do is remember to use it, and many datasets that use while researching are gonna be normalized to begin with, so the one we're about to use is already normalized so we don't actually have to do that. Later on the course I'll show you an example of actually using StandardScaler, however; and with that let's dive in and actually try it out. We're finally at a point in the course where we're going to actually write some real code and actually build a real neural network using tensorflow, so let's dive in in a nice lecture and see how it works in detail. 