 now moving on to the main event of this morning's general session are three keynote speaker presentations the keynote speakers of the General Sessions of Worldcom have always been delivered by individuals whose work has had a lasting and significant impact to our field in recent years the keynote speakers of the Congress included and this is just a partial list professor a J Siegel at the time he was a perdu but now at Colorado State University dr. John Gustafson now at masa de parler technologies is the CEO he was the inventor of the Gustafson's law professor John cosas from Stanford University inventor of genetic programming professor Richard from Chomsky but known as the father of machine learning from George Mason University professor very badiy Berko from MIT the founding member of the MIT Media Lab professor John Holland from University of Michigan father of genetic algorithms professor David a Patterson from University of California at Berkeley he's been presenting talks multiple times in this congress miss Anousheh Ansari CEO of pretty systems and also the first female private space explorer and the first astronaut of Iranian descent Professor Ian Foster known as father of grid computing of University of Chicago and also national Argonne National Lab dr. kay eric drexler known as father of nanotechnology is chief technology adviser to nano Rex dr. Jose Munoz formerly deputy director of National Science Foundation plus director of ASCII initiatives of NSF and tue professor Luffy's are they known as father of fuzzy logic University of California Berkeley dr. fear is not early as we should director project formulation and a strategy from Jet Propulsion lab and also Caltech NASA and head of NASA Mars exploration Sam he's the guy who who made the Mars rovers a reality he was a director of that that initiative Professor David Parnas from McMaster University he developed the concept of information hiding which is an important element of all object-oriented programming professor Eugene's passport from Purdue University and executive director of Silius a security center Purdue University professor Victor asking the skin English professor at Purdue University as an associate director of Sirius together with with his former PhD at Y Z at the Hebrew University of Jerusalem dr. Raskin developed a groundbreaking ontological semantic approach to NLP that for the first time aspires to near human comprehensive semantic capabilities for NLP systems well our first keynote speaker professor Amit Shah as chair of the steering committee of this Congress it gives gives me a great pleasure and honor to introduce our first keynote speaker professor Amit said Professor chef is an educator researcher entrepreneur visionary and leader is the lexisnexis eminent scholar and founder and executive director of the ohio center of excellence in knowledge enabled computing the acronym that he uses for for that Center is neurosis at Wright State University noices conducts research in social sensors semantic data and web 3.0 with real world applications and multidisciplinary solutions for translational research healthcare and life sciences cognitive science material sciences and others noises activities have resulted in writers State University being recognized as the top organization in the world in the world on world wide web in research impact professorship is one of the top authors in computer science world wide web and databases is h-index if you know about edge index is kind of a measure of how the quality of the work that is published and the citations associated with it his H index is 86 mine is 18 I'm so proud of it okay his his research has led to several commercial products many real-world applications and to earlier companies with two more in earlier stages of development one of these was tally then changed the name to per quit sick cymatics which was likely the first company founded in 1999 that developed semantic web enabled search and analysis and semantic application development platforms he is one of the highest cited authors in the world the science it is one of the largest academic research groups in the area of semantic wealth every year this Congress presents two outstanding achievement awards to have very few selected researchers and scientists in recognition and appreciation of their dedicated and outstanding contributions to their respective fields of research as the chair of the steering committee it gives me a great pleasure and honor to present one of these year's awards to Professor Amy chef in recognition of his leadership outstanding research contributions and pioneering work in the field of Semantic Web professorship please come to the podium professor chefs keynote address today is entitled smart data for you and me personalized and actionable physical cyber social Big Data [Applause] thank you it's all yours all right thank you very much for this opportunity to speak at this significant event this Congress Thank You Hamid for the invitation and the steering committee I have a title perhaps some of the names or other terms might not be totally clear to everyone I hope to introduce them to you and discuss their significance with you so we're talking about smart data for you and me for individual and for community and this emerges emergence of physical cyber social computing applications in which data from the physical world from the cyber world and social world come together to enable new class of very exciting very important applications very briefly about the big data and I don't want to dwell here much into that there'll be many other talks many other discussions on that later on but the growth is amazing in 2008 we lost the ability to store all the data we create and every year we kind of create more eita than we created before hand and that somehow the slide didn't come out too well but we have such a growth of data that only 0.5% of all the data gets processed it gets analyzed right so much of the data simply is not stored much of the data cannot be analyzed so this has the volume issue on which many people have paid some attention the other issue the one that actually gives more complexity more difficulty is the variety issue and that includes you have data of all kind semi structured unstructured and you have data of all modality audio video and images and the fact that the data continuously changes you talk about an event and in an event something happens let's say a hurricane sandy before the storm comes on shore there are some topics of discussion and then during the landing there are other topics of discussion like light goes out and after the storm has moved out there are other topics of conversation such as lack of some resources so these are changes fast and being able that's called velocity aspects of it so the volume variety velocity velocity these are some of the ways that people talked about in the context of big data but beyond that and the one that I will probably talk about more today is that for the first time in the human history we have one tool that majority of the humanity has right no and what tool is that that's the mobile phone right no time in the past we had a single tool that majority of the humanity had right so this is fun and because of that so there are you know 2 billion of the 5 plus billion people 6 months I think probably 6 plus billion people now that have data connections right there are smartphones buy in couple of more years majority of the people who have phones who have smartphones in addition to that there are not more sensors not going on and this term called Internet of Things is now becoming very popular at the two of the largest exhibitions and conferences the SXSW and the CES the two was the most important keywords or hashtags where IOT and digital health and in my talk you talk about challenges in actually using in dealing with these two issues so they're well over ten billion devices that communicate over the Internet these are in terms of things and that the numbers are growing and will grow very very fast so now and in that different terms Beyond IOT people have also called it Cisco's call it internet of everything that everything includes both the devices and the humans that use these devices to communicate so mobile phone you have human-in-the-loop computing human-in-the-loop sensing and humans when they see something they share with people through tweet or some other method with that then there is emergence of what I call this physical cyber special words coming together so what is not changing we are still working with rather simplified presentation of information majority of the computing that we do today a measure of the activity that our algorithms which you have the data that our good news process are usually a data of singular type there's a lot of work on text rustling there's a lot of work on image processing a lot of work on video processing unfortune being we are multi-sensory all right at the same time we are consuming the video of watching me and watching this thing you are reading and you're processing natural language and and you know so you have multi more multimodal information always coming at you and you take about a traffic event you will have a tweet about it you have Road sensors about it and many other things coming together so we need to be able to have a computer computational paradigm that deals with more diverse forms of data not only different types of data in a representational sense but the different types of data in the sense of the data capturing the physical world capturing the social interactions information of the web so the nutshell of this talk is that current data current emphasis on big data is on enterprise serving the enterprise and corporate needs there is significant opportunity for applications that deal with individual and community needs right so with the corporation's we can benefit so many people but if you develop applications that can serve your and my needs will be affecting a lot more people and interestingly I will show you if you are not already aware that on all of us already have a big data problem of source should we recognize all the data that is being relevant to things we do our existence our help so the applications in the domain particularly health fitness well-being disaster coordination personalized smart energy these are some of these where we can exploit diverse data and as a way of thinking as a way of new paradigm I'll talk about the smartedapp personalized contextually relevant actionable information as a better computational paradigm so think about data and from that data you get information that is good but there is not sufficient in fact what you need is information that is actionable that is relevant to you so your actual information could be that an apple a day do it does good to you it's good for you right so it's not important that there there's a data about how many that there's something is an apple what is important is how is relevant to your decision-making how is it important to your you know two interests inside that you need to have take an example of now the challenge that we face typical blood test has it has 30 biomarkers how the doctors are supposed to interpret this data into an information or end and decide whether there should be something some health related decision that needs to be made what would happen when the doctor would have 300,000 data points right so this is happening the amount of information that is being created is so massive but the computational Cavallini could the cognitive capability of a human brain is not going up that fast so the interesting thing and the challenge that we have is to convert all this massive more data and bring it to the level of abstraction at which it makes sense to human decision-making right that is the challenge we have so there is massive amount of I've been reading this book the the title is about top brain in the bottom braid and there's a lot of work in cognitive science in other disciplines but in particular in this case cognitive science many of you knew that know that in dealing with big data we you know we have done great progress in machine learning and being able to have some training data have the algorithms find some patterns and then give you new data they can do the classification they can find those patterns and such that's fine so you do pattern you know understanding or recognition mining and so and so forth at the same time we also need to engage top-down processing so the idea of the top brain is that you can plan it can you know basal planning kitten can say what other data to seek out what are the pattern to seek out so rather than just rely on the bottom of processing which is where you know techniques like machine learning are used we need to combine with that the top-down processing and to gather things we can develop much smarter much better algorithms right so if you think about and and in the computation I know terms we talked about infusion of models and the ground knowledge knowledge in reasoning things of that nature so think about the sensor in your refrigerator which has a count is it just count of number of apples in your fridge this is data but that is not very useful what and the fact that you have few apples in the not enough apples in your refrigerator would be information but there itself is not sufficient what will be important is that when you are at a grocery store at that point you being reminded that you are alone Apple and that you know basically that leads to the action of your buying more apples for you that is important that is where where what you that's where you want to go to and the other interesting thing here is that in all these all this context you cannot simply rely on the machine to do all the things for you what if you are tired of having Apple all these you know all these past months today you want to pick up another healthy fruit instead of Apple right so the human involvement in that process also very critical right so there are number of things that I talk about here about data to information to actionable information and the human role which makes for very exciting competition so we have contextual information that makes sense to humans not just machines make sense to have machines that's also good that leads to better timely actions that is what I call smart data right now it so happened that Hamid mentioned that I had started a company in 1999 a Semantic Web company and it got required a require and acquired it still exists by the way the technology develop still exists and this deployed at maturity of the biggest banks in the world for financial applications but the I wrote something there in 2004-2005 and that's so recently I was using this term smart data and then iGoogle and I found my own old article and white paper and industry talks because there was company so use of oncologists and data repositories to gain delivery insight and you can see this smart search and things of that nature you are you can see the you can see the the ontologies or knowledge representation all the notations of the data with regard to the terminology and then using all that for you know smart search this is very similar to basically knowledge graph you today these days you hear about Google knowledge graph and using that knowledge graph you you know basically annotate or tag all those terms and then the google has this result about when you do search you may get result about somebody we used to call reach media objects in those days in our company they call it something else now and much of this digital can find in a pattern that was only a warrior in 2000 2001 but anyway that's where we use the term smart data today I will kind of define that if smart eater make sense out of big data it converts all these challenges in volume variety velocity velocity into value and value is that of making this data you know converting this data into actionable information and help humans make better decisions I have another definition which is that smart eyes of human live by human for our human right so here the idea is that you want to extract value out of all this data and use it you know basically throughout the data creation processing in consumption phase to again improve the human experience those another term that I use is competing for human experience that the whole idea about computing is to serve human and to improve our experience IIIi don't like the general way in which we have computers do the job for us most of the time we have to convert our reach real-world environment into something very narrow or you know aspects of what you can convey to the computers using our compiler limited programming languages and rather limited data structures and data types and gets simplified answers to the simplified information we feel and then interpret that in the real world that is what generally what we do but things are changing now things with ambient intelligence and many other things that are happening the computing is becoming more aware of human activities rather than human going to computer and telling about everything that he or she is interested in let the computer in environment in the technology be more aware of you and what you are interesting a beautiful piece that Mac Wiseman wrote in 1991 computing in the 21st century he says that technology should disappear in the background right so that general you know theme is throughout what I try to do let's talk about of the human so you have data of the human whereby today you have all this quantified self the feed beat and smartwatches and all that thing that is data about you you have sensors around you on you and in the body there are ingestible sensor that will actually take photographs and then give you inside a video when the sensor is comes out then there is data that is about your your interaction online again that is data about you right that is what you do you say oh I'm at Las Vegas ready to for prepare all ready to give my talk there can be near field communications in our cells you know mobile phone can know who are among the people that we already knew man that we met today and so on so forth then there is data by human we will engage in creating lot of data on the Wikipedia for example we have all of these millions of people going there and basically entering the data and information that can then be reliever age for understanding new information so there is crowdsourcing there is the so called linked open data so lot of databases that have been created by human activities have been put up on the web as an open data right and then that can be used for other purposes for example geo names so the names of every location in the world is in this data set this can then be used to interpret the data and then data for human the most important thing so we can be capturing all kinds of data events such as visiting sound indoor temperature humidity does carbon monoxide level all this relevant to asthma but what is important is not this data most of the time to the child who is having a sama or to the parent of the child who in estimate or even to the doctor's individual numbers until data items are meaningless are not important right if you have for example hypertension then doctor is not paying too much attention to the hypertension every day what you have your blood pressure the doctor is paying attention through the sudden rise but significant change from your normal level that you have been able to manage right so all this data is not important what is important is something that you can act upon such as closing windows at home during day to avoid carbon monoxide in Gush so that you could avoid asthama at nights these are the kind of things that are important or I can give you the example from smart energy domain where York you can have data about lot of different things energy uses what the de devices were consumption so many different things what is important is to be able to basically have this actionable information the smart data saying washing and drying has result in two significant cost and if you were to change this schedule you could change significant and this also by the way needs to be smart it needs to be human centric for example if this comes with the conclusion that it will save you 3% money you know in the cost it's possible that you may not motivate that you want to act maybe there is not significant enough what is important is to recognize what is significant for the human and then perhaps you want to like say Oh more than 10% saving in the cost maybe you can make the human Act so everyone and everything has big data the example I gave you I have a kind of example the you and I can have right and this that if you think about it this data is being created continuously all the time the sensors are measuring it your human activities are you know capturing all the data and so that is a big data it is in fact big data is not just about volume which is an issue with some of these applications but lot more about complexity of the data the variety issue the velocity issues this is a very interesting example has anybody seen this particular picture and the nose of story behind it the gentleman on the left here is Larry Smarr he is a professor of computer science at UCSD insti of california san diego and he is a quantified self self buff so what does it mean he basically likes to use all these you know devices all the sensors to measure his body functions and his body activities everything all his organs in what he does and it's a long story and towards the end of this talk I have links to his one hour long talk very bit wonderful talk that if your time I really urge you to watch that but basically Larry came with his own conclusion that he has Crohn's disease the disease with regards to bowel and other things the bacteria and the gut and the stock big imbalance that occurs with the bacteria in the gut that leads to significant challenges and health problem so Crohn's disease and related disease is what something he came to conclusion himself now imagine now ladies a smart guy right he is a computer scientist he's not a doctor in yet he he can go to the literature he can read the literature and he can interpret the data right and from data he could get direction of information saying he has Crohn's disease he goes to doctors and then works with the doctors to manage a figure out how to manage this disease right now you and I may not be not all of us can be this smart and not all of this maybe that dedicated right so think about developing a solution where each of us have the ability to convert all those leda that can be collected about us with the privacy issue addressed into there's something that is actionable that tells us that that helps us manage our health better right and that is if that happens that means we can give this kind of solutions to somebody in rural India or China and so on so forth so for that I talk about these things first of all people leaving the physical world while interacting with the cyber in the social worlds right we are doing online search that tells us what we are interested in recently one of my stones working with Mayo Clinic did a significant analysis of the kind of search people do when they are interesting cardiovascular disease or when they are you know how much or major depressive disorder and so forth so that tells us something about cyber activities our social activities tells us about number of things and what if we can liberate completely you know all these observations from sensors knowledge and experience from people to understand correlate and personalized solutions so here a person may have electronic medical record and physical you know in health records you have reached knowledge of the medical domain your physiological data from sensors you help create interactions with other humans you have patients like me and other social media sites that where people go and say oh I am taking this new medication but I am having this side effect do you also have this side effect there side effect is not least you know known side effect for the drug when people talk about each other and they learned from that very often a drug into a you know adverse drug effect not known or declared by America pharmaceuticals have been known this way so all this can be then brought together for you know making the decisions where by here person is experiencing heartburn I will spare you all the details but ultimately what it all leads to a an actionable information actually saying you know contact your doctor right now don't read these busy slides and in fact don't read my slides you know they are just background information to think I want to point here is that how do we then convert this big you know ideas or this broad ideas and fuzzy ideas into something concrete you know I'm a computer scientist I want to build the things right so for that we analyze the you know we created this environment it consists of this horizontal apparatus and the vertical operators horizontal operators deal with all these variety shoes the heterogeneity of the date and so forth and for a long time I've been working this area of semantic web and that is that has developed a lot of techniques to be able to deal with heat rigidity but today I will because of the limited time I won't talk too much about that today I will give you some glimpse of the vertical operator right so that vertical operators typically are about converting data into knowledge and something actionable and so for that we developed this mechanism and we call it in Telugu in Telugu is to perceive it is based on a Greek word and the idea here is in this particular case we originally focused on making sense of sensor data but we already applied that to social data another data also so in the case of in Telugu we took the hour we took the ideas are not really techniques but ideas from cognitive science and we offer in science we often go to other disciplines and get the ideas so there were number of models for a perception human perception right and so you can learn for what what we need is to learn at a conceptual level from cognition models of perception and one of the things we also learned there is importance of prior knowledge I will come to that later so the perception cycle consists of an objective part of the cycle one where data is converted into hypothesis low-level data is converted into higher level abstractions or representation and directly part of the cycle nudity part of the cycle whereby you could basically say oh I have so much information and I have these possibilities and this hypothesis what additional data could help me reach a better understanding of where I the end to actually validate this hypothesis and things of that nature right so that is the reactive power cycle so there is this cycle that continuously goes on in fact that goes on in our brain all the time that is really the model that's the theory in that what happens here is that you are observing something I am saying something and then you say maybe he means this this that and then I say something more in the next few sentences you you you you know the what I said makes more sense in all that context you are also using your own prior knowledge your own understanding of the you know variety of disciplines some of you are interpreting what I am saying based on your knowledge of national English processing others are saying hey this is relevant to ocean learning and so forth we are using this background knowledge to also improve understand for infrastructure we use Semantic Web technologies and in that context what happens here is that a couple I think three years ago I initiated and then co-founded a w3c world wider consortium group on semantic sensor networking so these are slides related to that and the idea here is that you you have a compressive model of sensing and then you represent the facts for example elevated blood pressure is a property of hypothyroidism and with all this bunch of facts little domain knowledge you basically create a domain knowledge in this case there's a simplistic resolution in bipartite graph but often you need to have probably stick representation of knowledge and the same thing can be done for any domain so I can you again abstract representation and ontology creation in the case of whether now what we are trying to do here is that in the context of explanation and translation of lower knowledge knowledge into higher level we have to implement this we try to implement abduction with certain assumptions as directly processing using our DL again I won't go into this kind of details here there are papers on each of these things the links in my talk typically take you to the papers that are discussed in detail and then you have you know for example things behind the scene is that you find oh this prepetition it can be explained by all those things on the right hand side now you want to when you observe something you may have multiple possible or leash you know solution explanations so you want to say which is the right one right that leads to your second final cycle the DT final cycle right so for that then you do discrimination so here suppose you know what these are expected properties for those things but then you come across you ask for more information from other sensor and you say oh I find Comiskey in that case it is not hypothesis it is hypotension right so this kind of stuff in those happens in the cycle and that leads to this converting data into high love abstractions hopefully I painted a general mechanisms all the tears are left for you no further reading now even that is not sufficient of course to build a complete solution so I am also not a I'm very much a practitioner I like to build the system and so the thing that I am talking about we have system called K health I'll come to that we need to cover that all into something that is meaningful for medical decision making in this case risk factor for example so there is lot more that goes on when people suffer from multiple disease there are comorbidity and so and so forth so there are the things that happen there I won't go into details of those so when we try to implement this using our base reasoning the web ontology language and deductive reasoning the computation blew up with the with you know so in fact another important thing was that we wanted to do all this compression on mobile phone there were reasons for that for example there are privacy issues where because of which I didn't want to take the data of a patient and send it over the cloud and promise in the cloud that includes that creates certain privacy issues so I said the other issue is that I want the solution to be available for any person in the humanity remember I mentioned that half of the more than half of humanity have for now and very soon measured he will have smartphone so I want all of them to have the computational capilene necessary fault to for them to do this computation so for that we wanted to do this what we call increases the edge so you want to do the computation on the smartphone unfortunately doing so on the smartphone blew up you know the so the reason all didn't work well so we came up with a bit vector based mechanism and a operator implementation that then was killable from tens of thousands of nodes and reduced the time for minutes to milliseconds and so on so forth so there are some thinking now I'm going to quickly go through two applications one and there are more so but I don't have time to do all of them in fact I might have time to do just one of them so from healthcare we have applied this general thing I talked about of smart data with pcs data a physical psychosocial data to health and particularly dhf which is acute decomposes heart failure ask to mind children and GI patients and for hasta McCrory calluses in GI the main aspect is that of reducing readmissions so for example today so I have video here Allegan spare the details of that the internet is not very good here but the idea is that it cost 17 billion dollars a year because of readmission of chronic heart patients at a difficult Hospital 25% of patients get readmitted within 30 days 49 percent within six months each readmissions on the rich cost $50,000 eight to ten thousand dollar a day six days every state so this is a keep you know that goes for that disease I am going to talk about asthma which affects 300 million people worldwide right so here there is a kid in this case which consists of cells so drone and there are many sensors built in that I think nine or twelve sensors built into that and for pulmonary kind of disease the doctors wanted us to have this node sensor which measures exist nitric oxide but for customized very complex disease so it's not just related to the physical you know what you call physical data alone social data and a web data's cybertor can also be very important for that for example there are arrays information from Poland comm many of people have estimate in automatic allergic to high degree of pollen I used to be allergic to ragweed for example air quality outdoor and smog in the city is that it makes people automatic and many things of that nature right so these are available as web services so this is a cyber component right and people freely talk about tweets so you have here you can see real-time health signal from a personal level meaning of an individual public level coming from less than EMR electronic medical record systems and population level pollen level carbon dioxide and continuously you know this keep on coming continuously right I have passive sensing sensors around me that can continuously take data every second every minute I have ft sensing I can my mobile phone can ask questions that are relevant to particular you know suspected issues with regards to your disease and I have you know all this data coming on the web continuously being measured the the you know weather data and so forth so you have all this data coming and this is kind of broad level architecture your analysis of the data you have something in there there's Intel ago there's this more comprehensive system here and there are computational things algorithms for this model at the end of the day all you care about is take medication before going to work contact the doctor when the thrist has gone to high or or there is a setter onset of a stoma or avoid going out in the evening due to high pollen levels these are the things that matter to humans this is what we will go this is a smart data so to be able to do that I have to infuse the system with the domain-specific knowledge so in this case that this is about knowledge the protocol guideline that doctors use right there for example if so they have two things here severity level and the level how well control your asta mice and here if you have mild persistent asthma and it is not well controlled then you will take medium inhale corticosteroid right that will be the you know protocol that doctor will follow and the idea here would be that you want to figure out how well control they are some eyes and saying there's some very high level terms people don't want to get all those you know for a normal human who is not a doctor you don't want all the details you want something saying are you doing well well take a caution or no things are not gone not good or and the other important thing you need so that was you know diagnostic and prognostic saying how well control is my estimate and what are the risk factor today what are restricted now and so you have to say can I reduce mathematics at night from all that from that high level thing you need to understand really many next level of high level have sections saying what are the triggers what are the wheezing level and so and so forth and many of these things so and then you need to combine all those concepts those abstractions you need to unit even at the subsection in the context of all the data that you have right and so these are the data you see and then you you know these are the observations these sensors these are the observations you have model of the disease here so there are all these concepts that I am I showed you in previous slide and then you have ability to automatically I rotate all the data into this machine processable form this is message machine understanding of the data because the understanding is map this way and so there are well known you know now we have reached the level of science where we can automatically relate all these data from different sensors in this common way and on which then we can apply our processing you know Semantic Web this is the sensor model very comprehensive I will go into details of that and what we do is to convert the data comes from sense that that comes from self 152 8 a little data systolic blood pressure of that and represent rdf to a you know the interpreter saying you have elevated profession and that can all be represent into the model that I showed you before end can convert that into something that is meaningful for medical decision making is that hypothyroidism and that is where that Intel ago and the kind of you know small data processing abstractions come into picture okay so I alright now let's take another example here in this case I am talking about PC is computing for traffic analytics there are 1 billion or so cars on the road just 10% increase 20% increase in the vehicular traffic rage resulted in to two hundred and thirty six percent jump in you know in the kind of traffic related challenges and when the traffic will be when the number of cars will double in this decade to two billion we have a big problem we are part of this comprehensive large you funded project so we have number of partners this is one of the smart city projects that we are doing so in this case we have data collected every minute just as an example I'm giving this lot more here but I'm going to explain things with one example so we collected data for three months for San Francisco area and that data consists of every minute update of speed volume travel time and occupancy resulting in so many link status observation so many tweets related to traffic for that area alone so many active events on the web so their websites that will list events there's a sports game there's a music game there is a plan road closure schedule events and then these are all under only sample observations collected over humans so they say you can see it has volume variety velocity and velocity problems and what you're interested here is in that can be read rec onset of traffic congestion can we characterize traffic congestion based on events can we estimate traffic delays in in road network these are the kind of things these are actionable things that you are interested in understanding so you have data of different modalities these are sensor data these are tweets these are you know data on the web you have ability to automatically classify the data so this shows you how we could take the data and basically automatically classify based on all different departments the city has so this one relates to for example health another thing relates to the garbage and kind of a degree less - I know police or something like that right so if we can you know tag that then you have techniques for understanding you know and some complex issues here for example in this treaty to be able to understand that Half Moon Bay Brewing Company is a any an entity is a very college challenging task so everybody anyone here who does you know text processing would know the challenge in doing that because Half Moon is also a entity Half Moon Bay is an entity Brewing Company is an entity and so on so forth right so to understand these and so CRF model conditional random field models are particularly well suited for these kind of stuff other thing you need to do is to kind of then map the data internal observations internally sensor observations need to be mapped to the data out there exfol events like active and scheduled events these need to be mapped together by geography and time as an issue and then you need to be able to severe again the linking things going on you have raw data and you have a game data and they all come together and then they say Oh aha it seems like both of them relevant to this particular traffic related slowdown or event now what is what is important and what is interesting and why it makes these things possible rather complex processing possible is this use of background knowledge and so there is this thing about domain experts and knowledge graph data being coming from you know and so here in the concept you know I show concept net fight there is naught of knowledge about traffic rules and how one thing affects another thing you bring that in with regard to the data so everything comes together actually attract from the road network with the knowledge and understanding of location causal information other knowledge they all come together to help you make decisions and here I am showing you where of things that happened it up getting interconnected knowledge being infused and there is a you know just the architecture you have things like point of speech tagging you have hybrid NER named integration you have this holistic state map locations you have ontology smart city ontology data by IBM your file xi dot org human develop a taxonomy or hierarchy and then a whole bunch of algorithms and techniques for person Donita with all that then leads to for example finding information saying that things can be complementary here so slow traffic from sensor data and excellent from textual data come together right that you have corroborating information accident event is supported by a you know information on the web ground truth or you have timely information knowing more visibility before formal report from the ground to says that o IV a statistics lowdown on the traffic so there are a whole bunch of examples of these things about external event and ground events that shows you it shows you you know corroborate events it shows you you know complimentary events here and so on so forth this is an architecture and you know basically so so now I'm wrapping up I like to point to a couple of wonderful talks I mean there are many many things I I all the time spend a lot of I mean itís I myself spend a lot of time looking at number of videos and other things as you probably are doing so now it's so much knowledge on the web these two are particularly very interesting where Vinod Khosla talks about the future of healthcare he mentioned that example of you know the information overload on doctors when instead of 30 biomarkers when they have 300,000 biomarkers right so that's a big challenge that they would be facing the number of data with all these personalized medicine with zero mix data and so on so coming this is you know significant right and so that's problem and in hopefully I gave you some idea about how we are addressing this things how we will be able to address you with more than that Larry Smarr he's example of you know this himself identifying crows disease quantified self that's something I also talked about today and he has wonderful talk on human micro mic and uracil digital health right so some wonderful things there so let us wrap up right let's let's see what I talked about today big data is everywhere right but what is important is that it's not just about serving corporations need about how to improve their logistics and supply chain and targeting in advertisement that's where they are using it but big data is a lot about in fact a bigger issue because an interesting aspect of big data is that it affects you and me and it affects complete communities for example the traffic example I gave was commit example I did not have time to discuss with you on our work on disaster response for example we had collected data and created crisis mapping kind of information for traction from North Indian floods and a hurricane there or cyclone there we analyzed data for Rickon sandy with oklahoma tornado for example fema asked us are there any natural gas based track based fires and be able to analyze data are the tweets and tell them the last reported fire was 10:30 p.m. right like that and and many things like that the data that we process for indian floods and cyclone were used by Google crisis map and this was by the end beam this is my group and one of my school's a month Porat you know coordinated worldwide effort for that but then it was used by Google crisis map in doing the things so these are all life things so they were there the newspaper articles in some of the biggest media BBC news and the Atlantic and all that now hopefully I get you know sort of share with you and now you do think about it saying that data increasingly is growing because we have written about physical world cyber world social world analysis is not sufficient to find analysis will give you a pattern these are top 10 patterns data science data science is wonderful and and that's something that people need now for corporations but do you think you and I as normal human being interest in our health and well-being are going to do data science no we are not we will need actionable information contextually relevant personalized information that we can act upon right so the next step to data science and kind of things we do today is something I hope I increase rate I mentioned something bottom-up and top-down I didn't get too much time to discuss that in detail but this is very important that you know there is a well-known paper unreasonable effectiveness of Big Data written by a couple of guys I know at Google you know very very you know well-known scientist they say give me all the data and I'll tell you everything for that there is - no no that's not true you really need to have that top-down thing bottom of think by alone is not sufficient that focus on humans and improve human life and experience is I think the holy grail for at least for me yes making money in cooperation what they want that's fine so I introduced although I didn't talk use that word so far today I call it semantics perception right perception is very powerful but think about the data that you consume today just in this type talk the number of bits and bytes of data in all the video that actually hits your eyes and brain and all the text that you that you process is in many many gigabytes just the video have been several gigabytes right so from that if I ask you what did you or did I say you all will be able to explain that in few words they may be different takeaways for you but they are all different right so well that is the thing so the idea here is to convert all that big data into smart data something that helps improve our life I'll end at that these are my crew of the PhD students and many of them their work is represent here a little bit about our Center and no aces so let me end PhD students are concerned one of them is the fifth highest cited individual in that respective field so we are we are very proud I'm it used to be a colleague of mine at the University of Georgia for twelve and a half years so I've seen I've had the first-hand knowledge about the achievements that these students these former students have had thank you very much questions yes well for questions is it is a there is a microphone here or you can just shout from where you are if you have questions please raise your hand questions please and there is a there is a mic there so enjoy your wonderful lecture my name is Ashok nut I'm from Sydney base college Kolkata I have two simple questions the first question is you said that you you extract the data from the B data to define something called a smart data so my question is this monitor which you define today will you remain this smart in the future maybe say after five years or ten years will remain this smart or you can then you have to again redefine your smart data the first question the second question is this is slightly theoretical you shown in slide number 43 most only 43 the complexity of asymptotic thing is about of the order in three so that I want to understand how do you get this of the order industry I might have done something smart when I define it Martita and this is why I would say so see I packed the smart data to something that serves human lives right actionable information for human I did not defense my data in the context of something arbitrary something mechana stakes something and the point here is that anytime whether it's today or five years or 30 years from now once we hope will be there and you'll have needs yes our needs might be different because we have more technology more time for leisure or we we wall in some ways but no the less smart data whatever is something whatever you want to do whether we will be impor entertainment experience whether you want to implore health with regards to that when we have machinery that converts whatever relevant data that is out there into make my making better decisions that would be this monitor so I think the applica the definition changes by itself all the time with regards to that your question about the asset I know the use of bit vector based mechanism to convert actually the the very complex data processing technique into linear processing well we use the bit vector mechanism so the idea here was that the logic based mechanism is known to be a very high complexity and by using the big vectors you know you could convert that into sort of matrix kind of operations and that could become linear and so we were smart you know I think way of reform little problem and we showed that we can do the same competition we are doing in the derivative reasoning in this bit vector based thing and it's a published paper in detail and so that is the mechanism we use now there's lot more detail there that would require us to go to the board and so forth any questions there's a microphone thank you for this insightful presentation I just have one question so if I want to make a system which will extract the smart data from that domain so you worked in the healthcare but if I want to work in another domain the first thing I need is domain ontology so being a computer scientist how can you get that domain expert in that ontology or how do you build that down to us yeah so once again if the questions are clear in my talk in the mechanism I showed I showed that prior knowledge I also showed use of terminologies in the asthama actually I didn't show the ontology in detail but there is some knowledge I showed that metrics for example in that generally is the in a standard ontological knowledge and the question is how do you create the knowledge ontology creation as we now talked about for a while in the modern you know times it was 1992 report and theses by romnod wah that brought up that term ontology a conceptualization of the world and since then and my group has been both in the academic and the commercial side we've been very active in developing all kinds of ontological you develop well over 50 on trolleys many of them are public open source antibodies then you can develop them manually semi automatically and automatically just as an example we use electronic medical records we have some existing knowledge and then we analyze these sentences come across fact and asked whether that is a new fact and there are certain algorithms to decide whether there is medic eleven facts for this domain and then ask human expert is this a new knowledge that you like add to ontology and this is just an example of highly semi automated mechanism for creating highly enriched ontology but this also increasingly the case where there are about more than 300 ontology at a bio portal in CVO National Center for biomedical ontologies biophoton so in certain domains like biomedicine large number of controllers have been developed and are public and in equal views and then there are all collages that can be quickly created from high-quality information on things such as linked open data or things such as for example this concept file which had the transportation route knowledge he's out there in public and it was it has under external edges that means the knowledge is not as high-quality as we wanted so we need additional enhancement to put the directionality through certain data analysis so the point here is basically you can often bootstrap for many different sources and then do some automatic and semi-automatic ways to create the kind of oncology that you do and today when I had a file for pattern in year 2000 the word ontology was a is and industry would not accept it too much but now AI is much better accepted and now there has been all these years of experience in building all trollies of all different kinds and all different domains so I think we are pretty comfortable in doing that we'll have to be the last question and then to decide even but still we are unable to deliver the data efficiently to the users especially if someone is there in the who has interest about that data he can receive it your question someone is moving with for example 120 kilometer per are on the roadside then is it there any technique to deliver the data efficiently to anyone who is interested through the world because in room you have resources your means you can efficiently analyze and protect the decision between from other about that decision you must have an efficient network to deliver your data yeah but remember I do not so much address the issue of communication infrastructure all that so if your question is as much as the question of caution is that decision is fine to be due but still to inform the interesting people about that you let me good the question is about you know getting the decision or really smart data meaning to the right person the analysis that you've done that is actually able to the right person so we have to address this issue in a very concrete context of disasters where people have needs and wants and people who are willing to supply or can supply resources when we realize two million tweets for Oklahoma tornado only 1.2 percent of the fees we're asking for some help needs and wants only point zero two percent of all the tweets were about some love of qualified level of help they could help like for example somebody who has set up a community center for you know people to have you know temporary stay the big challenge was connecting the two that you know somebody has you know can give 20 miles away and in that disaster situation you can't go there that's useless right or in it if it's not in timely manner that's also useless so you have to do spatio-temporal thematic matching of all these things out so indeed there are in this case inactivity there is a paper on first Monday I will be happy to give you pointer to that whereby we were able to do matches of people needing something and people offering in fact interestingly this work of that we are done is already part of crisis net who Shahidi which is the world's most important organization in crisis response and management has the infrastructure in software called crisis net and this software for matching giving percent you know just the right personal information matching their work he's already operationally deployed but these are example so yes their techniques that have to be done to really do that matching beyond just identifying that something is relevant and that's the very important issue glad that you brought a bishop well once again let's thank professor Amy Chet for a very informative talk and I noticed that there were a few hands up for questions I'm pretty certain and it is going to be available after 2:00 p.m. in the registration area so you'll be able to kind of offline pose your questions to him thank you well thank you our 