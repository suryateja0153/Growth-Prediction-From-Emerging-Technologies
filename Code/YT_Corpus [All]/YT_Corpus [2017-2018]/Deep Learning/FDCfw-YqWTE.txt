 when training a neural network one of the techniques to speed up your training is if you normalize your inputs let's see what that means let's see the training sets with two input features so the input features X are two-dimensional and here's a scatterplot of your training set normalizing your inputs corresponds to two steps the first is to subtract out or to zero out the mean so you set mu equals 1 over m sum over I of X I so this is a vector and then X gets set as X minus mu for every training example so this means you just move the training set until it has zero mean and then the second step is to normalize the variances so notice here that the feature x1 has a much larger variance than the feature x2 here so what we do is set Sigma equals 1 over m sum of X I star saw two I guess this is element wise squaring and so now Sigma squared is a vector with the variances of each of the features and notice we've already subtracted out the means so X I squared element Y squared is just the variances and you take you an example and divide it by you know this vector Sigma squared and so in pictures you end up with this where now the variance of x1 and x2 are both equal to 1 oh into one tip if you use this to scale your training data then use the same mu and Sigma squared to normalize your test set right in particular you don't want to normalize the training set and the test set differently whatever this value is and whatever this value is use them in you know these two formulas so that you scare your test set in exactly the same way rather than estimating mu and Sigma squared separately on your training set and test because you want your data both training and test examples to go through the same transformation defined by the same Mew and Sigma squared calculated on your training data so why do we do this why do we want to normalize the input features recall that the cost function is defined as written on top right it turns out that if you use unnormalized input features is more likely that your cost function will look like this at a very squished out bow very elongated cost function where you know the minimum you're trying to find this maybe over there but if you're beakers are on very different scales say the feature x1 ranges from 1 to 1000 and the feature x2 ranges from 0 to 1 then it turns out that ratio or the range of values for the parameters w1 and w2 will end up taking on very different values and so maybe these axes should be w1 and w2 probe intuition I'll plot W and B be a cost function can be a very elongated bow like that so if you plot the contours of this function you can have a very elongated function like that whereas if you normalize the features then your cost function well on average look more symmetric and if you are running bathe in the scent on the cost function like the one on the left then you might have to use a very small learning rate because over here you know the grading descent might need a lot of steps to oscillate back and forth right before it finally finds its way to the minimum whereas if you have a more spherical contours than wherever you start breathing descents can pretty much go straight to the minimum you can take much larger steps but gradient descent need rather than needing to oscillate around like the picture on the left of course in practice W is a high dimensional vector and so trying to plot this in 2d doesn't convey all the intuitions correctly but the rough intuition that your cost function will be you know more round and easier to optimize your features are all on similar skills not all not from 1 to 1000 0 to 1 but mostly from your minus 1 to 1 or with about similar variants as each other that just makes your cost function J easier and faster to optimize in practice if one feature say x1 ranges from 0 to 1 and x2 ranges from minus 1 to 1 and x3 ranges from 1 to 2 you know these are fairly similar ranges so this will work just fine is when there are dramatically different ranges like ones from one to a thousand and another from zero to one that that really hurts the optimization algorithm but by just setting all of them to zero mean and say variance 1 like we did in the last slide that just guarantees that all your features are similar scale and will usually help your learning Avrum run faster so if your input features came from very different scales maybe some features are from 0 to 1 some from 1 to 1000 then it's important to normalize your features if your features came in on similar skills in this step is less important although performing this type of normalization pretty much never does any harm so often you know do it anyway if I'm not sure whether or not they were help with speeding up training for your algorithm so that's it for normalizing your input features next let's keep talking about ways to speed up the training of your new network 