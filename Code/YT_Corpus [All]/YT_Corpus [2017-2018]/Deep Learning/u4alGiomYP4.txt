 [Music] hello hi everyone so thank you for coming in such great numbers to this tensorflow session apologies is quite late in the afternoon I will need all your brains for this session because to today I want with you to build a neural network so now I don't need your brains to build in your no brain surgery in this session but it's another crash course to get developers up to speed on machine learning and deep learning and neural networks so I need all your attention the data set we will be using is a very classical one it's this one here handwritten digits academia has been working on these data sets for the past 20 years so you should go to the website but it's hosted you will actually see 20 years of research papers and that's what we will do together today will will go on this data set trying to build a network that recognizes this angers and digits from the simplest possible Network all the way to 99% accuracy so let's start just a question beforehand who has done some work with a neural networks before oh well ok quite a few people so feel free to help me and I hope this will not be too basic for you and I hope it will at least be a good introduction to tensorflow but if you have never done anything with neural network networks that's fine and I will explain everything from the start so this is the simplest possible neural network we can imagine to recognize our handwritten digits so the digits they come as 28 by 28 pixel images and the first thing we do is that we flatten all those pixels into one big vector of pixels and these will be our inputs now we will use exactly ten neurons the newest are the white circles what a neuron does is always the same thing a neuron does a weighted sum of all of its inputs here the pixels it adds another constant that is called a bias that's just an additional degree of freedom and then it will feed this sum through an activation function and that is just a function number in transform number out we will see several of those activation functions and the one thing they have in common in neural networks is that they are non linear so why 10 neurons well simply because we are classifying those digits in 10 categories we are trying to recognize a zero or one that you want Illinois so what you are hoping for here is that one of those neurons will light up and tell us with a very strong output that I have recognized here an 8 all right and for that since this is a classification problem we are going to use a very specific activation function one that ball researcher tells fellows works really well on classification problems it's called softmax and it's simply the explore an exponential normalized so what you do is that you you make all those weighted sums then you elevate that to the exponential and once you have your 10 Exponential's you can compute the norm of this vector and divided by its norm so that you get values between 0 and 1 and those values you will be able to interpret them as probabilities probabilities of this being to an 801 or something else you will be asking which norm any norm doesn't matter the length of the vector you pick your your your favorite norm there are several usually for softmax we use L 1 but L 2 which is the Euclidean norm would work just as well so what does soft maggots do actually you see it's an exponential so it's very steeply increasing function it will pull the data support increase the differences and when you divide all of that when you normalize the whole vector you usually end up with one of the values being very close to one and all the other values being very close to the zero so it's a way of pulling the winner out of top without actually destroying the the information all right so now we need to formalize this using a matrix multiply I will remind you of what a matrix multiply is but we will do it not for one image we are going to do this for a batch of the 100 images at a time so what I have here in my matrix is 100 images one image per line the images are flattened all the pixels on one line so I take my matrix of weight for the time being I don't know what these weights are just weight so I'm doing weight itself and I start the matrix multiplication so I do a weighted sum of all the pixels of the first image it is and then if I continue this matrix multiply using the second column of weights I get a weighted sum of all the pixels of the first image for the second neuron and then for the third neuron and the fourth and so on what is left is to add the biases just an additional constants again we don't know what it is only and there is one bias per neuron that's why we have ten biases and now if I continue this matrix multiply I'm going to obtain these wicked sums for the second image and the third image and so on until I have processed all my images and I would like to write this as a single formula there you see there is a problem x times W you know that's a matrix of ten columns by 100 images and I have only ten biases I constantly add them together while nevermind we will redefine addition and it's okay if everybody accepts it and actually people have already accepted it it's called a broadcasting ad and that's the way you do additions in numpy for instance which is the numerical library for or for bison the way of broadcasting ad works is that if you are trying to add two things which don't match not the same dimensions you can't do the addition you try to replicate the small one as much as needed to make the sizes match and then you do the addition that's exactly what we need to do here we have only those ten biases so it's the same biases on all the lines we just need to replicate this bias vector on all the lines and that's exactly what this generalized broadcasting ad does so we will just write it as a plus and this is where I wanted to get to this I want you to remember this as the formula describing one layer in a neural network so let's go through this again in X we have a batch of images 100 images all the pixels on one line in W we have all our weights for the ten neurons all the ways in in the in the system x times W are all all of our wicked sums we have the biases and then we feed this through our activation function in this case softmax the way it works is line by line line by line we take the ten values elevate them to the exponential normalized line next line ten values elevate them to the exponential normalized line and so on so what we get in the output is for each image ten values which look like probabilities and which are our predictions so of course we still don't know what those weights and biases are and that's whether the trick is in neural networks we are going to train this neural network to actually figure out the correct weights and biases by itself well this is how you write this in tensor flow you see not that is different okay 10 Central has this n n library for neural network which has all sorts of very useful functions for neural networks for example softmax and so on so let's go train when you train you've got images but you know what those images are so your network if you initialize your waiting biases at random value and your network will output some probability since you know what this image is you can tell it that it's not this it should be that so that is what that is it is called a one hot encoded vector it's very not very fancy way of encoding numbers basically here our numbers from 0 to 9 we encode them as 10 bits all at 0 and just one of them is 1 at the index of the number we want to encode here are 6 why well because then it's in the same shape as our prediction and we can compute a difference the distance between those two so again many ways of computing distances the Euclidean distance the normal distance sum of differences squared would work not a problem but scientists tell us that for classification problems this distance to cross entropy works slightly better so we'll use this how does it work it's the Sun across the vectors of the values on the top multiplied by the logarithms of the values on the bottom and then we add a minus sign because all the values on the bottom are less than 1 so all the logarithms are negative so that's the distance and of course we will tell the system to minimize the distance between what it thinks is the truth and what we know to be true so this will call our error function and the training will be guided by an effort to minimize the error function let's think of this work in practice so in this little visualization I'm showing you over there my training images receipts training so you see this batches of 100 training images being set into the system on a white background you have the images that have been already correctly recognized by the system on a red background images that are still missed so then only in the middle graph you see our error function computed both on the training data set and we also can't decide a set of images which we have never seen during training for testing of course if you want to test the real-world performance of your neural network you have to do this on a set of images which you have never seen during training so here we have 60,000 training images and I set aside 10,000 test images which you see in the bottom graph over there they're a bit small you see only 1000 of them here so imagine there are nine more screens of pictures like that but I sorted all the badly recognized one at the top so you see all the ones that have been badly recognized and below are nine screens of correctly recognized images here are two mm rounds of training so that a little scale on the side here it shows you that it's already capable of recognizing 92 percent of other images with this very simple model just and neurons nothing else and that's what you get upon the top graph the accuracies graphs as well that's simply the percentage of correctly recognized images both on tests and training data so what else do we have where we have our weights and biases those two diagrams are simply percentiles so it shows you the spread of all the weights and biases and that's just useful to see that they are moving they both started at zero and they took some values for the weights between one and minus one for biasing between 2 and minus 2 it's helpful to keep an eye on those diagrams and see that we are not diverging completely so that's the training algorithm you give it training images gives you a prediction you compute the distance between the prediction and what you know to be true you use that distance as an error function to guide a mechanism that will drive the error down by modifying weights analysis so now let's write this in tensor flow and I'll get more explicit about exactly how this training works so we need to write this integral so the first thing you do in tensor flow is defined variables and placeholders the variable is a degree of freedom of our system something we are asking 10 to the flow to compute for us through training so in our case those are our weights and biases and we will need to feed in training data so for this data that will be set in at training time we define a placeholder you see here X X is a placeholder for our our training images let's look at the shaping brackets what you have is the shape of this multi-dimensional matrix which we call a tensor so the first dimension is is none it says I don't know yet this will be the number of images in a batch this will be determined as training training time you forgive if we give 100 images this will be 100 then 28 by 28 is the size of our images and 1 is the number of values per pixel that's not useful at all because we are handling grayscale images I just put it there in you wanted to handle color images that would be three three values for six per pixel so okay we have our placeholders we have our variables now we are ready to write our model so that line you see on the top is our model it's what we have determined to be the line representing one layer of the neural network the only change is that reshape operation you remember our images they come in as 28 by 28 pixel images and we want to flatten them as one big sector of pixels so that's what the reshape does 784 is 28 by 28 it's all the pixels in one line all right I need a second place holder for the known answers the labels of my training images labels like this is a 1 this is a 0 this is a 7 this is a 5 and now that I have my predictions in my no labels I'm ready to compute my error function which is the cross-entropy using the formula we've seen before so some across the vector of the elements of the labels multiplied by elements of the logarithm of the projections so now I have my error function what do I do with it what you have on the bottom I don't I won't go into that that is simply the computation of the percentage of correctly recognized images you can skip that ok now we get to the actual heart of what tensorflow will do for you so we have our error function we take an optimizer there is a full library of them there here they have different characteristics and we ask the optimizer to minimize our error function so what is this going to do this is going when you do this tensorflow takes you an error function and compute the partial derivatives of that error function so that relatively took all the way and all the biases in the system that's a big vector because there are lots of weights and lots of biases how many w the wait is a drive variable of almost 8,000 values so this vector we get mathematically is called a gradient and the gradient has one nice property who knows what is the nice property of the gradient each points yes what it almost it points up we had a minus sign it points down exactly down in which space we are in the space of all the weights and endures all the variables and the function we are computing is our error function so when we say down in this space it means it gives us a direction in a direction in the space of wages and n biases into which to go to modify our weights and biases in order to make our error function smaller so that is the training you compute this gradient it gives you an arrow you take a little step along this arrow well you are in the space of weights and biases so taking a little setting to modify your weights and biases by this little Delta and you get into location where the error is now smaller well fantastic that's exactly what you want then you repeat this using a second batch of training images and again using the third batch of training images and so on so it's called gradient descent because you follow the gradient to head down and so we are ready to write our training loop there is one more thing I need to explain to you about tensor flow tensorflow has a deferred execution model so everything we wrote up to now all the TF dot something here commands there's not actually when that is executed it doesn't produce values it builds a graph of computation graph in memory why is that important well first of all this derivation trick here the computation of the gradient that is actually a formal derivation tensorflow takes the formula that you needed to define your error function and there's a formal derivation on it so it needs to know the full graph of how you have you how you computed this to do this formal derivation and the second thing is for use this graph for is that end result is built for distributed computing and there as well to distribute a graph on multiple machine it helps to know what the graph is okay but it means so this is all very useful but it means for us that we we have to go through an additional loop to actually get values from our computation the way you do this in ten to the flow is that you define a session and then in this session you call session dot run on one edge of your computation graph and of course for this 2m and that that will give you actual values but of course for this to work you have to fill in all the placeholders that you have defined now with real values so for this to work I will need to fill in the training images and the training labels for which I have defined placeholders and the syntax is simply the trained data dictionary there you see the keys of the Dex dictionary x and y underscore are the placeholders that I have defined and then I call session the run on my training step I pass in this training data and that is where the actual magic happens just rely on what is this training step well it's what you got when you asked the optimizer to minimize your error function so the training step when executed is actually what computes this gradient using the current batch of images training images and labels and follows it a little to modify the weights and biases and end up with better ways and biases I said a little I come back to this what is that learning a race over there well I can't make a big step along the gradient why not imagine you're in the mountains you know we're down is we have senses for that we don't know to derive anything we know we're down it and you want to reach the bottom of the valley now if every step you made is a 10-mile step you will probably be jumping from one side of the valley to the other without ever reaching the bottom so if you want to reach the bottom even if you know we're down is you have to make small steps steps in that direction and then you will reach the bottom so the same here when we compute this gradient we multiple we multiplied by this very small value so as to take small steps and be sure that we're not jumping from one side of the valley to the other all right so let's finish our training basically in a loop we load a batch of 100 training images and labels we run this training step which adjusts our weights and biases and we repeat all the rest of the stuff on the bottom it's just for display I'm computing the accuracy and the cross-entropy on my training data and again on my test data so that I can show you four curves over there it is just for display that has nothing to do with the training itself all right so that was it that's the entire code here on one slide let's go through this again at the beginning you define variables for everything that you want tensorflow to compute for you so here are our weights and biases you define placeholders for everything that you will be feeding during the training mainly our images and our training labels then you define your model your model gives you predictions you can compare those predictions with your known labels compare a distance between the two which is the cross entropy here and use that as an error function so you pick an optimizer and you RC optimizer to minimize your error function that gives all the gradients and all that it gives you a training set and now in a loop you load a batch of images you're on your training set you load a batch of images and labels you're on your training set and you do this in a loop and hoping this will converge literally it does as you see here it did converge and with this approach we got 92% accuracy smaller recaps of all the ingredients we put in our pops so far we have a softmax activation function we have the cross entropy as an error function and we did this mini batching saying where we trained one 100 images at a time do one step and then load another batch of images so he's 92% accuracy good now horrible imagine you're actually using this in production I don't know in the post office you're you're decoding zip codes 98 92 percent out of 100 digits you have 8 bad values no not usable in production forget it so how do we fix it well deep learning will go deep you can just stack those layers how do how do we do that well it's very simple look at the top layer of neurons it does what we just did it computes a weighted sums of pixels but we can just as easily add a second light that will compute weighted sums of the outputs of the first layer and that's how you stack layers to produce a deep neural network now we are going to change our activation function we keep softmax for the output layer because softmax has these nice properties of pulling a winner apart and producing numbers between zero and one but for the rest we use a very classical oxidation function in neural networks is called the sigmoid and it's basically the simplest possible continuous function that goes from zero to one okay all right let's write this model so we have now one set of weights and one set of biases per layer that's why we use c5 Ayers here and our model will actually look very familiar to you look at the first line it's exactly what we have seen before for one layer of a neural network now what we do with the output y1 is that we use it as the input in the second line and so on we change those it's just that on the last line the activation function we use is the softmax so that's all the changes we did and we can try to run this again so this one run run run and let's comment well I don't like this slope here should be shooting up really sharp it's a big flow actually I have a solution for that III lied to you what I said that the sigmoid was the most widely used activation function that was true in the past and today people invented a new activation function which is called the Rayleigh and this is irrelevant simpler it's just zero for all negative values and identity for all positive values now this actually works better it has lots of advantages why does it work better we don't know people tried it it works better I'm being honest here if you had a researcher here he would fill your head with equations and prove it but he would have done those equations after the fact people already tried it it worked better actually they got inspiration from biology it is said I don't think it is true but I heard that the sigmoid was the preferred model of biologists for our actual biological neurons and that today biologist thinks the thing that neurons in our head work more like this and the guys in computer science got inspiration from that pride it works better how better well this is just the beginning of the training this is what we get with our sigmoids just 300 iterations so that's just the beginning and this is what we get from Raley's well I prefer this each the accuracy chutes are really sharp the cross-entropy goes down really sharp it's much faster and actually here on this very single problem the sigmoid would have recovered it's not an issue but in very deep networks sometimes with the Sigma's you don't converge at all and and the Rayleigh solves that problem to some extent so the renovating is for most of our issues okay so now let's train let's do this for ten thousand iterations side layers look at that ninety-eight percent accuracy first of all oh yeah we went from 92 to 98 just by adding layers that's fantastic but look at those curves but all messy what is all this noise well when you see noise like that it means that you are going too fast you're actually jumping from one side of the valley to the other without correctly reaching the bottom of your error function so we have a solution for that but it's not just to go slower because then you would spend 10 much ten times more time training the solution actually is to start fast and slow down as you as you train it's called learning rate we can usually educate the learning rate on an exponential curve so yes I hear you it sounds you know very simple why why this little trick but let me play you the video of what this does it's actually quite spectacular so it's almost there should I have the end of it on a slide yeah that's it so this is what we had using a fixed learning rate and just by switching to a decaying learning rate look it's spectacular all the noise is gone and for the first so just with this little trick really this is not rocket science it's just going slightly slower towards the end and all the noise is gone and look at the blue curve the training accuracy curve towards the end is stuck at 100% so here for the first time we built a neural network that was capable of learning all of our training set perfectly it doesn't make one single mistake in the entire training set which doesn't mean that it's perfect in the real world as receipt on the training shoulder on the test data set it has a 98% accuracy but well it's something we got 100% at least on the inflation alright so we still have something that is a bit design look at those two curves that's this is our error function so the blue curve the test error function that is what we minimize okay so as expected it goes down and the error function computed on our test data at the beginning well it follows that's quite nice and then it's disconnected so this is not completely unexpected you know we are minimizing the training error function that's what we are actively minimizing we are not doing anything at all on the on the test side it's just a byproduct of the way neural networks work that the training you do on your training data actually carries over to your test data to the real world well it carries over or it doesn't so as you see here until some point it does and then there is a disconnect it doesn't it doesn't carry over anymore you keep optimizing the error on the training data but it has no positive effect on the test performance the real work performance anymore so if you see curve like this you take the textbook you look it up it's called overfitting you you look at the solutions they tell you over feeling you need regularization okay let's regular eyes what regularization options do we have my preferred one is called dropouts it's it's kind of quite dramatic you shoot the neurons no really so this is how it works you take your narrow network and pick a probability let's say 50% so at each training iteration you will shoot physically removed from the network 50% of your neurons do the Tough then put them back next iteration again randomly shoot 50% of you in Europe of course when you test you don't test with a half brain-dead neural network you put all the neurons back but that's what you do for training so in an intensive flow there is a very single function to do that which is called dropout that you apply at the output of the layer and what it simply does that if you take the probability and in the output of that layer it will replace randomly some values by zeros and small technicality it will actually boost the remaining values proportionally so that the average stays constant that's it that's a technicality so why does shooting neurons help well first of all let's see if it helps so let's let's try to recap all the tricks we try to play with our neural network this is what we had initially with our five layers using the sigmoid as an activation function it got the accuracy got up to ninety seven point nine percent using five layers so first we replace two sigmoid by the rayleigh activation function you see it's faster to converge at the beginning and we actually gained a couple of fractions of percentage of accuracy but we have this messy curve so we trained slower using the exponential learning rate in VK and we get rid of the noise and now we are stable or above 98% accuracy but we have that weird disconnect between the error on our test data and the error allowed on our training data so let us try to add dropout this is what you get with dropout and actually the cross entropy function the test cross entropy function the red one over there and on the on the right has been largely brought under under control you see there is still some disconnect but it's not shooting at as it was before that's very positive let's look at the accuracy no improvement actually I'm even amazed that it hasn't gone down seeing how brutal this technique is you should neuron watch while you're trained let's here I was very hopeful to get it up no nothing we have to keep digging so what is really overfitting let's go beyond the simple recipe in the textbook what over filling in a neural network is primarily when you give it too many degrees of freedom imagine you have so many neurons in so many ways in a neural network but it's somehow feasible to simply store all the training images in those weights and add variables you have enough room for that and the neural network could figure out some cheap tricks to pattern match the training images in what it has Ford and just perfectly recognize your training images because it has stored copies of all of them well if it has enough space to do that that would not translate to any kind of recognition performance in the real world and that's the trick about neural networks you have to constrain their degrees of freedom to force them to generalize and mostly when you get overfitting is because you have too many neurons you need to get to get that number down to force the network to produce generalizations that will then produce good predictions even in the real world so even you either you get the number of neurons down or you apply some tricks like drop out that is supposed to mitigate the consequences of too many degrees of freedom these opposites of too many neurons if you have a very small data sets well even if you have only a little a small number of neurons if the data set the training data that is very small it can still fit easily so that's a general truth in in a neural networks you need big data sets for training and then what happens here we have a big data set sixty thousand digits that's enough we know that we don't have too many neurons because we added five layers that's a bit overkill but I tried I promise with four and three and two and we try to drop out which is supposed to mitigate to the fact that you have too many neurons and didn't do anything to the access so the conclusion here that we come to is that our network the way it is built is inadequate it's not capable by by its architecture to extract the necessary information from our data and maybe someone here can pinpoint something is really stupid we did at the beginning someone has an idea remember we have images images with shapes with curves and and lines and we flattened all the pixels you know in one big vector so all that shape information is lost this is terrible that's why we are performing so badly we lost all of the shape information so what is the solution what people have invented a different type of neural networks to handle specifically images and problems where shape is important it's called convolutional networks here we go back to the to the general case of an image of the other color image that's why it has red green and blue components and in a convolutional Network one neuron will still be doing weighted sums of pixels but only a small patch of pixels above its head only a small patch and the next year would again be doing a weighted sum of the small patch of pixels above above itself but using the same weights okay that's the fundamental difference from what we have seen before the second neuron is using the same weight as the first neuron so we are actually taking just one set of weights and we are scanning the image in both directions using that set of voice and producing weighted sums so we scan it in both directions and we obtain one layer of weight itself so how many weights do we have well has many weights as we have seen put values in that little highlighted cube that's four times four times three which is around 48 what 48 we had 8,000 degrees of freedom in our simplest network with just 10 neurons how can it work with such a drastic reduction in the number of ways well it won't work we need more degrees of freedom how do we do that well we pick a second set of weights and do this again and we obtain a second let's call it a channel of values using different weights and now since those are multi-dimensional matrices is fairly easy to write those those two matrices as one by simply hiding the dimension of dimension two because we have two sets of values and this here will be the shape of the White Nights matrix for one convolutional layer in in a neural network now we still have one problem left which is that we need to bring the amount of information down at the end we still want only ten outputs with our ten probabilities to recognize what this number is so traditionally this was achieved by what we call a sub sampling layer it's I think it's what useful to understand how this works because it gives you a good feeling for what this network is doing so basically we were scanning the image using a set of weights and during trainings these weights will actually specialize in some kind of shape recognizers there will be some ways that will become very sensitive to horizontal lines and some ways that will become very sensitive to vertical lines and so on so basically when you scan the image if you simplify you get an output which is mostly I see nothing I see nothing I think nothing all I seen something I've seen nothing I see nothing oh I've seen something the subsampling basically takes four of those outputs two by two and it takes the maximum value so it retains the biggest signal of I seen something and passes death down to the layer below but actually this is a much simpler way of you know condensing information what if we simply play with the stripe of the convolution instead of applying is instead of scanning the image pixel by pixel we scan it every two pixels we jump by two pixels between each weighted sum or mechanically instead of obtaining 28 by 28 output values we will say only 14 by 14 output values so we have condensed our information and mostly today I'm not saying this is better but it's just simpler and mostly today people who build convolutional networks just use convolutional layers and play with the with the step to condense the information and and it's simpler you don't have you don't need in this way to have these sometimes subsampling layers so this is the network that I would like to build with you let's go to it there is a first convolutional layer that uses patches of 5x5 I'm reading through the w1 tensor and we have seen the dish in this shape the two first digits if the size of the patch you pass the third digits is the number of channels it's reading from the input so he ran back to my real example this is a grayscale image it has one value per pixel so I'm reading one channel of information and I will be applying four of those patches to my image so I obtain four channels about the values okay now second convolutional layer this time my stride is two so here my outputs become planes of 14 by 14 values so let's go through it it's I'm applying patches of four by four I'm reading in four channels of values because that's why our output in the first layer and this time I'll do I will be using eight different patches so I will actually produce 80 different channels of wicked cells next layer again astride of two that's why I'm getting down from 14 by 14 to 7 by 7 patches of 4x4 reading in 8 channels of values because that's what I had in the previous layer and outputting 12 channels or values this time because I use 12 different patches and now I apply a fully connected layer so the kind of layer we've seen before okay this fully connected layer I have remember the differences in this one each neuron does the weighted sum of all the values okay in the little cube of values above not just a patch all the values in the next neuron in the fully connected Network does again a weighted sum of all the values using its own ways it's not sharing with that's the normal neural network layer as we have seen before and finally I apply my soft max layer with my kenhub alright so can you write this in tensor flow well we need one set of weights and biases for each layer the only difference is that for the convolutional layers our weights we have this will have this specific shape that we have seen before so two two numbers for the filter size one number for the number of input channels and one number for the number of patches which corresponds to the number of output channels that you produce for our normal players we have the weights and biases defined as before and you see this truncated normal thingy up there that's just a random okay to complicated way of saying random so we initialize those weights to random values initially and now this is what our model will look like so tensorflow have this helpful cons 2d function if you give it a weight matrix and batch of images it will scan them in both directions so it's just a double loop to scan the image in both directions and produce the weight itself so we do those way sums we added bias we feed this through an activation function in this case the ready and that's our outputs and again the way of stacking these layers is to feed y1 the first output as the input of the next layer all right after with our three convolutional layers we need to do a weighted sum this time of all the values in this 7 by 7 by 12 little cube so to achieve that we will flatten this cube as one big vector of value that's what the flatten the reshape here does and then two additional lines that is that aligns that you should recognize those are normal neural network layers as we have seen before all right and let this work so this time it like it takes a little bit more time to to process so I have a video you see the accuracy is shooting a really fast I will have to zoom and the prom is tonight stone and percent accuracy is actually not too far we're getting there we're getting there how are we getting there we're not getting there oh yeah and so disappointed again I really wanted to bring this to 99% accuracy we'll have to do something more 98.9 damage that was so close all right yes exactly this should be your WTF moment what is that on the cross entropy loss curve okay let me zoom on it you see that a disconnect do we have a solution for this drop out yes let's go shooting our neurons it didn't work last time indeed this time it will so actually what we will do here it's a little trick it's almost a methodology for coming up with the ideal neural network for a given situation and what I like doing is to restrict the degrees of freedom until it's apparent that it's not optimal it's hurting the performance here I know that I can get about 90 to 99 percent so I restricted it a little bit too much and from that point I give it a little bit more freedom and applied result to make sure that this additional freedom will not result in over and that's basically how you'll pain a pretty optimal neural network for a given problem so that's what I've done here you see the patch is on a slightly bigger 6 6 5 5 4 4 instead of 5 5 4 4 and so on and I've used a lot more patches so 6 patches in the first layer 12 in the second layer and 24 in the third layer instead of 4 8 and 12 and I apply dropout in the fully connected layer so now why not in the other layers I tried both it's possible to apply drop out in convolutional layers but actually if you count the number of neurons there there is a lot more neurons in the fully connected layer so it's a lot more efficient to be shooting them there I mean you need to hurt a little bit too much to shoot neurons where you have one of your few them so with this let's run this again so again the accuracy should suffer is very fast I will have to zoom in look welding well the 99% is and we are above yes thank you I promise you will get above 99 and we are actually quite constantly above we get to 99.3% in this time let's see what our dropout actually did so this is what we had with a five layer Network and already a little bit more degrees of freedom so more layers sorry more patches in each layer you see we are already above 99 percent but we have this big you know disconnect between the tests and the training cross entropy later the slide result boom the test cross entropy function is brought in under control it's not shooting up as much and look this time we actually had a problem in this with just applying the result we got two-tenths of a percent more accuracy and here we're sure we win we are fighting on for the last percent between 99 and 100 so getting to 10th is enormous with just a little trick alright so there we have it we built this network and brought it all the way to 99% accuracy the plates nodes just social justice summary and to finish so this was mostly about tensor flow we also have a couple of three trained api's which you can use Justin a PRS if your problem is standard enough to fit into one of those clouds Asian cloud speech natural language or translate API and if you want to run your tensor flow jobs in the clouds we also have this cloud ml engine service that allows you to execute your tensor flow jobs in the cloud for training and what is even more important with just a click of a button you can take a train model and push it to production behind an API and start serving predictions from the model in the cloud so there that's I think that's a little technical detail but from an engineering perspective it's quite significant that you have a very easy way of pushing something to product thank you you have the code on github and this slide deck is freely available at that URL and with that we have five minutes for questions if you have any [Applause] [Music] [Music] 