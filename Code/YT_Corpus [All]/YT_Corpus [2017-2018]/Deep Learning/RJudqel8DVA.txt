 on this episode of AI adventures we will attempt to go through an entire machine learning workflow in one video pulling best practices from our previous episodes it's a bit of material but I think we can do it training a model with the end this data set is often considered the hello world of machine learning but that's been done many times over and unfortunately just because a model does well on M NIST is not necessarily predictive of high performance with other datasets especially since most image data we have today are more complex than handwritten digits delando decided it was time to make n this fashionable again and recently released a data set called fashion amnesty it's the exact same format as the regular m missed except the data is in the form of pictures of various clothing types shoes bags it's still crossed in categories though and the images are still 28 by 28 pixels so let's train a model to detect which type of clothing is being shown we'll start by building a linear classifier as usual and see how we do and we'll use tensorflow as estimator framework to make our code easy to write and easy to maintain as a reminder we'll first load in the data create our classifier and then run the training and evaluation we'll also make some predictions directly from our local model let's start by creating our model we'll flatten that data set from being 28 by 28 to 1 by 784 pixels and make a feature column called pixels this is analogous to our flower features from episode 3 plain and simple estimators next we'll create our linear classifier we have 10 different possible classes to label instead of the three that we used previously with the iris flowers to run our training we'll need to set up our data set and input function tensorflow has a built-in utility to accept a numpy array and generate an input function right from that so let's take advantage of it we're loading our data set using the input data module I've already downloaded the day set to a folder so we'll point to that here now we can call classifier trained to bring together our classifier the input function and the data set finally we run an evaluation step to see how our model did when we use the classic and this data set this linear model typically gets about nine the 1% accuracy however fashion and this is a considerably more complex data set and we can only really achieve an accuracy in the low 80s and sometimes even lower than that so how can we do better as we learn in episode 6 let's go deep swapping in the DNN classifier is a one-line change and we can now rerun our training and evaluation to see if our deep neural network can perform any better than the linear one and as we discussed in episode 5 we should bring up tensor board to take a look at these two models performance side by side it looks like the deep network could definitely use some more time to Train though estimators makes this easy all we need to do is rerun the call to the train and evaluate functions looking at tester board it seems like my deep model is performing no better than my linear no one did this is perhaps an opportunity however to tune some of my hyper parameters like we talked about in Episode two maybe my model needs to be larger to accommodate the complexity of this data set or perhaps my learning rate needs to be lowered experimenting with these parameters a bit we can finally break through and achieve a higher overall accuracy than our linear model can obtain it takes quite a bit more training but ultimately this is worth it to achieve those higher accuracy numbers notice also that the linear model plateaus earlier than the deep network because deep networks are often more complex than linear ones they can take longer to Train and at this stage say we're happy with our model we'd be able to export it and produce a scalable fashion M this classifier API you can see episode 4 for more details on how to do that let's also take a quick peek at how you can make predictions using estimators in large part it looks just like how we called train and evaluate that's one of the great things about estimators the consistent interface notice that this time we've specified a batch size of 1 num epochs of 1 and shuffle is false this is because we want the predictions to go one by one making predictions through all the data and preserving that order I've extracted five images from the middle of the evaluation data set for us to try some predictions on and I picked these five just because they were in the middle but because we my model managed to get two of them wrong both were supposed to be shirts but the model thought that the third example was a bag and the fifth example was a coat incorrectly and you can see we're looking at these images how these examples are more challenging than handwritten numbers if for no other reason than just the graininess of the images so how did your model perform and what parameters did you end up using to achieve that accuracy let me know below in the comments you can find the code that I use to train this model and generate these images also in the links below along with more links to the other resources we talked about in this episode our next set of videos will be focused on some of the tools of the machine learning ecosystem to help you build out your workflow and torch it as well as showcase even more architectures that you can employ to solve your machine learning problems I look forward to seeing you there and until then keep on machine learning thanks for watching this episode of cloudy AI adventures be sure to subscribe to the channel to catch future episodes right when they come out 