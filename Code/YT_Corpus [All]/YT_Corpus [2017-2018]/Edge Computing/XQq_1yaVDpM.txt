 Hi, thanks for tuning into Singularity Prosperity. This video is the third in a multi-part series discussing computing. In this video, we'll be discussing classical computing, more specifically - how the CPU operates and CPU parallelism. [Music] In the previous video in this series we discussed the shrinking of the transistor, allowing for more powerful and efficient computers, as well as the end of Moore's Law based on the miniaturization of the transistor within the next seven to ten years. Be sure to check it out for some background context for this video. Now in that video when referring to computing performance, we were focused on classical computing based on the CPU. Classical computing, is essentially the digital computer, almost every computing device on the market today is a classical computer. Classical computers operate in serial, in other words, as mentioned in the first video in the series, Computing Origins, executing various instructions extremely fast in 'order', but to the average user it appears to be running them in parallel, meaning multiple instructions at a time. This is due to many hardware and software optimizations to allow for asynchronous operation. By the end of this video the distinction between parallel and asynchronous operation will become clear, but first let's see how the classical computer works. As a disclaimer: the concepts we will be discussing are an over generalization of computing architecture, but for the sake of getting an abstracted understanding of classical computing functionality will serve well. Alright so first let's bring in the central processing unit, the CPU is the brains of the computer. Let's also bring in memory, the RAM, this is where the CPU accesses stored information it needs. Now the CPU also has built-in memory, this is called the cache. The cache is considerably smaller than the RAM, with sizes ranging in the order of 32 kilobytes to 8 megabytes. The purpose of the cache is to give the CPU the information it needs immediately. The CPU and RAM are separate objects, so when the CPU needs information, it takes time, albeit a very small amount of time to read the data from the memory. This time can add considerable delay to computer operation. With the cache being right on the CPU reduces this time to almost nothing. The reason why you don't need much cache storage is because it just needs to store little bits of important information that the CPU will need to use soon or has been using a lot of recently. There are various methods implemented to determine what goes on to the cache, what should be kept on the cache and when it should be written back on to the RAM. In a typical CPU there are various levels of cache, each with different read and write times and sizes, for the sake of simplicity we'll assume a single cache for our CPU. So now with the basic components out of the way, let's get into how the computer operates. When a CPU executes an instruction there are five basic steps that need to be completed: 1) Fetch: Get the instruction from the memory and store in the cache in some cases. 2) Decode: Get the appropriate variables needed for the execution of the instruction. 3) Execute: Compute the result of the instruction. 4) Memory: For instructions that require a memory read/ write operation to be done. 5) Write Back: Write the results of the instruction back into memory. Nearly every instruction goes through the first three and final step, only certain instructions go through the memory steps such as load and stores but for the sake of simplicity we'll assume every instruction requires all five steps. Now each step takes one clock cycle, this translates to a CPI, clock cycles per instruction, of five. As a note, most modern processors can execute billions of clock cycles per second, for example, a 3.4 gigahertz processor can execute 3.4 billion clock cycles per second. Now a CPI of 5 is very inefficient, meaning the resources of the CPU are wasted. This is why pipelining was introduced, bringing asynchronous operation into computing. Pipelining essentially makes it so each step can be executed in a different clock cycle, translating to 5 instructions per 5 clock cycles, or in other words, one instruction per clock cycle, a CPI of 1. Essentially what pipelining does is take the segmented steps of an instruction and execute them in each clock cycle, since the segmented steps are smaller than the size and less complex than a normal instruction, you can do the steps of other instructions in the same clock cycle. For example, if a step for one instruction is fetching the data, you could begin decoding another, executing another, etc - since the hardware involved for those steps isn't being blocked. Superscalar pipelines add to this performance further. Think of pipelines as a highway, now typical lane in the highway can execute one instruction per clock cycle. With superscalar processors you add more lanes to the highway, for example, a 2 wide superscalar also referred to as a dual issue machine, has a theoretical CPI of 1/2, two instructions per clock cycle. There are various other methods implemented to make the processor CPI more efficient, such as: unrolling loops, very long instruction words [VLIWs] - which are essentially multiple instructions wrapped into one larger instruction, compiler scheduling an optimization - aallowing for out of order execution and more. There are also many issues that come along with pipelining that decrease CPI, such as: data hazards, memory hazards, structural hazards and more. All these topics are beyond the scope of this video, but mentioned to satisfy curiosity if you wish to know more about them. So at this point, we now know about the basic design of a CPU, how it communicates with memory, the stages that it executes instructions in, as well as pipelining and superscalar design. Now instead of imagining all of this as a single CPU, let's take it further, all this technology can be embedded on a single core of a processor. With multiple cores, you take the performance of a single core and multiply it by the core count, for example, in a quad core by four. Multiple cores also have a shared cache as a side note. The use of superscalar pipelines as well as multiple cores are considered hardware level parallelism. The computer industry after years of stagnation is now beginning to divert more focus to hardware level parallelism, by adding more cores to processors. This can be demonstrated by consumer processors like AMDs Threadripper line and Intel's i9 processor line, with core counts ranging from 8 to 16 and 10 to 18 respectively. While these may be their higher end consumer processor, even the low and mid end processors from i3, i5 and i7 are getting buffs, with core counts ranging from quad, hex and octa core. As a side note, supercomputers are the best examples of utilizing hardware parallelism. For example, Intel's Xeon Phi and AMD's Epyc processors have core counts ranging from 24 to 72! With supercomputers having tens of thousands of processors with them. Now there's one key component that is required in tandem with hardware parallelism to truly use all the resources efficiently, software parallelism. This leads us to the final topic in classical computing will cover, hyperthreading also referred to as multithreading. Instead of being implemented as hardware parallelism this is used as higher level software parallelism. Think of a thread as a sequence of instructions, now with single-threading, that sequence of instructions just flows through the pipeline as normal. However, with multi-threading, you can segment your application into many threads and specifically choose how you want to execute them. Multi-threading can significantly increase computing performance, by explicitly stating what CPU resources you want to utilize and when. For example, for an application, the user interface, GUI, can be executed on one thread while the logic is executed on another. This is just one example of many instances where multi-threading can be used. Now multi-threading can't just be used for every application. Since classical computing isn't intrinsically parallel, there can be a lot of issues with concurrency, or in other words, when multiple threads are executing at the same time but depend on the result of each other. Thus, some applications end up being only single threaded. However, many individuals and groups are working on ways to best utilize hardware parallelism through new software practices and rewriting old software. For example, the latest Firefox updates now are bringing in multi-threading. Also some of the most computationally intensive tasks by default excel at multi-threading, such as: video editing, rendering and data processing - to list a few. Also as exemplified by the gaming industry, a lot of games are now moving into multi-threaded performance. [Music] So, in summary, classical computing is asynchronous not truly parallel. Instructions are still executed in serial, but through the use of hardware and software level parallelism, maximize the utilization of the resources of the computer, making them execute extremely fast; giving the illusion of parallel operation. Also if you want a deeper look into how the CPU works, I highly recommend you check out the two videos I've listed in the description below. At this point the video has come to a conclusion, I'd like to thank you for taking the time to watch it. If you enjoyed it, please leave a thumbs up and if you want me to elaborate on any of the topics discussed or have any topic suggestions, please leave them in the comments below. Consider subscribing to my channel for more content, follow my Medium publication for accompanying blogs and like my Facebook page for more bite-sized chunks of content. This has been Ankur, you've been watching Singularity Prosperity and I'll see you again soon! [Music] 