 We were talking about the fastest supercomputers that are extant at this point and you feel that there're still a lot smaller than what you would want to see to really be able to yeah the if we take you know all of the computers that are connected to the Internet and that you know doubling probably every couple of years as it is now well pretty soon have the equivalent computing power of one king brain really but that happens in a couple of years I think - could be Wow I don't really I haven't done the a very strict scientific analysis of that but I think that the that that's not going to still be enough and that we still may there still may be other ways to approach computing because that's always been the challenge and I think that the number of devices that we have that are being connected to the Internet is is growing even at a faster rate so when you consider all of the smart phones and all of the IOT devices that are that are coming about so we'll have computational power and now you have to say how do you communicate because in the analogy the brain you have a neuron that is connected to thousands of tens of thousands of other neurons so that connectivity and most of the brain is actually in those connections so which is why the connection making machine was particularly appealing because it's recognizing that the connectivity is really at the heart of computing and that the computing element the CPU or whatever can be very small so with the connect at least the cm-1 it had the 12 dimensional good Leandre hypercube that richard fineman had suggested in that that i ended up also using as a inspiration for the external form of the machine did that hypercube structure survive into the CM - and was it in the CM 5 yes but we we evolved in something what's known as a factory and the factory is another kind of a cube like that the whole point is it's not a strict hierarchy whereas you got higher and higher you go it's fewer and fewer connections going up you needed to be able to do what's known as also the cloths network today we talked about it whereby you have connectivity that that is sort of constant at each level and therefore you have a lot of communication bandwidth between the individual processors and even the data centers today we're moving towards that model and the networking space because we're finding that lo and behold inside of a data center today there's more traffic going between servers than there is in and out of the data center i still that's similar to a brain again there's so much more communication going between the different processing elements so that it's single in obvious you know example is sort of when you make a single query for google you know you might type in you know find me you know picture of a cat and that will be farmed out to hundreds of machines which will all be searching for that and their own cross their own sort of segmented data stores and then aggregate the results coming back so hundreds of machines are involved in answering a particular query so the input and that but can be rather small compared to the amount of communication amongst the processors themselves so for instance school really does across of course yeah thousands of machines us in across many multiple data centers as well and so every that you know every query that processed in parallel and that's where a lot but the we're not using languages as much as we're using this know today's sort of these microservices of being able to have explicit communication between different services that are returning results and analyzing the data so the the division of a problem into a parallel structure is done at a micro service level and yeah and we had we had to take shortcuts as well and one of the big shortcuts we took at that time was what's known as a single instruction multiple data machine we were ready running one program the same program on every machine and issuing those instructions as a way to most efficiently perform this parallel processing that model hasn't been continued today that model you do find however in Nvidia and in video processing and so now chips we actually have these Nvidia chips which are essentially small connection machines that are operating in this kind of single instruction multiple data way because they're processing an image and that's ideally suited for that kind of processing power whereas general intelligence is more difficult to achieve that way so there are other problems that Thinking Machines was working on right at the beginning with things like modeling of weather and many in many ways some of them in high-performance computing which I haven't been very close to in the last several years but they are using much the same algorithms for weather forecasting for you know QCD quantum chromodynamics a lot of these physics problems and they are using much the same algorithms and there you have being done a massively parallel supercomputers today so those still exist and that was really where the connection machine led to to using very very much and essentially tired Center running a batch process running with a single problem at a time quantum chromodynamics of course being the area exactly exactly right healthy so we're highly influenced by behind Richards work and that's also where high-performance and massively it was both Richard you know interest has only been in combining a very different way of thinking about a problem within the the technology you go about actually making it making it work so he was very influential and not only what we worked on but how we thought about the problems I can't stress enough the importance of what we did every day as a as a you know in terms of our problem-solving was to try to think about the problem differently you had to break out of the old mindset there were you know there was a lot of bets being placed at that time if I remember that you couldn't exceed by a factor like 10 a serial problem because you know which said that you know no you can paralyze as much as you can but then the remaining part that you can't paralyze will always slow you down and so there will be a limit to how fast you can though go when M don't forgot was data parallelism and that was know the data if you're matching if you're taking larger and larger and larger amounts of data you can have full power was him across that data and therefore we were consistently showing that I mean it was the difference between a number and one of the other great things is that the people who came to us with problems they were always most interesting problems I remember working in something about space junk which was NASA that NASA had a problem there was a lot of these remains of satellites and nuts and bolts flying around whizzing round space and all we could detect was a photon or two from each one of these things and so you get this like you know scatter plot and in the next scatter plot know scatter plot and you had to try to connect the dots to create the orbits of what these these particles were we're spinning around and the best known serial algorithm showed you just you need an enormous computing power you just couldn't do it instead we would take the position that you can either map each small segment of the sky as a cell and process all the cells at the same time you know and that way we can immediately paralyze the problem and that's the typical kind of thing through a course of a conversation you would you would arrive at a data parallel algorithm which allowed you to really exploit the power of the machine and even things such as sorting which you think of as being a completely serial algorithm that you go through we could break that up into smaller segments sort those merge sort merge and then using the power of the scan operation which was implemented in hardware bring those results together can you explain this scan this scan allowed you to to reduce it's part of the MapReduce algorithm that people are using today that in parallel you perform some function aggregating result out of a set some set and you can do those over many subsets at the same time and so it's an it's the first part of sort of mapping of of data to these different segments and then scanning is reducing those as a reduced operation and since we were on a hypercube we could do that in parallel and use the communications network where we had implemented the scan as this reduction operation and so it's a combination with today and it's data flow where we reproduce these things in a parallel sense quick quick quick question moving from the boolean and cube to the list of family the factory the factory if that happen at this cm 200 cm high low that was the same 5 seemed to was still a hypercube and but the the difference is very is very slight I mean a lot of it has to do with also the size of the machine that we were building we were getting larger processors in a Sam 5 and more memory and therefore the machine was spread out over many cabinets and we needed a way to be able to communicate now over distances and we were still one of the fundamental problems with speed of life I mean being able to have a signal that could go from one corner machine to another which is why we came up with a physical design but you did which was trying to minimize that by making it a cute and so that had to do it but constraints and and posed by this actual speed of light Hey 