 [MUSIC] Hi everyone. It is my great pleasure to introduce you to Manikanta Kotaru. He is here as an intern this summer. He is working with Manya Krishna Nair on a wireless project for VR. And he's going to talk today about his work at Stanford with his advisor Sachin Katti on position tracking for a VR using commodity Wi-Fi.  Hi all. Thank you Edwardo and thank you all for being here. So today I'm going to describe how we can achieve sub-centimeter level accurate tracking using commodity Wi-Fi chips that we can buy in a retail store. This is a joint work with my advisor Sachin Katti at Stanford University. Immersive experiences like virtual reality need extremely accurate tracking of the headset. And this is how a user experiences VR today. Current virtual reality headsets like HTC VIVE and Oculus Rift, they require instrumenting the environment with dedicated sensors like these in order to track the headset. Not only the sensors have a very limited range of one to three meters, but they also severely limit the user movement. For example, here we can see that the image rendered on the headset changes as the user moves. However, as soon as the user goes outside the field of view of the sensor, the headset cannot be tracked, and the image rendered on the headset no longer responds to the user's movement. Even more annoyingly, as soon as the user turns away from the sensor while experiencing virtual reality, the headset stops tracking again. Ideally, one would desire an infrastructure free, position tracking system that does not require the user to install any dedicated sensors in the environment. And moreover, the user should be able to move freely without any restraints in her motion throughout her entire home. However hard it may be to believe, this is the state of the current VR. Either it requires a cumbersome setup, or it provides a very limited user experience. For example, consider Oculus Rift. It uses an external infrared camera like this and tracks the infrared LEDs that are present on the headset. HTC VIVE uses a laser source and photo diodes on the headset in order to track the headset. These systems, in addition to requiring the user to install these specialized sensors, they also share their limitation of the restricted movement that we just talked about. There are other class of devices called as mobile virtual reality devices. For example, Samsung Gear VR and Google Daydream. So these systems use sensors that are already available in a phone in order to track the headset. However, these sensors on the phone, they can only track the orientation of the headset but not the position. So what it means is that the headset is going to respond as the user rotates her head, but does not respond as the user moves along in a space. Developments in augmented reality devices, specifically Microsoft HoloLens and Google Tango led to self contained tracking systems which is awesome. So what these systems do is they equip the headset with cameras and depth sensors in order to track it. However, these systems are known to have problems in low light conditions or when the user is facing a texture less surface like a plain wall. Moreover, these systems are too complex to be embedded in simple virtual reality accessories, like hand motion controllers. So we worked on developing the system WiCapture, that uses the ubiquitous Wi-Fi signals that are present around us, in order to track the headset. So before I get into details of how WiCapture works, I just want to show what this means for virtual reality tracking. So here I attached a Oculus DK2 headset with a Wi-Fi chip. Oculus requires this external camera to be placed within couple of meters from the headset in order to track it. However, the Wi-Fi access points that are tracking the Wi-Fi chip, they are located much farther away at the corners of this six meter by six meter room. So here to the bottom right, I am showing the actual experiment I am performing, that is how I move the headset. Here, I show how the trajectory estimated by Wi-Fi, and here I show the trajectory estimated by Oculus. In all these diagrams, I also show the position of the Oculus camera and the dotted line represents the field of view boundary of the Oculus camera. So as I move the headset, we can clearly see that the trajectory estimated by Wi-Fi resembles the trajectory estimated by Oculus pretty closely. However, unlike Oculus, Wi-Fi based system does not require the user to install any dedicated sensors at all. Now I am going to conduct a similar experiment as before. However, initially I moved the headset outside the field-of-view of the Oculus camera. WiFi, being a system which has no field-of-view limitation, tracks the headset without any issues. However, Oculus starts tracking the system only once the headset is inside the field-of-view of the camera. Now finally, I am going to show an experiment where the headset is turned away from the camera. This represents a scenario were the user is turning away from the tracking sensor. Here, for the entire duration, Oculus has no idea what is happening, whereas WiFi just keeps on tracking the headset without any problems. Now some fine details. If we look at how a WiFi chip works, the equations look very similar to how a time-of-flight camera like Kinect works. So in a time-of-flight camera, there is a light source which sends a reference sinusoidal signal. The camera correlates the received signal with the reference signal in order to obtain time-of-flight camera image. From the phase of this resultant signal one can then obtain the time-of-flight information, which is the time taken for the signal to reach the camera from the light source. Even in a WiFi system, at the start of each WiFi transmission, the WiFi signal source is going to send a reference sinusoidal transmission. The WiFi chip at the receiver, it correlates the received signal with the reference signal in order to obtain this quantity called S channel state information. So this channel state information, or CSI, is similar to an image obtained by a time-of-flight camera. Here I just want to mention that this channel state information is calculated by every WiFi chip that there is, for every WiFi transmission that it listens to. And we intend to get time-of-flight information from this channel state information quantity. Now let's say we equip it, virtually add a headset with a WiFi chip, and it is listening to WiFi transmissions from an access point. Now as the headset moves, the time-of-flight from the access point to the headset changes. Here, I represent the trajectory of the headset by using this vector Delta. And the phase of the received signal changes in proportion with the time-of-flight. Specifically, the proportionally constant is 2 pi times the WiFi frequency. WiFi typically uses the five gigahertz radio waves. So what it means, that even a small change in the time-of-flight, a few picoseconds, registers itself as a significant change in the phase of the received signal. This translates itself to saying that we can accurately detect even millimeter level motions of the virtual reality headset by using the WiFi signals. Sounds pretty good. However, I want to mention that the best prior work-  [INAUDIBLE] Lambda, half the bytes didn't come back again, so where do you go for that?  For the Lambda to actually come back over the two you have to be travelling at like 500 kilometers per hour.  So you're saying that humans move so slow that-  Yeah, the phase wrapping does not occur, yep.  [INAUDIBLE] keeps tracking the differential motions?  So here, the prior state-of-the-art, in WiFi-based positioning, could at best achieve a half a meter of positioning error.  But don't you have accumulation of errors? In the sense that, so you're just adding differences, right, at the end of the day?  Yes.  So is there any way to sort of, again, fix it? In a sense you have to correct it, right? Errors will keep adding up after a point of time, and they'll diverge. So how do you actually make sure that you can get it back to correct? How do you correct these accumulative errors?  So the corrections can be performed through localization techniques. And moreover, we can see the results, how these errors look like. We'll get to the quantitative results later in the presentation. So the state-of-the-art WiFi-based positioning could at best achieve half a meter of localization error. And so it is a long shot to go from half a meter to a few millimeters. So we are talking about a couple of major roadblocks here. So the first challenge is multipath. Given the WiFi wavelength, which is like few centimeters, and typical object size in a typical indoor environment. Everything is a reflector, like walls, monitors, tables, everything. So the actual WiFi signal that a headset receives is the superposition of the signals that is obtained from all these reflected signals. So here, I'm showing the time-of-flight of the signal along two paths, that are reaching the headset along directions R1 and R2. So the time-of-flight along each path depends upon the direction of the path and the trajectory of the headset. c here is just the speed of light. And the actual Wi-Fi signal that you obtain along each path is a simple, complex exponential of the stem of light. So here, this represents the Wi-Fi signal that is obtained from the first path. Here we have the Wi-Fi signal that is obtained from the second path, that is coming along direction r2. And the overall Wi-Fi signal is obtained as a superposition of both these signals. If one naively tries to estimate Time-of-Flight from the phase of this overall received signal, this is how it is going to look like. Although the Time-of-Flight along each path somehow looks like a headset's trajectory. The Time-of-Flight obtained from the phase of the overall signal looks very different from the underlying trajectory. So essentially, the multipath phenomenon is distorting the Time-of-Flight values. There is a even bigger challenge, actually. Remember how I said that a Wi-Fi chip is similar to a Time-of-Flight camera? Well, there is one major difference. In a Time-of-Flight camera, the light source and the camera, they are co-located. They share the same clock. So the Time-of-Flight value that you are going to measure corresponds to the actual Time-of-Flight. However, in a Wi-Fi system, the Wi-Fi signal source and the Wi-Fi receiver, they are different devices. For example, a Wi-Fi access point that is transmitting Wi-Fi signals, is different from your mobile phone that is receiving those Wi-Fi signals. So these devices being different, they use different clocks. And there is a time offset between these two clocks. And worse, this offset actually varies with time. Let's see why. A clock is nothing but a simple oscillator and it measures time by counting the number of oscillations it has performed. Now let's say we ask this oscillator to oscillate at a frequency of 5 gigahertz which is a typical Wi-Fi frequency. Then it cannot oscillate at exactly 5 gigahertz. It is going to operate at a frequency of 5 gigahertz plus or minus 100 kilohertz. And this frequency actually varies with time because oscillators are unstable. Now here, I show the oscillations at the transmitter clock and at a Wi-Fi receiver clock. We can clearly see that the number of oscillations counted by the transmitter is different from the number oscillations counted by the receiver, leading to a time offset between the transmitter's and the receiver's clock. And moreover, as the time progresses, this offset actually varies. So this random clock offset is going to further pollute the phase of the received signal. So the actual Time-of-Flight values that you are going to obtain at the Wi-Fi receiver is the Time-of-Flight values as though there is no clock offset, plus this additional clock offset that you're adding. So here, this represents the overall Wi-Fi signal that is obtained at the Wi-Fi receiver. And this term in the brackets, represents the effect of the multipath where I'm adding the signals from two different paths. And this additional phase offset is due to the clock offset between the transmitter and the receiver. And if we just do a quick math. Let us say that the headset obtained two Wi-Fi transmissions with a gap of 10 milliseconds. During this period, let us say that the headset moved by 10 millimeters. Then the Time-of-Flight actually changed by 30 picoseconds. However, during this same duration of 10 milliseconds, the clocks have drifted apart by more than 200 nanoseconds, assuming there is a 100 kilohertz uncertainty in the frequency. So what it means is that the Time-of-Flight change that you are trying to measure, which is 30 picoseconds, is overwhelmed by the noise of 200 nanoseconds that you're adding due to the clock drift. So we are essentially trying to find needle in a haystack here. And this random clock offset.  Sir, Wi-Fi receivers do clock offset correction, right? How much is that? I forgot the numbers. So every receiver has to do this, and then, for every packet. So they actually use the preamble to do clock offset corrections, right? The carrier frequency offset estimation and correction. Now, what is that? What is the amount that actually get on Wi-Fi receivers today?  So you need to correct the clock offset but the clock offset is valid for only that Wi-Fi transmission. So, you transmitted the Wi-Fi signal, the Wi-Fi signal is received. Now when you start the Wi-Fi transmission again, there is no clock offset correction between two Wi-Fi transmissions.  A Wi-Fi transmission does the. After Smithline Cox, there's a carrier frequency estimation that the receiver does and corrects the phase for every single subcarrier. So that is already done at the per packet level in Wi-Fi transmissions.  Per packet level, it cannot be done for two packets. Because you have to constantly send something called as pilot signals, which are reference signals, which allow you to track the phase change. No, but when you're not transmitting a Wi-Fi packet, you cannot do that pilot transmission and you lost that connection. So every Wi-Fi transmission has to individually perform it. So there is actually a clock offset between Wi-Fi transmissions.  That's true, but I'm guessing, you're also doing that, right? You're transmitting a packet and estimating time of flight or something.  Yes.  I don't know how you do transmissions. So your computer is sending it very often and I'm hoping. I don't know, how do you do it?  You have to send it every four microseconds.  Four microseconds.  Yes.  Microseconds.  Even shorter than that, actually.  So you could keep sending the STS sequence continuously, right? Because STS sequence is four microseconds, so if you keep cycling the STS sequence in a loop.  But you cannot always occupy the medium.  I see, so you don't wanna do that?  You cannot occupy, the Wi-Fi does not allow you to occupy, so.  And linear interpolation does involve.  No, no, actually, the clock of-  But it differs the offset.  This is the exact how the clock offset is going to look like. So the linear interpolation is not going to work, yes.  This is how it looks like.  When the clock offset has becoming the differences between the carrier frequencies, right? And that's changing a lot.  So actually, the Wi-Fi communities, so far, has considered the phase that your have ping from different Wi-Fi transmissions as a random independent signal. And this random clock offset is the primary reason why Wi-Fi community did so. And what I'm going to show today is how we can actually derive some meaning from this apparently random signal. Now, let us first solve the multipath problem. This is not something completely new. So by using multiple antennas at the headset, one can actual dissolve the signal that is obtained from different parts. So let us see why by cancelling the signal that is coming from a particular direction r, and by plotting the time of flight at different antennas of the headset. Now, here, we can see that the time of light at different antennas changes with time because the headset is moving and there is this clog drift. However, the relative time of flight of different antennas compared to the time of flight that is obtained at the first antenna is pretty much constant. And this relative time of flight just depends upon the direction of the path and distance between the antennas. And this happens because the signal from the transmitter to different antennas in the receiver travels slightly different distances. And this difference in the distances manifests itself as the small differences in the time of flight. And this relative time of flight remains constant as long as the direction of the signal from the access point to the headset remains the same, which is a valid assumption as long as the moment of the headset is small compared to the distance between the access point and the headset. Now, based on this insight, let us try to write the Wi-Fi signal that you're opting at the three antennas for the two paths that we considered before. So for the first path, if you just assume that the time of flight for the first antenna is tau 1, then the time of flight at the rest of the antennas have a small additional offset which depends upon the direction of the path. And as before, the actual Wi-Fi signal that you obtain from that particular path for the three antennas is a simple-complex exponential of this time of flights. We can write similar equations for the W-iFi signal obtained from the second path. And the overall Wi-Fi signal that you're opting for the three antennas is the sum of the signal from these two paths. I'm just going to relate this equation as though the signal that is obtained at the three antennas looks as if it is obtained from a linear combination of two vectors. Here, the elements in the vector just depend upon the direction of the paths, r1 and r2. And the weights in the linear combination depend upon the time of flight along the two paths, tau 1 and tau 2. And this time of flight actually varies with time as the headset moves and the clock offset drifts. So the channel state information that you obtain at the three antennas for different Wi-Fi transmission looks as if you are obtaining multiple linear combinations of the same two vectors with different weights. This is actually in a standard form to be form into an algorithm called as MUSIC. What this algorithm does is if you provide multiple linear combinations of a set of vectors, then it is going to split out the elements of the vectors and the weights in the linear combination. So here, once we feed the channel state information from different Wi-Fi transmission to this MUSIC algorithm, then we are going to obtain the elements of this vector. And once we obtain the elements of these vector, it is trivial to identify the directions of the paths, r1 and r2 because we already know their distance between the antennas. And if we obtain the weights in the linear combination, then it is trivial to obtain the time of flight along both the parts, the one and total. Because we know the frequency, the frequency at which the wi-fi is operating. So just to repeat, we now know the direction of the path. And we know the time of flight of the signal along both the paths. Let us just recall what this time of flight along each path consists of. It consists of the actual time of flight, which depends upon the direction of the path and the trajectory of the head set. Plus this additional offset, which is the problematic term. And here lies our simple k insight. This problematic term is actually the same for all the parts. This is actually surprising because what I'm suggesting here is to actually use the multipath to all work in the clock of the challenge. This multipath is usually treated as a complication. A challenge, in time of LED camera systems or a Wi-Fi localization systems. And even I described the multi path as a challenge, because it is actually distorting the time of lead values. However, the same multi path has come to our rescue to overcome the cloud drift that is our cloud drift. So if we just subtract the time of flight from both these paths, although we cannot estimate the clock offset. The clock offset term has just disappeared. And we know the rest of the terms except for the trajectory delta, which is of interest anyways. So I already showed a functioning of the system, where I move the headset and show the trajectory of the headset. But however, I just want to conclude by showing the performance of this system. Using some quantitative metrics.  The same intuition that this definitely is the same? The for the delta is the same, is we are combining the signal from both the paths. And the offset that you're adding is the same for both the paths. Because the offset is being added at the receivers, it has nothing to do with the paths. So you're sending some signal. It is traveling different.  [CROSSTALK].  Different time of light, yeah. Different distances. And it is going to accumulate different time of light. However, at the angle you're adding this additional offset, because of the hardware differences. But that has nothing to do with the actual environment. So it is only being added at the end. So that is why.  They'll have to arrive at the same time though? Let's say I have a measurement right now, I'm gonna have another measurement tomorrow. Will the offset still be the same?  It would not be the same. So for this to actually work, these two parts should be arriving at a close proximity. However, given the speed of light and typical distances that the light has to travel it is like three nanoseconds, nine nanoseconds, very small amount. The clock offset can be treated the same for the small [INAUDIBLE].  [INAUDIBLE] Has 125 nano seconds of 2125 nano seconds of.  That is the furthest part, but consider, the furthest part is 125 nano seconds. So to implement the system, I used Intel 5300 Wi-Fi chips. And I used a command line API available that allows me to query for the channel state information from these chips. And I placed four access points at the corners of this five meter by six meter room. And within my three centimeter by three centimeter area, a Wi-Fi chip is manually placed at different locations. So I actually created this trajectory within a space that measures half a finger. And I tried to recreate the trajectory by using the Wi-Fi signals. The trajectory could be recreated with an accuracy of 1.5 millimeters. Moreover, when a Wi-Fi chip is completely stationary that jitter in the positions that are provided by the white capture is just 0.25 millimeters. Both these numbers are very much comparable to an Oculus Rift, Oculus DK2 headset, but only slightly worse. But a Wi-Fi is providing much more additional functionalities like ease of deployment and resistence to occlusion. So now I conducted even more extensive set of experiments. So what I did was I performed 100 trajectories. And I tested Oculus DK2 headset, like in the previous demos. And here is a sample trajectory where the blue curve represents the trajectory estimated by WiCapture. And they measured the positioning error by position to position distance between the ground crew trajectory, and the trajectory estimated by Wi-Fi for each point on each of these hundred trajectories. And WiCapture could achieve a sub-centimeter level tracking accuracy. And this is the first WiFi-based system that could achieve a substance meter lower tracking accuracy. Just to compare the state-of-the-art WiFi based positioning could achieve a meter lower median Azure. One meter lower Azure is essentially useless for virtual reality tracking. So I performed 100 more trajectories, but here, I included the target from the access points by using wall and furniture. Even in these scenarios, WiCapture performs at a similar accuracy, because these obstacles are really transparent for the WiFi waves. So to conclude I present a Y capture which enables subsentimental level position tracking using commemorative chips. And by virtue of using WiFi signals, the system provides resistance to occlusion and is sensitive to illumination levels. And by improving upon the state of the art WIFI positioning by two orders of magnitude, this system can potentially enable applications that were previously deemed impossible with WIFI, like position tracking for virtual reality. Thank you and I look forward for any questions and feedback.  [APPLAUSE]  Let me actually [INAUDIBLE] this. I'm sorry, clap.  [LAUGH]  [APPLAUSE]  So when you actually tested this trajectory,and so my question is more with accumulation stuff [INAUDIBLE], right? So the [INAUDIBLE] point like the [INAUDIBLE] point the [INAUDIBLE] that you're showing ,which is the point [INAUDIBLE] that error is trajectory. So you have 100 trajectories and then you take an average over one trajectory and then how are you doing that? How is-  For each point or-  For each point, that's what I want to  So this, and each of these trajectories are separate trajectories. You do it, and then you do it, and so on.  Okay.  Right? And this is the relative thing that you're measuring, because initial location is not so your essentially, but here's, so if you have this, let's say 0.25, or one millimeter error. You start adding that, in the sense that each, so I don't know how quickly you're sampling this. So let's say, how quickly are you doing that?  Six milliseconds, generally [INAUDIBLE].  Six milliseconds. So that means about 300 times, or 200 times every second. So you know how standard deviations Keeps adding up, right? Square root of whatever. So the variance of your thing, if there's variance around a half of a millimeter, then over 200, you've already increased that by square root of 200, and that it keeps accumulating, which means over ten seconds, you're already crossing. So the problem that I'm seeing is, that there is no other localization scheme that can correct you when you reach that one meter mark. Because this gap is huge, so how do you correct this periodically? I mean, do you rely on optical or what do you do, because-  The way to correct is by using the WiFi access points' location. Since the WiFi access points are stationary, you can localize it periodically. Actually, you can combine both distances.  But that is nothing close to your ability [INAUDIBLE], right? It's much higher. So you've to wait until the point that your accumulates. So that is the kind of standard deviation, let's say 50 centimeters of 1 meter. So you wanted accuracy of whatever 1 millimeter or 2 millimeters, right? That's fine in a relative sense. Now it's accumulating, because now you have to use that one meter antenna to correct your millimeter level thing. So how do you actually, I'm talking with respect to the absolute location. So how do you actually manage your error accumulation level, without any other scheme to correct you?  So when I said that you can use the VIPER localization techniques, I'm not suggesting that you have like WiFi localization estimates in order to correct for these millimeter level errors. What I'm suggesting is this, the Wi-Fi access point is stationary, as you look at the phases. So a typical example would be like the phase would come back to, If you know the location of the Wi-Fi access points,  You mean the stationary effect?  Yeah, the stationary WiFi access points.  To maintain level of accuracy?  Yes.  Okay.  But that is a alternative problem.  It's a alternative problem?  Yeah, it's an alternative problem.  Let's accept that, we know them most likely.  Yeah.  Okay. So now you can know the phase difference between a known position of the wi-fi access point and your own wi-fi chip.  Isn't that what you're using in [INAUDIBLE]  For the time of flight, I'm using it. Since I described the phase change to measure the time of flight difference, that is why there is this doubt whether this error accumulates or not.  I mean, you are doing a little deliberation, so it's really cumulative.  Yeah. But given that, the phase information between a known Wi-Fi access point's location, if you know the difference between, you know the phase between the Wi-Fi access point and the oculus headset with the same accuracy.  Earlier [INAUDIBLE]  Except for the phase wrapping, you know the distance between the access point and your target Wi-Fi chip to the same accuracy.  Mm-hm.  Their millimeter limit, right?  That is very phase wrapping, so that's the only reason, that's why you're doing relative, and then you're saying-  That phase wrapping is on the order of few centimeters.  Yeah, lambda.  You can actually correct for that by knowing your distances to multiple access points.  Within Lambda accuracy again. It has some standard deviation.  Yes.  Okay, we can take this off, then it's fine. But do you see the problem that I'm asking, right? Which is that even for example in cellphones And then you have to correct it in some way. And they use the absolute of compass to get it right. So you need some other mechanism to make sure that you accumulate and going down and I was just wondering what the other because Wi-Fi-  [CROSSTALK] Okay let me just conclude this. So what I am saying is Wi-Fi itself is a correcting mechanism because not only the phase difference but the actual phase between a stationary WiFi access point and your WiFi chip can be measured with the same accuracy. The WiFi itself is a correcting mechanism, okay.  Okay, I think we'll discuss later.  What's the minimum number of Wi-Fi access points that you need for this to work?  One access point is necessary and sufficient to get these particular numbers, we used four access points. The actual numbers with a single access point, we'll be doing in an analysis, and making it public.  So we understand that. So with one access point, you're just getting time of arrival difference, which is just [INAUDIBLE].  Yes,-  [INAUDIBLE] the vector. So you're just getting a magnitude.  Because you are getting multiple parts.  So using multiple paths. So you get the direction from music, or something?  Yeah.  So you get the direction from music.  There is a single access point, but there are multiple paths.  So you know the location of the access point, and you know the direction using music.  Mm-hm.  And you know the change in. So am I understanding this right? So you know the location of the access point. You know the direction of the music and then you know the displacement. So you're using the music direction and the [INAUDIBLE] distance to find your vector, right? Is that what you're doing?  Sure, yes.  How many antennas that you need on the receiver side for this to work?  Two are necessary and sufficient. Two antennas. We use three antennas, because the commucating Wi-FI [INAUDIBLE] three antennas anyway.  So this work by he was also using music to find the error and he was using eight or six.  Yeah.  Why did he end up, did you do something different in the music algorithm itself, or something which, why did he have to use six or eight to get these results? To start with that is a traditional positioning work that can only achieve like a meter-level localizational error.  No, no, no, he was using estimating direction using music, right. So the question is, why did he need six to eight antennas to get his direction accurately using music?  Yeah, so that just builds upon my previous work. So in order to obtain this direction accurately, the number of sensors you need to have should be greater than the number of paths in the environment, right? So typically we have five to eight different paths in an environment. So you need at least those many antennas. But in Wi-Fi, the signal is transmitted not only on different antennas, but also at different frequencies. So when you also include like 64 subcarriers, you are suddenly having so many more measurements, so many more sensors, but the number of parts still remain the same. So by using this additional channel state information that you obtain from different subcarriers or different frequencies in addition to the different you can actually work on documentation. But is out [INAUDIBLE], yeah.  So is the device connected to these out? Are all these Wi-Fi access points accept for the same network and that's what the device is connected to and all of that?  Actually, in order to obtain this channel state information, the device need not be connected to anything. Say, your mobile phone, it actually listens to every Wi-Fi transmission that is happening, okay? Calculates this channel state information, looks whether if the Wi-Fi transmission is the strength to it or not, and then drops the packet or accepts the packet. So to obtain the channel state information, you need not be connected to any access point at all, so yeah.  What comprise an environment when you can have like many points [INAUDIBLE] in the same waves in channel. I mean, would that decrease [INAUDIBLE] your approach? I mean, normally, many apartments, not like the carpark neighbors, will be like 10, 15 legspan or something. How would that affect their position in this case and [INAUDIBLE]?  Yeah, so the thing I want to mention is whenever a Wi-Fi transmission happens, the rest of the Wi-Fi chips, they just shut up.  Okay.  They listen to an ongoing Wi-Fi transmission and they just turn off their transmitter.  Well, with that [INAUDIBLE] late because they had to wait longer for transmission?  Yes, so, yeah, they have to wait, okay? But we are talking about like selecting devices, trying to operate in a single Wi-Fi environment. The maximum Wi-Fi state transmission length is two milliseconds, right? The typical Wi-Fi transmissions are much, much smaller than a full packet size. So what is going to happen is you will have enough time to actually round robin. And ten milliseconds is actually, current position tracking headsets, the is 90 hertz. So even if you get one chance to transmit every ten milliseconds, still, you are good.  Right, and see, last question. Did you actually try to build their application using the system and get a feeling of how it feels to use, be like this tracking as opposed to using the built-in oculus or via tracking?  We built in application, but not when we were writing this paper. Yeah, these results were before, yeah. Then, these were the kind of project.  So basically, you're saying to use Wi-Fi instead of IR?  Yes, IR or laser.  And so this begs the question of, what was the fundamental reason that these companies decided to use IR? And secondly, what do you lose? Is it higher cost? I mean, is it all flowers and roses, or is there anything that you lose?  Yeah, people could not use it because 'til this was-  We're gonna ask Mike.  'Til this one, it's like a middle level error. Middle level error is useless, actually. So this actually changed the game by actually improving it by 100x increments, so-  So are these companies jumping up and down-  Yeah.  To use this?  [LAUGH] Hopefully.  So it is my question, can I make like a comment? So I believe that you seen IR with UV like a more predictable error? I exchange used of the error, you wouldn't change depending on obstacles or. So I guess, maybe, could that the reason that make the error and the position, and the latency is more predictable with IR than with Wi-Fi?  The Wi-Fi, I could say, definitely depends upon the kind of environment you are in. But the functioning of the system itself is independent of the environment. So yeah, the errors might change, but you also have an, well, what I did not mention in this talk is you're also given estimates of the error.  Okay.  So, that's, again, something that can be useful when you're talking about like unpredictable error.  How about like mobile hotspots, is there any way of determining if there are mobile hotspots in the environment and filtering those signals out?  What is a mobile-  Like if you turn your phone onto a hotspot. So it becomes an access point.  Okay.  Wouldn't that interfere with the signal stationary.  Yeah, so the system assumes that the access point is stationary. So if the access point is moving because it is support, then yeah, right now I think that it has a issue.  Right.  Maybe there is a trick to [INAUDIBLE]. I can [INAUDIBLE].  So do you limit it to the access points you know [INAUDIBLE], like do you configure it only for the access points that you set up? Or like, do you [INAUDIBLE] you've probably got like a 100, 115 WiFi signals. Would that work with that many?  Yes, it will. Because, as I said, a WiFi chip for every worker transmission, it has to collect at the CSI. And the information that I'm using is the CSI, and So. And each WiFi transmission is also encoded with this hardware address called MAC address. So you can always correlate this signal is obtained from this access point, this signal is obtained from this access -.  So assuming they're all stationary?  Yes, stationarity is important.  And in these experiments, did you filter it to those four MAC addresses?  Yes, yes, yes.  One more so if I come [INAUDIBLE] the initial configuration, you need to know the exact location of the access point to a position of millimeters [INAUDIBLE]. So how would you imagine like someone like buys the headset and Is to configure the router doing that simple way or way?  I did not give it completely complete thought to that.  But there are words like slam, which kind of like simultaneously try to localize and also track your. Like [INAUDIBLE] in slam, you are trying to figure out the environment but also trying to track yourself. Like a visual slam technique. Here the environment is like the position of the [INAUDIBLE] access point. And you are still trying to track the WiFi chip, so maybe we can borrow some techniques from  Yes, yes. In Slam, we are assuming the environment is stationary and you are moving in that, right? Same here, the movement is nothing but the the access point. [INAUDIBLE]  Yeah. Do you if the API is available to read the state on-  Yes it is available in the paper.  [INAUDIBLE] available on Windows.  I do not think so.  [INAUDIBLE] some systems chipsets do and some chipsets don't so, now I think more and more chips are now opening up in case, and also it's not just reading these sites it's also about what kind is the length you read and differences in chips.  So just to add on to that, every, all the major devices they have like a APA tool. Say Atheros, Intel, Qualcomm, you just query this channel's straight information. But not all the devices have a publicly available API. The reason I used Intel 5300 cards is it has a publicly available API, so that I can actually share the system implementation with others. The rest of the chips, for example, Atheros, if you sign an NDA with them, they are going to allow you access to that tool, that queries the CSI.  Okay, so then what operating system did you use?  I used Linux, Ubuntu.  Linux, okay, cool.  Yeah, but whether Windows can use it or not. Maybe they have some internal APIs that they are not Thank you.  [APPLAUSE] 