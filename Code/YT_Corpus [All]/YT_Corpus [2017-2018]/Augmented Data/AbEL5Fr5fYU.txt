 OK, why don't we get started. So good morning everyone. I'm Alex Bui director of the BD2K center's coordination center. So today we're continuing our computing overview, with our third talk in this section. I have the pleasure of introducing Dr. Trey Ideker from UC San Diego. He is a professor of genetics and bioengineering in the Department of Medicine, and is a director of the well-known National Resource for Network Biology and the San Diego Center for Systems Biology. He is a fellow at the American Association for the Advancement of Science, as well as of the American Institute for Medical and Biological Engineering. Some of you may already actually be familiar with Dr. Ideker's work. He is the founder of Cytoscape an opensource bioinformatic software platform. That some of you may have been used before for visualizing molecular [? interaction ?] networks. So today Dr. Ideker is going to share some of his experiences and insights into the practical challenges of running a research lab at the intersections of data science and biomedicine, something that several of you will likely face in the future. So Dr. Ideker, thank you for joining us today, and the floor is yours. OK, very good. Thanks very much for that introduction. What I'm going to talk about today is really four points that I consider to be key considerations when you build a data sciences laboratory. I'll talk a bit about my own work through vignettes to illustrate these four points. These, of course, aren't the only four points that one might want to consider as one builds out their own data sciences laboratory or improves an existing data sciences laboratory. But they're certainly four of the more important points in my experience. The first thing I'll talk about is lab mission, and this is first because it underlies it underscores everything that you do in the lab. Why do you get up in the morning? Is it technology, biology, medicine, or something else? And clearly articulating the vision as specifically as possible. But with as much vision as possible. I'll spend most of my time in the next hour, or in slightly less, talking about getting data. It turns out the more you think about places and sources of data, whether they be external or internal to the lab, through collaborations, or one on ones, it's a very key issue in the life of a scientist. So most of my talk actually ends up really focusing on one sources of data. Near the end, I'd like to touch on computing infrastructure. So of course, one needs to think as a data scientist about what is the hardware that one needs to have in one's environment. And then finally, I will say a few words about tools development process. Process ends up being more important than one might first realize in terms of one's research flowing into development and ultimately tool dissemination. So that'll come at the very end of the talk. So without further ado, let me take the first issue on. Lab mission. And the way I think about this is really why one as a data scientist would want to go to work in the morning, and what keeps you getting up in the morning and going to work every day. Of course all of us have our reasons, but there really are three important reasons that I can think of in terms of driving your research. One is biological discovery. So is the most important thing in your life questions or statements such as, I want to reveal how biology or life is structured, of functions, and evolves. Now as a data scientist, let me make it clear that you are going to be using the tool box, among other tools, of data science. A chemist could ask this question, a geneticist could ask this question and they would be using a different toolbox. And of course one's free to use multiple tools from those different boxes. But the point is it's very possible to be a data scientist, yet your first allegiance is to biological discovery, which might mean you largely use state of the art tools that already exist to answer those questions. The next question would be medicine. So independent of how life actually works, I want to help people get better. I want to cure cancer would be, if I had to stage my lab's goal, from the medical point of view. But again, you want to cure cancer using a data scientist toolbox and increasingly I think we're seeing that the ability to cure diseases and to push for biomedical research into those diseases is dependent on data science. And finally is listed here is probably what's the most common answer to this question of among data science individuals or labs which is, what gets you up in the morning is the urge to develop new technology or methodology. So you have, and this is commonly and unfortunately in a derogatory way, sort of referred to as a hammer in search of a nail. But of course hammers can easily find the correct nail lots of the time. But the point is, you've built a hammer and you have a data science analysis, you have an algorithm, a software or hardware device, etc. that you believe is going to revolutionize, largely, the previous two disciplines, either biological discovery, and or medicine. So now, like I'll do throughout this talk, I'll try to make some general points and then spend a few slides illustrating how I answer those questions in my own life and lab. So I am driven to get up in the morning because I have the as you'll see not an unambitious goal, I'd like to build a complete model of cell structure and function and fully automate that process, much like one can now build a full model of a genome through automated, or at least semi automated genome sequencing pipelines. And so just to give you a couple of motivations and arguments for why I think that is an important motivation in my life, I'm driven by this idea lately that networks, like people are generating on the left here, of either physical protein, protein interaction, or gene expression, or so-called genetic interaction. There's lots of ways that scientists are linking together molecules like genes and proteins and cells. The problem is with this common view is that cells don't look like that. Cells look much more like the picture you see on the right, and it turns out that both the picture on the right and the left are the same object, or representations of the same object. And in cells, this is the proteasome. The proteasome, as you may know, is the vacuum cleaner of the cell. We're looking at the proteasome on its side. So here is the core in the middle that eats the proteins, and here is the regulatory particle on either side, and you can see of course, the several [? faults ?] of symmetry there. It's very hard to see that structure on the left. Although if one starts to peer at it, one sees a higher density of interactions among the core proteins, which here are the same colors, in red and yellow. And one sees a higher density of protein and other interactions on the right side, among the regulatory sub units. But the question that, really, we've been addressing now towards a ultimately model of the structure and function of the cell, which starts to look much more like the picture on the right, is as follows. So we're trying to come up with algorithms and tools from data sciences point of view that will take all of these network data, that I just showed you here and here is in panel A. And first computing master gene-gene similarity network, based on all of those raw interaction data, but don't stop there, from gene-gene similarity network ask the question, how far can we push that towards what starts to look like models of the cell? And as far as we've gotten so far, we haven't, I should admit, gone all the way toward structure. That's a very, very hard problem. But we have been able to push the towards hierarchical models of the components of cells. So you saw in the proteasomal case, there were already several orders of hierarchy the proteasome contained a core and a regulatory particle. Each of those contains sub units, each of those sub units contain proteins. So we've at least gotten as far as being able to flesh out that type of structure, largely directly from these kinds of interaction data. That hierarchy begins to look a lot like what we've already heard about in this series an ontology. So we directly align those ontologies with the gene ontology to see what is the similarity with what's been curated from the literature. Of course this hierarchy I'm talking about now was entirely data driven. So just quickly, the way the way that works is, it's a clustering algorithm with a couple of key twists. Rather than build a binary tree, like most clustering algorithms, or many clustering algorithms do, you want to build what's called a directed [INAUDIBLE] graph, which is, not coincidentally, the structure of a gene ontology or other ontology. Which means that internal clusters, first of all, can have more than two children. So here's an internal cluster that has three children. Why is that a good idea? Because if you have a protein complex, it can certainly have more than two sub units. It can have arbitrary numbers, and we also want to allow for [? cliatropy, ?] the idea that an internal cluster can participate in two or more parent clusters. So it's a varying of clustering with that twist, and afterwards you align it to a literature curated ontology. of clustering with that twist, and afterwards you align it to a literature curated ontology. And then just to give you an example of how this works, this process was just recently applied by my very capable grad student, [? Mike ?] Kramer, to autophagy. And so here he's generated, or I should say, analyzed, and we'll get to this in a second, in the public domain, a bunch of network data, like protein-protein interactions, coexpression, and other types of networks. Among the autophagy genes. Autophagy genes typically start with ATG for autophagy. Now you'll suspend your disbelief here because we're not showing you the raw data but he's already processed these into an overall similarity score between pairs of genes. So you can see in the interaction data, genes like ATG 27 and 10 are more similar, genes like 10, 12 and 7 are more similar. Ah and look, ATG 10 participates in both of those clusters. So already you need a clustering algorithm that can correctly create both clusters for one component. And oh look, there's a much larger cluster here which encompasses all of these genes ATG 10 all the way to ATG 8. So in the hierarchy, at right, you can start to see that structure in what we hope is a much more clear and interpretable way than looking at either the raw interact omics data or these heat maps. So you can see here that ATG 10 is placed into two higher order components defined exactly by these data, and now we have a larger component here, which contains all of these ATG genes. The names that you see on, at least, these blue terms come from alignment to the gene ontology. In the case of these red, internal subsystems, or terms, those were not in the gene ontology but it turns out, have literature supporting them. And so we were able to name those orphaned terms and submit those names to the gene ontology and those are now in that resource. And then finally when you see a number here like 183, this is a truly orphaned component. We have evidence that it's a true component based on the data, yet there is no evidence and [? go ?] for that particular grouping, and there's no evidence in current literature for that particular and beyond [? go. ?] You can, of course, zoom out and look not just at autophagy but the entire cells worth of genes, and so this tree here starts with all genes underneath the root, again, through alignment to [? go. ?] This is labeled a scalar component, branch of [? go, ?] because that's what we aligned against. And then you see quickly you partition into a mitochondrial part, a membrane part, and intracellular part, especially the intracellular part, you can name because [? go ?] names lots of these internal structures. But there's lots that's unnamed, and those are exactly the new biological discoveries. So to get back to the example of the proteasome, here is on right here is the old picture that you would view that in a network browser, but now if you look at it from the hierarchical or ontological point of view here, again based on the exact same data you start to see the structure of the proteasome, at least in terms of the hierarchy, where you have a proteasome complex, which splits into a core and a regulatory particle, which split into subunits, and proteins, and so forth. So that's an example that I'll carry forward throughout this talk. But let's get back to how that relates to the lab mission. So that definitely is a new technology rather than a biological discovery or a medical discovery. However, of course, once you are driven by one of these goals-- which I'm clearly driven by the third one here-then you have ample opportunities, if you play your cards correctly, to imagine at least how those would then bear on biological discovering and medicine. And we'll see examples of that in a little bit. But first, let me now talk about the second and what ends up being the majority of this talk, which is the very exciting and sometimes thorny issue of where you get data as a data scientist. Like any topic, one can try to classify it into different parts. So here what I've done is try to spread out different ways of getting data from-- or across a continuum, where you have easy entry but less novelty and control all the way where you have a higher bar of entry but complete control and hopefully more more novelty. Although this, of course, is a broad generalization. And there's lots of novelty certainly to be found all throughout the spectrum. So the easiest bar to entry, as a starting data scientist, is of course public data sets. It's surprising the number of data science labs that have a perfectly good career with only looking at public data sets, without doing any of these other four activities that I'm about to talk to you. Now, if one does not like to operate in a vacuum-- and increasingly in our connected society we are encouraged and have ample opportunity to work together-- one can think about accessing data through different kinds of collaborations. There's really many ways to think about this, but I think about it in terms of two types-- many-tomany collaborations which are sometimes called consortia and one-on-one more focused collaborations between you and a coworker who can be either at your same institution or at a different institution. And then finally, pushing down this pipeline or this continuum, the last category would be to bring those experiments into your own lab. And the way to do that is if you yourself do not feel yet qualified, if you have not been trained in biochemistry or genetics or some other molecular biological laboratory discipline, then I've seen lots of cases where that lab nonetheless is able to run a very-- well, to start and then run a very successful experimental program. And the way to do it is with a host lab. So this requires a lot of trust with the host partner here. But if you have, say, a neighbor with whom you have a very good working relationship, then you can often come up with an agreement where you'll pay for an experimentalist in their lab or you've already hired a postdoc or a technician into your lab and they'll mentor that person within their own lab at least some of the time. The final position on this continuum is to go even further and to take on all that experimental laboratory activity yourself. And again, there is a lot of laboratories, including my own, that took this plunge at the beginning of their careers. And I certainly wouldn't go back at this point. And I'll talk a bit about that. But all of these modes I'd like to say are successful ways of running a data sciences lab. You just have to choose which kind of lab you want to be. So in terms of public data, because a lot of what I'm focused on is building models of cells with ultimate application to understanding and curing cancer. And again, every time I say that, I have to smile, and I'm sure some of you are as well. It's a lofty goal here. So-- but-- but-- but the point is you want to keep getting out of bed for 30 years, or maybe your entire career. You don't want to just have a goal that's going to get you out of bed this week. So in the case of cancer, there is-- in many ways, cancer is the flag ship of widely available public data. And the jewel in the eye of that flagship is the cancer genome atlas, or TCGA. Where at this point in time, there there is far in excess of 10,000 exomes, I should say here as opposed to genomes, for cancer available. This just shows one such genome that was an early genome generated by the cancer genome atlas. This is a so-called Circos plot here, for a single patient who had been sequenced, and also subjected to a variety of other modes of data collection and analysis. But here they're really focusing on the genome. So you can see here around the circle, you have the 22 chromosomes plus the x and the y. And then in successively smaller and smaller tracks around this circle, you can see different types of analysis they've performed to extract different kinds of information about the raw genomic sequence. So in terms of single nucleotide substitutions, you're looking at that here in this yellow track, just counting the number you see as you walk along the chromosomes from one to the other. You can then get into the different kinds of substitutions you see there and other kinds of changes. You can then get at the copy number variance or long segments of DNA that are amplified or deleted. So here you can look at amplifications, deletions, and so on. And so then if you take a step back and look at the collection of all data that's available through a resource like the cancer genome atlas, they've really made it very, very accessible. So a few clicks away, if you simply Google TCGA cancer, you can get to specific tissues where all of the patient data that have been collected for that specific cancer tissue can be accessed. So if one is interested in head and neck squamous cell carcinoma, here's the data page that you find. So there's a total of 528 cases currently in the TCGA, of which just shy of 500 have been sequenced, at least for the exome. But then I mentioned, they've collected a whole variety of layers of molecular and then finally clinical information, which are mostly survival. Occasionally you get direct response information in the clinical column here. But it's a lot of data and at least the first couple of tiers of those data are pretty easily accessible. If one wants increasingly raw data, one has to obtain the proper access but hundreds, if not thousands, of labs have taken that step, including my own. So it's not complicated to do. Now my lab being a network lab, we also are very interested in public domain data sets describing molecular networks, of which-- just like cancer genomes-- there are many. Here I just list general categories without mentioning the individual databases. I'll talk about that maybe a little bit later. But you have metabolic networks that are essentially akin to the whole Boehringer Mannheim wall charts people used to have in the labs-- on the lab wall. Transcriptional networks which have been measured increasingly systematically with techniques like chIP seq. Genetic and protein interaction networks-- protein interaction networks are being measured with technologies like [? authenity ?] [? purification ?] [INAUDIBLE] or the [INAUDIBLE] hybrid system and a variety of other related experimental technologies. And again, many labs are generating these data and depositing them in the public domain. If all else fails, one can always take the prolific MNRA and protein expression data that are out there in databases like the Gene Expression Omnibus, or GL. And from those profiles of MRNA levels for each gene, compute genegene similarities which can be interpreted and used as networks. We started putting, in my own lab, a lot of these data but more specifically, not just the raw interaction data, but some of the subnetworks in the cell models derived from those data into this database called the Network Data Exchange, or NDEx. This now contains-- here we show just a variety of sources of networks this thing contains. We certainly have a focus on cancer networks like the NCI pathways interaction database we're the host of here. Because we were funded by the NCI and our core interest is cancer. But there is a variety of diseases and pathways related to those here in the database. Now when you have various kinds of data in the public domain, I just want to give you another vignette of ways that our lab in a purely public domain, data-driven way has been able to put together these different data sets for productive line of research. And so, what we thought we could do is use these protein networks as a scaffold of potential pathways in which or on which to integrate heterogeneous patient cancer mutation profiles and heterogeneous patient MRNA expression levels. Today I'll focus mainly on the patient cancer mutation profiles, with the idea here, can you use the networks to group patients that may have different genes mutated into tumors, but nonetheless are mutating the same networks and pathways as a way of taking what have been called in [INAUDIBLE] patients-- those that can't be grouped with any others for diagnosis or therapy-- and getting extra power and insight for grouping them at the pathway level. The algorithm we used here to do this integration is a algorithm that's been very powerful in network science as a whole. It's called network propagation. It also goes by the name of network smoothing or sometimes diffusion, sometimes random walk models are invoked. All of these names are essentially synonymous for the same underlying method. And I won't go into the mathematics here, but simply take you through a high level intuition of how this approach works. I should say these-- I'm citing two papers here. One is [INAUDIBLE] lab from Tel Aviv, [INAUDIBLE]. And the other is from Ben Raphael's lab, who used this technique to develop a very successful approach called HotNet. And then I'll talk about in the next slides, some of our activities. But the idea here of network propagation is that if you have a patient mutation data set, I'm calling the patient genome type 1 in yellow or patient genotype 2 in blue, the data set will indicate some mutations in genes for that patient. So here are the yellow mutations in the yellow patient, and the blue mutations in the blue patient. These data are extremely heterogeneous, motivating the need for this approach. But occasionally, you do find patients will mutate the same commonly mutated cancer gene. Maybe this green gene here that's mutating in both patients is a very common player in cancer like p53. But the question is, can you find more commonality than that at the network level. And so the way propagation works is it pretends that each mutation is a source of heat. And that the protein interactions and other molecular interactions you have in this network can conduct that heat to their network neighbors, and their neighbors' neighbors and so on, to convergence. And convergence depends on one parameter. It depends on how much-- it's essentially the ratio of heat you inject to the amount of heat at every unit time being diffused into the atmosphere. So you can think of that heat evaporating or diffusing upwards from this network into the atmosphere. And so, at steady state, you get some spreading from the heat sources with those being the hottest, and their neighbors being the hottest. And then the temperature falling off around that, depending on the setting [? of its ?] parameter. And so having diffused those two data sets, you now see that there is a large green region over here, which represents hot proteins in both patients. That is to say, what we've done is we've rescored a patient profile from a few 1's in a sea of 0's for the genes that aren't mutated to a quantitative score for every gene indicating not whether it has mutated in a patient, but its network proximity to mutations. So if you have an upstream factor, it feels the heat. If you have a downstream factor, it feels the heat and some kind of signaling pathway. If you have a member of the same protein complex, then it feels the heat as the other member that was mutated. And that's how you can begin to get the striking now similarities between patients. Again Ben Raphael, using a HotNet algorithm has been very successful at finding cancer pathways using this approach. And in our hands, we pivoted the question and tried to stratify patients using this approach. And so here is a network that stratifies and identifies a particularly aggressive subtype of ovarian cancer. The idea here is that while hardly any two patients in this subtype mutate the same gene, all of the patients in this subtype mutated gene somewhere on this slide in this subnetwork. And that's how the network knowledge helps. Here I'm showing you evidence that these subtypes are significant at least clinically or with respect to survival this blue subtype is that aggressive network I just showed you. So if you're mutated in that, then you are in the blue subtype with poor survival. That poor survival replicates in future cohorts that have come out after our initial study. This is a second large public repository of cancer data, by the way, the International Cancer Genomics Consortium. This is an Australian cohort of ovarian-- or of women with ovarian cancer. And here we're essentially not re-formulating this diagnostic, we're just using it to show that if you mutate that same network, you also get poor survival outcomes. OK. So that's one vignette from my group, where it's really-- we're actually now seeing two vignettes, I should say, where really we're focused entirely on public data sets. And if you can argue TCGA was actually the result of a large collaboration. Although what's interesting is my group is not-- or at least was not at the beginning-- part of TCGA. We became involved near the end, only after the success with this method on the public data they made available. If you're interested in joining a consortium from the get go, with respect to cancer even, there are-- here I've made a short list of four different consortia that either are just completing or just getting started. So the Cancer Genome Atlas itself has officially ended. But that is already giving rise to what's been called the pan-cancer analysis of whole genomes. And that's well underway, transitioning from exome analysis to pan-cancer or full genome analysis of some of the same cancer tumors. There's other satellite projects, such as the Pre-cancer Genome Atlas, the Cancer Immunity Atlas. I'll talk in just a slide about the Cancer Cell Map Initiative that I've been involved in formation of. There's a Cancer Cell Atlas, which seeks to define all cell types at the single-cell RNA sequencing level and cancer. This project is just getting underway and it's also a consortium, and so on and so forth. Cell Explorer, for instance, involves imaging and advanced image recognition capability and building databases of tumor cell images. So again, this is just cancer. There is a short list in any disease-focused area now of large consortia to become involved in if you so choose. And depending on this stage of your career, I think it's either advisable or inadvisable. I think the clear pro of a consortium like one of these is that there is enormous energy in the critical mass of folks, and also funding that can be had in a critical mass of folks that come together around a common cause. The con is that one can become lost in all of that. And so if I had any advice to someone starting their data sciences laboratory, it would be certainly be involved and don't make it the only thing that you do. I think in this day and age it's still worth having your independent research laboratory activity and pursuing these consortiums or these consortia as a secondary activity. But the pros are really quite strong, I must admit. And so here's-- here's just the one we've recently formed around networks with the goal to-- having now sequenced many, many different cancer genomes, can we now understand the higher order proteins network structures that help us explain some of those mutations, and marshalling people together to try to systematically define those protein-protein in genetic interaction and transcriptional interaction networks. So now when we talk about one-on-one collaborations, one of the nice things about having started with public data and showing proof of concept of a particular approach with a tool that might come out of your lab, is now people might get interested. And of course, they can get interested by reading your papers. They can also get interested by you reaching out to them, depending on whether they're at your same institution or they're at a different institution. That's either easier or harder. But certainly, conferences are a great place to form those connections even across institutions. This is a collaboration that I became involved with after that network stratification paper we published in 2013. One of my colleagues here at UC San Diego is Joe Gleason. He was not studying cancer. He studies rare childhood diseases. In this particular case, he was studying a disease called Hereditary Spastic Paraplegia or HSP. Joe tells me that HSP is what they call a complex Mendelian disorder, which means that in any one family or child, it's one gene typically that causes that disease. But from pedigree to pedigree of unrelated families, then it's typically found that it's different genes. And that's the complexity. So here, we were able to work together using essentially-- not the exact same but what's essentially a very similar approach to the one I just described with network propagation here-- to find a number of disease genes for this HSP disease based on their proximity through networks to known disease traits. And so what you're seeing here are the seed or known proteins from before this study. And red are the candidate genes where we have power for some of these genes to find them only because of the network proximity that you see here. If you just looked at the marginal association of mutations or variants in these genes with the phenotype, without network knowledge knitting genes together, you would not [? be ?] [? power ?] to find those genes. So that's an example of playing between collaboration and one's own work with public data set activity. But now let me talk about the last two categories here, which are transitioning to get these kinds of experiments going in your own lab. And for this point, I want to invoke my hero Sir [? Ari ?] [? Fisher ?] who has two very relevant quotes. I don't actually know if [? Ari ?] [? Fisher ?] had experiments going in his lab. I think he did a lot of collaboration. But he had a very famous quote, and then a not-so-well-known quote that's relevant. The first point is that if you hold your experiments close, you control the experimental design and validation. And so the famous quote is to consult the statistician after the experiment is finished is often literally to ask him to conduct a post-mortem examination. He can perhaps say what the experiment died of. And so therein lies the problem that I think data scientists experience all the time. This one-on-one collaborator comes to you and it's essentially all the work in lab has been done. And you can immediately see five things you would have suggested they change about the experiment. And of course, if you had started at the beginning of the projects, you might have learned of course as much as they would have about the particular biological problem at hand, causing you to change your analysis or your plans for analysis. So the closer you can be involved at the start of an experiment, the better for all of us concerned. There is of course many ways of doing that. But one of them is by working closely with experimentalists either in a host lab or in your own lab. The other point I want to make here ends up being actually, in my career, even more important. And it was not something that I saw immediately, but saw over the first three or four years that I became involved in experiments alongside data science. And that's that as you begin to get your own hands wet or dirty or whatever the right analogy is here, this miraculous thing happens, and you become in touch with biology like no other method. You can certainly become very educated about biological systems as an armchair reader of those processes, either with text books or literature or hearing about them. But somehow, when you're at the bench, you gain a completely different level. At least in my own life it was transformative. And the real power that you then have is the ability to see these key biological computational coupled problems that it's harder to see if you're on just one side of that fence or the other. And it turns out [? Fisher ?] has an excellent quote on this as well. Sanity and realism can be restored to the teaching of mathematical statistics most directly by trusting it to men and women who have had personal experience of research in the natural sciences. And so here, of course, he may be talking about different ways of getting that experience, but certainly a laboratory is a wonderful way of really getting experience beyond data sciences, which he's referring to as mathematical statistics here. Really get involved in the biological discipline itself. Considering this quote is circa 1930, it's even more impressive. And then really, the third practical issue here is once you have data in your own hands, you're much more assured that you're not duplicating the efforts of someone else. Of course, it's certainly always feasible that someone else has done the exact same experiment as you are. Certainly molecular biologists, geneticists, and biochemists also get scooped. But it's a higher bar to be scooped certainly. And so, if you look at my lab, we have the pictures you might expect, like this is [? Eric ?] and [? Kay ?] in my group who are Cytoscape team developers, looking at a network in Cytoscape. But then you have, moving from the office to the lab here we're generating-- this is [? Brian ?] in lab generating network data and [? Anna ?] analyzing some of the network data, for instance. And so that's been quite a fun and frustrating and productive exercise. And to show how it has been used in our research plan or program here, let me go back to this example of constructing hierarchical models. So I already introduced this idea that we are trying to integrate network data of various kinds to together to build ultimately these hierarchies or data driven gene ontologies of a particular biological process and ultimately the whole cell as a type of whole cell model. Now, once one has some control over one's experiments, one can shift from a mode in which one is analyzing really public interactomics data-- that's A, B, and C-- to now asking what new data set could I generate that would most improve this hierarchical model. That is to say, the fidelity with which this hierarchical model represents real biology, actual biological systems. And to look at that, we of course don't know biological truth, but we do have the very good Gene Ontology resource that's been created from the literature over a long period of time. So if we use GO as at least a bronze standard here-- it's not a pure gold standard, it's not perfect. But if we treat GO as some sort of bronze standard and we ask, how will we recapitulate GO in our data-driven hierarchical model, that's what's been shown here on the y-axis, just as a fraction of our maximal ability. What I should add here is out of the box, this approach gets about 60% recovery of cellular component terms and their interrelationships that are in the Gene Oncology, at least for the budding yeast [INAUDIBLE]. For humans, it's a little bit lower than that. So you should be thinking perfect for us, or the best performance we can do with about 60% recovery of GO. And that's normalized here to one. And now what I'm going to do is remove studies one-by-one from the public domain, and various kinds-- genetic interactions, protein-protein interactions, and co-expression interactions. And after I remove a particular PubMed ID-- and that's what literally these are, is PubMed IDs-- I'm going to reevaluate my ability, having removed that study, at recovering GO in my hierarchy. And so what you can see here for at least these three kinds of data is that if I remove genetic interactions or protein-protein interactions, then I do start to hurt my ability to recapitulate Go. Whereas interestingly, if I remove co-expression studies all the way up into all 50 budding yeast co-expression studies that were in the database we accessed. Given that you still have all of the other data around in the model. You don't hurt your performance. And so this allowed us to choose, in this case, genetic interactions as the strongest negative slope here. That is to say, the interactions which at least at the present time appear to be the most important for driving this model. And so now, because we had a research lab that over a few years, I geared towards interaction mapping-- and I should say, our lab is certainly not the highest throughput interaction mapping labs out there. There are many labs that surpass our capacity of our jack of all trades lab, if you like. But nonetheless, we have some ability to generate interaction maps. And so here we took 50-- this is back from the autophagy model. And we took autophagy genes and paired them with about 3,000 yeast genes to look for genetic interactions in a matrix-like format. So all pairs of 50 by 3,000 are tested. And we test for interaction across three conditions. And then you can also look differentially between-- at interaction changes between those three autophagyrelevant conditions. And what you find is, here's another way again of looking at the similarity with Go. I apologize. We've changed here the metric. But nonetheless, here is how well you do at recovering GO with all prior data in the public domain. If you look at just this screen alone, given you've targeted one particular process in your lab with the right gene sets and the right conditions, it turns out that even not considering prior data, you can do better than all of those public domain data. And again, it's just because you've tailored the data in terms of conditions and genes. But of course, if you add both the new data and the prior data together, the recovery of the GO hierarchy gets even better. And over here are some controls showing that if you randomize things, then you [? erode ?] your performance as expected. So that's just the latest vignette, really, of how we've put together a public data set analysis leading to a collaboration with an autophagy lab with then let us generate our own network data, and come full circle on that systems biology cycle I showed. Now, I don't want to build this for everyone here. And I should say, there's many frustrations with getting involved in these kinds of experiments yourself as a data scientist. It is a massive investment of your time that could otherwise be going to data science research directly. Compounding this fact is that debugging lab work is much slower than debugging other kinds of computation. And I think several of us have appreciated that over the years. You can press Enter and test whether your program works in a split second, typically. But pressing Enter in lab can take days before you get a result. So two, it costs more. You have to not only maintain your space and personnel for the data sciences part of your lab, or the data analysis side of your lab, you have to come up with those resources of course for experimental side of the lab. And that is a challenge. You have to expand your focus, which means that you're distracted. You have more to worry about, and that may make you a less successful scientist at any particular task. This is what people call the jack of all trades. And so another way of saying this-- or another implication of this is that specialized biologist collaborators will not always be better than you, but have a distinctive advantage over you. You will never be as high as they are in knowledge of a particular biological process or the particular tool box of experiments that one uses to interrogate that process. And perhaps most frustrating of all, while you spend the time diversifying your computer science background to learn experimental biology, you're going to find that your colleagues who are staying strictly in computer science domains are going to now, over time, be better at you at computer science. And that's also frustrating. But you can collaborate both ways. I find at this point in my career even though I started with a degree in computer science and ended with a PhD in molecular biology, I now am in a position where I collaborate freely with both sides of that fence. And I've got to. OK. So that was the bulk of what I wanted to talk about today. I mentioned that getting data was by far the most weighty topic in my mind. But let me just say, in the last five minutes or so, a few words about both computing infrastructure and process. So I think about of course all of data science activities require computing infrastructure. And so how should we think about that? I think about it in terms of at least five tiers. And these are not meant to be mutually exclusive. So there's the connectivity and the interface, which sounds trivial, but it requires some thought in design. You're going to have to separate if you have a wet lab and if you separate that out from the dry lab or that's necessarily separate. So you have to think about how you're going to maintain connectivity from a data point of view. I [? fear ?] and I think certainly the goal in the lab would be to provision both environments with high bandwidth connections, both wired and Wi-Fi. You have to think about how your personal-- or personnel, I should say here-are going to be equipped with their personal machines. So again, sounds trivial but it requires someone to think about those kinds of things which as your lab grows can be a surprisingly large amount of someone's time. Now beyond someone's local desktop, you have two major resources in the data sciences laboratory to think about. You have the raw CPU power or compute engines that you'd like to be able to access and you're going to find are a necessary part of working with big data. And of course, storage of those data are necessary-- is a necessary part of working with big data. Here we use both private servers and the cloud. The other point to make about the compute engines is we're seeing a trend just in the past year or two towards these GPUs or Graphics Processing Units as ready-made cards by video game-- made originally for video games and high performance graphics, for those kinds of applications. Like companies like Nvidia. They're essentially highly processed or highly parallel CPUs that one can avail oneself of if one has the right libraries. And it turns out that now Nvidia in particular supports the libraries for scientific computing on GPUs, and it's really been revolutionary to not just my lab but to a lot of labs I know in the data sciences. So if you're just getting started, really look seriously at access to GPUs. From your own lab, they're relatively inexpensive. Now storage-wise, we do have private storage and also cloud storage. And the private storage, there's a variety of reasons one wishes to have private storage. Not least of which, as we begin to get data sets for patience, one has to worry about HIPAA requirements dealing with patient privacy. And it's easier to worry about those if one has a mobile storage unit. But of course, we're also seeing HIPAA compliance in the cloud. So whether you go local or cloud, really starting to become more of a personal decision than one in which it's easy to advise one way or the other. I would say the other big consideration here is latency. One would like to have quick access to the data, which of course if the connection is to a machine right next to you, that latency tends to be much less. But that's also changing. Here is what this-- all this looks like, which I think it always helps to get some perspective. I'm not going to spend much time on this slide other than to say, we here have a lot of the local machines I talked about that compute and the storage here at the San Diego Supercomputing Center, which is just a half campus away, so about a quarter mile away from my lab, with fast connectivity to both the experimental and computational lab. This is the schematic of that space. And so over here, you see that this is the compute part of that cluster. And here is the storage part of that cluster. And here is representation of the local machines. And so of course, the physical location changes between the local machines and lab and the STSC machines and so on. OK, so the final thing I wanted to just touch on is software development process. As we begin to think about our data sciences projects, it helps greatly-- and this, I should say, took me 10 years and a key conversation with Dr. Tamara Munzner at UBC who urged me-- she runs a very successful lab at UBC in data sciences. And a few years ago, she urged me to think about my tool development across four stages, and whether she would accredit this to someone else maybe she would. But I wanted to give her her credit here. So first is to identify and motivate the problem or the driving biologic question, along with the target community that most stands to benefit from answering that biological question or successfully solving that problem. Stage two is develop a prototype, and not worry about yet how you're going to convince the world that your prototype is useful. You've already identified the need. Now in stage two, spend some time internal to the lab and get out that first paper. Once that's out, then you can focus in stage three on making a robust software tool or a website or database as the case may be. And that's not the initial point of that first paper in stage two. That comes later. And finally, once you have a [INAUDIBLE] that begins to take a life of its own, perhaps you even generates funding behind it, then you can worry about broad dissemination and steps you can take to broadly encourage the community and make your approach available and useful. So the first time we thought about this type of four-stage process was in planning our national resource for network biology, where Dr. Munzner is an advisor. She convinced us to take all of our projects and to make this kind of chart. And so on the-- I'm not going to go, of course, into any detail here, but just to show you kind of the big organizational view of all of this. So [? and-- ?] [? in ?] rows of this chart, you have that different technology research and development projects or TRDs, the different groups in this consortium that are signed up for those. But then importantly, across the columns here, you see those four stages. And so now all of our research-- well, first of all planning in stage one, then research in stage two and three, and development in stages three and four-it all now gets thought about in terms of a four-stage model. In terms of, for instance, our Cytoscape software, here's how I think about the four stages. So, and this is, arguably for my lab, the first software tool that came out of my lab in collaboration with several others, started back in the year 2000. And so the first was to identify the driving problem in the target community, which here, at the time, it was clear that molecular biologists were generating ever larger molecular networks. We've already talked all about the fruits of that, 15 years later. But at the time, there really were no tools to analyze networks that large. There were certainly tools for Cytoscape, but nothing of the magnitude that was needed or the analytical complexity that was needed. And so that was the motivation. We first-- decode it ended up becoming did first was used in a series of prototype studies, which were biological studies performed and published over a few papers between the years 2000 and 2003. And the actual publication and creation of Cytoscape itself, its initial release was in 2002. And its initial marker paper was in 2003. But again, it was that real evolution of the code, stating all the way back to even the late '90s through 2003 when we announced that tool to the world. Now since then, of course, we continue to evolve the tool quite dramatically every two to three years. And so every three or so years, you want to think about updating that effort with a publication. But now, as the last thing here, how did we get broad adoption of this Cytoscape resource? And that really is where I think luck played as much of a role as skill or planning. But there are definitely lessons to be learned. And the first big decision we made here was to allow the Cytoscape core, or to define and separate out a part of Cytoscape which was called the core, and a part which were called plug-ins at the time. Now they're called apps after the iPhone came along. Giving people the ability to write their own plug-ins or apps really changed everything for us. And so if you want, if you're willing again to put in the software engineering effort, then it can pay off in terms of dissemination and crowdsourcing because you're giving them a mechanism by which to very effectively crowdsource. Many of these apps-- and there's over 200 of them that are now developed for Cytoscape-- are also themselves open source. And so what that means is we can actually absorb some of that technology and use it to benefit the core in future releases, and everyone's happy. And that's really been a powerful model for us. Other things that we've done are maybe more usual. So we have an international meeting around the increasing core development community around-- and plug-in developer community-- for Cytoscape, as well as heavy biological users of Cytoscape were invited to this. Over the past several years, we've had a very successful joining with the DREAM and the RECOMB Systems Biology Conference. We do Cytoscape in conjunction as a sort of three-for-all. And that's been a great development. Of course, one also wants to foster local community groups around your tool. And that's-- and one can identify individuals who are willing to take lead roles in that. And finally, one has to fund these efforts as they progress and grow. And here, I think if there's any one piece of advice I have beyond the obvious, it's let go of ownership. Realize that you're going to be successful if your software is successful. And you want people to write grants that are essentially extending your software or taking ownership over your software even. And so, becoming an open source project, facilitating a developer community, and just letting go of the reins as much as possible-- you know, of course, one of course wants to continue to contribute to the project-- has been a really, really key guideline for us. And so not only now do we have Cytoscape itself, but we have this whole satellites of tools that have been built around Cytoscape, including the app store where you can download the apps. Some of these apps get downloaded thousands of times and cited thousands of times. So they themselves in their own right are quite powerful forces. And I already talked about our connections to data basis, like NDEx. So that concludes my talk. And I have tried to touch, really, on four different points, which were key in building my own data sciences laboratory, and I think are clearly going to be key considerations for anyone getting started or building into the future. And so with that, I will thank you for your time, especially given that I think I just competed with Donald Trump's inaugural address. If you are still on this call, I think you made the right decision to stay. But with that, I will take any questions. Thank you, Trey, for this wonderful talk. So we actually have time for one quick question. So the first question-well, actually, we got a lot of questions for this. But I'm just going to choose one. So what are your thoughts on using-- makerspaces for data time science labs? Do you have any thoughts on that? Are there any data science makers out there who use a space like this? That is a wonderful suggestion. I have several lab members who are big participants in the makers movement. I, myself, have not-- have not tried going down that road. My guess is there's-- depending on what environment you are in. If you're in a university environment that has a biological sciences department, my guess is it might actually be easier just to go to your neighbor and use their maker space that's provided by the university. But I would love to hear people have had experiences with that. OK. Well, we're out of time. Thank you so much. And we'll conclude this talk today. Thank you. 