 my name is kevin O'Leary from the University of Wisconsin at Madison and I work on the image a project this is the pre-processing part of our bio image analysis and in this section of the of the video series we're going to be talking about the importance of understanding where your data came from image analysis has the goal to do measurements and further analysis but you can't blindly just take your data as it comes in you really understand where your data came from what were thickest trains to the acquisition whether it's limitations of the system or even mistakes that may have been made as part of the acquisition and it's really important understanding these constraints and this could be anywhere from parameters you want to collect such as a really big field of view where you want to stitch and bring your data together in big fields of view and understanding what are the constraints and necessary options for stitching to issues like your microscope may not always be properly uniformly illuminated understanding issues of illumination correction and understanding how your sample is lighted up austere registration you may have different views different cameras where you want to bring them in and correlate them or modality such as light sheet microscopy where you look at different perspectives you want to be able understand how to stitch and register those together but the number one thing is always keep in mind that you need good data and good data out the importance that you don't use the analysis to correct for failures in acquisition you want the best data said possible before you analyze and the big goal that is to actually have the best signal-to-noise if you want to find a signal of object of interest such as later talks where we talk about segmentation you want to make sure that you have good signal to noise which means good data and so we're going to start in this series about more basic methods and move to the more advanced so one of the most common things you might do is cropping where you want to actually crop a field of view and actually see kind of get rid of the unnecessary background the the elements it might be confusing distracting todayI but also distracting to the algorithm so it's a simple concept you might want to move the edges of culture discs or as I showed here just the unnecessary black backgrounds reducing the non information areas and it's very common we do this for colocalization as we talked about in other videos where you just want to see how two things of interests co-localize are in the same place and so having less background can help on that because otherwise more background could influence certain calculations such as colocalization and so sometimes you need to reduce the areas without the signal just because it's confusing to the scientists another thing you may want to do to make it easy for the algorithm is just invert your sample so again a very simple concept as illustrated here where you just want to actually revert it maybe the hour that prefers to have something white objects on black you can very easily invert it and it's very important to always do this in a context of knowing what is your pixel values what is your bit depth what is your samples parameters and really knowing what you're doing to your data keeping the original data when you make these kind of changes sometimes you also just want to improve the noise or smooth the edges of objects filtering is an incredibly powerful way to better have better contrast differentiate your objects and there are many classic filters such as median filtering getting rid of noise or embossing and filtering when you get rid of back on distraction and these filters can often be used together and they could be chained in a series to get the optimal contrast for your particular case and they're available in many tools such as image a and there are more specifics and filtering and other videos in our series as well many times you actually want to do color metric analysis for example when the gold standards in all clinical care is pathology where you use dyes and stains and for example as illustrated here you would actually have stains such as hematoxylin and blue and ES and magenta you have these color metric stains that can tell apart a tumor from a noun to myrrh but you want to unmix them and they'll extract these colors so both so profile or and image a have very easy to use tools for extracting these colors for further analysis as well a very common thing is just had better views we live in a world of 3d but when we analyze data we actually want to understand the 3d aspects and these 3d objects we want to go back and forth we have easy ways of showing them on the screen on video online but even in prints and Z projections are one powerful way of convincing your data using projections and the classic is a maximum intensity production which is really great for showing what's going on in your data set in a condensed form that might help you is shown here where I can actually condense all this data into this image here and I can analyze that data for example for segmentation and it's a very powerful way of being able to see what is happening in this case if I just want to understand the objects or if I want to intensity I mean want to sum all the signal as shown here and so the power of different views as a Z projections is very important as well you may want to have a three-dimensional view of your data set so as shown on this screen you have data where you really want to look slice by slice and yet you want to have a three-dimensional you don't want to be constricted just a projection view you want to see the full 3d rendering and so this is something that you can do in a tool called site view for example name is J where you can actually understand the 3d rendering and Loic Rohrer our colleague in another video we'll be talking about 3d rendering in depth that I encourage you to look at that another issue as I mentioned early on is understand the limitations of your system ideally you want to understand your acquisition so you can fix it when you acquire it a classic example is the bulb on a microscope being not set up and aligned correctly but there may be times where you can't deal with it it's already been acquired or it's not your microscope or other issues of the illumination correction and this image here shows us that on the far image here you are seeing issues of the uneven and light we've actually and panel B have shown it in more clarity and we can actually correct that through post-processing so that our pre-processing so that we can actually correct that frame another important thing for tracking a segmentation as my colleague and Carper will talk about it under one of our other videos is the importance of increasing the SNR and this is the idea of trying to understand our resolving capability and being able to TechEd objects from each other so it's shown in this frame here we actually want to be able to clearly make out each element each cell and be able to tell one from the other and so there are some very nice pre-processing techniques you can do to increase that and isolate those signals so then you can do decrease the noise and increase the signal so that you can do things like single processing and segmentation and so on on those images what happens if you have to align two more images of the same scene for example if you have sets of cells they're migrating a you to actually see from different microscopes or different cameras what is going on in that scene this is a process we called registration and it's the idea of transforming different sets of image data into one coordinate system and the concept is very simple you calculate a transformation function which is essentially the math for modifying the space relationship to between pixels and the idea having an input range where you transform against a reference image and then that transmission function takes into account the geometric distortions angle orientation shifting a distance as you can see in this pipeline here on the screen and the thing to keep in mind is there could be many sources for why you might want to register and deal with issues of air for example when you're sectioning tissue your knife blade may miss a cut you may actually miss a section and you may have to register when it's harder to see between different views you know it's the same sample or in the case of lightsheet where you actually have different views and you want to correlate them so registration is a very important concept and as I mentioned you know this general technique we use in many ways not only in life sheet and dealing with different views but also if you want bigger fields of views such as you want to actually analyze many different positions that stitch them together and you can do this fairly automatically you can actually have points and features share between the images and you can do it mainly by hunting and pecking and finding these or you can use algorithms that can find them automatically so for example at image a we have the ability to computationally find as you see here in yellow corresponding points that features in the images and use these to actually determine the best transmission function so it can do it for you to register these images which is incredibly useful which would be for large datasets and a very famous example as shown in this plugin freemium is Jay called unwarped you can actually use a type of elastic algorithm to actually correct for deformations in the image and still register him as you see here in the Eiffel Tower image that has been corrected and registered and this is an example of the registering of sequence images I talked about and there's in general they also the idea that not only do registration for a single field of view but what if I wanted a giant field of view so a good example is if you went to the Grand Canyon and wanted to take a picture of the Grand Canyon your favorite smartphone is capable of actually automatically taking bunches of photos and stitching them together and we wondered the same thing at imaging we won't actually have the high resolution view maybe part of the Grand Canyon but then like Google Maps be able to zoom out and see the entire thing and so you can do that in microscopy too where a single picture can't capture at all so we take many pictures and then we can automatically stitch that together and that's a common application of registration and stitching and so motorized stages allow you to do this where you can roll them around and you can actually have coordinates so the stage can be kept track of all the coordinates and you can computationally stitch all these coordinates together with the help of algorithms and these stitching tools have gotten quite powerful where you can do it from a whole bunch of samples or less samples and it can actually use a registration to deal with issues of overlap and lack of overlap and getting the best results based on whatever data you acquired for your microscope and this is an example of what you can do here you can see all the images that we've stitched together and the final results as you can see here you may not often need to correct for the brightness difference at the borders using correction techniques that we very much in very briefly so that you get the best result and the smoothest result so you don't see the little squares and the artifacts as you can see in particular in the efforts of the middle one there another really important aspect of pre-processing is deconvolution understanding the noise and air in your system when you acquire fluorescent images in particular these are very digital views of your actual sample and you have to understand the limitations and the air of your actual microscope and the defining optics and it's important to understand the convolution of air that can be introduced in your acquired images by the fundamental hardware on your system and we often call this the optical blurring and you can say the image is convoluted the convolution is trying to correct that systematic error this is due to the hardware on your scope you get rid of that blur and this can be very important because it could often result and while your microscope may look more noisy or have less contrast than a colleague's microscope that seems to have similar capabilities you want to be able to see smaller features for example and that can be prevented by problems due to composition or noise introduction and so what you can do and this kind of work is understanding for example a basic heed and try to understand what is the actual object trying to measure and trying to measure what's called a point spread function basically this mathematical measurement of what is your point spread of your sample understanding in Z what does the air produce by air of your optics your actual glass issues like optical oil or things being left on that might contaminate your view and we can correct for that we can understand this mathematical function and then use other algorithms together the point spread function to correct for the inherent convolution of your system so I think it was a way for correcting of the air of your optics and we can correct the raw image before we ever get to additional post-processing as we talked about in this video series so the importance of deconvolution can be used on fluorescence confocal microscopy another another types of data to get the best possible quality data and so with that we're going to end this part of the series but thank you for your attention and I hope you get a chance to watch the other videos in our image analysis series you 