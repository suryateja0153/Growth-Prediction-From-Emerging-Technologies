 The human fascination about agile flying beings like birds and bees have propelled decades of research and trying to solve the problem of obstacle avoidance on aerial robots. However most of the prior research has focused on static obstacle avoidance. Albeit in nature a bird's real life is very different. You often encounter predators or your not so friendly neighbor trying to attack you. To solve the problem of dodging, these masters of flight have evolved to sense at high speed with low latency even at night. Inspired by nature, a class of sensors designed to excel at these tasks are called event-based cameras Such event cameras collect only the asynchronous intensity changes in light instead of traditional image frames. This enables high-speed computation and fast reaction times on a quadrotor to avoid dynamic obstacles. To realize such a bio-inspired dodging solution one could train a deep neural network based solution but such a system suffers from high computational cost and a need for a lot of data. Obtaining such data from real world is arduous, expensive and time-consuming. Instead, we simulate objects flying it our drones and learn a series of shallow neural networks for the complete navigation stack flying and dodging dynamic obstacles. Our Intel Aero quadrotor is equipped with a front and down facing event camera, a down facing sonar and an IMU to achieve the task of dynamic obstacle avoidance using onboard sensing and finally all the computation is run onboard in Nvidia jets and tx2 without the use of any motion capture system. All our neural networks are trained and simulation however the real data looks very different from simulation as it suffers from noise which are hard to model. Instead of making the neural networks larger to make them resilient to noise at every single stage we propose to use a shallow neural network for pre-processing the raw event frames to remove noise in motion blur; We call this EVDeblurNet. Now assuming that the down-facing camera is looking at a near planar surface, we estimate the odometry using the combination of the sonar and the EVHomographyNet. Later this  odometry is used to warp the front facing event frames to perform motion compensation such that the dynamic obstacles stand out which is finally fed into the EVSegFlowNet to obtain segmentation flow. This information is then fed into the control policy to find a safe direction or a trajectory to avoid the dynamic obstacles. We successfully evaluate and demonstrate the proposed approach in many real world experiments with obstacles of different shapes and sizes. To our knowledge, this is the first deep learning based solution to the problem of dynamic obstacle avoidance using event cameras on a quadrotor. We also apply the proposed strategy to evade fast dynamic objects such as drones and territorial birds. By merely reversing the control policy we can also accomplish the pursuit task proving that our navigation stack can cater to different scenarios. Also the most impressive part about our research is that we only train our models on simulation which directly transfers to the real world without any fine-tuning which highlights the generalizability of our networks. 