 Mark Oskin: All right, good afternoon everyone. Welcome to the Computer Science & Engineering Colloquium where it's a pleasure to welcome, Dr. Huang who is going to be talking about his work on analog accelerators for supercomputing applications and his work on making quantum computing programs more correct. This is a virtual colloquium, and so it's still kind of in process of figuring out how this is going to work for everyone. So, we are going to take questions during the talk that are sort of clarifying questions. To do that, either use the raise your hand feature of zoom and I'll be continuously monitoring this and then I'll try and step in and verbally interrupt the talk and then hand it off to you. Or you can also just use the chat and say hey, I have a question and then we'll hand it off to you. And then at the end, we'll take technical questions from anyone, in more of a free-formed way. You can say you have a question in chat, or you can just unmute and take it away from there. So with that I hand the virtual podium off to Dr. Huang. Yipeng Huang: Okay. Thank you Mark. So, I'm gonna begin my talk with a little bit of computing history. Starting in the 1940s, there was actually the style of computing using analog continuous-time signals. And this type of computing was useful for solving problems for wartime needs like solving artillery equations, and it actually developed to the point where people were solving field equations, like air flow equations. And the technology actually developed fairly suddenly through the 1970s, but that came to a halt in the 1980s. We also know that in this early history, Researchers were also developing digital discrete-time computing, and again for wartime needs such as breaking the German Enigma. And over time, as the device technology developed, the transistors, via transistor scaling, Dennard scaling and VLSI: these transistor scaling milestones, coupled with these breakthroughs in architecture abstraction, such as using stored programming computers, microprogramming, ISAs or reduced instruction sets. In combination, these scaling trends and architecture milestones made it so that digital computing became the de-facto mode of computation and it made analog alternatives all but extinct. And we also see that, in more recent history, the trend of computing has been in heterogeneity. And this is really driven by the end of Dennard's scaling and, with that, we see that computers become more parallel, we see more specialized hardware in the form of PGAs, and GPUs, and a6. And this is really driven by the end of some of the key underlying micro-architectural scaling trends that enabled those types. As we look to the future, we see that some of the key problems that we need to solve to address humanities' grand challenges. Things like: addressing climate change, and understanding the human mind, and also solving optimization problems. These types of problems, not all of them can fit into the existing mode of using a von Neumann architecture coupling CPU to memory. And as we go forward, we actually see that researchers are exploring different modes of computation. So on one axis, we see that we're encoding data in digital form, or also, in more continuous variables in the form of analog signals. And these types of computing, on this axis, we see that there's researchers investigating, understanding, and performing neural network inference on underlying analog neuromorphic substrates. And also, in this talk, I'm going to talk about this idea of using analog continuous time hardware to solve problems in continuous physics. So in problems such as fluid dynamics and plasma physics. So that's one dimension. Another dimension of this heterogeneity is going forward, We're also seeing researchers exploring doing computing using quantum signals. And so on the quantum signal side, we have researchers performing problems on chemistry, on quantum circuit computers, and also performing high dimensional optimization on these devices such as d-wave quantum annealers. And so, as we see, going forward the space of unconventional emerging architectures is very broad. And my research is in the space of, as digital scaling motivates us to explore alternatives to digital computing, and also as these tantalizing quantum models of computation drive us to explore that mode of computation. Going forward, we need to find these mappings from extreme and urgent workload challenges, down to these fundamentally different models of computation and analog and quantum computing, right? And so, going forward computer architecture will be even more heterogeneous taking the form of using CPUs, GPUs, FPGAs, a6, analog and quantum and so forth. My research is in addressing how do we use these architectures. So these problems include: how do we map large problems down to this type of substrate? How do we borrow from existing programming abstractions to harness these types of architectures? And how do we interface these types of hardware with existing conventional von Nuemann computers? And so, my work is in both of these areas. So, here is a timeline of my research in both abtractions for our analog computing and also for quantum computing. On the analog side, I worked on at Columbia University, developing prototype chips for solving systems of equations. Culminating in a phase 1 STTR study with DARPA where I served as the PI. I went through grad and served as the PI for that. And more recently, in my postdoc work, I've been working on developing abstractions, and analysis techniques, and debugging tools for quantum programs and quantum algorithms. Ok, so let's focus on my dissertation work first at Columbia University And so in this line of work, I will use this map to chart out what are the areas that I worked on. So starting from the top, I'm interested in figuring How do we map these large-scale, important numerical problems to an underlying prototype, analog computer hardware? Which we prototyped Columbia University. And the way that we explore that is we created these numerical primitives that allow us to effectively go from the problem space down to the unconventional computer hardware. Okay, and so this takes on several three main thrusts. So one is in solving differential equations, another in solving linear algebra, and another in solving nonlinear systems of equations. Okay. So with that map in mind, the next part I want to talk about is a tutorial of how exactly do you map these differential equations into something that could be solved as a analog computer problem. Okay, and so to motivate this problem, I'm going to show you a differential equation. Here, this is a second-order ODE. What it says is the change in the value 'x' is going to depend on the second derivative of 'x' and time. And also, the first derivative of 'x' is in time and the variables are interrelated in this equation here, in the form of ODE. And the way we would have solved this, using conventional algorithms or conventional digital computers, is we would use something called numerical integration. So numerical integration, we would come up with some mapping of the values for 'x' and then as time progresses, we find and integrate the values of 'x' and 'dx/dt' so that we can do a numerical integration on this. And so this will involve both discretizing the values in space in terms of the variables and also in time. Okay, so that's how you would solve this in digital. On the other hand though, so we know that earlier in the history, we used Analog computers to actually solve exactly this type of problem. And the way that we solve these problems was in the form of these analog computer configurations. So the way that you would use something like an analog computer is, in this diagram here, what this is showing is you would set up a analog electronic circuit, such that, the behavior of the analog electronic circuit follows the same ODE that you want to solve right? So in this circuit, we have these components called integrators, They take derivatives and create them in time and give you integrals. And then you have these feedback loops which are multiplied with these coefficients. And once you set the values of this circuit to its initial values, you let it go. The behavior of the circuit would then follow the ODE that you want to solve. So, it would look like something like this, for example: So this is an example ODE trajectory. And if you were able to observe or sample or directly use this waveform, Then you would have generated the solution for solving the ODE and that would be useful computation. Okay? And so, at Columbia University, we were interested in trying out this type of computation, right? So this type of computation is how they worked on analog computers in the 1940s, 50s, and even the 60s, but no one has really revisited it, and used it, and reinstantiated it in modern integrated circuit technology and see if it can actually do this type of computing using modern tricks. Okay, and so that's exactly what we did at Columbia University. I was a member on a team, of a team of graduate students, consisting of two PhD students. PhD students, one EE and one CS, who was me. And my role on building this chip was, I was in charge of designing the connectivity network for the analog chip And doing the validation testing of the chip and also creating the entire stack that maps us from the problems face down to the hardware. And so what this chip constitutes is, it's basically a bunch of integrators that I find integrals in continuous time, you have multipliers and mirrors and also summing points to allow you to break out variables and sum them. And also analog and digital converters to allow us to interface with digital computers. All this is part of a much broader programmable crossbar that allows us to set up a bunch of different types of topologies that correspond to different types of ODEs. And using this prototype chip, we were able to show, Yes, we can actually revisit analog computing in a modern context using integrated circuits We were able to use this chip as a developmental platform to test different problems and algorithms without having to resort to simulation. And we were actually able to take this chip, package it up, along with its documentation, and send it out to different companies and research groups, including one at MIT. Where they were able to take this chip and compile problems to it, or experiment with different kinds of problems, and demonstrate it useful in different workloads. So, It seems like this type of computing has a lot of promise, but as we know, it actually disappeared in the 1980s and the question then is, why did this style of computing die out? So the reason for the limitations of analog is primarily these two, right? So I've told you how exactly we use a continuous time pipeline to solve problems in ODE, but it also means that we have to dedicate it physical integrators and physical multipliers, chain them together to solve an ODE of any problem size. And also, because we're working with the raw analog values, this means that all these non-idealities in the analog circuit. So not in idealities, such as analog mismatches, non-linearity and the transfer slope and noise, All of these characteristics are going to conspire to limit the amount of precision and accuracy that you can obtain in an analog chip. So were constrained in both problem size and also accuracy and precision, Which means that's difficult for us to reach to the capability to solve a lot of the problems that are interesting in modern problems, right? And so my thesis talked about exactly this problem. So how do we solve problems, useful problems? How do we obtain more accurate solutions? And how do we handle larger problem sizes? And we want to do all this because we want to find a useful mapping from an unconventional analog hardware and bridging the gap so that we're able to tackle useful problems that are interesting in modern scientific computing. And the key ideas that allow us to do this is...two (key ideas). So, the first key idea is we will use a digital conventional computer to chop up problems into problem sizes that do fit in the analog chip. And the analog chip, then, only serves to generate approximate solutions, which are then used as digital seeds and the digital computer would refine those solutions to obtain higher accuracy solutions. And this is done in the context of solving two types of problems, that I demonstrated. So one is solving linear algebra and the other, which I focus on in this talk, is using an analog chip to generate seeds for solving nonlinear systems of equations. Okay, so I will pause here before I dive deeper into solving nonlinear problems. Are there any questions at this point that you might want to jump in get out of the way? (silence, skip to 15:27) Okay. So let's dive in. So how exactly and what does it mean to solve nonlinear systems and equations? And so I'm going to talk about this portion via three key ideas So again, it's those two key tricks on how to leverage analog to assist digital and finally an evaluation of how this idea works in the context of solving problems that would be something that you might solve on a GPU. Okay So the first step of this is, let's talk about what it means to use the analog approximation as a digital guess. And I'm going to illustrate and motivate this problem in the context of solving this nonlinear equation Alright, so this is a nonlinear equation that we want to solve, We want to find a value of 'u', so that the 'u' satisfies '(u^3) - 1 = 0' And this is a nonlinear problem because there's a cubic term on the 'u.' And the way that you might solve this, using a digital computer, is using an iterative numerical method, something like Newton's method. And the way that this works is, it's a recurrent relation, You take some initial guess for the value of 'u,' You follow the recurrent relation several times and then eventually it converges on one of these three solutions that satisfies your 'q-1 = 0' Ok, and so, let's try this out for several different initial guesses. So here's one initial guess, starting at the top left corner. Newton's method is going to follow the green curve and eventually settle on the green solution here. Ok, so it seems to work out pretty well. Now let's try it for a different initial guess. So here it's the top right guess there, we're gonna follow it (inaudible) steps, and here it has an interesting behavior where instead of converging smoothly to the solution point and red. It actually has this behavior where the error increases, it swings out wildly and then finally comes back and converges in the convergence basin. Ok, so this is a tricky part of using Newton's method. And a canonical way of improving the performance of Newton's method is to use damped steps. So here, instead of taking the full steps We're going to take smaller steps and this allows the trajectory become smoother. And as we take smaller and smaller steps, such as taking 1/8 damping here Now we see that the trajectory is smooth, it converges more readily, And also you start to see that this is actually some kind of continuous trajectory. And so this continuous trajectory is exactly something that we might be able to solve in an analog accelerator. Ok, so here I'm going to illustrate exactly how we leverage this idea. So here I'm plotting the error in the solution returned by Newton's method over the number of iterations of the Newton's method And as you see over the first few iterations, The error does not decrease by much, it's still trying to find its convergence basin And finally once it hits this convergence basin and the 20th iteration, then it readily converges. Ok. So the idea here is we would use an analogue accelerator to fast forward us through all those first 20 iterations, started a good initial guess, from where the digital computer can then continue to refine the solution. And the way we set up the analog accelerator to do something like that is to use this method called 'continuous Newton's method.' So in continuous Newton's method, we've taken the Newton's method recurrence relationship, and we've turned it into an ODE Ok, and this ODE, you would solve it using integrators and multipliers on the analog chip. So you use the integrators to store an initial guess for 'u,' such as in this picture, You then feed that present guess of 'u' through these two other networks that evaluate the Jacobian and also the function of 'u'. So if you (inaudible), in this case it is 'u^3 - 1' And then you feed it into this separate network which uses negative feedback to solve a linear system of equations and this internal loop generates a solution which is an increment, something that's analogous to increment, that allows us to do Newton's method updating. Ok, and its important to know in this whole scheme, we're doing this in analog computer, which means that the analog and voltage current encodes the variables and also the whole circuit operates continuous time. So there's no notions of steps or discrete time clocks and so forth. And so with the analog log guess we get to fast forward all the way through the 20th iteration and refine it using the digital computer. Okay, and so the next step here is then how do we use a digital computer to tackle larger problem sizes that the analog computer itself cannot handle? It's the key idea here as you use a digital computer to discretize PDEs Following the conventional techniques that you use to discretize PDEs and then use the analog accelerator purely as a tool to solve for approximate solutions for nonlinear systems equations. And we evaluate this in the context of solving problems that come from fluid dynamics. And so this is the Burgers' equation. The Burgers equation comes from Navier-Stokes equations for fluid dynamics. And the Burgers equations is the key nonlinear heart of the Navier-Stokes equation and the 'u' and 'v' variables here encode velocities of 'x' and 'y' dimensions. And this is a nonlinear problem because within the equation, you have the terms for 'u 'and 'v' multiplying with themselves and the extent to which this whole problem is difficult or nonlinear is governed by this parameter called the Reynolds number. So as the Reynolds number becomes larger, the PDE becomes more hyperbolic in its character and also it becomes more nonlinear and therefore more difficult to solve. Okay, and in order to solve this PDE, we use the digital computer to chop up the PDE and to discrete spatial points and then we solve the nonlinear system of equations that results from this discretization and we solve it in the analog accelerator. And using this idea, we're able to use a scaled up analog accelerator, hypothetical scaled up accelerator, We're able to return an approximate solution a hundred times faster than just a purely digital approach performing Newton's method. So the valuation is shown here. What I'm plotting here is the time to solution for various problem sizes of the Burgers equation or various nonlinear problems that descend from the Burgers equation here. I'm plotting the time it takes to solve plotted against the Reynolds number Which governs how challenging a problem the Newton's method problem is going to be. Okay, and as we go to the larger problem sizes, We see that we're able to return approximate solutions using the analog approach in a hundred less time compared to a digital CPU performing a damped Newton's method solver, Okay. And so the next part then is can we use this approximate seed in the context of solving larger scale problems. Something that might be tackled within a GPU Okay, so we see that we're able to assist a GPU performing Newton's method we help it return solutions about six times faster because it no longer has to search for the convergence basin And because it no longer has to search for the basin and expend time and energy in that time period, we're also able to return a precise solution at about 12x less energy consumption. Okay, and so the valuation for that is shown here. What I'm plotting here is the solution time on top and solution energy on the bottom, for two problem sizes. So for 16 by 16 grids and also by 32 by 32 grids. And the 32 by 32 grids here, we're actually solving matrix equations that have 2048 variables at play. Okay, and so the various bars here that I'm showing are the various design options. So one design option is just to use a GPU naively, using Newton's method, to solve to high precision. The middle bar, which barely shows up, in red. That middle bar is the time it takes to generate approximate solution using an analog accelerator. And finally, the orange bar on the right is combining the two approaches using analog to accelerate the digital solver Okay. Now we see that for the larger problem sizes were able to solve for the same precision 6x less time and 12x less energy compared to the pure digital approach And it's important to note that saving time and energy in these inner loops matters a lot because as we solve PDEs, PDE solvers will call these kernels repeatedly. And these innermost loops and (inaudoble) kernel loops dominate the runtime of these PDE solvers. So just to recap, the idea here is we are able to bridge the gap between the problem sizes and problem accuracy that we want to tackle. And what is fundamentally offered by the analog accelerator, we're able to bridge this gap between the problems and the hardware Using these numerical primitives and solving nonlinear and linear algebraic equations. We're bridging the gap by using analog seeds and using digital solvers to break up the larger problem sizes. And, this picture here shows that we can actually revisit using analog acceleration and analog hardware as a way to assist some of the challenging problems that we have in modern scientific computing. Okay and before I move on to the next chapter of my, in terms of my postdoc work, I wondering if anybody has any questions we can get out of the way at the moment? Audience: I have a question. Have you looked at whether you could use that method for either training neural networks, or implementing Hopfield neural networks? Yipeng Huang: That's a really good question. So the question is...I'll answer the latter first, Which is, does this have connections to Hopfield neural networks? And so, another way to look at this whole dynamics is that it's exactly a kind of recurrent neural network operating in continuous time. So I think there is actually deep theoretical connections between this model of computation with recurrent neural network models. Okay. And the other question is, can we use this as approach to do neural network training? And I suspect so. So this type of solving nonlinear algebraic equations, It's analogous to doing a gradient descent on a nonlinear space. And so, intrinsically, what we're doing is we're actually solving an optimization problem, which may have uses in neural network training. Audience: I have a question as well. So when you're doing these comparisons between the analog versions and the conventional Newton iteration, the convergence is going to depend on where you start in the space and whether, for example, you're using the undamped or damped Newton method. What are the kinds of conversion comparisons that you're you're doing? Like, how can you, how are you coming up with sort of this one number? Yipeng Huang: I searched the space, I did this experiment kind of stochastically. So, I plotted a bunch of different initial starting points and then averaged the various starting points over the sample. In general, there has been some mathematical literature in terms of the continuous Newton's method, but in general, I would be interested in pursuing, kind of, a numerical analysis of these continuous time versions of iterative methods a little bit more. So there's a theoretical and kind of an analytical part that I'd be interested in pursuing as well. Okay So the next phase of my research, again, in the area of harnessing, and coming up with problems, and programming abstractions for emerging architectures is in the area of quantum computing. Okay, and so my research in this area, is I've done project on figuring out how to debug quantum programs and also how to simulate and perform analysis on quantum circuits and quantum algorithms. Okay, and I have some research proposals in this area as well. So first, let's go into some motivation. So the motivation for this area right now is, for decades, we've had understanding of these awe-inspiring quantum algorithms, So these algorithms allow us to determine the properties of chemical compounds from the governing equations And also we have these algorithms that allow us to perform integer factoring faster than any known classical algorithm. In fact, there is many different types of these algorithms, quantum algorithms that researchers have been searching and discovering over the past 20 years. And what's important, at this time point, is now we've actually had these prototypes that allow us to test a lot of these algorithms for the first time. So either in prototype hardware or also in simulation. And these are based on various types of technologies and going forward, that may be even the case that the most mature and successful technology has not even emerged yet. And the role of computer science, in this time period, is that computer science plays a role in bridging the theoretical algorithms and problems down to the hardware devices and the platforms. And my work in bridging this quantum software hardware gap is in several directions. So one direction, in a paper recently published at ISCA 2019 is in using assertions as a way to help programmers debug and understand why their programs are not succeeding. Okay. So let's launch into this portion first. And, before I go into it, it's helpful for me to motivate the challenge of doing quantum program debugging. So suppose that you want to determine the property of some physical object, right? And the way you do different properties is you would interact with it. So suppose you want to measure the speed of this car? The way that you might measure the speed, is you bounce an electromagnetic wave off of it. It bounces back and it's analogous is to throwing a particle such as a ball at the car. It bounces back and by measuring the difference in the speed of the ball as it goes out and comes back, you're able to do something about the speed of the car, right? Now, suppose that we want to determine the speed of this quail. If we use a similar approach as before, we throw a ball at it, instead of the ball bouncing back, the ball is so massive that it will actually impart some property and some momentum onto the quail. And you've no longer have a valued measurement of the very property that you are trying to measure, okay And so the idea is that depending on the properties of objects that we're trying to measure, Whether or not it succeeds depends on, depends on whether or not there's enough mass differential between the two particles. And this is gets at the very part of why it's challenging to debug quantum computer programs And so quantum computer, we know that they operate because they are working on these minut properties of particles that are very delicate. And conventional methods that we want to use to observe their states, are going to disturb the state of the algorithm and you won't have a valid readout to do things such as debug. Okay, and so the challenges of doing quantum programmed debugging are that, 1. You're not able to read out variables in the same way that you might read out variables in the classical computer. There's no such thing as printf debugging. 2. Even if you're able to obtain some kind of measurement information from the quantum program, then in general, it's very hard to use them as a tool or as information for debugging. Okay, and so to make this more concrete, I would like to talk about this example. Motivating the example, where we're going to debug this component of a quantum algorithm. This is the quantum Fourier transform algorithm. It's expressed in this format called a quantum circuit. What it plots is, it's basically it plots what operations we want to perform on qubits and time. So to unravel this more, I'm going to give a very short tutorial on how to interpret a quantum algorithm upon a circuit and also why it's difficult in to do debugging on this quantum circuit, quantum algorithm. Ok, and so in this tutorial we have two qubits. I represent them as quail's and they start up at this classical state. And quantum information scientists represent these states. Intrinsically, what they are, mathematically, is they're just these vectors and we manipulate them using matrix vector multiplication. Ok, and The way that a quantum algorithm works is we want to put various gates, in this case, for example, this operation, had a Hadamard gate on it. And the Hadamard gate has the action or has the effect of taking one of the quails and put it into this state called a superposition state where it's going to be simultaneously pointing both left and to the right. And, again, quantum information scientists have a notation for that. It's just matrix vector multiplication. And this state, tells us something interesting, Which is that, at this point, this quail the upper quail is simultaneously pointing left and right. And it's this property that qubits can take on multiple states at once This is a property that underlies the power, one of the properties that underlies the power of quantum computing. Ok and continuing on with our tutorial, we continue to work on this circuit by introducing more gates So this next gate I'm going to introduce is a CNOT gate It has the effect of taking one of the qubit states and then parting that information onto another qubit Again, it's just some more matrix vector multiplication But when you do the math that shows that there is a interesting property that descends out of this state, which is, at this point, the two quails are what we call entangle. Entangle means that these two pieces of data are now intrinsically intertwined, So they are no longer treated as two different pieces of data and instead they're part of one description. Okay, and if we were to try to observe this state as part of the algorithm or as part of debugging the algorithm, It would be analogous to throwing balls at the quail. And what comes out of it is a particular property, where now, you're going to see that two of the quails are either simultaneously pointing left or simultaneously putting right. Okay, and so this is actually a property that would not be something we expected in purely classical systems, But in quantum computing, because the states are so delicate and because there is such thing as superposition and entanglement, we see these interesting outcomes. And the takeaway I wanted to talk about from this whole tutorial is that in quantum computing, there is no such thing as printf debugging. Because as you printf debug or if you perform measurement, you're going to disturb the very delicate states that you were trying to observe in the first place, and you're no longer kind of have a accurate observation of the program state. Okay. And coming back to our tutorial again, just to remind you, So we're trying to build out the code for this module called the QFT, the quantum Fourier transform. And what I just explained to you is an interpretation of what these circuit diagrams mean. Okay. And so next, I want to talk about what does it mean for a quantum programmer to actually implement this code. And so what it means, is you would actually have to take something like this. It's a gate. And you would create the program code that specifies this gate in the form of more fundamental operations. So here, I'm going to show you an example of how we decompose more complex gates into more elementary gates. And the job of a quantum programmer is to write out this code specification so that the code codifies and represents this decomposition. Okay, and this is a decomposition because we're taking a more complex gate on the left, a controlled unitary gate, control new gate operation and we're going to decompose it into several more elementary operations consisting of single qubit operations and also, a couple of two-qubit operations that actually could exchange data between the two qubits. And the subtlety here is that, there are actually several ways to do this decomposition. Right? So I'm going to show you two correct ways here. So here is one such correct decomposition where because of subtleties in the particular unit rotation that we're trying to implement, In this case, you can omit the 'A' gate and purely use the 'C,' 'B,' and 'D' gates and the CNOT gates to achieve a correct decomposition. Okay, and another way to do this decomposition is this middle column here. So this middle column is also correct because we can omit the 'C' gate and instead just use the 'A,' 'B,' and 'D' gates to do the same decomposition. Okay, and it's subtle here because you have to make sure that the current lines of code are correct and you want to make sure all these angles are rotating in the correct positive or negative direction. And so, a very tricky thing that a programmer might get wrong is, as you're writing out this code you accidentally permute the code, where you drop the wrong gate or you put the signs on the angle incorrectly, linking to you code that looks very similar to correct code but is actually incorrect. So, okay. No big deal. Programmers make mistakes all the time. We just unit test this code and make sure that it has the correct behavior, right? And that's actually something that's even more subtle about this debugging task here. Which is all three of these versions of the code, If you were to give it some input stimulus, such as the '1 1' state All three of these codes will give you the same output of '1 1'. Alright, so this is very insidious, it means that even incorrect code might lead to unit tests that pass but are only subtly wrong in very subtle ways, And in this case, this code is wrong because it actually introduced a wrong angle, a wrong phase angle on the output state. So this type of mistake might not be catched until much farther down along the line, when it's part of some larger module. Okay Any questions before I continue onward? Audience: So your point is that if you measured it at that point there'd be no observable thing that you could see, no observable problem. But, because the phase is wrong, something could happen later that would lead to a problem you'd actually see. Is that what you were saying? Yipeng Huang: Yeah. Exactly. Okay, and so what I'm advocating for to address some kind of subtle problem, such as that, is we need more robust debugging assertions and bring up strategy analogous to bringing up digital hardware components when we're trying to bring up quantum circuits and quantum code. And so, for example, that mistake that we just had, where the phase angle was wrong, it would lead to incorrect implementation of the QFT and we might be able to see something like that here. Ok, so I've basically covered some of the key challenges of debugging quantum computers, 1. That you can't really observe states and 2. That even when you do observe states, they might not tell you enough information to perform effective debugging. And so, our proposed solutions that to help programmers do debugging is to introduce this idea of breakpoints and assertions that allow us to assert properties on the quantum states so that as we're bringing up the hardware we're oftentimes checking for the correct states. And we brought up this toolchain in the context of IBM's Qisket framework. So it's a open source framework for experimenting with IBM's quantum hardware, back-ends, or simulators. And also it allows us to implement quantum codes and simulate them and execute them on real quantum computers. And what we introduced to this existing framework is these breakpoint annotation, and assertion annotations, and a modification to the compiler tool chain So that the compiler will help us in testing for these annotations, It will actually check at these breakpoints that the assertion is passed and that they match what you expect And so this next slide here, I'm going to show this in actual action. So here, I'm showing you the actual code for a test bench that helps us validate the QFT code that had built up in this example, right? So this is a test harness. We instantiate in this test harness the components, the QFT components. So here we have the code that we submit, that implements quantum Fourier transform and inverse quantum Fourier transform. And in this test harness, we would give it some input stimulus, in this case, a qubit state that encodes five. We assert, at the beginning, that it should be five and after these two operations of QFT and iQFT, which undoes it, we should also see that it returns to five. And in the between of these two components, we would assert that the states on the qubit should be a uniform superposition. Okay, and so when this code is submitted to a simulator or an execution back end, it would pause and stop the computation at each of these points, do a statistical test on the measured outcomes, and make sure that each of these assertions are correct before starting again and continuing on to the next breakpoint. Okay, and so it's using this methodology that we can validate something such as the QFT. And in our paper and in the code that we released we demonstrate using these assertion frameworks to test these other codes that we implemented, including Shor's Integer factoring algorithm, Grover's database search algorithm, and also, a benchmark in solving a quantum chemistry problem. Okay, any questions about this debugging work? (silence, skip to 44:25) Okay, so very briefly... Audience: Actually, before we move on, I have a question. So my confusion is kind of in the previous work, right? One of the challenges in debugging is that you cannot observe anything without changing it, but if you're implementing then the debugging tools on top of the simulators, then it seems like, at that point, you can pretty much observe whatever you want since you're operating within the context of a simulator. Yipeng Huang: So, right. Exactly. So if it's in context of a simulator as opposed to a physical execution, you have a lot more information that you can work with. But even in that context, The assertion annotations sprinkled within the code is extremely helpful from a programmer's perspective because it helps the simulator tests for these expectations, right? So in the simulated setting, it just becomes an annotation that allows us to codify expectations. Audience: Right, but it seems like its... So then the question is kind of, why is just like such a challenging problem to do this in the simulators? Since at that point you can, you know, you have the opportunity to observe anything that you need to observe. I think this is what I missed. Yipeng Huang: So in the simulator setting, What I would say is that kind of in existing coding frameworks there just hasn't even been these annotations that would allow a programmer say that, at this point, this state should be classical, or this state should be a uniform superposition, or that these two registers should be entangled. In existing programming language frameworks that simple straightforward annotation just doesn't even exist. And so, our contribution is that we introduce some simple assertions that allow us to codify those expectations. Audience: Yeah Yipeng Huang: So, in the last few minutes, I want to talk a little bit about some near-term plan work, Where my research is in finding contributions or finding new tools and new types of analysis that aid in this area called quantum programmed quantum circuit simulation. And the motivation for doing research in quantum simulation, quantum circuit simulation is that allows us to develop next-generation algorithms. It allows us to validate the noise models that we use to model real quantum prototypes. And it also allows us to probe the limits between classical computing capabilities and quantum computing capabilities. And so it has insights on what's the boundary between these models of computation as well. And my research, really quickly, in this area is I'm interested in finding new encodings and new types of program abstractions for quantum circuits. So specifically, I'm interested in using probabilistic models that come from classical computing, classical inference. Using those models with some change in the semantics and using those as tools and models for doing quantum simulation and analysis of quantum algorithms. And so the first step of this is to reencode these quantum circuits as Bayesian networks. And we are able to do this encoding because we can take the matrices for the gates that we had before, When we take the topology of the quantum circuit, put it into the topology of a quantum Bayesian network, And then the matrix values then become components of these conditional probability tables, but now in this context we're working on complex values instead of probabilities. And then once we have this translation, we can then use a lot of the tools that were very successful in classical inference I'm converting these quantum circuits into other encodings so I'm interested in converting this into a purely logical coding. So we would use tools that take Bayesian networks and convert them into logic equations and For example, if we represent the quantum circuit as a CNF, valid assignments to the variables that satisfy the CNF then mean they correspond to correct paths through the quantum circuit. And by enumerating all these paths through the circuit, we perform quantum circuit simulation. Ok, and so this is just a very short example that I'm going to jump through of how we actually re-encode the circuit, the Bayesian networks as logical circuits. So these descend from the normal tools that we would have been using in classical inference to re-encode Bayesian networks as CMS. And we show that we're able to do the same for quantum circuits And once we have that re-encoding we can use a lot of the existing tools and classical CNF manipulation as a way to Help us analyze or recompile quantum circuits for the purpose of analyzing algorithms or doing quantum circuit simulation. And so one result, for example, from all this is that we can use knowledge compilation tools for Bayesian networks. These tools compiled the Bayesian network to CMS and then onward to other representations. We can actually use those tools as a way to compress the representation of quantum circuits. And so what this plot is showing here is, I'm showing the resource usage for representing quantum algorithms versus the size of the quantum circuits itself. And as we see, the compiled results for three different types of quantum algorithms the toolchain is able to extract different amounts of structure from each of those algorithms leading to different sized compiled outcomes. Okay, and so this would be useful as a tool for enabling new types of queries and new types of inference and new types of debunking techniques for quantum circuits and quantum algorithms. And so this belongs to a broader picture of some of these near-term projects that I'm interested in, Leveraging this idea of using classical inference tools as a way to understand and represent quantum circuits and quantum algorithms. Okay, and finally, just to wrap up, This is part of a longer range plan where I've been working in the area of emerging architectures, First, in analog computing for my PhD work and more recently, in quantum program analysis and debugging in my postdoc work And, broadly, I'm excited in the landscape of future computer architecture where we're trying to build out all these abstractions that allow us to map important problems onto unique hardware substrates that uniquely support those problem types. And I, as a computer engineering educator and researcher, I'm interested in working this space, particularly I'm interested in understanding the principles of how they operate and what potentials they offer I'm interested in teaching students to build them, use them, prototype them, and apply them to industry and research. And I am also interested in understanding what types of limitations they have and security ethical risk that they may pose. And with that, I like to thank my co-authors at Columbia and at Princeton and also UCLA and also my teammates. That's a summary of my talk. Thank you for your time, and I'm happy to take questions. Audience: All right, thank you very much Dr. Huang. Let's let's all do that virtual clap thing that's down on the bottom. So, we have time for plenty of questions, if anyone wants to ask. And I'll get started, I have question which is, how can you determine that the assertion in your code fired from a bug in the program and not a bug in the hardware implementation of the signal processor or just the fidelity of the quantum device itself? It could be in the entity, right? That could be bugs anywhere in this chain. Yipeng Huang: Yeah, so it really depends on where in the development time line that you're using the assertion. So if you were to be using it purely in simulation, for example, then the assertions allow you to test for programmer mistakes. Okay, and then if you're able to say you have confidence that the program code is free of programmer mistakes, then the next part is these assertions allow you to test for potential problems, systematic problems, that the hardware may be introducing into the algorithm. But in general, right? So in general, these assertions don't operate in the sense of you know, they don't operate in the sense that they are not error correction schemes. Once they detect that an assertion is correct or incorrect then you've terminated the algorithm at that point and you can't carry on as you might carry on with classical computing. So in general, this is primarily a scheme to test for systematic errors. (silence) Audience: I had a question about the... You mentioned that you have a test for entanglement, and I'm just interested in what you mean by that or you have an assertion that this state is entangled, did I hear you correctly? Yipeng Huang: Yeah, so an assertion for entanglement just means that if you were to... So entanglement involves two registers, right? It involves two pieces of data. So entanglement assertion means that if you were to measure both of those registers at that time point, the measured outcome should be corresponding or correlated in some way. And the way that you would test for that kind of correspondence or correlation is you had set up a Chi-squared test or also like a contingency table that actually checks if you know something about one of the registers, Does that tell you something about another register, and if that's so, that means that the two registers are entangled in some way. Audience: I see, so it's only between individual registers It's not for for larger sets of states since that's known to be a hard problem for just figuring out entanglement of larger sets of data. Yipeng Huang: Yeah, so this is not this is not performing some kind of analysis on the full state vector or the density matrix, this is purely an assertion that the programmer has to say, I want to know something or I know something about the entanglement or lack of entanglement between two registers. Audience: So you talked about the debugging, as you mentioned, stopping the program early, But from the example you gave, it almost seems like running the program longer would be the way to see the problem in the sense that that phase that was wrong is sort of unobservable, But if you did more processing later, that could lead to observable things, So I mean does that idea make sense rather than stopping it early? Sort of, you know, do some random computation with the current state as the input or the state you're interested in as the input. So it's kind of like a hash or something that would actually amplify the phasers. It sort of seemed like that, you know, does that make it make sense? Yipeng Huang: Yeah, so what you say is true, which is the particular example that I'm showing, you would also see that there was a mistake in the code if you just ran the program and just saw that the QFT failed, right? But the problem is okay, so now we know that the QFT failed, now what? So the programmer would want to...if somebody who's writing the code would want to have a richer set of tools, have more checks throughout the code, so that you can nail down exactly where the mistake was. And so this is the part where I think it's... It's very analogous to you know, building up iterative test harnesses for something like hardware design. It's a very similar idea, You would want to have small test harnesses to make sure that your sub components are correct before you bundle them on to larger sub modules. (moderator calling on someone, skip to 58:15) Audience: Yes, to be clear, the quantum assertions stuff, this isn't anything that you run on an actual quantum computer? Or is it like a... Yipeng Huang: It's both. So you could use it in the simulated setting or also in the real quantum computer setting. And so we've released our debugging assertions and also the compiler changes that check for those assertions and they're in the process of being pulled into Qiskit, IBM Qiskit, as kind of a first cut debugger for these quantum programs. Audience: Right, but for simulation you wouldn't have to use these like measurement stuff? It would mostly be for actual quantum computers? Yipeng Huang: Right, so it's useful in setting. In simulation, it's just like an annotation. (moderator calling on audience, skip to 59:26) Audience: So, yeah, I had a question. So when I teach my students how to design hardware, I often say that it's about 10x harder to buy hardware than it is software and they should invest the extra time to do defensive programming and also lots of code reviews, So, where would you put analog computing on the debug-ability scale and also quantum computing? Yipeng Huang: I don't know if there's one uniform scale. When we were in the process of bringing up the analog chip for validation, We basically had a very similar, you know, problem where we can't really observe states and so the keys to getting the analog chip to work correctly as it is... So first of all, we had hundreds of tests harnesses and validation tests in simulation before we taped out the chip. So these test harnesses would actually be mixed signal simulations where you use the digital control circuitry to set up the chip into some state. And then we do mixed signal analog simulation to show that simulated chip would then go on to solve an ODE. So that was a huge effort in making sure that the chip would work correctly And then once the chip was actually taped out, there's actually still a similar debugging challenge of knowing that an analog accelerator configuration is solving for a meaningful ODE. Okay, and so I could tell you a little bit more about that. So let's say that we're trying to solve something like a stochastic differential equation like Black-Scholes, right? So what I would do, in the process of knowing that Black-Scholes was correctly implemented, I would take Black-Scholes and decompose the ODE, or the this case the SDE, into all of its different components. So I would first validate that the noise component has some sensible results, then I would then see that its noise free component has some sensible result, I would see that it has the right exponential growth process and so that I know that each of these different components of the DE, the differential equation, are being correctly implemented on the analog chip and then I combine them together. Audience: Yeah, thank you. So, you, by now you've worked in a few different non-traditional domains as well as like the traditional computing domain, I'm wondering if you have any like high-level takeaways about, you know, is there hope for building shared infrastructure between, you know, quantum analog, boolean domains, or do you have thoughts on that? Or do you kind of just think the future will mostly just be highly specialized compiler language stacks? At least for the near term. Yipeng Huang: So what I advocate for... So, I think that's actually a much bigger problem I don't have a good answer for. But I could try to narrow down the scope. I don't think it's...I think there is hope for different types of shared infrastructure. And one of the challenges of harnessing unconventional hardware is to expose a sensible interface so that programmers can target it, right? And so in the analog context, what I advocate for is that the sensible interface is some kind of mutant solver or some type of problem specification for a nonlinear system of equations, right? And so, so long as you have that kernel in your program code, then it would be sensible to retarget that to a, either a digital solver back-end or analog solver back-end. So I think that interfaces, going forward, is not so much, it might be in the form of shared infrastructure, But it also might just come in the form of shared problem specifications which then have different backends that support those problem specifications. Mark Oskin: Alright, let's thank our speaker again and takes everyone for attending. Yipeng Huang: Thank you Mark. 