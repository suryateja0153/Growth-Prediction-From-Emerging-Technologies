 so with Nigel and Mike you've really seen kind of the state of the art right now that startups are pursuing the data center as well as on the edge really pushing the limits of what's possible with performance and performance per watt one thing that that Nigel touched upon was the Colossus this first innovative computer and that really kind of speaks to the heritage that the UK has in fundamentally developing computers in computing architecture and one fundamental type of computing architecture of course is quantum computing and quantum computing is something that if we can solve we would really revolutionize the kind of problems that all of humanity can tackle and it's great to see that the UK has its own a representation in terms of a startup that is pursuing quantum computing so I'm very happy to welcome Alana whispy on the co-op walk Oxford quantum circuits Alana please come on thank you so much for that kind introduction so we are an Oxford quantum circuits and we build quantum computers we specifically build quantum hardware so we're taking a bit of a change of pace from the some of the a I chip type stuff we've been talking about but in actually listening to those things there's actually quite a lot of parallels compared to the processes that we're currently going through we're just a lot earlier so just a show of hands who knows anything about quantum computers oh nice excellent job so this is kind of aimed at on the introduction of quantum computers with an AI kind of tilt to it but here goes so first of all for those of you that aren't already interested in quantum computing why should I care about quantum computing well as a previously touched on quantum computers will revel revolutionize the kind of calculations and provide an entirely new form of computation that's fundamentally not possible with classical comply school computers so inherently a quantum computer can for example simulate at the smallest scale quantum interactions between individual atoms of matter which you fundamentally cannot do with classical computers even if you have some of the world's biggest supercomputers there are a number of problems you cannot solve at the moment so as a really exciting side of it on the other side you've probably heard about speed-up so the potential of speed-up obtained through using quantum computing is also a really exciting element so it's very strange because the rain is quite strong so I think as it's been already touched on the opportunities for quantum computing are very very diverse so we tend to think of this in two ways so on the left-hand side here we've got the quantum laboratory this is anything to do with simulation so if you have any type of materials design process this is where that will be really useful and of course that's got applications across battery technologies across healthcare with the drug discovery process for example catalysis fertiliser trying to understand for example the nitrogen fixation process and how you might do that at room temperature without the incredibly high amounts of energy and that currently is required within that area there's all things that quantum computing could do on this side of things we tend to think about early stage applications appearing with commercial viability in the next five to seven years on the other side of things we have the quantum boost so with this this is much more about portfolio and sorry about any type of large-scale data processing and speed-up in that sense so in this way you're utilizing the fact that quantum can provide parallel computation as opposed to having to do linear computation and of course that's got applications across many industries and particularly around recommendation and optimization engines for example but of course the key point here really is that the applications are wide reaching and yet unknowns we're still at quite an early stage point in this process so as we begin to develop Contin computers and we can start to play with them will more and more about the kind of applications and opportunities that quantum computers can provide so touching on exactly specific touching on what might be more interesting for an AI type and this is where I'm a hardware girl I've got PhD in experimental quantum physics so the algorithmic side is something that's that's less in my comfort zone but fortunately we have some very good partnerships and collaborations with people that do work on the quantum software side that have been able to help me understand exactly what this might mean fundamentally for quantum computing in AI and for example these are some of the things that we've got so with the top one here with data generation with quantum computers you can start to do generate new data on the fly this would be incredibly useful for say materials properties and understanding those without full sets of data on the other side with model representation now this one is a little bit more contentious it's much more kind of as an academic type level but with a quantum computer you could start to think about simulating systems whether they're quantum systems or classical systems so this one's a little bit more contentious simply because a lot of the a lot of the advantages that we self quantum computing are based on quantum computers being able to simulate and replicate quantum systems so why would it be better at simulating or replicating regular classical systems so there's still areas of exploration to be done in there the final one of course is quantum speed-up so this is much more around large-scale data processing with recommendation engines that might start to provide solutions to say for example the Netflix problem especially when you have incomplete data sets there was a paper out I think there was like some 16 year olds that said that actually quantum speed-up doesn't exist so it's been some debate about that but apparently it does so we're all good and actually on the speed-up side if we start to think about so my understanding is that we can do Lenny linear regression like pretty pretty well but actually if you start to think about topological solutions so larger scale or broader scale feature recognition that's not a quantum computer would be able to do and there are a load of academic papers to back this up now I can move on to things I feel much more comfortable talking about and then I'll turn this on its head and say what can a I do to help develop quantum computers at the end so a high-level overview of actually what quantum computing is it's based on quantum phenomena which is very very well described by finance so if you want to learn more about this check out the Fineman lecture series they're online they are absolutely incredible he communicates concepts in a very accessible way but then backs it up with a buttload of math so I really like that approach but we're gonna summarize it in two slides so quantum phenomena we need to think about the fact that energy is now quantized so energy is now discrete as opposed to a continuum we need to also exploit the concept of superposition so this is where a quantum state can be simultaneously in more than one state at the same time a little bit that a bit more in a second and then the final one is entanglement so this is the fact that if you have one quantum entity here one quantum entity here you can join them up and they will behave in tandem with each other or dependent on each other even though they're not physically interacting we all got that great so we start to think about the quantum bits this is the building block for the quantum computer the quantum bit is typically described through this thing here which is called a block sphere so I like to think about this as the world so if we have the North Pole we have the South Pole we know that humans can exist all the way around the world and can operate in parallel will here some people are in Tokyo some people in Australia and that's very much the same with a quantum bit so I can define a state and it can be either a zero state or a one state or it can also be anywhere else on the edge of the world and then in that way you have a vector and you can have many of those vectors in superposition all around the edge of that Bloch sphere and calculate them all at the same time so you therefore don't have to have one bit with one piece of information you have one bit with many pieces of information operating on the same sphere and yet this is basically the the fundamentals of quantum computing so you're doing well there was some maths we started to think about describing them with Hamiltonians which you'll probably actually way more familiar with the typical audiences but yeah so there are math behind this that bottom one there you have a wave function that's describing so the zero state is having it back so which is up the Wednesday it's having a vector which is I wrote it down and then if you have a wave function of a mixture of those states for the given amplitude that's the point where you're operating around the edge so how do we think about building a qubit a quantum bits start operating on well there's actually many different ways in which you can think about building a qubit fundamentally what you need is that discrete energy levels splitting so you can have two energy level states that you can start to operate between them and start to manipulate them and nature actually gives us a numerous like number of natural qubits so if we think about silicon quantum dots natural atoms impurity spins trap ions they're all systems which give us a qubit state that we can start to manipulate and start to do computation with also a quantum circuits we work with superconducting circuits which I am biased but it's definitely the best approach to quantum computing superconducting circuits are man-made system so they're engineer able they're very much similar to standard microwave circuit type things but of course they use superconducting metals as the name might suggest and this is actually an artificial a turn so these ones are natural atoms whereas superconducting circuits gives all the maths all the same quantumness behind it but in a man-made controllable an engineer away so you conducting sir we lost the kids okay so you want to think about building one you start with a basic LC circuit so you have a inductor resonant circuit it's someone's laughing it really is this simple it's a really simple circuit it's got an inductor it's got a capacitor and of course they're made from superconducting metals now superconducting metals in order to superconduct need to be below their transition temperature which tends to be around transition temperature tends to be around water 10 Kelvin depends which metal you're working with typically work with aluminium or no avium nitride so you need to get them cool now getting them cool is actually pretty simple so we have these fridges and they work very very much like the fridges at home except they pump on cryogenic gases as opposed to air and in this way this means we can get down to milli Kelvin temperatures very very simply and there are numerous suppliers that provide these things off the shelf so we don't think of that as a problem what we then need to do is take it from that 910 Kelvin which is minus two hundred and sixty three degrees down to 30 million or ten million and at this point we start to open up that degeneracy safe so we start to open up that quantization so then we have two discrete energy levels that we can start to manipulate around because quantum requires very low temperatures or very low energy environments so it's not obviously just an LC circuit there has to be something more than that and actually what makes it quantum is this magic component which we call a Josephson Junction now a Josephson Junction is basically an in a superconducting layer with an oxide layer and another superconducting layer between it and what happens whether you have a superconductor is you have electrons that then buddy up into what we call cooper pairs and they then begin to tunnel through this oxide layer in a specific way which gives a non-linearity which then makes this LT circuit not just a Stan resonant circuit but then makes it into a artificial atomy or a quantum bit so then all the maths becomes beautiful and as I said if you read Fineman you'll read the maths and it's beautiful this is some pictures of what they actually look like so we have one here from Google we have one here from IBM we have one here from Righetti in slightly poor resolution so sorry about that we also have one from OTC and we'll talk about that a bit more and this is the fridge this is where it operates so we operate the circuits down here at around 10 million as you can see they don't look too scary the background here is typically a superconductor that's usually as I said aluminium or now you might try it this is all done on substrates typical silicon sometimes sapphire which again isn't too bad and all the processing in developing these kind of circuits is very very similar to standard micro fabrication the difference being that Josephson Junction layer which requires a double angle evaporation steps so there's anybody here with a clean room that wants to start thinking about the future we should think about adding in a double angle evaporator and then we'll all be dandy and we can all build thousands and thousands of kibbutz and I want to work with you so of course that's about the quantum the qubit the device itself but you can't just have a quantum device and then it will be dandy what we need to think about is creating a whole computer to support that so this is kind of my diagram of what that begins to look like and where QC operates so of course you have your end user you have a problem at your end user level then of course that problem must be converted into an algorithm which is I'm at something to solve and then of course once you've got to that point you then need to compile it into something into signals that your quantum computer can understand so at that point it goes through your quantum processor there's some control software some control hardware onto your quantum device your qubit is manipulated it's around and algorithm is done it comes back up again and we weed it out and we understand a calculation has been performed and we can look at the signals that have come out versus signals that have come in and we can understand that so Oxford quantum circuits we have a very unique quantum device it's unique qubit architecture so that's kind of a founding premise of the entire company we're also working on some of the control software and control hardware layer because we want to develop quantum computers we need that and ultimately our quantum computer will be accessible through a cloud system at this kind of layer here you have to do some part of the compiler so people can access it meanwhile we have quantum software people working on this side of things and end users and it's as simple as that so why don't we have a quantum computer today that's useful so to clarify there are plenty of continued peace in the world we just can't do anything for mostly useful with them yet in order to get to a point where we can do something useful on them we need to have systems that are bigger so typically 50 to 250 qubits we can start to do commercially interesting problems we need to have systems that are a higher quality there's a load of terminology that you can get bamboozled by about fidelities and error rates but ultimately they just need to be better you also need to be able to control and manipulate every individual qubit very well as well as turn on and off the interactions between them coherently and that's something again which is quite challenging so achieving all of these three things in parallel is basically where the industry is at we're all trying to work on developing bigger systems whilst maintaining that high quality and also facilitating that control so ops with quantum circuits we have a unique approach to this you saw some of the other circuit designs skip skip skip here and what you can see is you have your qubit and then you have your measurement and your control elements that you can pass signals on and off them and manipulate your qubit and you can see that they've started to scale them so you have more and more qubits to your device if you scale in this way currently the quality of the devices goes down which is a problem and it's a problem that everybody like IBM Righetti Google and others are all trying to solve and they're doing this by taking inspiration from what exists currently within the semiconductor industry so suit through silicon fears or through flip shipping for example but quantum systems are fundamentally different to regular classical systems so if you do that you have some problems for some interfacial layers and two-level systems and quantum noise so it's just not translatable so these are big challenges which people Charlie trying to overcome and what happened was our founder Peter leak who's based at the University of Oxford saw that people were scaling very rapidly and were finding that that quality was d'etre was at was before decreasing there we go hard one and he thought well what would I do because actually what they've done is taken something that existed within academia and scaled it up very quickly to meet some arbitrary milestones that really don't mean very much what would we do if we were doing this for commercial applications from first principles so he developed a novel qubit architecture within the university which is called the coax Mon which if you're physicist has some extra meaning and the Collatz mon as you can see looks pretty different to those other circuits so instead of having all of that control and measurement in that same plane instead we've taken it after the plane so we have this really neat unit cell that's very scalable so you can just begin to copy and copy and copy and copy it across you could even think about having 3d arrays if you really wanted to and we've basically decoupled everything so you have your key bit on one layer you have your connect connectivity there on another it's completely decoupled it's completely scalable it's completely flexible because as you add more in this way you're not adding quantum noise or mess near it that's what our foundations are and that's what we're currently doing atoms quantum circuits so we are currently building I've no idea how much time I've got left quantum computers I'm out but we apparently building 20 computers to a bigger scale right so this is exactly what we're doing okey see final thoughts were again bringing it back round to AI what can a I do to help build the next generation of quantum computers well we can think about automated searches for plot sequences for example circuit device designs you can use AI there for sure people already do as well as through smart compilation of arrow mitigation and with that I will call it a day just to note that we are hiring and we are also fundraising right now so have a chat after if you want thank you this is working okay Thank You Alana the quantum computing talk where you kept the crowd which is really um something new for me this is a really really break Harry stuff we're gonna have a I'm gonna ask Nigel and Mike to come back on we're gonna have a panel discussion to to kind of dig into some of their their products and afterwards we do a bit of Q&A so that you can get your questions answered as well that's quite a storm okay it's great hearing all of your individual presentations I think maybe just to start off recently Hennessey has been saying that we're in a golden age of computer architecture and that that's um and we're seeing that with a reflection of the amount of startups that's happening now and building new AIA companies do you agree with that and what does it feel like building a new chip company today versus maybe a decade ago or during the private days at the Silicon Valley why thank you very much yeah I was I was a very early employee at a company called altaira back in the days when FP J's were being created and you know that was an amazing experience the company grew very rapidly you know it was a new technology that kind of transformed the way that people did logic design and you know there's definitely strong parallels with with what's going on now but I think that this is actually more fundamental this is it's it's like the 1970s when micro processors were first coming out which I don't remember but you know it's it's just this period of new compute which needs new architectures and so there's lots of innovation around that and I think you know there's obviously gonna be some big companies that are created as a result hopefully from here in the UK I'll just quickly add that um when we first started the company its 2012 and we wanted to be venture backed and we were watching the amount of venture funding going to semiconductor companies year after year just approach zero it was very disheartening I could never believe in a million years it would have just all of a sudden skyrocketed up but it just goes to show that you know it is a new wave of technology but it was it took many many years of persevering through the dark days of semi funding for sure mic in 2012 alex net was just getting invented was the original idea of mythic actually to go after neural nets or did you start off somewhere else and you actually found fit with AI yes actually I think the reason we exist right now is that we started off with a simpler problem so we were actually the very first days of the company building GPS receivers for the Air Force and there's a correlation step in GPS that is similar math to the matrix math in AI but much lower precision so we got it working at a lower precision and then we said enemy but then Alex net came out and we just pivoted the whole company we looked at it and we said that there's no way this is not going to be huge and then okay so now all we have to do is get the precision from three bits up to eight bits the problem of the analog is that for every bit you go up it's like two times as hard so it actually took us four years to get that working but it was focusing on an easier problem first it actually got us to the solution we have for analog computing with um with maybe you can tell us a little about the founding story of Oxford now circuits quantum circuits sorry both sound very techie as you mentioned in your slides there are a number of companies worldwide that's pursuing quantum computing how did Oxford come up come about and what are you all doing differently than maybe your global competitors yeah sure thing and so as you said there's a number of companies that are working in this space and there's also a number of companies working so we specifically do superconducting quantum circuits but I touched on other approaches so there are other companies going for other approaches to quantum computing what happened basically was you reached a trigger point within academia that it's no longer suitable for an academic space and with that I probably happened about five or six years ago and actually d-wave were the first kind of quantum computing company that really kicked off investment in that space and since then there's been more and more so IBM had a research element that then commercialized its own technology in the last few years so effectively what we've seen is a lot more attend come across to quantum computing within the superconducting space but nobody has met their kind of quantum supremacy which is everybody hates it's a terrible terrible milestone presented by Google unfortunately huh yeah and that those milestones were never met people started to get disheartened by it and what happened was our academic founder from the University of Oxford Peter leak saw this and really wanted to revolutionize what was going on and think about it in a different way more commercially and so that's really where we came from with Moore's Law slowing down we're seeing a lot of companies pursue different means of accelerating compute as we get smaller and smaller does this help startups or by kind of closing the gap since Intel can't always be ahead or does it hinder you how does this dynamic play out for for you guys yeah so I think I think it helps us because again we're in the inference space and we're especially going after edge inference right and you're in it that's an area where there's a lot of fragmentation a lot of different opportunities but also a lot of cost sensitivity and there's a lot of companies that just can't afford to you know go to steady their semiconductor nodes and so you know they're gonna license some IP from somebody for their SOC and build a seven nanometer SOC you know we just don't see that happening and so you're seeing I think I think it's gonna help us in two ways I mean the first way is that we're gonna have competitors also try to build accelerators for edge inference and they're gonna find the only way they could come even remotely competitive with you know is to be in the state-of-the-art nodes you know they'll have to be in seven or ten nanometer but that's just gonna create a product that's way too expensive you actually see it where a lot of the other edge inference accelerator companies that are claiming these big numbers they all seem to be going after automotive because right now they everyone thinks automotive is not very price sensitive I think the second way it's going to help us is really this chip lit revolution where people are gonna Mitch and Mac silicon as opposed to building this big monolithic SOC and you know considering we're gonna be selling a variety of different sizes of silicon having a chip lit strategy can be great for us it'll have a little give us a lot more doors to go in in order to win sockets I think it does help the startups so what happens is if you look back you would have semiconductors we just improve every two years also driven by Moore's law and architectural innovation would would be harder to bring to bear and it would be the big companies that would drive forward you know leveraging Moore's law to just continue to push their architecture forward and forward probably Intel being the classic example so as Moore's law slows down that obviously means we need to rely a lot more on architectural innovation and I think something which is far more fundamental is is actually what we call Dennard scaling which is the fact that we've actually reached the power density limits of silicon and and so we've actually hit those limits ninety nanometers like 2006-2007 so your laptop now has tens of processor cores but still runs at two gigahertz just like it did ten years ago but how do you get from ten processor cores to 100 process of course how do you get from ten mm process of course that needs a fundamental rethink in computer architecture and it comes at exactly the same time as we start to make breakthroughs in machine learning where suddenly we need these parallel computers and it's fundamentally a parallel problem so it really is about creating these new architecture that fit in a new world I believe that we've still got quite a bit more to go with Moore's Law it's just going a bit slower but what we're really dealing with is a post entered world and that's really what we've got to try and design around from the quantum side I guess silicon is when we talk silicon we talk about Moore's law when we talk quantum computing I think I've heard of a different law where you also get this kind of improving performance every X months either Rosie's law or something to that effect I think it's like tracking the number of qubits how do you track progress in quantum computing not with a number of qubits yeah so actually quantum benchmarking and understanding progress within this sector is something that is a really really good question right we need better benchmarking you can't say you have five cubits 10 cubits 100 cubits 2,000 cubits and therefore each time your quantum computer gets better you know behind that scale quality and control there are a number of interdependent very technical parameters which you need to optimize across each of them right so actually finding a way to benchmark and measure progress is something that we're currently we're still looking at now IBM have put forward a quantum volume which is a really neat way of benchmarking it's and it's an algorithm that's open source that you could think about running on on everybody's quantum computer of course it's still manipulatable to optimize for optimized for a specific computer but yeah that's a really great question and there should be a better answer Mike you had a great quote for from a competitor of yours from Cerebrus about the changing nature of workloads in AI when when you all started your companies I'm sure you had a suite of benchmarks instead of like these are the benchmarks we want to win and if we do they would have a chance of competing against Nvidia and others how has the nature of workloads changed since the design the initial design of your chips graph course on the second generation now you guys about to sample have those change in a way that's really surprise to you and are the assumptions that went into building the initial products still good for what's happening on the word of AI yeah actually I mean it it's interesting you you asked that because in the earliest days of mythic we were saying 100 X 4000 X better and that's because alex net was so driven by fully connected giant dents fully connected layers and those fully connected layers do its work extremely well in our hardware whereas you know convolution the digital the digital approaches can get some leverage some advantages out of convolution and so you naturally saw everything move to convolution because it ran so well on on digital hardware and you know these later networks that are coming out try to get more and more you know weight reuse and data reuse and things like that and so you know the workloads did change from the early days but I think it's it's I still say that's okay I mean because again our philosophy is just give a vector matrix engine that's incredibly fast and powerful and you can run convolution you can run fully connect it you can run RN ends and things on that and keep up with where you don't want to do is you don't want to make really domain-specific accelerators where you literally built like a ResNet 50 accelerator chip but in reality that's what other companies are doing when you see these really big claims for numbers like resonate 50 you look under the hood yeah it's full of like why no grad 3x3 accelerators and things like that and they're gonna fall into a big trap with that because the latest network that came out of Google is gonna completely falter on that time for accelerator ii with the chart you showed the second fastest next to yours is the Habana chip have you guys looked into that how are they they seem to be quite a bit faster than video which is impressive but how are you feeling versus the competition like that so that was energy efficiency I was showing that was not performance and so you know the two the two numbers that we really benchmark is performance per dollar and performance per watt that is an X that's in my mind really what matters for inference all right so performance per watt you're seeing like the really four edge you're seeing a really focused like 3x3 convolution accelerators start to you know get close to us I'd say within five to ten X of us for performance per watt but then they're really hitting this kind of ceiling where you're just not going to get much more out of digital and you're in a state of the art process but then when you go to performance per dollar that's where we all have likely 2010 to 20x advantages there that are just gonna be really hard to recover because we're in a cheap process getting a lot of performance out of it but kind of the point of my speech is the machine learning is a very different workload what most of the startups are doing is they're building a better GPU or trying to build a better GPU incremental in in some way maybe bringing more of the data on chip or you're trying to be a little bit more efficient in terms of the arithmetic mmm but that's not really a sustainable advantage and you know as many who are going even further and building accelerators which are very specific to one workload ResNet as an example that's been spoken before you know we fundamentally believe that these knowledge models will become much more complex and that's really what we built the graph core architecture to support it can do anything a GPU does because you can connect the cause together and they can replicate whatever a GPU does and it will do it as well in many cases better than a GPU but what it allows you to do is to create the next breakthroughs and that's really we think very fundamentally important the other thing I think we're seeing is the size of the models is increasing we're in a phase at the moment where people are exploring the size of models that's something that's really been driven by TPU where you've got lots of TP use being able to connect together so we're in a we're in an exploration phase around how big should models be we fundamentally believe they don't yet too large because you know they just become very very difficult to train but actually what you end up with is ensembles of models you know many different types of models all working together it's kind of what happens in your brain not that we really know exactly how the brain work I certainly don't but you know it's these ensembles of models work together but it's it's lots of processes connected together to be able to either train or deploy and then and I think that is the other big difference as people talk about inference as though the models been trained and then it doesn't change I think that's completely false what happened is that the models we'll need to learn from experience you know if a human kept on doing something and they kept on doing it wrong and you kept telling them you're doing it wrong you wouldn't think they were very clever if they didn't change if they didn't learn from that experience and and the same will be true with AI will really need to learn from experience even in the deployed phase so continuous learning and being able to improve you know it's a big issue in automotive at the moment where many of these models are being created for the internet it doesn't matter if you get the cat wrong you know the accuracy of the model is is is not that important whereas in a car you know you want to be safe and you want to know why it got the answer wrong and what have you done to improve that model so it won't make the same mistake again so things like statistical environments around the models that use Bayesian approaches you know one of the things we built into the G into our IP U is the ability to create lots of random numbers entropy which becomes very important in these kind of statistical systems last question we'll turn it to the crowd for Q&A the the way people treat kind of tected the relationship between technology and government I feel like has taken a sharp turn it used to be that if you're a technology company and you supply it to the government that was an okay thing Raytheon is considered to be an okay company but now a lot of technology companies are very adverse to being providers for government and defense I think I heard one AI startup companies say we're very happy to sell our products but if the government asks us I think we'll say no and this seems like a very dramatic change in sentiment and especially when arguably if you're if your enemies have AI technology and you don't that's not exactly serving the interest of your country so how do you feel about this would you be comfortable having the government and having the the Defense Department being a customer and how would you rationalize and maybe communicate that with your employees I'm so simple so we we have investment from Lockheed Martin we have investment from Inc you tell which is a u.s. group that bridges you between startups and government entities I think that you know us as our leadership team and me and my co-founder when we you know when we saw what happened at Google and their response and then we saw what happened at Microsoft and we saw Satya's response where he defended working with the government we felt really strongly with Satya's views and I'm not gonna try to quote him I can't remember what he said it was very eloquent and if I tried to spend something out it'd be probably a lot worse I'd say just go read what he said and then that's what we think so I think it's a it's a difficult line and we as a hardware company a slightly struggling this because the other day if we provide our hardware say to a large hyper cloud and then they go on and use that to serve government how can we really control that you know we might have some control might be able to exercise some control actually the biggest control I think is by being at the leading edge and being one of the major suppliers that's how you probably get a voice but but I think it's it's also difficult because you think about surveillance you know on one level surveillance is good keeps us safe but where does it become spooky you know and it's it's really it's about where do you draw those lines in terms of the use so I think it's super important that has a community that we really get involved in the whole ethics debate that we we challenge people on how this gets used we challenge people on data and how it gets used you know and it's really weird isn't it that you know you're happy to give you data to Google you're happy to give you data to Facebook because they give you a service in return but you're really scared if the government uses your data whereas actually some of the health data here in the UK the National Health Service hold you know is phenomenally important and it's all anonymized and and you know very safe and could be used really for incredible good so so there are all kinds of lines around which government what's it going to be used for it's a very difficult argument yeah of course this is equally applicable to the future of quantum computing and the UK government has taken a very cutting edge approach to providing a quantum computing program so the UK had the first quantum computing program which phase one has just finished phase two is just kicking off I believe there are some announcements today or this week around that further so they've been really supportive of taking the quantum industry out from academia and into commercialization of it there are of course many kind of questions around what that looks like going forwards whether we would supply to the government or not is a question that once we've got a useful quantum computer we'll definitely have to start thinking about what but yeah they're interested they would want to buy one of our computers and the other side of it is government funding for example and finding investment within this area and the you know quantum computing is a sensitive area therefore there are certain sources of funding that we can and cannot take certain sources of funding that people may or may not feel comfortable with and that's something that's really really relevant to to our area right now okay we'd love to take some questions from the crowd I think sir microphone ready to go so please raise your hand mr. Nigel you started your presentation with the thinking of decision or the human mind is working totally different than what the current technologies is working with so what do you think about artificial neural network is this the this is the good approach or we need to change this approach for something different inspired by the human mind so if you look inside most of the machine learning systems you know what you're finding the fundamental underlying structure is a mathematical graph which is really defining how the data is connected together where the the vertex is that the neurons if you like connected by edges the spine apses if you like that the neurons that the vertex is really tensors and you're doing mathematical operations on those tensors you know so whether it's a convolution or some other kind of transform function for example on those graphs so I think you know there is really the way we need to think about how we build these systems the brain is just one implementation of how you build a graph computer we're obviously building a graph computer in silicon and we're going to build it very very differently and and so you know there's things you can learn from perhaps how the brain works but also you know you're inspired by the different things that the brain does different parts of the brains work in different way the plasticity in the brain the fact that you know for example you know my example about tracer may earlier you know what tends to happen is you hear these very famous people you almost grow and you are on for that person in its parks near lots of other connections in the brain and so you know from a from a silicon point of view you know we need these highly parallel systems that can learn and can adapt but they're going to be different from what we're building you know in wetware and then in a in a human hi are there any unique fabrication challenges any of you have faced and how have you overcome them sorry what challenges fabrication fabrication challenges application or fabrication fabrication fabrication oh that's a great question I think the you know we did an initial step in 2012 or 2013 of selecting what was gonna be the non-volatile memory technology that powers our architecture that lets us cram 8 bits onto it with enough retention to at least get a day or two before we have to refresh it and you know there were there are a number of technologies that fit the bill and when you stack them all up you see like face change in our RAM and flash memory and flash memory jumped out at us is you know very production stable and you know mature technology the problem we actually ran into is more of a business problem than a technology problem because flash memory was very much a commodity highly protected you know trade secret driven piece of technology and when we went to the initial foundries and said we want internal access into your flash memory IP they all told us to go away right and actually the back on us uh actually why a lot of academics went to our RAM and like logic flash and these other approaches was they just couldn't get access to flash memory because it's all very proprietary we finally got through with Fujitsu and they gave us access and you know we're off to the races but that took years and years to get that through so yeah there's a lot of challenges when you're doing something new but probably I don't know I'd love to hear the quantum side of it yeah I I think it's constant as the big challenges you know we're building in standard silicon technology billions has been invested in how to build to look in a scale and we just leverage that we work with TSM C as our fabrication partner and and leverage their technology but in Quantum's the difficult one there are still many similarities so I don't like stand him oh yeah it's so left-field but we couldn't possibly do it we do take a lot of inspiration from the semiconductor industry specifically that Johson Junction component with the aluminium the oxide layer and that's the process which at Oxford University we've got some of the best reproducibility of it's like less than 3% which is some of the best in the world but actually being able to reproduce the thickness of that insulator layer and have it very well bonded to your super conductor layer themselves is a challenge which I would say is still got to you're going to create tens of thousands of qubits that's not an area which needs to be improved we do kind of combat that by also making sure we develop in a modular way so that you can kind of have that redundancy built in yeah I guess if we you know we work on silicon so that's that's useful but obviously when you then first go through creating qubits with different materials you have to go through the same development of recipes a lot static analysis so when we're starting to think about how to build higher quality qubits from the first things we've done actually it's higher and we're still hiring on people from industry that are used to developing new recipes from semiconductor industry right so that they can apply the same you know 3040 year processes and development to these systems so we can continue to improve them robustly in that way question to my tech company we have working Khalid silicon was its first one yeah we do so it's basically if you think of our architecture as a big tiled system and you know I showed up there 108 tiles connected with a network on chip and we also had we have like a RISC processor in each tile we have a vector engine in the digital domain all of that has been what was been under development for the last year to year and a half the analog portion we've been working on for seven years and the latest silicon we have is is I think our sixth version of it and that's the one where you know it has all the analog it has the digital analog the FPGA basically serves as the digital architecture that attaches to the chip but with that chip we're able to feed in I think it's a it only has 4 million weights of capacity so we were able to do an amnesty roll network on that chip and the digital version of that got 98% accuracy and our analog chip got ninety seven point nine percent accuracy so we're able to you know and we're able to like move the temperature around and show that it contract temperature and not lose accuracy and things like that so the proof that we have all you have to do is say ok now can they build a digital architecture around that and if you look at our team yes we can so that's what's left and we're we're like I said literally days away from production tape out though as Nigel - say we were on a panel a few weeks ago and I said the same thing days away so I need to be very careful here but according to my engineers we're days away so we'll be sampling later this year the full product I heard Mike mention the use of risk 5 in in designs I wonder what the panel thought the role of open-source Hardware would be in you know I guess most of the open-source processor projects have been focused on a fairly specific opportunity which is how can we go after our arm and just make something which is free particularly in the IOT market yeah so the process that come up with you know they're reasonably flexible Habana for example as a snake in a @n silica core and has kind of played with that to get something out quickly but but I don't think it really hits the nail in terms of actually architecting something which is really designed for this space it's not just any old processor will do it's you know for example we are taking a piece of data from here in the memory a piece of data from there in the memory bringing it to the computer writing it back somewhere else and you know orchestrating that all within one cycle in the process different transcendental functions that you need to actually support the different machine learning functions random number generation inside the processes it's a very different process of design but at the end of the day it works with standard LLVM compilers it runs a complete see a plus plus program and has many similarities to other processes but I don't think you just taken off the shell processor and build anything that's going to be long-term competitive as a machine learning processor yeah for us I think it's a similar to what Nigel said that the bulk of the work was outside the processor and so we have a risk 5 processor in each tile that thing is only running at about 250 megahertz it's doing a lot of bookkeeping and it did shortcut the development for us and it saved a lot of money going with arm you know so we were able to have a compiler up pretty quickly but again that compiler and that stuff running on the wrists 5 is not doing a whole lot of really complex stuff we had to put a lot of thought into our vector engine into our matrix engine so it but it did it did actually save us money saved this time so you know we're happy with it hi I've got a question for Elana it's kind of themed with the temperature of the tent how do you intend to scale the cooling I mean your technology depends on getting down to super conducting and cooling temperatures how do you do that in a data center or even worse a handset yeah that's a great question actually there are the companies that are currently building these cryostats have much larger systems and they're very very hedge on the development it's not necessarily the cooling in terms of scaling that I would be so concerned about because you can you know you have these bridges you can keep them cold for years right so you could think of a multi fridge type system which is kept very stable very cold without without there being too much problem it's actually more around the say stick getting the signals down in an efficient way within that space without compromising the cooling power so he starts to think about the need for multiplexing of signal processing down wires so you don't have a single wire per cube like that's not scalable that's an area I would say is much more of a concern to me than how are we going to build big stable fridges hi I have hue schhh and also to Ilona like also about temperature is it possible to build a quantum computer which would operate at room temperature yeah absolutely so you use other types of technologies for example ions they operate on big laser tables at room temperature yeah they exist if you do silicon you still need cold temperatures because ultimately with quantum you need that very low energy state so that it's finding a way to achieve that state you either need low temperatures or you need energy thank you very much for an interesting panel can I ask what are your aspirations for your business should I think of you guys as trying to be like the next Intel or do you want to be a a kind of Big Bird notable niche in a specific bit of the market I think this is the future of computing and there will be big companies that will look similar to an Intel but in this space so to the extent you think this is Aneesh actually I don't think it is I think this is becomes bigger than computing today then big companies will be created that certainly are ambition I think there are a lot of startups out there who are you know fairly cynically trying to build something quickly to get bored and and flip and make some money you know we've raised 300 million over 300 million dollars in venture last around 1.7 billion valuation we've got other people wanting to address than the company you know we're in this for the long term to build a big company and make it very successful yeah I mean basically reiterate everything Nigel said I think it's very rare that these big technological waves come through and every one of these waves you know a few big companies get create out of it and the fact that you know we all the way back in 2012 when the first Alex net paper came out and we said oh we think this is gonna be huge and we pivoted everything through there you know all the technology we developed we pivoted to AI you know we had to go through four years of going in front of investors and I'm saying who the heck are you what the heck's a neural network analog are you crazy what's your previous company experience oh you have none like we went through four or five years of that so we kind of you know we've been through the long slog and we really wanted something big and great get create out of this because we put so much effort into getting this technology to work yeah that's also our ambition right you know you don't have build a quantum you do one specific problem and solve that you we're building a universal quantum computer to solve any type of problems and yeah thank you so much to our panelists for joining this concludes our session please give them round applause you 