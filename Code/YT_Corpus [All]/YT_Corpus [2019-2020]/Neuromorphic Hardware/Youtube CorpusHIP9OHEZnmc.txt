 Good morning everyone we're gonna go ahead and get started even though people are still coming in I am Mary Hall from University of Utah and it is my great honor to be able to introduce the FCRC 2019 plenary speakers these speakers were nominated by the FCRC conferences and they were selected for the cross-disciplinary impact of their work this is a really exciting time for designing systems that can scale to meet the challenges of the new applications that we heard about in the turing lecture last night and that we'll also hear about this week so we're going to look at these applications or look at the systems and we're also going to look at the societal impacts of some of the work energy efficiency and the end of Moore's law that inspired new and diverse computer architectures which have in turn inspired new ways to program and manage these systems today's talk envisions a creative solution to this challenge inspired by the brain's neocortex function our speaker is Jim Smith he's an adjunct professor at Carnegie Mellon Silicon Valley and he's a professor emeritus at University of Wisconsin Jim has been an integral part of the computer architecture community for several decades and he's navigated both the academic world and worked in industry designing architectures for cdc ACA Cray research and now and and also software companies Google and Intel his work significantly impacted the development of superscalar processors for which he received the ACM IEEE Eckert Motley award in 1999 and he's also a recent recipient of the Distinguished Achievement Award from the University of Illinois where he received his PhD he's known by the computer architecture community as a pathfinder see someone who leaps out ahead to explore the next challenges for the field and that's no doubt we're going to hear about today with no further ado I'll introduce Jim Smith okay thank you very much I'm told that I have to do some corrective action here that good okay well it's certainly an honor to be given the opportunity to talk about the things I'm working on I'd like to thank Mary and for this opportunity and also Hillary Bobby Eric I'm told my friend Joel Emmer was also partially responsible for putting my name forward this is this is something I've been working on for a few years and it's I was excited about it now as I was when I started yeah but I must say the you know the Turing lecture is a a rather hard act to follow you know especially given the topic of this talk and and it it then is I guess it kind of appropriate that I that I quote that I have this quote from Carver Mead and the work I'm doing follows this principle in this principle III think it's actually quite different from the principles that now underlie conventional artificial neural networks although although I think no I wasn't there at the time but although I think neural networks you know started out with this principle it's kind of diverged over the years and I don't think that many artificial neural network people today would argue that it really the neural networks really emulate despite their success really emulate the way that the the brain actually works so in a sense you know the work I'm doing is almost like a like a master reset you know let's go back let's start again and and and furthermore you know that what doing neurons in the brain works it has evolved quite a lot since the first neural networks were put forward so let's go back and kind of start you know from this principle and and see what we what we can do it's my opinion that you know that all the good features of the brain that we'd like to have and I'm going to list them shortly rest on the on the underlying foundation if you don't have these principles at the start in the foundations of the of the paradigm they're not going to magically appear later as a paradigm evolves so the motivation I don't think you really need much motivation for this topic but the you know the the human brain is capable of accurate sensory perception you know input high-level reasoning and problem-solving processing and then driving complex motor activity output but it also has a very impressive features you know 20 Watts from an engineering perspective it's incredible it's flexible it learns dynamically quickly concurrently with operation and it far exceeds anything conventional machine learning has achieved and so that so so one of the questions is will the trajectory of conventional machine learning ever achieve the same capabilities or should we look for new approaches on the way the brain actually works and that I guess that's you know kind of where the master reset comes in let's let's you know have another go at it and as further motivation this the title this talk was roadmap so and and if you want to emulate you know all the operations of the of the brain or the neocortex that's a tremendous you know order so I'm putting forward first it's kind of the first major milestone which is you know much more modest in in the objective and so the first milestone temporal neural network is to be able to provide continual unsupervised clustering we're basically by clustering you mean we're going to learn and identify similar input patterns and then map them to concise cluster identifiers and and the point is that training and inference are done concurrently and continually so you're taking a sequence of input patterns you perform inference you you know you basically compress it down to a to a simple cluster identifier but then you also feed the input the output back around combine it with the input and you adjust synaptic weights to either further refine the clusters you've got or if the patterns should change at a sort of a macro level to adjust for that and so maybe maybe the clustering changes dynamically over time it's going to be emerging all the neural operations are local and and somehow global behavior emerges we want a hardware implementation as fast energy efficient and we would like to implement it with conventional digital CMOS now I should emphasize that this is if we achieve this it's really only a processing core it's not a complete system you're gonna have interface this with the external world and for many applications that and itself is going to be a challenge being able to come up with a network like this is almost a you know a two-edged sword because if it works like this if it's truly unsupervised continual etc to some extent it's going to have a mind of its own I mean you can kind of point it in the direction you can you can summarize the kinds or general characteristics of the cluster you'd like to have based on the data input patterns but you don't really have a lot of control of exactly maybe the number of clusters it finds or how they the inputs are mapped on to identifiers it has a mind of its own and in the in the various models have been put forward to this type and including mine there's almost always a pseudo-random number generator somewhere buried in that thing and and I would argue that that's the pseudo-random number generator is like atomic mind of its own it's it's I think it's really kind of an inherent kind of thing you're gonna have here so just brief outline I'm gonna talk briefly about the biological neocortex because we're reverse architecting I'm gonna say a little bit about computer meta architecture I should say that then I take sort of the expansive you know Glenn furred Meyers view of computer architecture where it extends all the way from from essentially CMOS circuits all the way up to application software and architecture is all about defining interface and abstractions and so I'm going to discuss the first primitive abstraction the really critical one the one getting from the biology into the computational domain and then I'm gonna I'm gonna briefly discuss the next layer up which in to be to be analogous to computer design it's like sir like the register transfer level I have some slides on mathematical underpinnings digital CMOS implementation depending on how much time I take elsewhere I may or may not get to that but I'm gonna make the slides available and it's on the it's in the slides also a lot of that material is in the disc of paper from from last year so just a brief overview of the biological neocortex the neocortex is essentially a new shell that surrounds the older you know inner brain and it's the neocortex or interest again it's it's the thing that does the sensory perception cognition intellectual reasoning generates high-level motor commands it's it's the thing that we really want to be able to emulate physically it's it's it's thin only two to three millimeters 2500 square centimeters as talkin says it's like the size of a dinner napkin the folds are there to increase area just one example of the of the great engineer that not natural evolution is about a hundred billion neurons 10 K synapses each although most of the synapses at any given time aren't actually active only maybe 10% there is a physical architecture and it's generally considered that the physical architecture probably corresponds to a functional architecture and so what I'm showing here is a picture you know similar to sure you've seen which is which is basically the highest level which is there's a neocortex itself divided into lobes which are sort of the next higher level of the hierarchy lobes are divided by regions and so the major regions are labeled here in this figure regions are divided into sub regions lots of layers of sub regions at some point when you get closer to the bottom there are macro columns micro columns neurons and I have a few slot the next I'm gonna talk about what those are so now here's a physical Architecture from the bottom up and and in the research to date we don't get very far from the bottom up so we have a neuron at the bottom neurons are organized into micro columns of about a hundred neurons each micro columns are organized into macro columns of on the order of a hundred sometimes more micro columns some number of macro columns form the lower-level regions and then you have a hierarchy of regions sub regions of regions of regions till you get up to the lobes and in the full neocortex a neuron this is actually for neurons the ones that were interested in first here are the excitatory pyramidal neurons the one in the middle there with a shape like a pyramid you know neurons are generally you know named after you know what they kind of look like and so focusing on the excitatory pyramidal neuron there's the body there are inputs the dendrites there's an output the axon and and then the connection points between the inputs and outputs are at synapses it's a synapses where the kind of the the learning you know takes place because the synapses have have weight or efficacy okay so to jump immediately in the modeling here's a an excitatory neuron model that's very commonly used it's an example of a spike response model due to kiss Lauren about Hamlin ninety seven a lot of this this work really started kind of in the mid 90s so there's already like a 20-year history which when you compare it with with conventional neural networks is actually short so this is this is a simple example of the SRM model SRM zero model it's called and and the way it works is you you if you apply a volley of of input spikes or action potential voltage spikes at the inputs and you notice these occur at different points in time time flows from left to right here when the spikes reached the synapses they basically open you know conductive gates you might say metaphorical gates which which caused the neurons body potential to rise according to tor response function and the response function amplitudes are a function of the weights the higher the weight the larger the amplitude zero weight there's no response function at all and the in this model the response functions are summed linearly in the body and if the sum ever reaches a threshold level theta in this example then an output spike is emitted in some cases you never reach the threshold there's no spike now one of the beauty of this of this model from a from a kind of theoretical modeling perspective is you can use any response function and I've shown four of them here they weren't doing at the top there is a bi exponential function it's it's closer to the biology and you'll notice I put those discrete dots there and that's because I modeled this thing in my modest thing as a sort of a coarse green but discreet model because the actual lab actual biology only has a resolution or a precision of about three bits four would really be stretching it and so I start out from the beginning with a sort of a coarse grain discrete model it's it's immediately a kind it's basically a a an integer model that's based on discrete mathematics and that in some way sets this the work I've done apart from other people working this field which starts with you know real mathematics and the disparate ization doesn't happen at a convert to floating point I start out with with a coarse grained discrete model from the very beginning okay other other response function there's this on the on the left there in the middle there's this triangular thing which is sort of a piecewise linear approximation to the by exponential there's on the right there there's there's a step nolleke function which a lot of people actually use on the bottom there's a but I call a ramp with no Li which happens to be the model that I'm I'm currently using well so if we run a reverse architect let's talk Matta architecture okay so engineering as all the people here know engineering highly complex systems requires abstraction and conventional computer architecture again taking the sir the broad view of architecture contains lots of levels of abstraction and if we kind of briefly walk through those Thursday at the very bottom there's this really fundamental abstraction from from Electrical you know CMOS into the logical domain logic gates the next layer in the computer conventional computer domain is to what we typically call the register transfer level or RTL level where that where the blocks are al use you know shifters counters registers things of that sort and I guess my observation here is that when we do computer design we don't literally design with gates we instead more you know start more at the register transfer level and then from the register transfer level we build functional blocks and blocks of blocks of blocks until we have the entire computer hardware system and then there's another no fundamental abstraction from hardware to software where where we where we shift into machine language and again we don't write code in machine language we use the next higher level hll statements we bite procedures and procedures procedures oops until we get all the way you know up to the top of the of the hardware stack so we have these sort of fundamental abstractions where it seems to be just all we can do to kind of get over the hump to get through that abstraction and then we kind of design at the next layer and then once we're there we just sort of stack up layers in a kind of a uniform way okay well in the in the in the neural architecture again we're gonna we're gonna have to rely on levels of abstraction if if there are no layers of abstraction from from our you know from the human perspective trying to understand this it will probably be hopeless fortunately as we've seen the physical are texture actually kind of is been good to us you know it is divided into blocks of maybe 10 to 100 you know each box maybe 10 to 100 lower blocks and in this case we're starting with the biology biological neurons and the first critical step is from from the from the electrical you know biological circuits their electrical in the sense it's ions and biological materials it's not it's not electrons and copper but their electrical nevertheless so that was that first fundamental abstraction to to the model neurons and then we get we get to as I said micro columns columns regions there's a region hierarchy if I want to to a widely you know wildly speculate there's probably another layer sort of fundamental layer of abstraction somewhere because at the lower level is we're going to see that the computation is primarily temple and we don't usually think in temporal ways whenever whenever we have the reason about something that's temporal we tend to cast it first in two spatial terms and I've already done that you know when we show you know that this diagram with time going from left to right we show the waveform we've converted a temporal thing into a spatial thing and we do that so often in so much we don't think about it so so so if the lower-level processing is temporal and the higher-level thinking is spatial something is happening in between and it may be decades before we figure out what that is but when we were able to cross that throw shell that's where we have something that's really an intelligent device okay so the road map here's the long term roadmap we'd like we'd like to start at the bottom of the stack do the primary abstraction and then just work our way up the stack so and and we're gonna in this talk we're gonna we're going to start that process I'm going to do the first primary abstraction do the next layer in two columns and at that point we're pretty much going to be at our milestone which is that temporal neural network in terms of definition of terms that I used to avoid the word neuromorphic because it just you know it was a little bit too fancy for me maybe but I've come to embrace it because I you know I I define a neuromorphic architecture as being something that implements the paradigm and I've tried it and one reason I've embraced it as I've separated it from neuromorphic circuits where a neuromorphic circuit it is simply an electrical circuit that functions in ways that are similar to neurons and this is one way you can implement architecture the inner morphic architecture but you don't require neuromorphic circuits this is the important point to implement neuromorphic architectures ok neuromorphic circuits typically are you know actually literally use voltage spikes they're they tend to be more custom kinds of circuits you don't literally need to do that in order to achieve a neuromorphic architecture near-term let's focus on that first abstraction you know look at results from experimental neuroscience consider models from theoretical neurosciences and quite a bit of work has been done and I've and most of most of this talk is actually drawing from models that exist in theoretical neuroscience but but I guess the challenge comes in there's so many of them and so many things have been put forward the challenge is in deciding which ones to use and which ones not to use which ones make sense to you which ones don't ok so so then you know sort of you know assimilating and all that you know sort of you know information that's out there possibly the set of basic elements and then use those to develop these sort of quasi standard building blocks a kind of the RTL that I've been calling it because it's highly analogous to RTL and then use these to construct an experiment with it but the but the major milestone TNN that we're trying to do that that unsupervised continual you know highly energy-efficient thing that we would really like to have that's the first major milestone deep TN ends and so so as you're doing this research they're really all really there are three layers that are always that are in play the way you modeled the neurons you want to be able to kind of tweak those and you know see how that fits in in the grand scheme and then and then the RTL that what I call columns and then the actual macro column level so that the simian simulation and doing the thing of the day-to-day experimentation you're kind of working between these three these three layers at any given time okay so here's that first that first layer of abstraction and and and the point I want to make here is there aren't that many architecture elements to keep track of if you if you can can model and deal with these five that's a really good place to start it may even be sufficient I don't know so I'm going to go through each of these in turn the first is is temporal coding and that is that the idea here is that information is communicated by transient events in the case of the neurons voltage spikes but they don't have to be voltage spikes that the point is they're transient events in time and and the relative points in time or what mark out the values and so from here on I when I say spike from from a high level you know computation perspective it doesn't literally have any voltage spike it's it's a transient event it's just convenient to call them spikes so bodies are coded by spike timing relationships beginning and here I'm showing in this diagram a volley of spikes time is again going from left to right here's a case of where I you know I'm showing time as space because that's the way we think so so so at T equals zero the first we have this sort of volley or wave of spikes the one the first one at T equals zero we're just going to give that two values zero and then all the others are relative that so you see the second from the bottom that's like one unit time that's a 1 and what the force on the top happens to be for value 4 in many cases a particular line you know that the the axon to dendrite connection doesn't have a spike at all and I'm showing that with the infinity symbol if you want to think of it intuitively it's sort of the spike that happens infinitely far in the future and in in two comments here this this is not a toy example I mean not again we're using the coarse green discrete model so simulation that I do it just uses the C's of small integers you know synaptic weights range from zero to eight and another point is that in practice and the as you get into the into the network the coding is actually sparser than this there are a lot of infinities which which matches the biology it leads to efficiencies and so on now I kind of a key point here is that the flow of time in this kind of model where we're using we're using time differences to mark out values the flow of time can be used as a communication and computation resource and from an engineering perspective the flow of time has you know ultimate advantages okay time requires no space it consumes no energy and the flow of time is free it flows all the time whether you want it to or not you know you kind of feel it right it's flowing but when we when we when we designed conventional computer systems we try to eliminate the effects of time it's almost like time delays are our enemy so we use use synchronizing clocks delay independent asynchronous circuits to sort of remove the effects of time from the equation and and I'm not suggesting that we should have done otherwise it may be that for the kinds of computing problems that we're interested in solving for the technologies we have this is a good engineering choice but natural evolution when it was developing the neocortex was looking at a different set of computing problems I mean the reason we're interested in this in the first place is it can do things that normal ordinary computers can't do and it had a different technology so it so the thesis is that natural evolution use time as a resource because it has all of these and boo and a briefly component compare this temporal coating with putz another sort of alternative that people have proposed and this is actually I think kind of the genesis way back when of neural networks is use rate coding if you were to tell an engineer on the street that you're gonna use voltage spikes to communicate information this rate method is probably the one they would come up with it would almost assume that's what you're going to do where the relative rates of spikes encode the values on this diagram here then I what I'm showing is the sort of a rate method of communicating values along with the but the temporal method that we just talked about on the same biological timescale and what I'm showing here is is that the temporal method the volley of spikes lasts about ten milliseconds experimental results show that you can resolve about one millisecond out of ten there's that that's three bits of resolution on the other hand you know from a practical perspective the fastest rate you're going to achieve is about a about a 100 Hertz ten millisecond clock period so these are the same scale but but what you see is that the temporal method is an order of magnitude faster and it's also an order of magnitude more efficient there are two orders of magnitude difference so so you know it's sort of a parent you know which the good engineering choice would be and not only that but the rate method Gennifer up for a lot of tasks have been measured experimentally would just be simply too slow so so the temporal method has significant broad experimental support the rate method does not okay the temporal neural network model is basically the one that I'm using and other people use begins by encoding an input pattern into into into the temporal you know into temporal the temporal encoding a volley of spikes and again as an initial model the the the temporal neural network computes as a as essentially is a wave friend of spikes it sweeps from input to output at most one spike per line per computation and you kind of see here that the Spyke times that I'm showing you know monotonically increase there's some infinite infinities in there again and round it'll be more more sparse than that at the output you have the output again in temporal form and you're gonna have to decode it back into a sort of a human understandable version now so so what this is is a form of spiking neural network but I want to but I want to show how that this this temporal neural network separates itself from other proposed spiking neural networks and so I have a neural network taxonomy the first two branch of the taxonomy x' is you know III the taxonomy is based on theory implementation the classical neural networks at least originally we're based on rates the the values that are communicated have the same sort of mathematical properties of rates so so so classic neural networks all the way to deep convolutional neural networks rate theory rate implementation if I were to characterize those with three sort of keywords classification supervised compute intensive one class of snn is is the same theory the only difference is rather than abstract the the individual spikes to a value a rate they leave them with spikes use a spiking neuron to do the computation so they have the same classification supervised same applications compute intensive however accuracy becomes a struggle you know these these networks they had a great deal difficulty matching the accuracies of the of the conventional neural networks and energy efficiency is in doubt I would say because yes an individual spike is low energy but when you do rates you have lots of spikes to compute a value the energy energy efficiency is in doubt there's also a branches in taxonomy which is temporal coding which is what we're talking about and use spiking neurons but they borrow training from conventional networks in some cases they literally train a conventional network and then take the weights and move them into the spiking you know temporal network perhaps some transformation same applications you know same keyword accuracy is a struggle but you probably do get energy efficiency the thing we're looking at is this one where the training is different it's localized unsupervised the keywords now are clustering unsupervised simple dynamic accuracies you know improving on accuracy is not the point it's the learning it's the it's it's the it's the new learning capabilities there are probably some energy efficiency benefits for completeness as I'm cycling through the primitive elements here's just a repeat of the excitatory neuron we talked about inhibition is isn't isn't really the dual of excitation and inhibition reduces the body potential of a neuron it sucks potential out and has it so it has the opposite effect of excitation but inhibitory neurons tinactin mass over a large volume someone's called this a blanket of inhibition and so you and there they don't have to be as precise as excitatory ones they're not the excitatory neurons are doing the computation inhibitory ones are more of a more of a control and so a few inhibitory neurons can control a lot of excitatory ones and a typical inhibitory neuron may synapse with the same excitatory neuron 30 and 30 different places including on the body including on the axon so when inhibition kicks in it sucks the potential out of that neuron really fast and it basically throws a blanket of inhibition over the over thing actually an inhibition works at sort of at the column level it it it it diminishes the potential of all the excitatory neurons we're going to call this is typically modeled as a winner-take-all inhibition and I've got a little in animal animation here we've had to look closely so I have you know I have three spikes coming into a set of excitatory neurons and when the first one goes through it triggers sort of this kind of feedback thing which which throws this blanket of innovation over the excitatory neurons so only the first spike gets through only the one that actually triggered the inhibition gets through the others get nullified if you like and so it's winner-take-all the first one get through is the winner and and and it should be observed that this mechanism is proudly built into a sort of a soft synchronization method which I'm not going to talk about but but there are these the brain is always sort of oscillating and this these oscillations are generally in inhibitory oscillations that kind of control in a sort of a soft way that you know that and synchronize the the operation I've got a reference that to an interesting paper on this if you're interested okay stdp spike timing-dependent plasticity that's where the magic is you know if we're saying this is all about this sort of the emergent continual unsupervised learning this is where it comes from and I'm using stdp here and kind of in a generic sense it's it's the idea is that that each synapse updates its weight based only on information local to it okay the only thing that a given synapse sees is it's it's only input spike of course it's aware of its own weight the weight is like a finite state machine the weight is the state and it's also aware that it's neuron the thing it's attached to when it spikes so that's it you know so the only information does it get an input spike and what when it occurs is it's neuron output spike and when it occurs and it's current weight and based on that local information alone it updates the state there are different ways of model is I use a decision tree right where I you know there are only really four leaves as their input spike is there an output spike although the the upper leaf here if there's both an output and an input spike then then the actual spike timing relationship depends on whether the the weight is increased or decreased so so I kind of divide this in a decision tree whether there is or is not any input or output spike at the bottom is sort of the simple case where if there's no spike at all the weight doesn't change I model these things as Bernoulli random variables because I'm using again a sort of a small integer model and that that works well when you read the the saw that the textbooks or the the papers everyone focuses on this on the case where there's both an input and an output and you know if input precedes the output the a and you increase the weight if the input comes after the output you know I had nothing to do with the output so you decrease the weight everyone focuses on this but these other leaves are vitally important it's these other leaves actually that make or break the the paradigm here's a simple example it's telling me that I have no internet okay um here's a simple example I'm this is a synaptic you know crossbar so I have inputs at the left the the neuron bodies and outputs are there on the right and it's connected with a full cross bar with synapses at every cross point and I'm not showing the zero synapses there you know every cross point has a synapse I'm only showing the nonzero ones I'm assuming the in this model and and you know and generally people model it this way the weight distribution tends to the by mortal synaptic weights tend to either be zero or the max and so in this simple example on the input there I'm showing three inputs with spikes at time zero so they're not like logical zeros there times zero and the dashes are like the infinity cases there's no there's no spike and Madore this thing stdev will sort of magically configure the cross points into a decode array in such a way that the most frequent patterns will be decoded and the less frequent patterns won't so so in this case look let's assume it's already been it's already been trained or has proceeded into training in this particular instance when I apply you know that the three spikes on inputs 2 3 & 4 they match three maximally weighted synapses and if in this simple example I'm using the step nolleke so I'm kind of in way I've kind of removed time from the equation but for simplicity so if I use a step nolleke their total weight will be twelve so they'll lift the potential of twelve the threshold is nine you'll get an output spike and because it's stefan nolleke it'll happen at time zero now if you use a more sophisticated functions and in the general case we're going to actually decode clusters rather than individual patterns and so as you apply a pattern you'll get you know you'll be decode out the clusters the stronger the match for the cluster the stronger the more high weighted synapses the early at the output spike okay now those are all the elements the model we're going to put those together and build neural networks so I you know I think there's a lot of room for skepticism that a model for something as complicated as this sort of mess that's on the left of these all these neurons and that are entangled with dendrites and synapses and axons how can you have such a simple model too that will characterize something that looks this complicated so let's again let's let's sort of go to a digital logic as an analog and look at this circuit which is kind of become one of my favorites now that's also a real you know kind of entangled mess I've kind of intentionally you know sort of entangled things to make it look I've labeled the inputs it's kind of a generic way but if you were faced with this circuit even an inexperienced logic designer I think it would be hard-pressed to say what it does this is what it does it's a half adder you know one of the first circuits you learn when you're in in logic design 101 it's a it's a half adder that uses a really clever method called quieted logic which was the you know developed in the 60s it's basically a way of sort of redundancy with computation in such a way that if you get any failure on a line it'll be corrected within the thinnest I think to gate levels and so if you if you perhaps use redundancy or maybe far even more and you're worried and you look at transient failures this even if you pepper this thing of transient failures and still work well you know the brain is notoriously unreliable you know neurons are notoriously unreliable so I there's really no question in my mind but what it must use some similar kind of technique there's the redundancy is entangled with the with computation it's as I say here it's it computation is inextricably combined with an obfuscated infrastructure now you wouldn't teach a beginning logic design student that thing at the top okay you do the thing at the bottom you know you'd assume perfect reliability and talk about the actual paradigm and and we're you know we can do that when in the computer architecture lab and we're studying this thing and doing simulations we can assume perfectly reliable components and that alone will remove a lot of complications and there are other things there you know they're they're mechanisms that provides temporal stability and you know I'm one who knows what else so the model is simple because we're trying to extract the paradigm from all the complicating infrastructure and this is a sort of thing it's not you know you have to kind of use your own so researchers judgment I guess as far as how you do that okay also now that I've laid out these sort of primitive elements I want to kind of give credit where credit is due this is a pantheon of neuroscience architects they don't these are people who don't consider themselves to be architects but that's what they're doing and and all these principal components I've talked about have have been put forward many of them twenty years ago so I just want to kind of list these very quickly there's there's Simon Thorpe and his students and his students doing this there's actually a large number of them who were some of the first people to look at temporal coding stdp and TN and architectures in general it's this group more than any other that's kind of carried the torch through the through the past 20 years there's Wolfgang mas a really top theoretician who first defined he called them SN ends what I'm now calling T n ends because SN the name SN n is kind of broadened out to all these other things Henry Markram you know famous guy he's kind of an all around her both experimental and theoretical he put he did some of the first experimental work that established SCDP kind of in parallel almost independently Gerstner who is it really a top-notch modeler came up with a sort of a model for stdp and these two kind of his model kind of predicted what Markram came up with and Gerstner also has done a lot in her own modeling santur boda I like this because he's done some interesting architectures written some survey papers that are quite good and then singer and freeze this is the the work on inhibitory oscillations if you want to kind of get a know a big picture for how the thing works as a computing device this is really a good paper and I guess the only kind of an observation all of these people are working in European research labs okay so have these elements let's now try to move up to the next layer column layout column level this is what I'm what I call a computational column but essentially all the people working in this space do something that looks just like this one way or another the dimensions may be different you know the way st DP'd is done may be different but they all kind of look like this and this is the thing that learns and maps inputs having similar features to what I'm calling cluster IDs here okay the input lines if you want to give them a kind of a machine-learning interpretation the presents are so each input line kind of corresponds to a feature and the presence or absence of a spike indicates the presence or absence of a feature the timing of the spike indicates the strength the earlier two spike the stronger that feature the cluster ID in this model and you know to keep things simple is basically a one hot idea there's only one spike that's going to come out and it's so turn pro the better the match the better the closer it is to the you know the better match it is with the features that identify the cluster the sooner the spike and then it's not you know then also then because we're going to stack this up in a hierarchy a plus or ID output a one-layer becomes a feature you know going in to the next layer okay so I'm letting out a roadmap so here's some waypoints on the way to our first you know major milestone we have to do it if we have to decode inputs we have to take input images in this case map them onto volleys of spikes at the first layer though so after that first translation typically you're going to have dense volleys and so the first layer maps you know as a sort of a dense too sparse mapping and from there on it's going to be sparse so we have layer 1 which is dense the sparse layer 2 is sparse 2 sparse it may be it also may be kind of a special case maybe not but at some point we would hope to have a general sparse 2 sparse column and then at the output there's going to be a sparse - question mark - whatever it is it's kind of application dependent and so to achieve our miles or a major milestone we have to hit each of these waypoints Waypoint zero input encoding just very quickly that look let's use our in this you know numerals I just based the leverage biology I use the equivalent of on/off retinal ganglion cells where you have a where you have a center with a surround and depending on the contrast either your the center is brighter or less bright you know there's the on and there's the off you basically do a but what amounts to edge detection which works remarkably well so what I'm showing here the first one is it eight inist numerals with the on and the off and in the work I'm doing now I basically I take away time from the primary inputs I basically binarized the input and I do this so I can so I can more clearly you know look at temporal computation I want to any temporal effects from the inputs so all the temporal effects at the output of the first layer are do the computation because I have to know I I want to know how um I wonder how you know temporal values are produced so when when I go back and put temporal values into the inputs I kind of know exactly how to map the different you know pixel intensities let's say to a spike timing it isn't necessarily an obvious linear mapping in fact I don't believe it is okay so so so at the top there are the full images at the bottom there are six by six receptive fields I think machine learning people call these patches taken from the center here's that so Waypoint one is this dense the sparse computational column unsupervised clustering against that 6x6 so here's your some arbitrary inputs go in and after the thing has been running for a while and it's done a little bit of learning here is a the synaptic weight matrix black is the maximum weight white is zero and this these are the actions the actual matrix and when you first start applying inputs it only takes just a small number at most a few hundred and then this the most common clusters are already generating outputs that can be passed up some of the more rare clusters you know take a lot more cycles to fill in not these aren't just pure black and white you know some of them are kind of shades of you know light gray well that's because this thing is always learning there's this kind of stochastic bubbling that's taking place they're always kind of on the lookout for a change in pattern so they can you can grab it and change the clusters so you're always going to see a little bit of sort of stochastic bubbling activity going on and then here are the outputs in this example you know and it basically you can kind of see how it sorts the inputs into and a similar sorry similar patterns into the same cluster and here I'm pointing out sort of the the the state-of-the-art for this this is a 2018 paper is you know this is this isn't I'm not doing exactly the way they're doing it but I'm able to reproduce their results SCDP works you know or at least for this kind at least at this layer and enough to something see the light at the end of the tunnel it gives us I think a reasonable optimism that this thing is actually gonna gonna play out what we would really like though is what I'm calling a cookie cutter column we'd like to be able to take these computational columns make an arbitrarily deep arbitrarily wide network set up the you know the parameters you know which was gonna have to be done by someone who knows what they're doing and then turn the thing loose and it'll do all these you know wonderful things this has not been achieved people writing papers in this area they spin their work like we all spin when we write papers they spin they're working in a kind of an optimistic way but but in reality this really hasn't been achieved yet this is where the kind of a leading edge is okay so we both we've you know if we briefly wanna look at the research space if you were if you want to say well you know how much is there to do the turbo coding thing I'm saying that's kind of given either either that method of something like it excitatory neurons well there's a large parameter space you look at different response functions something we're looking at is maybe some sub linear summation I think that may have some promise it in inhibition the winner-take-all thing that's a large parameter space something's again it's not talked about much as why happens if there's a tie you know if you're using this low resolution thing you're gonna get a lot of ties and how you resolve tides actually makes a difference stdp I'm calling that a large idea space that there's I think a lot of exploration yet to be done there the neural networks themselves it's a large interconnect space to use full crossbars partial crossbar pseudo-random partial crossbars is there's all kinds of variations there there are two things I didn't mention at all you know there is some computation that takes place in the dendrites this may be analogous to pooling it's it's what I'm calling a large idea space it's practically untouched and then there's compound synapses a given given upstream synapse neuron synapses with the lower you know downstream neuron in not just one way but several there multiple paths and it's kind of obvious they're gonna have different delays you know the chances they have the same delay is almost nil okay so when you take that into account now you have a much more you know complex you know space of aggregate response functions that's practically been untouched uh you know I spent literally years working on that before I kind of backed away and I simplified things to a simple synapses okay so I'm getting near the end I want I want to quickly stick this in because this is an important part of the whole scheme as I see it and this comes back to what I was saying at the beginning that that neuromorphic circuits are not the only way to implement neuromorphic architectures the key thing with spikes is that they're a way of encoding values as temporal events but you can do that in other ways edges are one so so a signal change from from a one to a zero again now we're in the logic domain so so if you're using normal logic circuits and heads change from one to zero is a transition it's it's a transient in time and you can use those and when you do that you can use this thing called whole race logic and you can wind up actually implementing these neural elements with ordinary off-the-shelf CMOS where you're using edges rather than spikes and so you get a lot of the same efficiencies that you might get with a neuromorphic circuit by using off-the-shelf CMOS and I'm gonna take my shortcut here the disk of my escapade from 2018 talks about that all that's that part of it the race logic the temporal algebra in in some amount of detail okay so I just have a couple of closing remark if you want to do research in this area and I encourage it you it's got in a way it's kind of hard to do because you have this bright shiny object of successful artificial neural networks and you're kind of taking you know you're doing this mastery sentence you know stepping back you know twenty years in time to kind of try to develop this new paradigm but the barrier to entry is low that the actual tnan literature is really relatively small the development isn't that far along I kind of set off how long it is and so there isn't a lot to learn it isn't like lots of people have been beating on this it's actually been a small number of people the computational requirements are law I mean it's supposed to be simple so it's simple you know you don't need a GPU you know I just I use a high-end desktop that runs multiple threads and I'm perfectly happy with that and that's going to get me all the way up to this milestone TNN I'm sure and so the bottom line is you can get up to speed on this research area in a few months and a good way to start is the right is simulator because this is you know that's that's probably the best way to learn this stuff so I guess I guess a big part of my message is I'd like to kind of encourage people looking for something really new and different rather than making little incremental tweaks to do something big because again the research space is huge this is a good place to be okay finally I think an important question in all this and which may affect whether you would want to do research in this area is are we at a tipping point okay experimental neuroscience is span over a hundred years the published literature neuroscience is vast and it's growing all the time there's it's almost overwhelming but but but the question is one of all experimental research were to cease tomorrow do we already know enough is enough and we learn enough already to build these networks if so then I'd say that from a computer architecture research perspective that's a tipping point we don't need any more and in my view we're either already there or fast approaching it so um you know at the tipping point we know enough it's it's only a matter of combining things in a coherent and effective way and so let me close by saying that if you are you know a researcher looking for something where you can make a huge impact where there's a lot of stuff to be done but but the but the potential rewards are really big if you believe we're at the tipping point then I would ask you know what are you waiting for you know now is the time you know don't wait you know don't wait until you know hoping experimental still somehow find some magic thing which I don't think is going to happen don't wait for someone else to do it you know now is the time so that's my last slide I have a slide here of bibliography there's a there's a monograph that I did for as a synthesis lecture if you look at this be sure to look at the twenty hundred already have look at the 2019 preface because in the last two years my my view on a lot of these things has changed you know this is this is a very dynamic research area the second thing here is the iske you know 2018 paper and then I've got some papers here they're sort of like for the most part seminal papers you know paper by hopfield middle nineties excitatory neurons you know modeling middle 90s stdp middle 90s you know there's a theme here tn enzyme giving you know mosses you know seminal work and but I'm also giving that state of the art 2018 paper the paper body freeze acknowledgments a lot of people acknowledge I'll just name a few my wife Raquel for her longtime support which I greatly appreciate John Shannon who had started to work with recently at CMU Silicon Valley and I'd like to also thank our Eckhart Mach leeward winner mark hill who encouraged me early on and you know we all know his technical accomplishments but the contributions he made with in Wisconsin and the architecture group are were tremendous and I it's really a deeply felt I think mark for all he did for the architecture group of Wisconsin so thank you I'll be happy to answer questions so so we have time for some questions from the audience but please identify yourself before I say hi I'm from at this and universities in your talk it was heavily based on this abstraction of neurons and try to architect how the human brain works but there is growing amount of evidence in the neuroscience that neurons are not enough to explain how brain works and non synaptic information transfer and glia cells and astrocytes play significant roles to explain how brain works and achieve the efficiency echo here so bad I okay so the question is whether the other than neuronal cells in human brain that helps to explain how brains achieve their function and efficiency whether we need to abstract them and include it in this architecture or whether you think that neuronal attraction alone can explain it and achieve your goals of meeting well human brains do but the philosophy is that I've taken is to keep it as simple as possible and so so so I tend to add something only if I need it otherwise I assume that's there for some other reason that's there for this infrastructure support that I talked about for fault tolerance for temporal stability for some other thing okay so so so you have to kind of be aware of the literature and what you know what people experimentally determine about the various types of neurons especially inhibitory neurons but but but my guiding principle has been has been to keep it as simple as possible only put something in if I need it okay and then I implicitly assume that if I don't need it it must be there for something else other people take different philosophies hi by other UC San Diego sorry but annoyed very inspiring talk thank you I had a question like it seems like what you presented was mostly based on analysis of human brain so we have simpler organisms insects that you know have simpler newett nervous you know tissues and they accomplish certain like you know complicated tasks have you or anybody else like looked at validating some of the theories that you have by looking these two these simpler organisms or like is part of the modeling taken from such you know such organisms in nature the other some work that I'm aware of what insects sphinx moths are kind of interesting kind of creatures to some extent this request some Tetris on an issue that kind of separates I would be sort of the computer architecture research like I'm doing for month in neuroscience theoretical neuroscientists are doing the theoretical neuroscience as in as in many science has had this interplay between experiment and theory from the computer architecture perspective is a little bit more asymmetric you know if I don't I don't something does it isn't required that something being experimentally verified before I use it I'm perfectly willing to come up with something that may or may not be there okay and use it in other words my standard is not that it's whether it's verified to be plausible that it's there it's more like it's not implausible okay and it's certain you know that isn't unlike what happened with the original neural networks you know they kind of went off in a direction that kind of separated themselves from the actual biology but it led to really good things I mean I'm willing to do that you know although what I found is if I separate myself from plausibility too much that's usually a sign that I'm on the wrong crack and I wind up getting back you know back to something that's closer to applause more at least not you know sort of provably implausible Thanks thank you hello thank you for your inspiring talk and I was mainly wondering yesterday we hear her at Turing lecture where we had a question that whatever it's possible to explain neural networks deep neural networks and basically the answer was no because we don't understand the brain so you're working on understanding the brain do you think that understanding deep neural network is something that is another milestone in your project or do you think is mostly orthogonal no perhaps I mean it would it would be great if this could actually feed back into into neuroscience although I thought the the answers you got from the speakers were pretty good answers you know I would I don't know that I would that I would say much that's different I think in the end we're not gonna totally understand how this works especially when you get sort of higher up in the hierarchy you know as I said earlier you have this you know this pseudo random number generator buried in there and that alone is gonna you know kind of you know make things you know what I said it's you know sort of it has a mind of its own and so I don't you know I'm not I don't know that when you put a different way it may be inherent that we don't really kind of understand how it works totally it's all maybe almost like the uncertainty principle you know but it's it's it's sort of fundamental to having this higher level knowledge that we don't know you know it's I thought about this some you know I'd like to think that at least sort of an informal way and kind of explain what's going why don't you see some of that in this talk but ultimately when you get hired when you get beyond tenants we get way up in that stack maybe several decades from now at some point I don't think anyone is really gonna know how it works you know thank you and now it's time for lunch oh no we can't take more but questions you can take it offline okay so in your talk like sorry about that okay so as from your talk I understood that like a neuron is connected to maybe like thousands of other neurons yes right yes and they are communicating with each other right yes so let's map it to a processing element and there is a processing element and it's probably communicating to thousands of other processing elements right there's lots of communication happening yes and brain takes 20 watts of power right how we are going to achieve that well I think this point just doing me you may not achieve 20 watts but you know but our technology it is a different technology we're gonna be using rubies in silicon we can deal with 200 watts you know I think I think we'd be willing to compromise and he's 200 watts and we'd be happy right it is a different technology so I I don't think anyone is saying we're gonna match it to 20 watts but I think we're gonna get something that's really efficient okay so there's another philosophically question we have been studying theory of relativity maybe for like more than hundred years you 