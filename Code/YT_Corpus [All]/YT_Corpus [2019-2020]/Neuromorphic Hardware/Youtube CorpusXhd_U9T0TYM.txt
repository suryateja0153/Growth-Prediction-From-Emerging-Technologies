 Hey everyone and welcome to the Bosch Global sofa sessions. We are here to talk tech again and in order to do so this time, we'll even leave earth and embark on a journey to space. To the international space station to be precise. Last year, a very special Bosch project was sent to the ISS, which is called SoundSee. SoundSee is a sensor system that uses artificial intelligence to, ultimately, make the everyday life of the astronauts on board the ISS easier and better. Here to tell us more about that and, world's first in the history of the Bosch Global sofa sessions, answer as many of the questions that we have received beforehand on our Bosch Global social media channels is Sam Das, artificial intelligence researcher and SoundSee project lead at Bosch in Pittsburgh. Hi Sam! Thanks for joining me on my virtual sofa today. How are you doing? How's life in Pittsburgh? Very good, very good, very excited to talk about SoundSee today and it's been really exciting to be part of this mission from the get-go. Wonderful, super excited to hear more about that. Last week, last Monday, was Space Exploration Day and a couple of weeks ago, when I asked you if you would like to join me in a sofa session, you told me that this is pretty much your favorite day of the year. How was it for you? How did you celebrate and why is Space Exploration Day your favorite day? I mean there's no surprise right that anybody who grew up dreaming about you know space exploration and then be part of it someday it is a special day right. Neil Armstrong has been my childhood hero you know and so every year I try to celebrate in some way. Last year was special because it was the 50th anniversary of moon landing of course. This year of course so we are at home but you know look at some NASA feeds I guess. That's the only we could celebrate maybe this year but so as always it's been always special. Cool! Sounds great! I think we've received two pretty great starting questions from our community on Facebook. First one: Is there even any sound that can be analyzed in the vacuum of space? Sure. So that's a very good question. I've gotten that before. First thing is that SoundSee is not floating around in outer space meaning in the vacuum. So it is inside the space station. So space station is where astronauts live for long duration. So the way the environment is set up is just like here on earth, the typical atmospheric pressure, so that they have a comfortable stay there. So from that point of view: Yes, sound do exist and you know they communicate from there with regular voice and speakers and microphones. So it's all similar from acoustic standpoint. Only thing that is different is, of course, Zero-G. And that's where all the interesting things happen. So just imagine, it's a school bus, but in space. So inside it's very similar but of course we have no gravity there.  Does it only work on board the International Space Station then or would it also work on earth? It will also work of course on earth. So there's various advantages that you are leveraging from the space station is because, again, I'll repeat Zero-G because things can float around there so we have really unique opportunities to try out different mobility systems that can go around and sense interesting things but in terms of sound sensing and everything all those technology we're developing as a part of this research mission could also be applicable here on earth. Now, that we're already talking about where SoundSee is located and where it's operating, I think that you have a model of the ISS at hand, right? Okay maybe could you show us where exactly SoundSee is located and what it does all day long? There it is! It's a replica of the ISS so i could probably introduce a little bit. This front side over here is the US and European segments and the back is the Russian segment so of course our SoundSee module roams around in two modules at this time That's at least where the experiments are planned. This area right over here is called the Destiny Module. This is the US lab and this is Kibo, the Japanese research module. So SoundSee is expected to perform experiment in both those modules. Right now, it is stored in the Japanese segment because that's where another prop, this robot, this is the Astrobee robot, a souvenir from NASA, so this robot is actually roaming around in the Japanese module. The reason being that this inside of this module has been mapped and using this map the robot navigates so SoundSee rides on that robot. So initially the experiment is expected to take place in the Japanese Kibo module and then of course in the US module. So yeah, it's stored right over there. It arrived at the ISS through this spacecraft, I don't know if you can see, there this is the Cygnus spacecraft, I also have a model for that, so this segment is called the Unity module. Could you just move it a little bit? Yeah, I could probably put hold it up or something. Yeah, there you go. So if you look at this is the spacecraft cargo to the space station and SoundSee was part of this NG-12 mission back in November that docked here. So of course, you unload and then bring it to the US lab and other modules. So this, by the way, is the Columbus Research Facility, the European space agency module, so yeah, it's all collaboration, it's international collaboration, so we could use all these different labs as we plan our research experiments. This gives you some perspective of where things are. But this, this is one of the greatest things that humanity has built aside the football field and as you can imagine it took bit by bit different modules to be added. 250 miles above the earth to build this whole thing. It's just, you know, this just boggles my mind when I look at it. Thanks for preparing that and that you just have an ISS model at home. Inspires me when I put it next to my desk. Did you have the chance to to somehow monitor the arrival of SoundSee? Yeah, it was a live feed from NASA TV. So we could see it. So the spacecraft comes in and there's a robotic hand grabs it and basically latches it onto the one of those modules. It was pretty exciting, official arrival. Our communities were quite interested in how your entire SoundSee and space journey began and what the process afterwards were. So very essential first question: Where did the use case even come from and how long did it take you to develop SoundSee and the technology? Sure, we started thinking about SoundSee as a space mission, I think, is mid to late 2017. That that is when we wrote the proposal to you know ride on this, again, robot to actually monitor things but the core idea of SoundSee to use artificial intelligence and signal processing to understand sound at a deeper level started way before the space mission. We started I would say five years ago when we started researching on, you know, how could you use state-of-the-art AI to get more information out of sounds. So again, audio signal processing and traditional way of measuring vibration and sound for understanding machines has been there and we are part of that community so we are signal processing and machine learning researchers. So we wanted to see that, okay, so signal processing has given us all these different tools to understand, you know, the environment and state of machines through sound analysis. What more could be done with artificial intelligence? Meaning when you have a lot of data how do you really use this artificial intelligence model to extract more information as compared to the traditional signal processing techniques. So this is what we started with. How do you infer different operating state of machines, how do you recognize different events in the environment, physical events happening like loud bangs happening or glass break happening. So these are more benign everyday experiences they don't do you know see how a machine would understand that's the way we do right. If these are not easy probably it might seem easy you could just recognize pattern. It's always but how well could you do it? You can't afford to do too many mistakes. Then people will not take those systems seriously. This is where a lot of the AI and signal process research is going on. That's how we started five years back and we continue to improve and still we are continuing to improve and this research mission is actually part of that. So as we were working on those techniques and this opportunity came to us it was exciting on a different level because, again, I'll bring this prop, because of this flying robot. This kind of flying robot doesn't exist anywhere. Unfortunately, here on earth, we have strong gravity stronger than at least, you know, in the space station. So because SoundSee can ride on this free flying robot it gives a new opportunity to do experiments with how you could do sound sensing on a mobile platform and try to map different acoustic signatures from all these machines and equipment. Sou could also imagine the same thing in a factory floor or a warehouse but with NASA, with this opportunity, we could do it for the first time on a flying robot and that gives us a lot of flexibility to do interesting research like i say is a research mission, it's a journey, it's not a, you know, a done deal because that's how research is done and we're continuing to discover new things as we do so this is kind of our timeline right that our audio AI research didn't start with SoundSee mission SoundSee technology started way before that. Great, that you mentioned that it's an ongoing journey because we also received the question about what the three top challenges were while developing SoundSee? and what are the challenges that you're having right now? I think first challenge is that we are familiar with building proof of concept here on the ground and tested on the ground in our labs. The moment someone says that you are part of a space mission and you have to send the hardware you built, the sensor, physical sensor to the space station where astronauts will work with it. That means it involves human space flight right there are different levels of engineering, specifications and requirement that you have to meet for this different level. If it is a CubeSat that's a different requirement. You are in the outer space, nobody touches it. But then when you involve space station and human space flight there is a lot of engineering requirement you have to meet the safety, the standards that you have to meet with, you know, not actually messing up, you know, the communication infrastructure by doing EMI from the sensor by electromagnetic interferences. So all the specimen has to be met and that was a learning curve for us. That is number one, the engineering challenge to make sure that we we meet all these requirements doing all these tasks at NASA and make sure and then a lot of things you need to know, that was a learning process and then we learned and then we fixed it. So that was a big challenge. The other thing was how to really replicate the ISS environment to test as we build the senso. So we built a mock ISS facility at Bosch Pittsburgh where we actually used a ground robot to move things around and try and understand how to design the sensor elements and stuff. That was also not that trivial  because you cannot exactly replicate the movement trajectory here on the ground. We also went to NASA. They have a micro gravity simulation lab so that was again also really exciting but nonetheless a challenge to really make sure that we built it right and of course as part of the experience we'll figure out what are things we missed, what are the things we learned. So that's two. The last thing would be, I would say, is the algorithm design itself that our core challenge because we understand that in a I would say control chaotic environment that we'll be dealing with, meaning, it's not a nice clean room where you have just one sound.There are so many different, you know, machines and audio signals coming from all these different physical processes what you call you need to really figure out how to really focus on the right thing. So it's a combination of algorithm design and designing a strategy how you execute the experiments whether or not you just zoom through the ISS module and hope to capture whatever you want or you plan the trajectory of the robot, go to specific machines and try to focus on that. That's an ongoing process but in the process of designing our algorithm, we spent a lot of time, you know, thinking about those and addressing some of the challenges that associated with it, we believe that we have addressed most significant challenges and the other aspect that we need to figure out is part of the experiment that we're going to conduct later this summer. So it's like, I say, challenge has been addressed but challenges are ongoing and to be addressed and that's the part of the excitement for us as researchers to really understand these problems and then discover new challenges and solve them. So that's how best describe the situation in terms of, you know, challenges we addressed. Well that very last challenge that you mentioned also great because another person was asking, let's see I think this fits quite well: Isn't it difficult to identify which of the surrounding equipment emits the noise as the emissions of many devices overlap? Let me explain a little bit, yes of course, that's the most obvious thing that comes to mind and anybody who does audio analysis and audio understanding would know that that is one of the challenges of audio and this is what we are trying to solve not just I would say AI. AI is a buzzword people throw around. We want to use AI whenever it is necessary so to address that exact problem you're talking about, here, we use a combination of classical signal processing and AI where necessary. So let me explain. So there are two aspects of SoundSee. Again, let me use the prop. So SoundSee rides on this tiny robot, again, not as tiny, it's actually one cubic feet size, and rides along. So one thing we're trying to do first with SoundSee is not just use machine learning to monitor anomalies with machine but first you want to map the acoustic characteristic of the ISS Literally you want to map the noise intensities and patterns coming out of all these different machines. We really appreciate that there are different noise sources and would like to map it just like in optics using light you're using your lens in your eyes to really map different intensity that's the image you see Imagine doing the same thing, somehow, with sound and map sound intensities in the space where we have different acoustic sources or sound sources. The way we do it is that you use what we call, I myself call this, acoustic lens right and it's a traditional technique nothing we invented as such is that we are using a microphone array just like you can focus your attention to a certain area as you look using this array of microphone and focus on a certain direction as we move around the space station so that's part a so that that is nothing new right people have been using microphone array, of course our microphone array a little more sophisticated than the consumer microphone array that you'd find in consumer products but that does one thing we could steer the listening beam around and focus on different machines and equipment. That's how we can probably map a little bit. But other interesting thing that we're doing probably doing for the first time, to the best of my knowledge, is that as you move around the space what you can do is that what you call a synthetic aperture meaning using the motion of the robot you could create a virtual large microphone array that just looks like a cylinder and one spot is just a circular array as you move you can create this really large aperture listener like a big ear you have a small ear now you have a big ear as you use the trajectory and use that to have a much more higher resolution mapping of the acoustic sources this is a well-known technique in single processing called synthetic aperture typically used for imaging but we are using it for the first time with sound synthetic aperture acoustic imaging we are calling it that will enable us to map these different sound sources with much higher fidelity. That's the name SoundSee, seeing with sound clearly living by its name right and there are challenges to why, how you would be able to do it because you know somebody might argue that SoundSee exist You use ultrasound to see things inside the body. So that is sound seeing right. But the caveat there is that these are really high frequency sound right of course and you have to be very close to the target to be able to image it. What are you trying to do we are using regular everyday sound that you can hear to do imaging and that poses a lot of challenges and this is how we're using state-of-the-art signal processing techniques called compressive sensing where you actually built in the synthetic aperture imaging in the framework itself and try to do this imaging and we got some really interesting results. Of course, we cannot talk about everything here but that's part one the imaging of that. That will already help us segregating different noise sources and then on top of which we use then the machine learning and AI to look for changes in those patterns over time. That will tell us, hey suddenly maybe some new noise source just appear which you didn't expect maybe you should check it out more. It's a combination of machine learning doing something or alerting the astronauts to check out. So it's not going to be one system doing everything, you know, it's a collaboration between AI and human, the astronauts, so like I said, you know as you can tell this is not just using a data set of sound and just putting it all together and crunch AI to spit out some results and again that's, of course, a part of it is that. But there's so much more to how SoundSee operates in space station and of course same techniques could also be explored here on the ground. Wow, I do get the feeling that you're totally in flow right now. So i'm confident that I can confront you with the high level expert tech questions that we've received. First one: Will you use a supervised learning approach or are you going to use an unsupervised learning neural network? Very good question. It depends on what aspect of the problem you're looking at. So if you're just looking at anomaly detection suddenly you hear something suspicious which is off-nominal. This is where we're using unsupervised neural networks. So the way it works a little bit technical detail is that in signal processing also again like i said all of our work is inspired by signal processing then applied to AI. It's not, you know, the regular data science, at least, that's how I would like to articulate here right. So in classical signal processing sudden model change or gradual model change what I mean by model change is that anything that you're monitoring, it's an event or a physical process or a machine is a dynamical system right, it has its own rhythm if you will. It operates and suddenly the rhythm is broken right because of some anomaly just like someone is walking and suddenly stumbles or falls right and there has been work where there has been you know from control theory you can identify a sudden change in model that is actually the anomaly right. Suddenly your dynamics underlying system dynamics changes. So what we have done in this case is inspired by that work or those area of work. We are using neural network to model the system dynamics and try and see if we could predict the future behavior, the immediate future behavior of the dynamics using neural network. If you can't do it so well that means the underlying dynamics has changed, the machine is going through a off-nominal state just like someone walking and then trying to you know or going to fall sorry similar situation and then without going too much into detail neural network has I would say a really incredible modeling capacity if you train it properly to model such complex dynamics. So long story short, yes, unsupervised neural networks for anomaly detection is a really promising direction as many people are exploring. We're looking at it from a little bit different perspective  inspired by the system identification and dynamical model change work that has single crossing work community has done and we are you know like I said continue to really test the system and see how robust they would be so but that is definitely part of SoundSee's exploration and research. I just keep going because I was also so impressed that our communities have such a great interest in what you're doing and apparently also the knowledge and the expertise. So let me keep going. Do you have plans on using some neuromorphic hardware to improve accuracy and reduce power consumption? Certainly, in future, right now of course we are using commercial of the shelf hardware to just proof our hypothesis what could be done and with what extent to how good it is. But eventually when you figured out that, okay, a certain architecture, a certain type of algorithm works better you need to optimize your hardware so that you could deploy them at scale with very limited power to operate them continuously. So definitely, I'll be super excited we haven't unfortunately looked into that yet as much as you like to because we're focusing more on the algorithm side but yeah sure I mean i am really really interested to see that happen. Great, thank you also for that. Could the SoundSee technology help detect medical equipment malfunctions? I would hypothesize again we haven't tried it yet right but I would hypothesize as a researcher that it'll be super interesting to really see how SoundSee could be used for monitoring such equipment and the good thing how we are designing SoundSee is that each machine each what I call physical process at a more fundamental level right some kind of physics is happening right that makes a life support system different from a ventilator or a machine that is manufacturing things right the kind of different components work together and SoundSee's AI algorithm if I may use that again is designed to really learn the salient features from acoustic signature for those physical process. So let's say you want to really adapt the what they call feature extraction for audio AI to be purposed for a certain kind of medical equipment, you could do that of course, then you have to understand the operations and collect data for it but the way it is designed you can do that rather than using some regular of the shelf audio features that typically people would use in speech recognition for example. So on a higher level, what we're saying is that you could adapt add up SoundSee to monitor different types of physical events and different equipment so the way we are designing it and so yeah in principle it could be part of, you know, adaptation to you know such equipment monitoring as well and yeah I'll definitely think more about it and what sort of, you know, machines could be monitored, which are like for example I know that MRI machine for example are really critical right to do schedule and then if you have to do maintenance on such a machine it's a lot of downtime and this actually impacts human life so I thought it's a really good question in short. Right, wow. Thanks for that also. There seems to be so much potential it's insane. What's next for you and your team right now regarding SoundSee or a different project maybe? So in short, we're excited about the fact that we're working on AI and in this particular mission AI for space exploration which could definitely have a future from a scientific as well as from a business standpoint and with our relationship that we have with NASA we'd like to explore some more of such opportunities as well I'd say. Without going to too much details right now we do have some proposal in the works to work on let's say a little bit further beyond low earth orbit maybe to other celestial objects but I'll keep it at that because it's an ongoing work and I'll be happy to talk more about it when you add more details. Little sneak peek, thanks. Thank you very much, Sam for sharing all this information, your knowledge, your passion. It was so great talking to you and I think by answering all of these questions you made a lot of people very happy. I just wanted to read out loud one comment that we've received on Linkedin to you which is: Thank you for this wonderful opportunity to ask questions to the technical lead of such an impressive project I'm really excited to get your answers. And I think that sums it up perfectly. Thank you for taking the time. Thanks to everyone out there for watching and stay tuned for the very next episode of Talking Tech. Thank you very much. Thank you Melena, it was a pleasure! 