 on Hawkins with the san diego surfing camp uter center and this is this is our second virtual tech forum breakfast so it's YouTube IOB bring your own breakfast this time but we do look forward to the day when we can get back to doing nice in person and so please take the rain check on breakfast and we'll have a breakfast for you down the road I do want to make a couple of administrative notes first of all as you've probably seen been alerted we are recording the presentation we will we will record up until the Q&A session and then then we'll turn off the turn off the recording so you don't feel you have to go on the record to ask your question now if you do if you do pose a question during the course of the presentation and we take that then then that will end up on there on the recording so our next our next event is July 16th and we have Dilip Ramachandran from AMD and he's going to be talking about AMD processor technology for high-performance computing so we look forward to that but today we are excited to have Koon Leola cotan from Samba Nova systems talking about software 2.0 and I did last night tear myself away from Netflix and listen to one of kun least talks on YouTube and found it found it very interesting so I think I think we're in for a for an interesting and engaging engaging talk this morning just quick biographical details dr. Ellicott and his co-founder of Samba Nova systems and is the KDS design professor of Electrical Engineering and computer science at Stanford University he's also the director of the perfect parallel lab and a member of the data analytics for what's next or dawn lab dr. Alec Odin is an ACM fellow and I Triple E fellow and he recently won the prestigious I Triple E computer society's Perry H good Memorial Award dr. Alec Odin received his PhD in computer engineering from the University of Michigan so we are very pleased in prevent privilege to have him address the the forum this morning and without further ado I'll turn it over to dr. olla code thank you Ron for that kind introduction and hate to think that I'm competing with Netflix I sure I wouldn't watch me I'd watch Netflix but just me okay so this morning I want to tell you a little bit about Sabbah Nova systems Rican people dataflow hardware some of the research that we did at Stanford that led up to it and talk about sort of next-generation acceleration for machine learning and other sorts of things that one wants to do at HPC okay so you know let me begin by kind of setting the stage and you know where we are today these two big trends in computing the first as most of you might be aware is that you know the long run that we've had to last 50 or so years with Moore's Law is basically slowing down right the time between generations of process technology is spreading out and the costs for each new node is is getting into the tens of billions of dollars but more importantly is denial stealing which is kind of the companion law which talks about the power efficiency of the silicon technology and that has been dead for a few generations and so now computation is is limited by power and the implications of this is the conventional computer systems that we've all grown up we're going to be powered by these general-purpose CPUs are being beginning to stagnate so that's the first big trend the second big trend which you know you couldn't have missed in the tech arena is the overwhelming success of machine learning for solving some really difficult problems in an image recognition m'angil natural language processing and knowledge-based creation right and so these capabilities that machine learning is providing as having societal scale impacts that changing the way that software is being developed at making autonomous vehicles possible they are impacting scientific discovery and you know HPC sorts of operations are only being impacted by new ML techniques and they're changing the way that that medicine is being personalized and it turns out that of course machine learning needs a lot of computational power for both training the models you know trading very large detailed models and also for inference especially when you want to supply or you want to serve inference to millions of users so this you know confluence of these two big trends demands a new approach for design and computer systems for ML turns out that machine learning and computation have been inextricably linked for a long time so if you look at the idea machine learning has been around for you know over 50 years and and if in fact training these networks using back propagation has been around since the 1980s but what made it dominant is the fact that we if you kind of look on the x-axis here where we look at that at data size and model complexity is as you had more data you were able to Train large and more complex models and these models attained high accuracy and the Sacre see surpassed the accuracy of conventional algorithms developed manually and this happened both for you know very complex problems like like image recognition and also for things like not like many natural language processing and what enabled the ability to kind of process huge amounts of data and train these large models was the increasing amount of computation available typically in the form of GPUs and so this notion of kind of training models to solve complex problems instead of developing manually developed algorithms is known as software 2.0 right and so software 2.0 fundamentally kind of changes the way that we develop software or has the potential to develop change the way that we develop software so if you think about software 1.0 as traditional software development that starts by you know taking a problem decomposing it into components and developing manually developed algorithms in to conventional programming languages then software 2.0 the idea is that you start with a training set and then you use that training set to develop a model and the code that gets written is written in the weights of the model right and so this dramatically reduces the lines of code that you need to develop and allows you to solve these very difficult problems that were not possible to solve with some point software 1.0 techniques the other thing of course it's happening is that if you want to train these huge amounts of these huge models then as I said due to the demise of Dennard scaling with fundamentally power-limited right but we still need more performance and so this formula which is probably the only formula I'll show you a day you know shows the relationship between power performance and energy and energy efficiency and what it shows that given the fact that the amount of power we we can dissipate going forward is fixed and we still need to increase the amount performance that we get out of our systems to Train these more complex more testing machine learning models which means that fundamentally we need to become more energy attrition which means we need to decrease the amount of operator the amount of energy each of the operations takes one way to do this of course is to become more specialized right to reduce the amount of overhead associated with doing the processing and this gives us better energy efficiency but of course becoming more specialized also means becoming potentially less flexible and so the real challenge in accelerating software point-o is delivering this hundred X mm X improvement in performance which really of course has to come down to an improvement in performance per watt since I said that the power is fundamentally limited because we want to do enable you know more ml to be used in a wider variety of applications with more capabilities and so providing this quarter kind of performance and energy efficiency increase while maintaining programmability right so ultimately what we like is the energy efficiency of an ASIC like implementation of our algorithm with the flexibility and programmability of traditional general-purpose processes so that is the attention of the the solution that we'd like and so how are we going to achieve that well it's really gonna take a full stack integrated solution that combines innovations in machine learning algorithms innovations in the software used to translate those algorithms to run efficiently on the hardware in the form of the main specific languages and compilers and also new hardware architectures to to match the needs of machine learning applications right so these are the three elements of the solution that I'm going to describe the the talk and but before I you know delve into these elements let me say a little bit about Salmonella so salmon OVA was founded at the end of 2017 and the co-founders Roderigo Liang who was a VP at an Oracle and basically was the lead for all the Spock CPU architectures developed with Oracle and Chris Rea who's a colleague of mine at Stanford then is an expert in machine learning and databases and and machine learning learning algorithms and is a you know certified MacArthur Genius and so you kind of look at the kind of areas that we spend we span across the whole stack and the goal is to develop new the next generation of AI hardware to enable this greater this dramatic improvement in the sorts of things that people can do with AI and enable new ML algorithms to be developed so think going back to to the ML algorithm transit let's think about how things are changing and one of the most dominant trends is the increasing complexity of the models to provide high accuracy and this is quite evident if you look at large transformer models which are completely changing the way that we do natural language processing right so starting with the but the GPT models in the middle of 2018 from open a a AI through to the Bert large models that are being developed what we see is these large transformer models are the best way of improving the state of the art of natural language processing and to bring large models are being developed at the year there's the Megatron system for the grumpy eight point three billion parameter models and they're using lots of GPUs because of course the amount of memory per GPU is limited and because of course you've got to cross lots of GPU boundaries and the efficiency of the compute is not as high as you would like you've got 400 GPUs using 20% at 20% efficiency to get to train at one point eight billion parameter model and then more recently Microsoft using a different way of training models has come up with a 17 billion parameter model and this model is uses a thousand GPUs at six percent efficiency so where you can see the trend here right you know as we get to larger and larger models more more GPUs are required and the efficiency goes down and so we can see yeah when this ends here so clearly we need a different approach in training these large models that can get you terabyte size you know private parameters that have trillions of parameters with much higher efficiency okay so so how do you deal with these these huge models and how can you make the amount of memory required to to represent these models less one of the techniques of people of Pi inside the pioneer in machine learning is the use of sparsity so instead of having all the layers densely connected as shown on the left here you have sparse connection so not all the connections are present and the challenge here is to come up with a model that maintains the accuracy as you have sparsity and so a number of techniques are being developed and people are showing that you can use paucity to maintain the accuracy while keeping while having a large number of parameters in your model another example of of sparsity is the use of Groff neural networks and here what you're trying to do is you're trying to model the world right you're trying to model the people places and things to build a more accurate model of the world and this naturally results in a sparse a network and here the challenge is to compute efficiently with sparsity because deep use design or work best when when the matrices and the models are dense and so achieving high computational efficiency in the face of a sparse network is is the challenge and so this is the thing that needs to be supported by future processes that support the next generation of machine learning and neural networks and you know graph neural networks are being used in all sorts of applications and they used to model natural languages social networks communication networks recommendation systems so you see you look look at software 2.0 and it's providing the number of benefits right so it's making the amount of code smaller so it's often easier to develop models by training by craning using training data than to manually develop algorithms the other benefit of course is that the execution of the model is is more predictable and the memory requirements that sometimes reduced so as an example you know Google took their you know five hundred thousand lines of manually developed code for language translation and reduced it to five hundred lines of dataflow code so this dataflow code was developed using a distillation of gland which called tensor flow and if you look at the types of flow codes in dataflow and so let's look at the Mesa civic languages and see how these dataflow ideas are expressed so the idea of domain-specific languages is a fairly old one right so it's the idea of instead of having a double purpose language that panels all sorts of applications you have a tomato language has restricted expressiveness and with operators and data types the match of the problem at hand and typically here what you have are high level usually declarative and deterministic representations of your application and traditionally domain civic languages were focused on productivity right so the whole idea was by matching the problem that the programmer was trying to to develop that you could make them much more productive the thing that we looked at you know you know in research about 16 12 years ago was the idea that you know could you get both high productivity and high performance so we call these high performance domain-specific languages and one of the areas that we focused on was the idea of a high performance to make Civic language for machine learning that we called optimal to other examples of the Main Street languages are MATLAB in the area of dense linear algebra matrices in linear algebra and sequel in the area of relational algebra so here's an example of of optimal right so optimal is this language for doing machine learning and one of the algorithms that you might Express is k-means clustering right so KB's clustering here what we're trying to do is we're trying to cluster these green dots around a couple of centroids the the red X and the blue X right and the first thing we want to do is find out the distances of all the green points to the two centroids and so we can do this with a couple of lines of optimal and so now you've got a a red set of points in it and the glue set of points and then you're gonna move the means to the centroids of these two clusters and then of course you're gonna iterate until the the centroids or the means stop moving so a few lines of code there and this is you know high-level representation of the k-means algorithm and this we did as I said you know about ten years ago more recently tensorflow has used the same sorts of ideas and you can express k-means clustering intensive low like this and so the key thing that we discovered in the development of the high performance tomato speak languages is the underlying you know these dataflow kind of languages was a representation that we call parallel patterns so parallel patterns allow you to express almost all the kinds of operations that you like to do in data processing and machine learning as operations on collections so these collections could be sets they could it be arrays it could be tables they could be tensors and so proud patents give you a way of expressing both the compute requirements and also the access patents right so two are common parallel patents are Mac and reduce others are zip which is kind of a multi input Mac and flatten it and group by which are are more interesting operators that get used in data processing operations that are such as a sequel and so if you look at representing domains of languages what you'll find out is that you will need to support a set of hierarchical power patents right and so you don't just have map followed by reduce but you've got some nesting of Map Reduce grouped by zip and so on and so what you end up with is a high-level parallel instruction architecture that is represented as a hierarchy of power patterns that produce a data flow graph right so you've got harka cool data flow graph of power patents and that can be used to represent any number of domain-specific languages they can be used to represent machine learning languages like pi torch and tensor flow but also data processing to make Civic languages like sequel they can all be reduced to this hierarchy of data flow of Prowler patterns and so given that this kind of is this fundamental way of representing the computation then the next thing of course is to optimize that representation to get better performance right and so one of the things that you want to do of course as we know high performance is all about managing memory managing locality optimizing locality and exploiting parallelism right and so with a high level compiler focuses on Paula Patton's you can tile the computation to make it fit efficiently on chip you confuse computation to to reduce the amount of intermediate memory you can do all this in the context of Prowler patterns which goes beyond what you typically do in compilers that deal with what are called affine loops which was formally what people did in this area you can also exploit parallelism you want to exploit parallelism at multiple different levels you've got a hierarchy of power patents and so there's a hierarchy of parallelism that you want to be able to exploit you want to exploit not just traditional parallelism where you're doing multiple things at the same time but it are completely independent but you also exploit pipeline parallelism where you have a sequence of things that are dependent and you want to run them at the same time on different cohorts of data right so this whole idea of pipelining and hierarchical pipelining called meta fiber pipeline is important and this can all be done in the context of parallel patterns okay so what this is mine so we talked about machine learning algorithms we talked about the trends larger models sparsity we talked about the role of the nations of languages and how domain speak languages can be represented by power patents now let's think about how we might accelerate a software 2.0 applications with these ideas in mind right so when thinking about building software 2.0 accelerators and that Semenova three trends became evident one you know we've already alluded to it's the fact that the idea of putting multiple processes on the core I'll put them up so pretty multiple cores on a chip in all to improve performance whether those be complex caused from Intel or more streamlined high throughput cause from Nvidia is basically running out of steam right and so turns out it turns out with the more fundamentally that the cores are inefficient and then as you try to put more of those cores together you get even more inefficiency the other trend that we saw is the convergence of training and inference so traditionally people think well I'm going to do my training on a GPU and I'm going to do my imprints on some very high efficiency device that might even be something specialized or could be a CPU and and the issue here is sort of you know how many things am i doing at the same time how big is my back size right and so GPUs can exploit large back sizes and maybe they're not so efficient for small batch sizes but what we've seen is this convergence of training and inference because in many instances you want to do training while you're doing inference all right and so what you want is an accelerator is just as efficient at large batch inference large batch training at least I should say and small batch inference and then lastly what we the last trend that we saw is that this idea of data flow computation is much broader than ml and data processing that many other problems in HPC also fall into this general class of data flow operations that can be optimized by an accelerator which focuses or natively supports data flow so we see this general applicability of acceleration beyond software 2.0 so what kind of support do we need our accelerator to provide the capabilities that we need for the next generation of ML computer so clearly I've argued that hardcore power patent dataflow is the native is a natural ML execution model and should be natively supported in the accelerator the need to do to enable very large models to be trained is becoming increasingly important for these large language models and also for large recommender models which are dominant computation in lots of areas and large a piece HPC models right I talk to people at National Labs and they want to to Train very large models that come from computational simulations and and so that is the thing that's driving the need for terabyte size models the use of sparsity right so large models and sparsity go hand in hand in order handle very large numbers parameters eventually things have to becomes fast and the other driver of course is graph neural networks then in terms of sort of mapping these models to the to the architecture there are a number of ways of looking at how one takes a ml graph of operations and extracts the parallelism and you want to be able to do this as flexibly as possible you want to be able to exploit what's called model parallelism between the different elements of the graph and also you'd like to exploit what's called data pal ism where you essentially are running different sets of data on the same graph so flexible support for mapping these graphs and lastly you know as I said data processing is a key component of the whole amount or pipeline and supporting that efficiently is becoming more and more important so how are we doing this in Samba novo we're using the idea of a reconfigurable dataflow architecture or RDA and this is an architecture that is designed to accelerate the execution of parallel paths right so it's a tiled architecture of course you want to come up with an architecture that's stable and so tiling makes it scalable and makes it efficient to implement and as you see the cartoon on the right here there are kind of three key components to the architecture so the first is the what's called a pattern compute unit which is where the computation reconfigurable computation component of the architecture there's a pattern memory unit which is of course you know you've seen multiple copies of each of these units of course which means and of course that the memory is distributed distributed so it's a you know naturally on the on chip the the memory is distributed and then you also have a interconnect which is a set of switches and interconnect which connects the different pn use patent compute unit and patent pan memory units together and then lastly to connect to off chip memory we've got to address generation units and units that take multiple addresses and and configure them and coalesce them in order to connect and to support off chip memory so let's look in a little more detail what the patent compute unit looks like as I said it's it's a configurable unit which is essentially a pipeline of Cindy functional units and so each of these simply unit units is of course capable of doing multiple operations at the same time in a Sindhi fashion so that's one axis of parallelism the second axis of power is a course across the pipeline right so you've got multiple you've got both powers and within each stage of the pipeline each of the 70 units and then of course you've got powers and that comes from the fact that you are operating on different sets of data in each of the different pipeline states and so you've got multiple elements of parallelism just within a PC you then the PMU essentially is a multi banked memory with a number of functional units designed for doing the addressing right so what you have here at the high level is a distributed decoupled access execute reconfigurable architecture right so that's a lot of words but essentially what what that comes down to is the ability to address independently lots of different sets of data and then stream data both within the functional units and across the the network to other functional units such that you can set up computation which is very efficient and very streamlined so as an example of how that works we see this snippet of a graph from a machine learning application and and you see that the different components convolution pooling Bachelor and some and so on are laid out spatially on the the network of compute and memory components and what you have is you have the communication between [Music] the unit's it's very efficiently captured by the connectivity between the PC use and the PM use right and so this avoids having to go off chip in order to communicate between the different kernels and which makes it possible to reduce the amount of off-chip families and so this is an important consideration getting very high performance and so this dataflow execution maximizes compute bandwidth by exploiting the parallelism within the simply units the parallelism across the pipeline and the streaming of data and and between the different compute devices and minimizes the memory bandwidth because the communication is local right it doesn't go off chip so the initial implementation of this dataflow architecture is the SAB anova systems cardinal SN 10 reconfigurable data flow unit and so this chip was designed in TSM c7 nanometer which is basically the the most recent TSMC process it's it's a huge chip forty billion transistors more interestingly 50 kilometres of wire provides hundreds of teraflops of computing capability hundreds of megabytes of on chip and direct interfaces to terabytes of off chip memory and of course to other SN ten rdu chips so what this gives you then in terms of program ability and efficiency is the ability to break out on the traditional trade-off curve that we see as defined by conventional architectures you know you've a six which give you very hi energy efficiency but are not programmable at all and then CPUs and GPUs which give you much lower efficiencies in terms of energy efficiency but are much more much more programmable because they are instruction execution based now the odd to you is data flow based and it's reconfigurable and so it allows you to give you get much higher efficiencies much closer to ASIC but it's still flexible enough that you can reprogram it efficiently for machine learning and other data processing sorts of applications so if you think about high performance there's this fundamental you know tension between optimizing computation and computation that's essentially what one wants them to manage in a high performance implementation and you know traditionally if you look at conventional architectures CPUs and GPUs they haven't given you the ability to program the data or the communication the data flow right so they focus on programming the compute now with the Rd you you can program both the compute and the data flow and the communication and this gives you the ability to get tremendous performance improvements you know in excess of ten times performance improvements but also allows you to unlock applications you simply couldn't do with conventional architectures we call these kinds of applications zero to one applications right so things that you couldn't do before now that you can do and the key idea right which I've already alluded to is the traditional execution on accelerators such as GPUs does things one kernel at a time so you do the matrix multiply kernel and you stick the results in high bandwidth memory you do the norm calculation and you stick there you pick the data up from high bandwidth memory for them you stick the result you do the sick the result in high bandwidth memory and so on and so there's lots of data that crosses the boundary between the accelerator and the off chip memory now the data flow spatial way of course is to lay this computation out especially on the chip and move the data very efficiently between the kernels using the on chip communication network and so eliminates the overhead maximizes the utilization and gives you people to need to use off chip memory which has lower bandwidth which much higher capacity and that enables you to do things that you couldn't do before so as an example of you know how we generate these optimized spatial mappings with our software environment that we call Samba flow we could take a PI torch representation of the ml algorithm which will give you a data flow graph and if you were to just map this graph naively you'd see communication going all over the place and of course if your communication is going to off chip memory you don't care right because you're gonna pay that cost to go and pick that data up but in the off chip memory you don't care where those addresses necessarily are coming from however if you analyze the graph and layout the graph to optimize the data flow that you can get a very very regular layout which gives you very high efficiency and very high utilization of of the interconnect on the chip and the result is that we can take a single data scale system right or even and and that or even as small as a single are you and we can do and we can train models which have you know hundreds of of billions of parameters in the model and so this training mechanism that we call salmon over one so you know at the beginning of this year we announced that with a single data scale system we could train a hundred billion parameter model and so this enables you to take what was you know a thousand GPUs with represented 64 dgx two systems and reduce it to a corner rack with a toddy use right and so doing this enables you to increase performance and the cost performance dramatically and do it with a footprint and an energy a physical footprint and an energy footprint which is much lower and the key capability as I've mentioned is this ability to to do data flow and move data efficiently on chip reducing off chip bandwidth and then that reduce off chip bandwidth enables you to use conventional D Ram at your option memory which enables you to handle much much larger models efficiently and so you can have the same programming model so in fact in order to use 64 DDX two's you've got to kind of change the programming model and figure out how to make things very data parallel with the rdu based system we had a single programming model which takes a advantage of model parallelism to scale the model across multiple audio chips very efficiently so that we can do these hundred billion parameter models and of course in the future we're going to scale this approach and so the goal of course is to get to in parameter models and you know once you get to this size model the accuracy which you can get on that natural language toss is dramatic so you know so talked about the machine learning a component of the data science pipeline you know which starts with training a model from the start we call de novo training where you train it from from the very beginning but typically what happens is you start with an existing model and you might tune it to your particular environment particular problem right so this this idea of scaling and optimizing a model is typically done you know at you know by taking an existing model maybe a bird model or a recommended model and then tuning it for your particular environment and so then you might deploy that model and of course you'd want to do that very efficiently and you might do that today with a CPU because GPUs aren't good at doing single batch inference but with a Noddy you are a single batch inference is as efficient as our large batch training and so you can do that efficiently you do efficient in print serving on the odd you and the advantage of doing inference on the IDU is that there are certain models where you might want to incremental e-train and this will enable you to get even higher accuracy so you can train them to the particular data that's it's actually being being used and so this capability is provided by the rdu because you have the capability of doing training as well as inference and then if you go further forward to the beginning of the data science pipeline you may also need to do a bunch of extraction extraction transform asian and low ETL and in order to prepare the training data for the training exercise and so this data preparation step is also something that the rdu is capable of doing by mapping sequel style data flow graphs to the architecture in much the same way that the ml graphs have been mapped right so here you see a much wider set of things that can be done to accelerate the whole data science pipeline and this all is enabled by rika treatable data flow units and the ability to very efficiently process these data flow styles of execution which as I said is kind of the predominant execution model across the different things that you want to do in data science so in conclusion then you know if you look at the three trends that we see emerging in next-generation compute what we see is the replacement of multiple cores on a chip with this new execution model that we call reconfigurable data flow and we see how reconfigurable data flow allows you to do very efficient large batch training but also very efficient you know single batch inference because you don't need to exploit huge amounts of data parallelism in order to get high performance you can use model parallelism within the graph very efficiently and furthermore you can use the memory bandwidth very efficiently because you keep most of the communication on chip and then lost the last trend we saw of the applicability of software 2.0 compute beyond ml I didn't get a chance to go into a huge amount of detail here but I've made the case that that a lot of the computation that you want to do outside of ml I show you an example of sort of data processing but also thinking about high performance computing sources applications such as FFTs material processing and the like they all have this data flow like component and what we have is a general-purpose reconfigurable dataflow architecture and so you can map these sorts of problems to the architecture get very high efficiency we've got lots of compute lots of interconnect on the chip and the whole result then is that you can accelerate both the ml components of your application and also the more traditional high performance computing components of your application at the same time so with that I'd like to take questions if you want to contact me you can contact me XM and over at the following email address if you'd like to talk to to care about doing things in partnership with Shaban over you can contact ken there so at this point let me stop and ask if there are any questions can they think thank you so much for that presentation and I think this is really interesting to think about in the context of HPC for science and engineering and and how it might affect both our approaches to computation in the future as well as the infrastructure we might put into place so that we think we have a few minutes for questions and well welcome those via via chat or or voice 