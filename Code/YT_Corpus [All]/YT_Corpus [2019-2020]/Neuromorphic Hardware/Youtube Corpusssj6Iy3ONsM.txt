 Hello, my name is Galen Reeves . I'm an Assistant Professor here at Duke University. I have joint appointments in the Department of Electrical and Computer Engineering and also the Department of Statistical Science. Broadly I use my background, which is in information theory, signal processing, and statistics, to try and attack modern problems in high dimensional inference and learning that often involve complicated distributions connected with large amounts of data. So one application area that I'm working on is a community detection in networks. So this kind of a problem has a long history in the social sciences and biology. So maybe you observe a network of interactions between different individuals. Based on these interactions, the goal is to find meaningful community structure. And this is an important but difficult problem, and the problem is that there's lots of possible structures that may appear to be consistent with the data. So a huge amount of algorithms have been devised, and recently there's been a lot of theory trying to pin down exactly when do these algorithms succeed, that is they find meaningful structure if it's there, and when did they fail? What we're trying to do now is use even more advanced theoretical tools to really understand this problem more generally in kind of applications that are less of kind of a toy or a theoretical toy, and more at least motivated by behavior seen in practice. So again, this is a problem where it's you start with a network, there's just edges, and you wind up connecting this with a random matrix theory and seemingly unrelated problems involving estimation and Gaussian noise. Another problem I'm interested in involves a question of what types of structures or dependencies can be modeled using deep neural networks. So empirically we've seen a lot of good performance. If you want to see classification, reconstruction, they can get great performance, but there's some questions that we don't yet have answers to and it's impossible to answer these empirically. These actually involve questions about how well are you modeling interactions in a distributional sense, and so this is where we can use ideas from theory, drawing upon ideas from statistical physics, random matrix theory, to at least provably understand what can and cannot be represented using these neural network architectures. The idea is that once you have this understanding, it leads to more principled techniques that also have theoretical performance guarantees. The great thing about being at iiD is that it has people from a bunch of different backgrounds working on related problems. So I can constantly talk to people that may say, "ok, this problems relate to something we've seen over here, here are some techniques we've used" and from that I can think of new solutions to the problems I have. I can also learn about interesting problems that other people have, and this leads not just kind of me think about my own problems differently, but also to collaborate. So I've collaborated with probably five or six other faculty affiliated with iiD and as well as with their students. 