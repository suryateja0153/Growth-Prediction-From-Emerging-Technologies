 okay why don't we start good morning my name is someone huh from a Microsoft quantum team I'm one of the researchers in the team and I'm going to tell you well I'm going to give you a brief overview of on computing years what is it and why we do it and how we do it so the first well we would like to understand what quantum computing is computing by in general means that it is to manage and process and communicate information for various purposes I don't want to define what the information is I just rely on your intuition so we give you an what the one of the first general purpose computer computer looked like it is a Aniak in the installed in the pennsylvania making the 40's and it was designed for the purpose of calculating the trajectory of a ballistic missile and then you put the first program in ran was actually not about that ballistic missile but about some nuclear physics calculation initiated by just don't run for more money on others for the manhattan project so it was Kathleen some physical process actually now with the invent with the invention of transistor the machine does so smaller and so much faster and the physical reason that the Technol the physics that enable this techno technological innovation was based on the quantum physics but quantum computing is not about that quantum physics here it's more of a fundamental level so what is that so to give you an idea let me begin with the probabilistic computing what is a problem based computing let me give an example say I give you a function that's defined on a hundred variables and I ask you to find the integral of that how would you do that it's 100 dimensional so if you make a fine mesh along say 100 mesh points along his ration and the number of points you have the sample would be 102 100 astronomical number you shouldn't do that but more straightforward way is that you randomly sample points in the theta average that will give you a very reliable estimate of the integral and for all practical purposes that's the best but you know another random algorithm is that if I give you a black box of function polynomial function I guarantee that you this it function is a polynomial in say hundred variables then how would you say this is nonzero well if I don't tell you anything about the function in the only deterministic algorithm that we know of is essentially going through all possible inputs there will be like well if the x1 in our binary are variables then there will be two to the hundred so you shouldn't do that but the problem is method is that you simply randomly sample input points and test a perceiver and surprisingly there's no deterministic algorithm is known for this problem another probably a practical your active useful algorithm is that if I give you a factorization of a1 matrix C into a times B and how would you check that the as a factorization is correct well you could do the matrix multiplication and that will impact this close to the end cube complexity but randomized algorithm can do it faster because you can randomly sample on the input vector and then check the output of the two equations and you could do it sufficient many times then you have a high confidence that the equation is correct and that's good for our purposes so theoretically we can imagine the this problem is computing in this way we start with just a flat distribution over some number of variables and then my algorithm transforms that distribution under the classical rules and we end up with a some concentrated distribution that is hopefully constant concentration around the desired solution this is the this is what's going on now observe that the probability space the space of all probability distributions at the individual nav it has exponentially many entries and in any propolis computing we don't keep track of this probability density function but we just work over examples similar principle carries over to the quantum computing the difference is that you take a square root so if I had a bit then there will be a two numbers well it's actually essentially one number because I am constrained to the condition that the probably probabilities had to sum to one and I can denote that probability as a two component vector with the positive reals in the quantum computing we take a square root so each of the entries the state of my well square root space of square roots probably the probabilities will reside on the complex numbers and each entry takes values in the complex number whose modulus is between 0 and 1 and with a rule that the amplitude squared is equal to 1 now here the amplitude is a technical word to mean the square root of probability so that's cubed and it is apparent that if I have a n qubits then there will be exponentially many complex numbers to describe that distribution however in analogy with the probability computing we do not keep track of each amplitude we shouldn't because there are too many and some fundamental feature of the quantum physics relates the two models if I start with a some quantum amplitude vector if I look at it look is a rather not to appropriate but this most close term in the libyans word if I look at it then I would get the I'll come 0 with the probability given by the amplitude squared and probability 1 with amplitude given by this I would probably be given by them play the square again and in a sense this fundamental property of quantum physics limits our ability to use the quantum physics because we're back to the probability instead of the world of amplitudes and to exploit that quantumness here are the elementary gates unlike the classical circuit where everything can be generated by NAND gates there are several in a quantum case but not too many there are only well I listed actually over complete set I only need this much this and that and that one you can omit that one for example because this one is is a application of twice of that and the one notable feature of those matrices is that they are all unitary why unitary why do we need a linear algebra here so the the fundamental feature that quantum phase is quantum measurements maps of amplitudes to probabilities we shouldn't measure them we shouldn't collapse them you shouldn't intervene the intermediate step of quantum computation well if you do so then you're about to the classical probability computing you're not exploiting the qantas here so we shouldn't look at it and the only way to not look at it but somehow operate is to go through the unitary operation and and therefore we have a unitary set and I want to emphasize that unlike some popular social media exploits it quantum is knots are real in the following technical sense in the probables computing I'm repeating myself it was all handled by the exponentially many positive real numbers that's up to one taking the square root your end up with exponentially many complex numbers whose modular squared sum is one and quantum circuit is nothing but a unitary matrix multiplication on that gigantic vector therefore a logician will say Oh anything that can be done with the quantum computer can be done on the classical computer there is no empirical difference anything that can be decided in the classical computer can be done in quantum and vice-versa however as a computer scientist not every finer things are equal some things are much harder because you have to go through many more steps to accomplish a job and that's where the component computing is so why do we care about the quantum because it can sometimes address classically intractable problems and I'm giving a 2 or 3 most famous examples here so there are quantum algorithm primitives that we use over and over again we are waiting for a new primitive but so far these are pretty much complete least so one probably the famous one is the first one the starch over the database of n entries can be done in the square root time instead of looking at n entries there's a quadratic speed-up and a similar principle can be applied to the sampling problem where you where your accuracy converge is much faster instead of the in order to achieve the absolute accuracy in your statistical measurement then you have to go through one over epsilon squared number of samples to achieve that goal but you know particular quantum setting that accuracy convergence can be made quite radically faster and the Fourier transform which typically oh yeah which in the classical world takes about n log n time where you have a n component vector this becomes exponentially faster you lose you you you you you forget about that ant vector but left with a log in that and certain aspects of linear equation can be obtained and exponentially faster in the number in the dimension if you have a matrix of condition number Kappa then classically you need n square root of Kappa number of operations to calculate something about the solution but quantumly the the dependence and the condition number remains but the dimension factor the dimension dependence gets exponentially better but note that B all these primitives are applicable only for the data that are generated on the fly it cannot be used for real data for a simple reason it takes linear time to even to load the data so there's a bottleneck however there is a exponentially exponential speed-up in the case of for the user-defined data when it comes to the simulation of quantum physics so remember that ENIAC was first used to simulate classical physics now quantum computing is natural to solve problems that involves simulation of quantum physics and the in this exponential speed-up is very typical natural so what can we do with that the one appealing example is the following you may be surprised that the ammonia molecule consumes well the production of ammonia it consumes like a 5 percent of all global natural gas consumption or like two or three percent or one or two percent of all global energy consumption because it could eat the this chemical process in the at the industrial level it requires like hundreds of degree of temperature and the hungriest times times the atmospheric pressure so to to make that environment you need a lot of heat and surprisingly the there are some bacteria in at the roots over some vegetable that does this job at the room temperature at the natural atmospheric pressure and after several decades of research actually we don't know why it's likely more in depth there there the bacteria has a special enzyme and the in the middle of that enzyme there there lies a this particular molecule I'll iron Malita Malita and cofactor it's called it's not too complicated molecule it involves like a hundreds of atoms it's not terribly complicated and this molecule is believed to be the key for this process that converts nitrogen a molecule in combined with the hydrogen to produce ammonia and to understand how this works we need to compute the energies of various chemical configurations and that is computationally limited even the the fastest supercomputer is not sufficient to address this problem and you may expect that some experiment will tell us how this molecule works but it is still crude maybe this process involves like a several tens of steps each of which is hard to compute and we don't know how it happens if we understand how it happens then there's a check good chance that we can utilize them and to make the ammonia production much more efficient and some estimates so I'm very shy until a reasonable estimate tells us that even very small scale quantum computer can help us here like a one that has only two hundred cubits or so may be able to help under help us understand how this molecular works in another famous example for the application of quantum computing is the the factoring problem almost all cryptography practical protocols may relies on the hardness of factoring if you could if I give you a composite number and don't tell you about the prime factors then it is hard for you to figure out what those private vectors are and based on that we build out the public encryption schemes and if you have a like I say hundred the course in the classical computer then merely factory and thousand bit numbers takes about order of a year and if you increase that number to a two thousand then that will take a order of the age of universe to compute extrapolating the current technology but if you have a quantum computer that you that has thousands of qubits then that problem can be solved in the order of days well this all this is due to the exponential scaling you see initial gap here the for smaller sizes quantum computer will not be a cup help because of the huge constant factor I will get to that huge constant factor later so if we then then this poses a great problem or great concern to the world of cryptography if the pub if the current technology is broke it will be broken by on the computer what should we do the quantum physics helps again here there is a private key distribution protocol based on the quantum physics in such a way that any investor pain will be detected and is guaranteed by the laws of physics any inherent Lee uses the quantum correlations so called entanglement and robust implementation is still being developed and to to protect our information in the post quantum era in the future there is a quantum resistant classical crypto methods and there are candidates and it will replace the current probably keep crypto systems based on the factory and with a requirement that it should be interoperable with the existing facility the mentioned the quality private key distribution is not it's not considered in this proposal and such a purely classical algorithm is now being standardized by Liszt it's I heard that it is round to where like a tens of protocols are the competing with each other to find the best one so so that was major applications or easy to access application so upon the computing now how do we make one what are the challenges and what where we are now there are many obstructions in the in architecture of a computer in any computer there should be an hardware that actually operates something and there should be a some programming support to control that hardware and on top of that we have to think about what algorithms can we learn and at Microsoft we have investments in all stages and how to make one in this talk will be focused on the hardware side and you will hear more about the quantum programming side in today later so there are many qubits proposed developed by that various industrial companies some are based on the faults on its polarization some are based on the superconducting devices and you take a very particular physical states in this device and some are thinking about the quantum dots your your separate where you isolate very tiny atoms or artificial atoms in your leg some are thinking about the atoms that are trapped in in an optical lattice we are thinking about a topological qubit what is a topological qubit that may sound scary but not so much so let me tell you a bit more about so to motivate let me give you the classical error correction the overhead let's talk about the classical era crashing overhead here is a picture of deep space probe that we have to communicate over and if the communication is very noisy you can't really hear the clear sound over that probe that are in the deep space lightly well not light years but it takes it takes for light to travel it takes about I don't know a few hours to try it for the light to travel to that probe so imagine that each video sent is corrupted by the other bit it's it's your bilious fleet but up with some small probability then then you can correct those you can fight with those noises by make redundant message so you send 0 0 0 in place of 0 and triple ones in place of 1 we say that these beats are physical bits are actually sent and these beats are logical piece that we meant to send now the receiver which can be at the probe or us decodes potentially corrupted data if I receive triple zero or two two zeros and one that I and I would infer that message was just zero in the other case I infer the message to be one and that's reliable because the logical error probability that is I inferred it was zero but the sender actually meant one or vice versa that logical error probability gets smaller for any for a given physical error rate by this repeating our procedure and if you do it more clever well you could do that analysis and given the fiscal error rate the number of actual physical beats that you have to send as a function of this physical error rate given the target logical error rate scales like this so as long as your error probability is less than half then you can achieve you can send some information but the number of actual physical piece that you have to send increases quite rapidly if you're if your physical noise is the exactly the same thing happens in the quantum world too so even with even though we are dealing with the quantum information the square root of probabilities we can fight with errors by encoding our according information into the correlation of quantum particles and there exists a quantum error correcting code in as much as we have a repetition classical error correcting code and the similar analysis tells us this figure if you want to achieve logical error rate to be 10 to minus 10 here the error rate is the the probability that the qubit is corrupted given a clock cycle or a message if you wish if we wish to achieve this very small logical error probability then if you're a physical error rate you say 0.1% then you would have to deal with thousands of qubit you have to send oh you need a thousand cubits only to send one qubit but that number dramatically decreases if the physical error rate is decreasing now not a difference the clock in the classical world the threshold below which we can send any information was half zero point one in the quantum case it is order of magnitude smaller and that's the challenge we are facing that's why we do not have a general-purpose quantum computer so it is critical for scalability to achieve low physical error rate otherwise the overhead from their from the air crashing will be too large and here comes well and that that differentiates our approach from more conventional approach the one of their prevalence technology to build a cubit is based on the superconducting circuits and they have error rate around here so you will need approximately of thousands of qubit to make a general-purpose cone computer but topological qubit if we have one it is expected to have much lower error rate hence much smaller overhead from the economy crashing that's why we are focusing on good care of it and that is very analogous to the this peak this picture if you have a chalk board you and you write something here the writer was counting something obviously the small small particle in your chalk is attached to your blackboard by a some very weight chemical bond or friction but it can be easily washed away it doesn't really have a correlation between the amount of tooth and it doesn't fight with noise anymore but if you count using knots as the old ink indeed then it can last for thousands of years because as long as a string is unbroken the fact that there is not isn't invariant so topological qubit conceptually in a very abstract level does the same job so here's an actual example where the where that autonomous correlation helps this is a figure from 1980 paper where they measured so-called Hulk conductance across the sample under a hype low temperature and the high magnetic field and they just measure the resists resistance of that sample across the certain big configuration what they found was that there is a plateau in the resistance there are pretty wide and resilient to external parameters they tested various geometries and various other settings they even alter the current year ascending and they didn't see any change and this accuracy is so remarkable that it is for what know for a long time used as used as the calibration standard whatever example you make how dirty you are it doesn't really matter the the autocorrelation amount of electrons gives you robustness and we want to qubit that exhibits a similar phenomena so there is a proposal theoretical proposal that the signals that oh there if you fabricate such a device by putting a some very small wire on top of very special with material in a low temperature and apply certain magnets then there will be a some hidden states that we can use to build a qubit and one signature for that purpose has been experimentally observed so we are optimistic that we will be able to find a material and setting such that the the many-body electrons help us to build a qubit and that's what we are focusing on that's how we are going to build a quantum computer so that's a recap what I have said this morning so don't be scared about the quantum computing it's just a square root of probability a probabilistic computing and to make use of that quantumness we should remain in the space of amplitude instead of probabilities and that's why we should use the unitary operations instead of other classical analogues and naturally this quantum the setting can simulate quantum systems efficiently whose job is intractably on a classical computer and therefore what a computer will likely have applications in the common chemistry and material science and on top of that there is some computational problems including the factoring is becomes much easier on the quantum computer and the quantumness is more subtle than excuse me more subtle than the classical counterparts so to overcome the overhead from the error correction the physical qubits fidelity is very important and we are focusing on a very good qubit instead of many and we are our qalam covers whole every aspect of that from hardware to intermediate program language to higher level algorithms and others and you may wish to check out our websites that illustrates further conceptual explanations and our programming our tool sets and you will hear more about it later this today and I'm ready to take your questions thank you how long do you think it'll take before you have a functioning topological qubit is it one year ten years well I you're not allowed to say yeah I personally don't know it's still under investigation so iBM has already functioning quantum systems in their cloud but that's not based on topological right that's the residents magnetic residents I think or I'm not sure so do you have any as already thought that that's what they're doing is any good or is it if there's something interesting device in the interesting in that you can actually implement some experiments and apply gates there but it's only five cubits or well I I forgot the exact number maybe I shouldn't say five it anyway small number and anything that can be done Condor machine it can't be done I see malaria to very easily so scientifically interesting and we'll see where we go get your example of the ammonia locking so how is it that a quantum model helps us here is the steady state or because it doesn't give us anything with with respect to time C time involving Oh actually a good question in fact in quantum computer is much better to simulate dynamic scenes that are estimating static properties however but the time scale where the chemical reactions happen is far smaller than other thermal time scale so that's why we are asking about the stepping numbers there and the quantum computer can help there too because you can estimate the energy of intermediate steps so you know the catalyst is like a financing you will make a profit in the end but to do to make to that to make a dot profit you have to invest it too much and that's why some processes are as too slow in nature and the catalyst is like intermediate financing facility where you learn small money and then make a small profit and you to keep doing to to reach the eventual goal and how we land money and where to run money depends on the molecular structure and the typical method would be well okay here here is my catalyst to understand that well let me attach an atom here how much energy do I get and you need to calculate that oh if I do if I know that energy then I can calculate the rate the chemical reaction happens along that direction and you do with many times here the bottleneck used to estimate that energy and the current computer will go facilitate that process you you encode your molecule into account quantum computer and oh and let it find the ground state and you make a measurement the classical method exists but there are very crude so you're basically finding the global minimum in a high dimensional function it is it is like that yes yes yes but it's much more it's much better than the combinatorial case because it is supposed to be a nature thing so you can simulate what nature does in the quantum computer and find the ground state energy one more question so I read a lot about quantum inspired algorithms yes as as people talk about as quantum computing will overtake classical computing then they keep especially in computations as they thought there are no fast classical solutions quantum inspired algorithms keep popping up is that something that that you see in your work and the research looking for new algorithms that you run across quantum inspired ones I think you will hear more about that like it in ten minutes so I defer that quick answer to that talk okay thank you very much [Applause] 