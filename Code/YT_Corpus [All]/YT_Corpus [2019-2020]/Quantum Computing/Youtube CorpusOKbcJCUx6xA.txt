 - [Jin] When data is mapped from its input dimension into the quantum computers Hilbert space, it intrinsically becomes cast into a higher dimension. I'll cast you into into a higher dimension. (upbeat music) Welcome back to another episode of Coding with Qiskit, I'm your host, Jin. This week we're gonna be combining two really hot topics, quantum computing and machine learning. If you're new to this series, each week we talk about different quantum algorithms. So make sure to like, and subscribe. The example that we will be coding today, is a Quantum Support Vector Machine. So let's first talk about, what a normal support vector machine does. Essentially, this is a way for a computer to learn how to classify data into two different sets. Let's go through a simple example. Let's take a look at my aquarium. So in my aquarium, I have a school of fish. These are ember tetras, and a colony of shrimp. When I feed them, the fish will crowd to the spot where their food is, and the shrimp will crowds to the area, in the tank where their food is. And if I'm looking at the tank from the front, and I have a list of the locations of each shrimp and fish, and the tank in this xy plane, I can infer by the position of each critter in the tank, whether it's a shrimp or if it's a fish. So now, if I add a new critter to the tank, the idea is to be able to classify it as a shrimp or a fish based science position in the tank. So if we have this training set of data, we can find a line or more generally a hyperplane. I can divide the space, so that on one side of the tank, the critters are classified as shrimp. And on the other side, they're classified as fish. So this is all very nice. But what happens if the animals are arranged so that we can no longer draw a nice line that separates our two classes? Let's see what happens when I'm not feeding them. Because the fish are schooling fish, they'll tend to bunch together. However, the shrimp will tend to disperse themselves randomly in the tank. So how do we classify the animals into shrimp or fish based on the location data now. The answer is to introduce what's called a feature map. So what we're gonna do is map the location of each critter from the 2D plane into a higher dimension 'K'. Now what we can do is compute the optimal hyperplane in some higher dimension that can separate the two classes. This involves computing the distances between our data points within the higher dimensional space. So if 'K' is very large, finding these distances can be very computationally expensive. So simpler thing to do, is to use what's called the 'Kernel Trick'. The kernel is some easily computable function that takes our two data points and gives back a distance. And the kernel can be optimized in order to maximize the distances between our two classes of data. Unfortunately, some kernel matrices are difficult to compute classically. So this is where the quantum computer comes in. If the kernel can not be estimated classically, quantum machine learning shows a lot of promise in being able to use the multidimensional computation space of the quantum computer in order to find the hyperplane. When data is mapped from its input dimension into the Hilbert space of the quantum computer, it intrinsically is cast into a higher dimensional space. And as we showed, finding a dividing hyperplane is often only possible in a higher dimension. So now let's program a quantum SVM with Qiskit. So the first thing I'm gonna do as always, start up my environment. Kinda activate kernel with Qiskit, and this is where we have Qiskit installed. And I'm gonna quickly install one thing that I need for this. So I'll do PIP install, Qiskit, Aqua CVX. (gentle music) All right, so now I'm gonna start up a Jupyter Notebook. (gentle music) I'm gonna start a new Python 3 Notebook. All right, so I'll start with my imports, import Qiskit. Gonna import some stuff from (indistinct voice) lips. I can do some plotting. (keyboard typing) Important numb pie, (keyboard typing) Import some of my machine learning tools from Qiskit. And this data set is the ad hoc data set, but we have a couple of other data sets that can be used as well. And the ad hoc data set is just a curated data set that we can use for demonstration purposes. (keyboard typing) We're gonna import my basic air simulator. (keyboard typing) We're gonna import my quantum instance. This will run my experiment. (keyboard typing) I'm gonna import my feature map, here's my quantum feature map. (keyboard typing) I'm gonna import my quantum SVM QSVM. And a few last utilities. (keyboard typing) Split data set to data and labels. And that label to class name. Oopsie, Qiskit. (gentle music) Typo, there we go. All right, so let's set up our Chrome SVM. I gonna set my feature dimension to two, training set data size. So it was my initial training data set. (gentle music) (keyboard typing) My testing data set. This is the dataset that I want to classify. Plug in some random seeds to randomize things. (keyboard typing) I'll set my shots to say 10,000, just for my simulator. Okay, let's set up my data, sample total training input test input class labels. We'll pull these from my ad hoc data. (gentle music) Training size is equal to training data set size. Test size equals testing data set size. Gap is equal to 0.3. So this is a gap in my higher dimensional space, that will separate my data in the ad hoc data. And it's gonna be my feature dimension. And then let's go ahead and plot this data. (gentle music) Okay, and then set up my data points. (keyboard typing) Split data, set to data and labels for my test input. Print, class 2 label, okay. (gentle music) All right, so this is my ad hoc data. This is my training data that will train my QSVM, in order to classify some test data. So now let's set up my backend, to actually run the QSVM. I'm gonna use the basic air chasm simulator. (keyboard typing) Okay, let's plug in my feature map. (gentle music) Easy feature map. (gentle music) Plug in my future dimension and the reps. All right, and let's set up my support vector machine. Plug in my feature map, my training input, test input. Okay. Plug in my random seed for SVM. (keyboard typing) Set up my quantum instance and this will actually run my experiment. (keyboard typing) Plug in my backend shots equals shots, seed simulator. It was gonna be a random seed. (keyboard typing) Seed transpilar is also a random seed. Okay, and then let's get the result back. (keyboard typing) Opsie! A little typo here. Shots is equal to shot. Here we go. So I just stayed here, was I set up my backend. I set up my feature map. I plugged it into my qualm support vector machine. So I have my feature map here, my training data and my tests data that I wanna classify. I'm gonna have my backend run this in a quantum instance. Alright. So let's take a look at what our kernel matrix looks like from our result. And once again, this is sort of the distance between each of our points in the higher dimensional space. (keyboard typing) Kernel matrix is equal to the results and just grab the kernel matrix during the training. (keyboard typing) And I'm gonna plot this. (keyboard typing) Matrix, the kernel matrix... (keyboard typing) Terp deletion, set that to nearest... The origin, set that to upper. And then our color map will set to, bone 'R'. Now let's show this. So this is what our kernel matrix looks like. And you'll notice that along the diagonal, we just have black squares. And all this means is that, the distance from each point to itself is zero. And what we're seeing here is the computed distance from the kernel in the higher dimensional space. All right, so now lets look at how well, our QSVM has predicted our ad hoc data. (keyboard typing) Let me predict. Data points, zero. Data points. Predicted classes, map, label to class name, predicted labels, and SVM labeled to class. (keyboard typing) Okay. So we're gonna print the ground truth. This is what we know the labels, the classification to be oops, ground truth. (gentle music) (keyboard typing) Format data points, and then print the prediction and see how well QSVM has predicted our test data points. (gentle music) (keyboard typing) Predicted labels. And then let's also print the testing success. So see how well our prediction, works overall. (keyboard typing) Testing accuracy. (soft music) All right, so here we go. So we have this set of test data where we know the ground truth of what they should be classified as your one. And our prediction, which was trained from our training data has accurately predicted how to classify our testing data. And we get a testing success ratio of 1.0. So that means we've perfectly predicted our testing data. So there we have it. In this episode we use Qiskit in order to classify our ad hoc data, using a quantum support vector machine. Why don't you try to run the quantum support vector machine with another dataset. We'll put links in the description below. Next week, we're going to talk about the most famous quantum algorithm Shor's algorithm. So be sure to like, and subscribe. Okay, see you next time. (gentle music) 