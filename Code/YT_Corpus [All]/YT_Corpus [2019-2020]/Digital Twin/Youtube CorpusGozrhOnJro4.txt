 Hello, I'm Larry Wasik with Aurel Systems. I'm going to talk today about a couple of pulp and paper process digital twin case studies. I will also describe why the use of a process simulator and dynamic data reconciliation is an essential enabling technology. Our digital twin approach is to use a first principles, dynamic process simulator to track a process live every minute of the day. Essential to that live tracking is the use of dynamic data reconciliation to correct erroneous measurements and allow a process model to be fitted to the changing dynamic process. This superior approach has allowed reliable and continuous tracking of pulp and paper applications, and I will describe two commercially implemented case studies. A plantwide energy system and a stock grade tracking. I will also briefly discuss other applications for a process digital twin. The goal of a process digital twin is to first find the current state of the process and then optimize it. This requires a good process understanding that will not succeed without a good model. The best path to understanding is a simulation model. But there is a problem with initialization of a simulation model with the current process dynamic operation. Aurel has uniquely solved that initialization by tracking the process with a proprietary and reliable dynamic data reconciliation. Dynamic data reconciliation is the concept of using many measurements from the process and validating them against each other. We want to eliminate faulty measurements and correct for bias in the data. We want to turn noisy and erroneous process data into actionable knowledge. This data reconciliation becomes the foundation of a successful implementation of a process digital twin. So how does data reconciliation work? In this example, if we had two measurements and we trusted them equally, we might average the flows to give 1100 litres a minute. You cannot take these three flows, add them up and divide by three to get a sensible answer. However, a mass balance model understanding what flows or input might suggest the net throughput is somewhere between eleven fifty to twelve hundred. Conventional methods would suggest that a feed of 800 litres a minute here is too high as a flow of 200 would give an exact flow and temperature balance. However, if we have a bit more information that the process is a tank and the inlet flow was one hundred a few seconds ago and now it is 800, this starts to explain some things. The outlet flow is on level control, and is working its way up from 1100 to 1800. And the temperature is logically on its way up from 15 degrees to near 40. Everything is actually perfect with these measurements. And these would fool other systems which do not use a dynamic process simulation that understands transient behavior. In order to take reasonable process action, one needs to know where the process is currently operating without being fooled by measurements that do not add up. Dynamic data reconciliation fits the first principles model through the data to detect and correct measurements error by trying thousands of trials each minute. Making sense of the data moves the bar from collecting data to understanding the current process operation. This supports suitable action such as better process decisions, accurate real time optimization and better monitoring, reporting and alerting. I'm going to demonstrate this with an example where I will introduce some false measurement errors. This is an example of dynamic data reconciliation in action. Measurements are read in as shown on the sensors on this drawing. Data reconciliation tries many trials and ends up with a fit of the model to the entire measurement set. The green numbers on the charts are the measurements and the colored are the data reconciliation results. Now, let's look at what would happen if we add some arbitrary error to two of these measurement. I'm going to add an error to the flow measurement here of 200. And an error to the temperature of 20. And now when I started running again, these measurements are out. This right now reads 1900 and 89, but the simulation is still marching along, knowing that those are wrong now. And it can figure out from the rest of the measurements what is really happening here. Here is an example where dynamic data reconciliation was able to ignore a faulty pH meter, but when the caustic pump failed an SMS Message was sent to my phone. A screen capture of that phone shows a trend chart of the measured and actual pH for the last five minutes to go along with that alert. We've picked power generation as an ideal project because it is complex and therefore difficult to understand by Kraft Mills. For our first process digital twin case study, we will look at the Power Island at the Crofton Mill. This mill has several boilers making high pressure steam, which is converted to medium pressure and low pressure steam. Measurements of all of the steam users do not add up to all the sources. Often, steam flows can be out by 50% or more. At this mill, at one point, the addition of a condensing turbine was considered. The CADSIM Plus Data Reconciliation computer connects to the mill process historian gathering new measurements every few minutes and setting back balanced flows and additional key performance indicators. Each morning, an e-mail is automatically generated showing the five worst measurements from yesterday and any measurements that were out of range. Prior to the CADSIM Plus that a reconciliation, we had implemented a MASSBAL based data reconciliation, which used conventional methods for reconciliation. However, there were problems with conventional methods. With process areas going down, it would fail several times a day. Sometimes it would fail two thirds of the day. In October of 2006, we implemented the CADSIM Plus Data Reconciliation and it is not failed a single time since. The dynamic data reconciliation computer takes raw measurements from the historian, fits a dynamic model through these points, and then writes back balanced numbers and KPIs to the historian. Operator information can come from these KPIs or from real time optimization. A clone of the dynamic model can be used by real time optimization to find the best move to get to an economic sweet spot. A clone of the model can also be used by an energy engineer to take the process for a drive into the future. And I will now demonstrate such a process test drive starting from current operation. Here we can see a power generation system that is typical. I'm going to take this for a drive starting from where data reconciliation left off a few minutes ago. What we have here is a recovery boiler that is burning black liquor, a hog boiler that is burning bark off the trees, a packaged boiler. We have pressure reducing valves which are going to reduce the steam pressure, but they produce super heat, which must be de-superheated and produces a bit more steam. On the other hand, we have turbo generators which can reduce the pressure and create electricity, which reduces our electrical costs. The system is fairly complex to understand how to operate these. Which are the best fuel, what ratio do we run the turbo generators and how much steam do we put through the pressure reducing valves? I'm going to look at this system by watching these panels and these gauges to take it for a drive and try some different What-If scenarios. First of all, let's look at the gas boiler. It's currently running at 15 tons an hour. Let's see if we can reduce that. Let's change that to 12. That's worth .6 million dollars a year. Well, that was good. Let's see if we can shut it down. Shutting it down would save us 3 million dollars a year in natural gas and saves us 17000 tons a year of greenhouse gases. So far we haven't done much for electrical. Let's reset our gauge here, because we know now what we're going to do with the gas boiler. We're going to just look at the turbo generators. We currently have 390 of flow going through the turbo generators. Let's change that by 10 and let's go to 400. That saves us .96 million dollars a year in additional to the savings with the shutting the gas boiler down. Let's add another 10. That's up to 1.4 million dollars a year, and you can see here the energy savings that we're making from not buying the electrical power. So that looks good! Let's try another 10. Let's go to 420. So we're currently at 1.4 million and adding that we dropped in energy savings. So somewhere between 400 and 420, there's a sweet spot. That sweet spot can be found either manually like I'm doing, taking the process for a drive or with a real time optimization program. And that optimization program can then feed the operator a dashboard that shows where he is right now, where he could go to make the optimal energy savings and how much money that would save during the year. This important slide shows the complexity of the problem and the energy savings that can be gained four different production rates. The horizontal dash lines indicate where the power systems can operate successfully without changes as the production changes, but money is left on the table. Each different production rate has a different optimal solution. At low production rate, we should shut the gas boiler and give priority to the condensing turbine. The next case is the average production rate, which is similar to the case that I just demonstrated with the simulation. At a 10 percent higher production rate, we minimize the natural gas on the hog boiler, maximize the turbo generator two and minimize pressure reducing valves. At higher production rates, we maximize the gas boiler and put more through the PRV's, while maximizing the turbo generators as best as we can. The benefits can be less than a six month payback. The dynamic data reconciliation based process digital twin allows one to visualize the optimal energy operating strategies in real time, reduce energy costs, reduce greenhouse gases, run What-Ifs scenario analysis, perform future prediction, obtain measurement correction and more information with the KPI's calculated. You can have better reporting, accounting, budgeting, and there can be reduced risk. Shown here is the actual mill data at the top and the data reconciliation results at the bottom. One should note that the steam of the actual measurements is shown as a higher value than the water coming in as feed water. That's impossible and data reconciliation corrects for that. At the bottom, we're seeing KPI's such as boiler efficiency and hog energy versus that from natural gas. If we learn, correct, initialize and only then optimize by playing a process by trying different moves, a bit like a chess game, that is a game changer. A second case study was the tracking of grade changes through the specialty cellulose mill at Témiscamingue. In this case, the CADSIM dynamic data reconciliation computer was linked to a PI historian. Using the results of stock tracking with dynamic data reconciliation, a dashboard was also generated to allow operations to visualize with a slider. The grade change can be tracked. You can go back in time and see where it was in the process or forward up till now and then into the future to predict when the process stock grade change might reach a particular spot in the process. And here this is going to meet this up flow tube at 17:44 and that's when we should make our chemical changes. I have shown to actual commercial applications of our model based process digital twin. Other pulp and paper applications might be soda-sulphur balancing in Kraft Mills, load shedding in a TMP Mill during high electrical cost periods, consistency control, wash water control, more accurate data mining and pulp property prediction. My hope is that this presentation has demonstrated why a first principle simulation based method of tracking the process is essential for a good process digital twin. And the benefits are to turn inaccurate measurements into actionable information, enable accurate real time optimization, create model based soft sensors, monitor and alert problems, and take the process for a drive into the future. 