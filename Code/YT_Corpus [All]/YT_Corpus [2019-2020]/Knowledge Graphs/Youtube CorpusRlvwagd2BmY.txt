 alright last time we started talking about pseudo-random graphs and we considered this theorem of chunk Graham and Wilson which for dense graphs gave several equivalent notions of quasi randomness that means that face now you do not appear to be all that we cover them but they are actually you know you can deduce one from the other there was one condition at the very end which had to do with eigenvalues and basically it said that if your second-largest eigenvalue in absolute value is small then the graph is pseudo-random right so that's something that I want to explore further today to better understand the relationship between eigenvalues of a graph and the pseudo randomness properties for much of pretty much all today we're going to look at a special class of graphs known as MD lamda graphs this just means we have n vertices and we're only going to consider mostly other convenience t regular graphs so this will make our life so much simpler and the lambda stands for that if you look at the adjacency matrix and if you write down the eigenvalues of the adjacency matrix then well what are these eigenvalues the top one because it's the regular is equal to d and the lambda corresponds to the statement that all the other eigenvalues are at most lambda in absolute value so the top one is equal to d all the other ones in absolute value right so could be basically the maximum of these two it's bounded above by lambda and at the end of last time we showed this expander mixing lemma which in this language says that if G is nd lambda then one has the following discrepancy type pseudo randomness property namely that if you look at two vertex sets and look at how many actual edges are between them compared to what you expect if this were a random graph of a similar density then these two numbers are very similar and the amount of error is controlled by your lambda particular a smaller lender gives you a more student reading graph so the second part of today's class I want to explore the question of how small this lambda can be so what's the optimal amount of pseudo randomness but first I want to show you some examples so so far we've been talking about pseudo random graphs and the only example really I've talked about is that a random graph is pseudo-random which is true random graph is pseudo-random with high probability but some of the spirit of pseudo randomness is to come up with non random examples come up with deterministic constructions that give you pseudo random properties so I want to begin today with an example a lot of the examples especially for pseudorandomness come from this class of graphs called Cayley graphs which are built from a group we're going to reserve the letter G for graphs I'm gonna use gamma for group and I have a subset s s of gamma and s a symmetric in that if you invert the elements of s they remain in s then we define the KD graph given by the scoop and set s to be the following graph where we the set of vertices is just the set of hoop elements and the edges are obtained by taking a group element and multiplying it by s to go to its neighbor so this is a Cayley graph and no Cayley graphs are you start with any group start with any subset of the group you get a KD graph and this is a very important constructions of graphs they have lots of nice properties and in particular an example of a kini graph is a Paley graph they're now related so a Paley graph is a special case of a Cayley graph obtained by considering the group the sitter group Marty where P is prime 1 mod 4 and I'm looking at s being the set of quadratic residue small people yes actually non-zero quadratic residues so elements of marpie that could be a square so we will show in a second that this penny graph has nice pseudo-random properties by showing that it has an ND lambda graph with lambda fairly small compared to the degree just a squirts historical note so raymond pay me pay me graph named after so he actually so it was from the earlier part of 20th century so from 1907 to 1933 so he died very young at the age of 26 and actually died in an avalanche when he was skiing by himself in Banff so Banff is a national park in Alberta in Canada and when I was in Banff earlier this year for a math conference so there's a also a math conference center there so I had a chance to go visit the Raymond baby's tone right so there's a there's a grave yard there where you can you can find his his home it's very sad sure mathematical timespan actually he managed to do a lot of amazing mathematical finding a lot of amazing mathematical discoveries and there are many important concepts named after him to you things like Paley Wiener theorem Paley zigmund little wood Paley all these important ideas in analysis named after Paley and Paley graph is also one of his contributions so what working or what claim is that this Paley graph has the desired pseudo-random properties in that if you look at its eigen values then the top eigen values accept so except for the top I can value Oh the other eigen values are quite small so keep in mind that the size of s is basically half of the group so P minus 1 - so four especially larger values of P these eigenvalues are quite small compared to the degree okay so the main way to show a Cayley graphs like that have small eigenvalues is to to just compute what the eigen values are and this is actually not so hard to do for candy crush so let me do this explicitly so I will tell you very explicitly a set of eigen vectors and they are first eigen vector is just the old ones vector the second eigenvector is the vector coming from one Omega Omega square Omega to the P minus one where Omega is a primitive root of unity the next one is one Omega square Omega 4 all the way to Omega P Omega to the 2 times P minus 1 and so on so I want to half okay so I make this list so these are my eigen vectors and let me check that they are actually eigen vectors and then we can also compute eigenvalues or so the top eigenvector corresponds to a one spectra in the d regular graph that's always an eigen vector D and the other ones we'll just do this computation so instead of getting confused with industries let me just compute as an example Jeff coordinate of the adjacency matrix times V 2 so the jet coordinate so whether it comes to so yes the following some if I run over s then Omega raise to j+ s s is symmetric so i don't have to worry so much about plus or minus so I say J plus s so if you think about what this KT graph how it is defined if you hit this vector with that matrix that Jade coordinate is that some dim but I can rewrite the sum by taking out this common factor Omega 2 J and you see that this is the Jade coordinate of V 2 so and this is true for all J so this number here is lambda 2 and more generally lambda K is the following some for K from 0 so from the K for being 1 through P when you plug in K equals to 1 you just get D and the others are sums of these exponential sums now this is a pretty straightforward computation and in fact we're now using anything about quadratic residues right this is a generic fact about Kayla graphs of Zima P so this is true for all KD graphs s not necessarily for quadratic residues and the basic reason is that here you have the set of eigenvectors and they do not depend on s ok so you might know this concept from other places such as circular matrices and this is true and the simple computation so now we have the values of lambda explicitly I can now compute their sizes I want to know how big this lambda is or the first one when K equals to one is exactly D degree which is P minus 1 over 2 but what about the other ones so for the other ones we can do a computation as follows so note that I can rewrite lambda K by noting that if I take twice it and plus 1 then I obtain the following some ok so here I'm using that s as a set of quadratic residues consider there's some here every quadratic residue gets counted twice except for 0 which gets counted once and now I would like to evaluate the size of the Sun this exponential sum and this is something that's no matter cows um okay so basically a cow sum is what happens when you have something that's like a quadratic an exponential some with a quadratic dependence in the exponent and the trick here is to consider the square of this sum so the magnitude squared now if I expand the square okay so the squaring is a common feature of many of the things we do in this course really simplifies your life you do the square you expand the sum you can reap ramírez one of the summons do two steps at once and repro matera Singh and I'm expanding but now you see if I expand the exponent we find so that's algebra and now you notice that this sum here the sum over a is equal to what when B is nonzero claimed that this sum is zero I won't be is nonzero then I'm summing over some permutations of the roots of unity so here I'm assuming that K is bigger than not you're sorry parameterizing K a little bit case zero then when B is not zero the sum over a is zero and otherwise the equals to P so the sum over here equals to P and therefore lambda K I'm just up K change zero so then lambda sub k plus 1 is equal to plus minus P plus 1 over 2 for all lambda not equal to zero so really except for the top eigenvalue which is the just a degree all the other ones are you know one of these two values and they're all quite small okay so this is an explicit computation showing you that this Paley graph is indeed a pseudo-random graph it's an example of a quasi random graph yes okay the question is do we know what the sign is so we actually so here I am not telling you what the sign is but you can look up actually people have computed exactly what the sign should be so this is something that you can find in a number theory textbook like Ireland Rosen any more questions there is a concept here I just want to bring out that you might recognize sums like this right so this kind of sum that's a Fourier coefficient so if you have some Fourier transform this is exactly what Fourier transforms look like and it is indeed the case that in general if you have an abelian group then the eigen values and the spectral information of the corresponding KT graph corresponds to Fourier coefficients and this is a connection that we'll see also later on in the course when we consider additive combinatorics and giving a Fourier analytic proof of Rosse theorem and therefore-- analysis will play a central role but this is actually this analogy I said Witten it is only for abelian groups if you try to do the same for non abelian groups you will get something somewhat different so for non abelian groups you do not have this nice notion of Fourier analysis at least versions that generalizes what's above in a straightforward way but instead you have something else which many of you've seen before but under a different name and that's representation theory which in some sense is Fourier analysis except instead of one-dimensional objects no complex numbers who are looking at higher dimensional representations I just want to point out this connection I will see more about later any questions so let's talk more about Cayley graphs all right so last time we mentioned these notions of quasi randomness and I said at the end of the class that many of these equivalences between quasi random graphs and they fail for sparse graphs if your density if your edge density is sub constant then the equivalence is no longer hold but I what about for cainy graphs in particular I would like to consider two specific notions that we discussed last time and try to understand how they relate to each other for Kayla drafts so for tense KD graphs it's a special case of what we did yesterday so I'm really interested in sparse or Cayley graphs even bounded degree even bounded degree so that's much sparser than the regime we were looking at last time and the the main result I want to tell you is that the disk condition is in a very strong sense actually equivalent to the eigenvalue condition for all Cayley graphs including non-abelian hailey grasp so before telling you what the statement is I first want to give an example showing you that this equivalence is definitely not true if you remove the assumption of Kayla graphs for example if you okay so example that this is false for non Kayleigh because if you take let's say a large so let's say D regular graph so let's say a large random D regular graph D here can be a constant or growing with n by this pretty robust example and then I add to it an extra destroying copiers K sub D plus 1 that's much smaller in terms of number of vertices the big large random graph by virtue of being a random graph has the discrepancy property and because we're only adding in a very small number of vertices it does not destroy the discrepancy property right so discrepancy property if you're just adding a small number of vertices it doesn't it doesn't change much so this whole thing has discrepancy however what about the eigenvalues claim that the top two eigenvalues are in fact both equal to D and that's because you have two eigenvectors one which is the old ones vector on this graph another which is the old ones vector on that graph these two days components each give you a top eigenvector of D so you get B twice in particular the second eigenvalue is not small so the implication from disk to I can value really fails for non Cayley graphs for general graphs the implication the other direction is actually okay the fact that eigenvalue implies disk is actually the content of the expander mixing lemma so this follows by explained your mixing lemma and that's because if you look at the expander mixing them up for a king aguar therefore for Cayley graph or if you have the eigenvalue condition then automatically you would find that these two guys here are a most M so if lambda is quite small compared to the degree then you still have the desired type of quasi randomness so I'll make the statements more precise in a second so the question is how can we certify how can we show that in fact disk which sits seemingly weaker property implies the stronger property of eigen value for KD graphs and what is a special ball Cayley graphs I would allow you to do this that the statement is generally false for non Kayla grass okay so let me define okay so let me first tell you the results so this is a result due to David : myself two years ago so so many of you may now have been to too many seminar talks where you know there's this convention in mathematics talks where you don't write out your full name only by the initial out of some kind of false modesty but of course know we all love talking about own results but somehow we don't like to write our own name for some reason okay so here's the theorem so I start with a finite group gamma and let me consider a subset s of gamma that is symmetric and consider GP Katie graph let me write n as the number of vertices and D the size of s so this is a D regular graph let me define the following properties the first property I'll call disk with epsilon so I give you an explicit parameter the number of edges between x and y differs from the number of edges that you would expect so as in the expander mixing lemma so the disk property is that this quantity is small relative to the total number of edges the second property which will call I can value property like is that G is a it's an MD lambda graph with lambda at most epsilon D lambda is quite small as a function of T the conclusion of the theorem is that up to a small change of parameters these two properties are equivalent in particular eigen value implies epsilon implies disk of epsilon and disk of epsilon and this is a the second one is the more interesting direction it implies like while you lose a little bit but at most a constant factor I have eight Epsilon okay any questions about the statement so far so as I mentioned this is completely false if you consider non Cayley graphs and we also using expanded mixing lemma using that implication up there this direction follows one of the main reasons I want to show you a proof of this theorem is that it uses this tool which i think is worth knowing and this is an important inequality known as scrolls and digs involve so many of you probably know growth and ich asked this famous French mathematician who reinvented powder in algebraic geometry and spend the rest of his life writing homes and homes of texts that have yet to be translated to English but he also did some important foundational work in functional analysis before he became an algebraic geometry nerd and this is one of the important results in in that area that he so go from Dick's in equality tells us that there exists some absolute constant K such that for every matrix a so real valued matrix we have that the so we have that if you okay so here's here's the idea let's consider the supremum okay so let's consider the following quantity this is a bilinear form so this is a bilinear form this is basically a bilinear form if you hit by vector x and y from the two sides and i'm interested in what is the maximum value of this bilinear form if you are allowed to take x and y to be plus minus one valued real numbers okay so this is an important quantity and give you a matrix and they're basically asking you know you get to assign a plus or minus to each row and column and I want you maximize this this number here this is important quality that we'll see actually much more in the next chapter on graphing limits but for now just take my word this is a very important quantity and this is actually in quantity that is very difficult to evaluate if I give you know very large matrix and ask you to compute this number here there's no good algorithm for it and it's believed that there's no good algorithm for it on the other hand there is a relaxation of this problem which is the following it's still some but now instead of considering the bilinear form there let's consider the excise and why is not take the knock from real numbers but take vectors so let's consider this some where taking similar-looking some accept that excise and why ice come from a unit ball in some vector space with an inner product where P it's the unit ball in some RM where here the dimension is actually not so relevant dimension is arbitrary if you like you know you can make em and or 2n because you only have that many vectors so this quantity here just by very definition it's a relaxation in the right hand side of this this quantity here so it's at least as large right so in particular if you have whenever plus minus you can always look at the same quantity with m equal to 1 and you obtain this quantity here but this quantity may be substantially larger it's the x and y's have more room to to put themselves in to maximize the sum and growth and dekes inequality tells us that the left hand side actually cannot be too much larger than the right hand side it exceeded by a most a constant factor so in other words the left hand side was just known as a semi definite relaxation you are not losing by more than a constant factor compared to the original problem and this is important in computer science because the left hand side turns out to be a semi definite program SDP which does have efficient algorithms to compute so you can give a constant factor approximation to this difficult to compute the important quantity by using semi definite relaxation and growth and inequality promises us that it is a good relaxation you may ask what is the value of K so I said there exist some constant K so this is actually a mystery so the current proofs have been improved over time in the growth and become self prove this theorem but a constant has been improved over time and currently the best known result there is something along the lines of K Guffey 1.78 works but the optimal value which is known as cotton the constant so this is growth indeed constant actually this is what I've written down is what's called the real growth in the economy can also write a version for complex numbers and complex vectors and that's the complex growth index constant yes if your lower bound that is no yes so it's no night it is strictly bigger than one so there are some specific numbers but I forget what they are you can look it up okay any more questions so well leave growth and equality we'll use it as a black box okay so if you wish to learn the proof is I encourage you to do so it's there's some quite nice proofs out there and we'll use it to prove this theorem here about quasi random Cayley graphs so let's suppose disc hos so what we like what do we like to show we want to show that this eigenvalue condition holds and we'll use the you know some min max characterization of eigen values but first some preliminaries suppose you have vectors x and y which have plus minus 1 coordinate values then by letting okay so let's consider the following vectors where i split up x and y according to where they are positive and where they're negative so here these are such that X plus is equal to so if I evaluate it on coordinate G then is 1 if X of G is plus 1 and 0 otherwise its G sub minus this one if X of G is minus 1 0 otherwise so X splits into x + - x - + y splits into y plus minus y - let's consider matrix a where the G comma H entry of a is the following quantity I have this set s and I look at whether G inverse H lies in s and that cake considered an indicator of that which 1 or 0 and then subtract the / n this value has mean 0 so this is a matrix and now if I consider the bilinear form hit a from left and right with x and y then the bilinear form splits according to the plus and minuses of the axis and I claim that each one of these terms is controlled because of disk so for example the first term is if you expand out what the sky are so here's an indicator vector that's an indicator vector and you know if you look at the definition then this is precisely the number of edges between X plus and y plus minus D over N times the size of X plus times the size of Y plus where X plus is the set of group elements such that X sub G is 1 and so on all right so the punchline up there is that this quantity so this quantity is and most you know by discrepancy epsilon the end so so this sum here by triangle inequality is at most for epsilon D all right so so far we've reinterpreted the discrepancy property and what we really want to show is that this graph here satisfies eigen value condition so what does that actually mean to satisfy the eigen value condition so by the min/max characterization of eigen values it follows that the maximum of these two eigen values which is the quantity that we would like to control is equal to the following it is equal to the supremum of the spy linear form when x and y are vectors and this is simply because a is the matrix it's not the adjacency matrix it's not the adjacency matrix a is the matrix obtained by essentially taking the adjacency matrix and subtracting that constant there and subtracting that constant gets rid of the top eigenvalue and what you remained is whatever that's left and you want to show that whatever you remain has small spectral radius so we would like to show that this quantity here is quite small well let's do it give me a pair of vectors x and y and let's set the following quantities where I take a twist of this X vector by rotating the coordinates setting X super s sub G the coordinate G to be X sub s G so so X is a vector indexed by the group elements and then rotating this indexing of the group elements by s so that's what I mean by superscript s and likewise y superscript s is defined similarly okay so I claim that these twists these rotations do not change the norm of these vectors and that should be pretty clear because I'm simply relabeling the coordinates in a uniform way and likewise same for y okay so I would like to show this quantity up here is small so let's consider two unit vectors and consider this by linear form if I expand out this by linear form it looks like that I'm just writing it out but now let me just throw in an extra variable of summation Oh what do yes essentially look at the same sum but now I add in an extra s so convince yourself that this is the same some include simply reprimanding there's some so this is the same sum but now if you look at the definition of a there's just cancellation so the 2's cancel out so let's rewrite the sum 1 over n then G H s all group elements now if I bring this summation of s now I bring it inside and then you see that what's inside is simply the inner product between the two vectors X sub G between the two vectors ok so what's inside is simply the product inner product between these two so I may need to redefine yeah so when you're looking at when you're talking about non abelian groups there's always the question of which side should you multiply things by and there okay well I need to change this asked over here but anybody see it should work yes good YH thank you yeah I think great so I maybe I need to switch the definition here playing any case some version of this should be okay figure it out later in the notes but no okay so so you have this you have this here and if you look at this quantity here it is the kind of quantity that comes up in cloth and digs inequality right so this is basically the left-hand side of growth index inequality what about the right-hand side of growth index inequality well we already controlled that we already controlled that because we said whenever you have up there little accent of the Y so the conclusion of this board was that the conclusion of this sport was that this by linear form is bounded by a most for epsilon T for all X's Y being plus minus 1 coordinate valued so combining them by growth and eke we have an upper bound which is quoting the constant times for epsilon so for epsilon T and and therefore because the Courtney constant is less than two we have a pound of eight epsilon T and this shows that this variational problem which characterizes the largest eigenvalue in absolute value is at most eight epsilon D thereby implying the eigenvalue property okay so the main takeaway from this proof two things one there's growth in the extend equality is a nice thing to know right so it's a semi definite relaxation that changes the problem which is initially some more intractable to a semi definite problem which is both you know from a computer science point of view algorithmically tractable but also has nice mathematical properties that for this application here there is a nice trick in this proof where I'm cement writing the coordinates using the group symmetries and that allows me to obtain this characterization showing that eigen value condition and this discrepancy condition are equivalent for Cayley graphs let's take a quick wave okay any questions so far or so we're talking about n D lambda graphs so T regular graphs and the next question I would like to this address is in and en der graaf how small can lambda B so smaller lambda corresponds to more pseudo random graph so how small can this be and the right kind of setting that I want you to think about is you think of D as a constant so think of T as a constant and n getting large so how small and lambda P and it turns out there is a limit to how small it can be and it is known as the alone open a bound which tells you that if you have a fixed T and so G is an N vertex graph with adjacency matrix eigenvalues lambda 1 to lambda n sort it in non increasing order then the second largest eigen value has to be at least basically two root D minus one minus a small error term at all n little one where the little one goes to 0 as n goes to infinity so the aluminum panel bound tells you that the lambda cannot be below this quantity here and I want to explain what there's a significance of this quantity and you will see it in the proof and this quantity is the best possible and also say you know what do we know about the existence of graphs which have lambda too close to this number so this is the optimal number you can put here question okay question does they say how negative lambda and can be so I'll address that in a second but essentially if you have a bipartite graph then lambda N equals 2 minus lambda 1 okay all questions so I want to show you a proof and time permitting a couple proofs of a Lobel panel bound and they're all quite simple to execute but the you know I think it's a good way to understand how the spectral techniques work okay so first as with all of the proofs that we did concerning most of them concerning eigenvalues were looking at the quran fischer characterization eigenvalues it suffices to show to exhibit some vector Z okay so a non zero vector such that Z is orthogonal to the all ones vector and this quotient is at least the claimed bound right so by the Quran Fisher characterization of the second eigenvalue if you vary over all such d that orthogonal to the unit vector then the maximum value this quantity of things is equal to lambda2 so to show that lambda 2 is large it suffices to exhibit such Z so let me construct such as G for you so let R be a positive integer and let's pick an arbitrary vertex V so we it's a vertex in the graph and let V sub I denote vertices distance exactly i from so in particular V naught is equal to B and I can draw you a picture so you have V naught and then the neighbors of V naught and each of them have more neighbors so I'm calling be not the set pick V not and then pick the one V sub 2 and so on okay so I'm going to define a vector which I will eventually make into Z by telling you what is the value of this vector each of these vertices and we'll do this by setting very explicitly set X to be a vector with value X of U to be WI where WI is t minus 1 raise to power minus 2 whenever you lies in set big V sub I so if US distance exactly I from V I said it to this number so notice that they decrease as you get further away from D and I do this for all distances less than okay so this is my X vector and I said all the other back all the other coordinates to be 0 the distance between you and be leashed so like if you this vector and I would like to compute that quotient over there for this vector and I claim that this quotient here is at least the following quantity this is a computation so let's just do it so why is this true well if you compute the norm of X so I'm just taking the sum of the squares of these coordinates or that comes from adding up these values I neighborhood so I have W squared and if I look at that quantity up there ok so what is this a is the adjacency matrix right here a is the adjacency matrix so this quantity I can write it as sum over all vertices U and I look at X of U and now I some again over all neighbors u Prime it's that some day but this sum I have some control over because it is okay so also what is what's happening here I claim it has at least the following quantity consider where us so you could be only nonzero if you lies in the R minus 1 neighborhood so in that neighborhood I have mystified possible choices for the vertex U for that choice this XW is W sub R but what about its neighbors so it could have neighbors in the same set going left but there's okay so there's one neighbor going left and all the other neighbors are maybe it's in the same cell maybe it's in the next set but in any case I have the following inequality there's one neighbor in the same in the left if you look at that picture just now and then all the remaining neighbors have except you Prime's at least W sub I plus 1 because these weights are decreasing so the worst case so to speak is all the neighbors point to the next set so I had that inequality there there's an issue because if you go to the very last set if you go to the very last set think about what happens when I in that very last set I'm over counting neighbors I no longer have weights so I need to take them out so I should subtract D minus 1 times ok so this is the maximum possible weights um I could have maximum possible over count okay so each vertex here has t minus 1 neighbors at most all right so this should be pretty straightforward if you do their counting correctly but now let's plug in what these weights are and you'll find that this sum here this quantity is equal to I mean so the key point here is that this thing simplifies very nicely if you consider what so what ends up happening is that you get this extra factor 2 root t minus 1 and then the sum - okay so it's pretty straightforward computation using the specific weights that we have and one more thing is that notice that this ok so notice that the sizes of each neighborhood it cannot expand by more than a factor of P minus 1 because you only have D minus 1 outward edges going forward at each step and as a result I can bound this guy what you find is that there's this whole thing here is at least 2 x 3 t minus 1 the main term is the sum and this here is less than each individual summit so I can do a 1 minus 1 over 2 R ok putting these two together you find the claim all right so I've exhibited this vector X which has that quotient property but that's not quite enough right because we need a vector called Li up here that is orthogonal to the old ones vector and not you can do because if the number of vertices is quite a bit larger compared to the degree then I claim that there exists U and V vectors vertices that are at distance at least 2 R so if I let so this is the size of this tree if you if everything is within distance R distance to are from a vertex then they all lie on the street and if you come a number of vertices in that tree it's what I have there's some everything here so if I consider these two vectors so be so X be the vector obtained above which is in some sense and I'm being somewhat informal here centered at B and if I let Y be the back sure but I century now at the vector u then I claim that essentially x and y are supported disjoint vertex sets that have no edges even between them so in particular this in your product it's bilinear form in the product but this by linear form is equal to 0 since no edge between the supports ok so now I have two vectors that do not interact but both have this nice property above and now I can take a linear combination let me choose the constant C that's a real constant such that this Z equal to X minus CY I can choose this constant so x and y are both non-negative entries they're both non zero so I can choose this constant C so that it is the Z is orthogonal to the old ones vector and now I have this extra property I want but what about the inner products these two vectors x and y they do not interact at all so the inner products split just fine and the bilinear form splits just fine so you have this inequality here as desired and our notice that I can take R going to infinity as n going to infinity because T is fixed and goes to infinity then R can go to infinity roughly a logarithmic N and that proves the loan Papa not bound X just to recap to prove this bound we needed to exhibit by the Quran Fisher some vector with a nice this quotient such that this quotient is large and we exhibit this quotient by constructing the vector explicitly around the vertex and finding two such vertices that are far away from constructing these two vectors taking the appropriate linear combination so that the final vector is orthogonal to the unit vector to the old ones vector and then showing that the corresponding bilinear form has this large enough any questions I want to show you a different proof which gives you a slightly worse result but you know it's the proof is conceptually nice so let me give you a second proof slightly weakening and just that what show so what show so the earlier proof show that lambda2 is quite large but next will show that the max of lambda 2 and the lambda n is large ok so not that the second largest eigenvalue is large but the second largest I can no absolute value so it's slightly weaker but for all intents and purposes it's the same spirit show this one here and this is a nice illustration that what's called a trace method sometimes also a moment method here's the idea as we saw in the proof relating the quasi randomness of c4 and eigen values or C force are most I can dollars are related to counting closed walks in a graph so we'll use that counting closed walks in the graph and specifically the cue paved moment of the spectrum is equal to the trace the 2k power which counts the number of closed walks of length exactly 2k okay so to lower bound the left hand side we want to lower bound the right hand side so let's consider closed walks starting at a fixed vertex so the number of closed walks of length okay starting at a fixed vertex B here when I do a regular graph so here we are in ad regular I claim you know whatever this number is may be different from each V it is at least the same quantity if I do this walk in an infinite D regular tree so infinity regular tree yes this is infinite through regular tree start with the vertex and then you growth D regular so why is this true so think about how you walk let me just explain this is I think pretty easy once you see things the right way to start with the vertex V think about how you walk and whatever way you can walk well you can walk the same way on the Infinity regular tree well I mean whatever walk you can do on at infinity regular tree if you label first vertex first edge second edge if you do a corresponding labeling on your words no graph you can do that walk on your original graph although the original graph might have some additional walks namely things that involve cycles that are not available on your tree but certainly every walk you can do every closed walking you can do on the tree you can do the same walk on your graph so you can make this more formal right so you can write down a batch action or injection to make this more formal but you know it should be fairly convincing that this inequality is true but this is just some number all right so this is a number of 2k walks and the D regular tree starting at a vertex and this number has been well studied I mean we don't need to know the precise number just need to know some good lower bound and here is one lower bound which is that if there's at least a Catalan number the cave Catalan number times D minus 1 to the K where C sub K it's the case Catalan number which is equal to ok so k2 k2 is K divided by K plus 1 so then we remind you what this is you know one there has many combinatorial interpretations and fun exercise to do by directions between them but in particular C 3 is the equal to 5 which counts the number of ups and down walks of length 6 that never dip below the horizontal line where you start so then this corresponds to going away from the route versus coming back to the route certainly have at least time anyways and when you are moving away from the route you have D minus one choices which branch to go to good given that the right-hand side is at least then and the number of vertices times the quantity above related to Catalan numbers on the other hand the left-hand side is at most here were using a queue case even number is at most D to the 2k plus okay so all the other eigen values are a most lambda in absolute value rearranging this inequality we find that lambda to the 2k is at least this number here yes sir I'm changing a minus one to end that and now what can we do we let n go to infinity and K goes to infinity slowly enough so if K goes to infinity and n goes to so K goes to infinity with N and but not too quickly but K is not a rule of log n then we find that this quantity here is essentially 2 to the K 2 to the 2k so and this guy here is the tall one so lambda is at least 2 root D minus 1 minus 1 so that proves essentially the alone Poupon abound although a small weakening because we are this big I can value my find might actually be very negative instead of very positive but that's ok before applications no this is not such a big deal these are two different proofs and I want you to think about are they really the same proof are they different proofs are they related to each other ok so it's worth thinking about they look very different but how are they related to each other and one final remark do you know you already saw two different proofs as two shows you this number and you see where this number comes from and let me just offer one final remark on where that number really comes from and it really comes from this infinitely regular tree so it turns out that to root y -1 exactly is the spectral radius of the infinite irregular tree and that is the reason in some sense that this is the correct number occurring along bopanna bound this is you know if you've seen things like algebraic topology or topology this is universal cover for T regular graphs some general remarks and you already saw two different proofs so the beginning of next time I want to wrap this up and show you to explain some what we know about are there graphs from which this bound is tight and the answer is yes and there's lots of major open problems as well related to what happens there and after that I would like to start talking about graph limits so that's the next chapter of this course 