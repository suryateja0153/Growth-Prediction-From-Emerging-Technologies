 Hello everyone, Thanks for viewing this  supercomputing conference 2020 presentation talk.   In this talk, I'm going to introduce CallFlow,  a tool to visualize hierarchical performance   profiles of parallel codes, and also explain how  we enable scalable comparative analysis on an   ensemble of such profiles. This is a collaborative  effort from visualization researchers   from the University of California, Davis,  and High-Performance Computing (HPC) experts   from Lawrence Livermore National Laboratory  and University of Maryland, College Park. Large-scale computational resources like sierra  enable scientists to explore complex scientific   phenomena and advance the frontiers of  human understanding in multiple areas,   such as astronomy and climate change. In order  to maximize the amount of science per dollar   computational scientists and HPC experts  continuously strive towards optimizing   the performance of their simulation codes to  do these computational experts continuously   ask themselves "Is their application  performing as good as it can?". To answer this question performance  analysis usually performed to identify   and eliminate bottlenecks. Typically,  large-scale scientific applications   demand a lot of optimization effort to  understand how different application parameters   affect performance. Diagnosing the true  cause of performance bottlenecks typically   requires a simultaneous understanding of  the code execution which usually comprises   multiple libraries and the runtime  performance from multiple parallel processes.   In particular, performance analysis commonly  comprises of four analysis tasks: 1) To detect   performance issues it is crucial to pinpoint call  sites that consume most execution runtime, 2)   To ensure consistent workload distribution  across processes we need to identify call sides   with high load imbalance, 3) To determine which  optimization is better we would require to compare   the performance across executions, 4) Finally,  to understand the effects of various optimization   techniques we need to study the performance  variability across multiple executions. Key indicators of performance analysis are  usually captured through performance profiles.   Sampled performance profiles are collected by  forcing an interrupt at every nth instruction.   In general, they record two kinds of information  exclusive metric and inclusive metric. The   inclusive metric represents the time consumed by  a call site along with its children's call sites.   Say, for example, we have foo one calling bar one  and bar two, so the inclusive runtime is added up   for foo1 and it becomes six the collective  call sites can then be aggregated by their   call paths to form a tree structure popularly  known as calling context trees or CCTs. CCTs can   further be aggregated by merging their call sites  with the same function name called call graphs. A majority of existing tools generate  CCTs for the whole application,   like HPCToolkit. These include the call  sides of the libraries it depends on   however typically only a small fraction  of these nodes in a CCT are of interest   but such nodes are usually buried deep in the  tree and identifying them could be challenging.   For example, HPCToolkit uses an expandable  tree visualization requiring users to toggle   and expand multiple hierarchies from the top-down  and bottom-up call stack. However, expandable tree   layouts do not scale well with the size and depth  of CCTs, also the user would lose context during   analysis. On the other hand, several tools  have combined traditional node-link layouts   and visual encoding techniques to direct the user  towards the performance of the calling context.   However, visualizations using node-link  layouts become intractable as well this   is especially true for large-scale applications  containing hundreds to thousands of call sites. Recently introduced a performance analysis  tool, Hatchet provides a programmable interface   to perform performance analysis on a variety  of formats such as HPCToolkit, Caliper, gprof,   cprof, and so on. They enable analysis by  converting hierarchical performance data   into a pandas data frame and associate the call  sites in the call graphs to their performance.   Although hatchet provides basic visualization  techniques to visualize the data   these visualizations are mostly static and do  not support interactive analysis. For example,   on the left is shown a LULESH simulation on 512  cores, the average exclusive runtime is color   mapped on the text and we can immediately  find out the most expensive call site   CalcHourGlassForElems. However, the interface  requires the user to programmatically change   each time before performing the visual analysis.  Additionally, their analysis is also limited to   a single execution. Similar to other node-link  layouts this tree-based visualization approach   would still suffer for executions that  contain a large number of call sites.   CallFlow, on the other hand, extends the interface  of Hatchet to provide an interactive exploration   of profiles. CallFlow enables ways to encode  information based on both inclusive and exclusive   metrics this helps to find the most important call  size easily. To help associate the performance of   per-process information CallFlow also  provides additional views such as histogram   scatter plots and box plot views that  help to associate data much easily.   We further aggregate the contextual information  using the same available semantic information to   what we call super graphs. In this example, the  call graphs are grouped by the module or library   they belong to this helps us immediately spot  the most expensive call sites inside a library.   Our interface also extends the analysis  to multiple profiles at the same time. Studying a large number of profiles is critical  as large scale applications require significant   optimization efforts to understand  how different application parameters   may affect the performance. In this process,  experts conduct a variety of test runs by   varying multiple configurations to identify  optimal execution parameters and configuration.   To support this call flow enables the user  to modify the profile they want to view.   Although, CallFlow supports studying individual  call graphs comparison of two or more call graphs   requires the user to analyze them individually  comparison of multiple call graphs is essential.   To identify the overall trends across the  performance metrics in an ensemble so to   improve this, in this work, we improve CallFlow's  functionality to introduce two new analytic modes   ensemble mode which visualizes the performance  variation across multiple call graphs, and a   diff mode which directly compares two call graphs  and a selected call graph against the ensemble. In the remainder of this talk, I will first  cover how we construct the super graph and   then discuss the multiple profile scenario  where we construct an ensemble super graph.   Like I mentioned earlier the scale and complexity  of CCTs and call graphs forces significant   challenges for interactive visual exploration.  CallFlow's Super Graphs, on the other hand,   utilize the contextual information from  the code to provide a high-level overview.   This is particularly useful in the case  of modern software abstractions that have   numerous intermediate call sites which are not  relevant to performance analysis. For example,   library 4 is being called from different code  modules library 1, library 2, and library 3.   These repetitive calls from different code modules  form similar subtrees with different parent nodes.   To reduce these repetitive calls, we first  filter out these call sides by providing a   user the provided threshold and aggregate the  CCT to a super graph based on the available   contextual information. These super graphs  can interactively be modified and studied   using the splitting operations to further reveal  the callers and colleagues for further analysis. To encode this super graph we use a Sankey  diagram. The Sankey diagram provides a high-level   overview of for the code execution the nodes  can be colored based on the module or inclusive   or exclusive runtime based on the user's  choice the height of the node corresponds   to the inclusive metric, whereas the edge flow  corresponds to the control flow of the execution.   To highlight the nodes of the interest users can  modify the runtime range and CallFlow will update   the encoding on the super graph the interface  supports various interactions such as zooming   panning and for exploring large super graphs.  Hovering over a node reveals detailed information   of the call sites inside a particular supernode  and their properties. Further, each node in the   super graph is embedded with a mini histogram  that shows the process distribution. This   histogram allows the user to spot load imbalances  among processes across different supernodes To create the ensemble super graph, first, we  perform a unify operation on individual CCTs to   construct an ensemble CCT. The unify operation  merges all the calling contexts that share the   same caller-callee relationships across the CCTs.  A call site from CCT is considered equivalent   to the call site from another CCT if they have  the same calling context across the two runs.   The corresponding metrics are aggregated and  stored as individual vectors. The unification   enforces graph equivalence by assigning a  null value to the missing nodes in any CCT.   For example, the bar call site is actually  missing in the second and the third CCT,   so they have been assigned the value of null.  Next, the ensemble CCT is converted to an ensemble   call graph by merging the call sites with the  same function name. For example, the bar call   site occurs in multiple calling contacts which are  further aggregated in a call graph. The associated   vectors are element-wise added to summarize the  performance runtime on each call site. Finally,   the ensemble call graph is aggregated to a  module level super graph ensemble super graph   through user-defined filtering and grouping  operations similar to a single super graph. To encode the performance variability, we  encode each supernode using ensemble gradients.   This corresponds to the distribution  of inclusive or exclusive metric   by user stores mapped vertically along with the  height of the node using a white to red color   map. The colors are mapped consistently across all  nodes to allow the evaluation of distribution not   just within a single node but across supernodes  as well. Such an encoding preserves the notion of   resource flow from one module in this  call graph to the other across executions.   For example, the distribution of the MPI module  is spread across multiple bins, on the other hand,   the execution runtimes for the calculated grants  module is actually along the extremes making a   clear distinction of performance variability.  Additional runtime information can be revealed   by toggling the text guides by clicking on the  supernodes these text guides mark the bins of the   histogram and also display the minimum and maximum  runtimes of the execution among the ensemble. During analysis, the user might find two  executions that exhibit a variation in the   ensemble distribution. To facilitate a further  comparison of analysis, we compute a diff super   graph that subtracts the mean runtime across the  supernodes between the call graphs. The resulting   diff view is visualized using a Sankey diagram  and is colored using a red-green color map,   where the hues of red color highlight  the performance slowdown and the hues   of green color highlight the performance speedup. CallFlow additionally provides two supporting  views that enable performance correlation   analysis. The histogram view provides  an enlarged view of the mini histogram   along with the shadow lines the shadow lines  indicate the rank-to-bin mapping to assist the   user to explore the connection between slowdowns  in the MPI ranks and the physical domain. Although   shadow lines can create visual clutter  especially for large processor counts,   it is a desired visualization since it  indicates the code behaves normally.   More crisscrossing indicates efficient  mapping to the hardware domain   while a less visual collector denotes that certain  runtimes are not well correlated to the rank ids   this implies a load balance as shown in the  bottom figure. The scatter plots help reveal   the correlation between inclusive and exclusive  runtime metrics. If there is a correlation then   we expect the processing elements to form  clusters as shown in the bottom image. Next, to enable performance correlation  across ensembles of super graphs we show   histograms and scatterplots of a selected  supernode in three modes: 1) Call site mode   shows the distribution of runtime metric  with respect to individual call sites, 2)   call graph mode shows the distribution  with respect to the call graphs, 3)   finally the MPI rank mode shows the distribution  across the rank ids. These three modes enable the   user to compile the runtime performance  of a selected run across the ensemble. CallFlow supports two graph splitting operations,  1) split node by entry call site and 2) split node   by parents. Both operations allow the user to  prune a high-level overview and reveal low-level   call sites that consume most resources. To split  by entry functions, we show the list of call sites   on a site panel from which the user chooses  multiple functions to reveal. Once chosen the   graph view is updated to include the selected  functions in real-time. Splitting by parents,   on the other hand, reveals the caller's call sides  of a selected node from the hierarchy. In this   example, we show the two entry functions of the  LEOS module and also reveal its parent functions. The histogram can further be used to enable a  side-by-side comparative analysis to compare   the runtime among processes by allowing the users  to click and drag on the histogram using a brush   this splits the graph view into two  halves revealing two super graphs   for the selected and  non-selected bins respectively Finally, we show CallFlow's  usefulness on a real-world application   released by studying for load  balancing across multiple processes.   LULESH is a proxy application used for  modeling large scale hydrodynamic stimulation.   For this study, we compare the performance of two  execution configurations. The first experiment is   a traditional MPI run where a single MPI rank is  placed on every process and the second experiment   runs relation with over decomposition where  multiple MPI ranks are placed on every process.   In general, over decomposition  enables communication among processes   and improves performance. Comparing the  heights on the nodes we realized that the   application performance improved with over  decomposition at an average of 44 percent.   Next, by comparing the histograms, we notice  that the per-process distributions of the   supernodes are skewed conveying the load imbalance  exists due to inefficiency in the communication.   These two inferences would have been difficult  to introduce with traditional CCT visualizations   since modules like "libpsm" and "AMPI"  would contain hundreds of call sites. We further investigate this phenomenon  by clicking on the LULESH supernode to   reveal the corresponding histogram view  we can immediately spot the few processes   that are overworking with 170 seconds are  shown on the right side of the histogram.   The performance of heavy MPI ranks can  then clearly be distinguished using the   side by side overview by performing the brush  interaction which splits the call graphs into two. As a next experiment, we study the weak scaling  of LULESH across eight execution parameters by   varying the number of processor points. The  aim of the experiment was to identify call   sites that exhibit inconsistent  or unexpected runtime behavior.   Although, the ensemble gradients across super  nodes exhibit good weak scaling as they have very   little variance in runtime the text guides  reveal two cases of out-of-order runtimes   especially for the MPI module  and the CalcForce module.   This can be seen through the text guides as  we can notice the 27-core run is followed by a   216-core run and then followed by 64 and 125 this  implies that the 216-core run is much faster than   64 and 127-core runs which seems like an anomaly.  To further surprise we also notice the same   pattern existed in all the nodes of the CalcForce  module by performing the split graph operation. Next, we further investigate  this phenomenon by comparing   the 64-core run with the 27-core run and  also comparing the 216-core with 125-core.   As we notice the diff view clearly distinguish  the performance variability exhibited by the   force module there is a clear performance load  on in the case of the 64-core and there is a   performance speedup in the case of the 216-core  case. The diff view clearly distinguishes not   only the performance degradation but also the  degree to which the performance has degraded   in comparison to the calc force module  the other modules like, for example,   the "LeapFrog" or the "Timer" module or the  roulette module show little performance variation. To summarize, CallFlow helps users identify  performance bottlenecks in their parallel codes   effectively leading to potential optimizations  and improved overall throughput of applications   catering specifically to the target  audience call flow customizes and   enhances the understanding of call graphs using  a module-level representation called supergraph   and also uses multiple link views to provide a  holistic exploration of CCTs through a set of   interactions on the underlying super graph call  flow provides a high-level overview of the CCTs   and also provides the ability to drill drawn  for detailed information call flow also supports   visual mediums that enable comparative analysis  on two or more call graphs. Additionally, CallFlow   also supports a comprehensive list of profile  formats like HPCToolkit, Caliper, and cProfile.   To reach a wider audience, CallFlow is now  publicly released under the MIT license   and is available on Github. Please feel free to  clone the project and check it out. Thank you! :) 