 Richard Pinch: In this second session we welcome Professor Danilo Mandic from Imperial College London. He is Professor and Co-Director of the Financial Signal Processing Laboratory at Imperial, he works in the areas of statistical signal processing and machine intelligence, he's a fellow of the IEEE and has been a Member of the Board of Governors of the International Neural Networks Society. He has more than 500 publications and has received the President's Award for Excellence in Post-Graduate Supervision at Imperial. His work related to this talk includes his book “Recurrent Neural Networks for Prediction”, two recent monographs and ongoing work on Data Analytics in Graphs. He has the Dennis Gabor 2019 Award from the International Neural Networks Society and a 2018 Best Paper Award from IEEE Signal Processing. So it gives me great pleasure now to ask Professor Danilo Mandic to speak on: 'Tensors, graphs and deep networks: convergence of concepts and ideas', Professor Mandic. Danilo Mandic: Thank you very much, it gives me great pleasure to talk to our audience today and this being an event by the Institute of Mathematics so actually I'll try not to have any equations in my slides although there will be a couple. So let me see if i can share my screen now - this one will be correct. So I'm hoping that you can see my screen now - Richard Pinch: Yes that's fine, thank you.  Danilo Mandic: Fantastic, so my talk is really on sharing some, perhaps, high level ideas on the commonalities between big data represented through tensors, graphs for data analytics on irregular domains and deep networks. So these are three seemingly disparate concepts however through linear and multi linear algebra you'll see hopefully how it can be considered under one umbrella and how we can benefit from domain knowledge and graphs say to have a better neural network and how can we use sensors to dramatically decrease the the dimensionality of of deep networks. OK without much further ado, so the crux of my talk will be on tensors for super compression of data and we'll see how that achieves - super compression can be used to optimise neural networks. OK so when you talk about big data processing then it's of course very interesting to to talk about biological neurons versus artificial neurons and such like and in this slide we can see some simple, simple, simple comparisons. Computers do excel at algorithmic tasks - So big data processing, so of course we're talking about biologically inspired data processing and such like which is the motivation for neural nets but you see the computers are excellent at solving algorithmic tasks so we program them and they can perform search in an unprecedented way however they're not that well equipped for ill-posed problems while biological systems are. For example, a single pigeon brain with one billion neurons and cycle time of 0.1 seconds is very similar in its operation in terms of number of operations per second to an old PC, however clearly the pigeon can live perfectly well and make fantastically complex decisions while this type of computer can't. So what can we do? We can go one way which is to increase computing powers in a brute force approach until we are hoping to reach the performance of simple brains or can we maybe try to make sense somehow from big data in order to use standard computers to do to the same? Now intentionally I've taken this from the year 2011, it's the -basically visioned by the McKinsey Global Institute - it says that in 2010 there will be four billion mobile phone users in the world now we are in 2020, clearly we have more than four billion mobile phone users which - who soon will be having a 5G network and constant video sharing and the data traffic will be will be very, very uncomfortable for the network. Now with the 40% prediction in the growth in global data per year and 5% in the growth of global I.T spending we clearly have a problem because the IT networks, the infrastructure cannot cope any longer with the amount of data or soon it won't be able to. You think that during this COVID situation where most of the broadband providers were struggling to cope including myself back home. Now this gives us the opportunity to look into how can we maybe deal with data in a more interesting and intelligent way in order to to make sense from such data and help decrease the computational load which brings me to the well-known - the four V's of big data. This being the Volume because when I started my career a kilobyte of data was plenty, now I just bought a hard disk of two terabytes and a petabyte is not really unimaginable. Variety, because we are now examining time series images, 3D images, probabilistic data, semantic data all under one umbrella. Velocity, whoever has trained the neural networks in the past, they remember batch processing many, many epochs of data. Now we are trying to move towards streaming data, so indeed people like researchers at McLaren Formula One Team they, at their headquarters in Woking they're analysing data from any racing circuit in the world online the data about the car and physiological parameters on the driver, circuit and such like are being streamed to the UK, the decisions are being made immediately and the decisions are being sent back.  Veracity, which is basically an interesting way to say some sort of noise or uncertainty in data. OK, so how do we deal with this? One way is to look at how data acquired and try to learn from the data acquisition process which brings us to graphs. So graphs have a very, very long history and go back to Euler who at that time lived in Königsberg, today's Kaliningrad and there's seven bridges in the city the question was whether somebody can cross all the seven bridges without crossing the path, basically. So that's the beginning of graph theory. Now the most interesting modern applications of graphs are in internet topologies, power grids or finding anomalies maybe in internet traffic like in the recent Enron insurance scandal. OK so graphs by themselves are not new obviously they are 300 years old however the analysis of data on graphs which is what we are concerned with is fairly new and we could talk about maybe last 10 years of research. I like history so I looked back at the prospectors at my home institutions then in the year 1978-79 I could find a course on communications actually just talking about graphs for coding while back in 1984-85 we can see a course on graph theory. So those were taught by my predecessor, industrial professor Anthony Constantinidis and those graphs were basically static still you can do plenty of interesting optimisation graphs but now if you have to optimise graphs themselves and find meaning in data and graphs that becomes much more interesting and underpins many new applications. So I myself have tried to get myself into data analytics and graphs and the way I thought about this was a simple temperature measurement so this is a site on the Adriatic Sea and you can see the sea, a bay medium height mountain, high mountain, this is a capital, this is a lake and low land so these are the temperature sensors. So here's a snapshot of temperatures recorded by these sensors indexed 1 to 64. You can see something very, very interesting so, A: the sensing grid is irregular so that's very interesting. The second thing is that if you would like to make any very simple analysis or data conditioning for example to find the average temperature here you have a sensor number 30, 36 which is next to the sea and the sensor number 37 high up on the mountain. So have very, very high difference in temperature so that anything that you do with standard data analytics which assumes regular sampling in time or special sampling is not going to work very, very well so instead we need to think how to accommodate those irregular data structures of course that's more complicated but the advantage is that we can talk about neighborhoods about processing local information. So this 64 sensor or grid cannot be thought about as a set of neighborhoods so indeed this is a neighborhood next to the sea, a neighborhood at the low mountain, neighbourhood at the the high mountain, a neighborhood near a lake, also low land. So clearly if you like to do say simple averaging you should do it with respect to the neighborhood slide rather than the global averaging. The second thing you can do is you can establish some connectivity between those sensors and neighborhoods for example it's clear that this neighborhood next to the sea is not going to have a direct link with the neighborhood high up in the mountains but it can have an indirect connection through some of those, others not. So the second thing is that you can think about the temperature difference in terms of the physical distance between two sensors and the distance in the altitude. This source makes it possible to design a graph which represents temperature sensing in this geographic area. Now how about a signal on a graph then? So the black dots here are the sensors, the red lines signals on the graph and now such something interesting happens so now I can no longer use the standard equidistant sampling in time or spatial sampling but I still have signals on graphs which can be represented by these bars or by those circles with the color designating the value of a signal. OK, it becomes more, more interesting. Now, how do we describe a graph? So this is from our recent three volume work which we are hoping to publish in one of the series of the publisher. So the graph can be represented by a set of vertices which are nodes here one to seven, these are connected by edges in blue and each edge can have a weighting telling us how confident, how dependently the nodes are. So, which brings it to a weighting matrix, so each node can have a circle degree - a sum of the weights that it's not and some combination of the weights and the greens gives you the Laplacian matrix for a graph. Now when we say that we know a graph basically we mean that we know the Laplacian matrix which describes the graph. It's a big step because with matrices we can use linear algebra to do many, many operations and perform operations on a graph and we know how to do this. OK, the second thing is that if every matrix of course in this case it's a square matrix says eigenvectors and eigenvalues we know that the eigenvectors 'u' are orthogonal in eigenvalues lambda can be sorted in a decreasing order. What happens on a graph is that the first non-zero eigenvalue is the smoothest and the next one is less smooth and so and so and so on and we know that - sorry the eigenvectors obviously are the smoothest and then less smooth and so on - and that they are orthogonal. Which brings us to the notion of a basis, so indeed very much like the harmonic basis here in the Fourier domain, Euclidean domain we can talk about basis or spectral analysis of graphs in the Eigen domain and indeed these are the first several Laplacian eigenvectors on a Minnesota road network graph and you can see that this one is the smoothest and less smooth less smooth and so on, so we have a new representation space. OK so that's very, very good and if you have a spectral representation so very much like in the Fourier transform a convolution in the time or vertex domain is equivalent to the product in the spectral domain and that gives the basis for convolutional graph of neural networks. Another thing that we need to think about is the graph shift. So in standard data analytics if you have a pulse here on a graph a shift would mean that this pulse spreads on the neighborhood nodes but actually the information diffuses if you have more than one shift and the second thing is the more you shift your data the more you increase the energy in the data so that's a big problem. So we can talk about graph neural networks, ground signal processing but even a simple system on a graph which includes a shift is compromised by the inability of the graph shift operator to to bound the signal energy. So our first basically in roads into this area was to perhaps suggest a new way to to establish the graph shift operator which is both preserving the energy and is unbiased and with that we can then talk about filtering in the graph domain so if these are the temperatures which are noisy and unfiltered in the simple filtering in the vertex domain you can have really nice clustering basically of sensors or temperatures near the sea side in this for example near the capital and on the mountain. OK so just to summarise what graphs do to us, so we can talk about processing data on irregular domains if data are encoded as a graph you can have many interesting ways of visualising such data identify clusters in that if you have clusters you can maybe separate graphs to graph cuts into smaller entities and in such a way perform dimensionality reduction. Again through some causality chains and such so you can arrive at very simple ways to perform decisions like in this case we are talking about a recommender system seller or buyer, OK. Graphs and neural nets if you think about these nodes and neurons and those connections as weights in graphs it's very easy to make a so-called graph cut whereby you can disconnect parts of this bigger graph into two sub graphs and you disconnect again along the line of least sum of the weight so the least disturbance. You can think about this as a way to simplify huge neural networks and clearly the difference between the standard convolution neural networks we need layers like input feature, extraction classification such like and the graph wants is a bit of graph to accommodate for irregular data domains. So my own work in this area basically includes - this is a lecture not basically nightly personal processing magazine trying to to illuminate how we can move from standard data analytics to update analytics in a seamless way and this is a three-part hopefully monograph on data analytics and graphs. The first part deals with basic definitions, second is analysis of data graphs and third is machine learning graphs. Now going back to big data, I'm going to talk a little bit about tensors now and then try to converge with tensor graph neural networks. So when you think about data types you basically as you shift the gears you have scalar vector matrix and a tensor so it's a collection of matrices but of course the tensors is a data entity by itself so so we can talk about the 4th-order tensors which is basically a vector whereby each element is a tensor and such like. So you can have this multi-way relationships between data which happens to be very, very, very interesting. So we're dealing with tensors routinely without really realising we are talking about tensors so for people who have been analysing color images, we know that they are recorded in the r g and b domains. So if you'd like to compare this apple and this tomato one way to do that would be to split the color image on apple into its red, green and blue components, do the same with the tomato, build a tensor so a collection of matrices from from the apple and from the tomato and perform some some comparisons and there's many interesting results in so-called ensemble learning for tensors and such like which showed that in this way using tensor algebra, multi-linear algebra you can perform this in a much more interesting and insightful way. So going back to tensors, I'd just like to demystify them. So tensors is not only a 3d entity, it can be 10d, 10-dimensional, 5-dimensional whatever but in all cases you can talk about its elements. So 2d components are called slices so in this case we have for let's say this sensor horizontal slices lateral and frontal but of course you can think about all the vectors that are contained in a tensor so you can - they're called fibers so column, row and tube fibers and such like. So in many ways this helps us to reshape tensors. So recently in 2016 and 17 we published a two volume monograph with Now Publishers which is also standalone articles in foundations and trends in machine learning. So one part is maybe 180 pages the second is 250 pages and particular attention has been paid to performing all these complicated multi-index transfer operations in a graphical mode. So for example here we have a matrix, so that makes this an entity with three edges, so two dimensional. This matrix we can vectorize, we can sample it column by column and concatenate it to build a vector, very long vector but that is an entity with dimension one, one edge, or you can combine these matrices separated by black lines into a cube so as to have a tensor of data so it's an entity with three edges that tensors can be further split into in many ways and this is basically a 3d tensor where each element is a 3d smaller tensor. So here, an entity with six dimensions. So this is called reshaping of a tensor, so again I'm giving a very high flying overview because the concepts are seemingly very, very different and I'll try to converge a little bit later. So I'm going to show you why transfers are so interesting. So consider a simple video clip, it's The Lion King and suppose that each frame here, each image is of 1000 by 1000 pixels size so if we have 20 seconds with 50 frames per second rate which is high definition video we have 1000 frames from those 20 seconds in the horizontal arrangement. Now of course for a brain or even for a pigeon brain I think 20 seconds is quite kind of short memory we can remember what's happened in a video or 20 seconds but if you want to use a computer to analyse, to make an inference between slide number one and slide number 1000, the search space on this slide would be 1000 by 1000 pixel, the search space along the horizontal time - evolution of video, of frames would be 1 million so 1000 slides with 1000 pixels. It's very, very clear that this short and wide matrix is not very amenable to to analysis and if you'd like to use brute force to express it you have a matrix of 1000 times one million pixels so one gigapixel size of a matrix even performing simple correlations will be prohibited on your computer. However, if you combine these slides now into a cube - slides frames into a cube then you would have for each frame 1000 by 1000 pixels and you have 1000 frames along the time mode. So we're talking now about the mode horizontal, vertical and time so clearly within tensors you can combine different physical quantities into one unifying framework and what's more interesting is that now the search space on one frame would be 1000 by 1000 pixels and the cell space along the 20 seconds of data would be also 1000 frames. So we are talking about much, much more compact representation. In many, many other disciplines like the brain science, you talk about EEG recordings so from all these channels you have time series which you can matricize by time, frequency, representation, for example horizontal axis is time, vertical axis frequency and these yellow vertical lines are basically eye blinks which are very wideband. So we can combine all these into a cube, a tensor - 3d tensor. Now, for each of the sessions you can combine them again into one tensor so the fourth dimension would be session, then the fifth dimension would be subject you can combine responses from all subjects into one tensor and the seventh dimensions would be stimuli and so on so and so on. So easily you end up with the eight, nine ten dimensional tensor from many experiments on many subjects perhaps all the same on the same paradigm which is much more amenable to the analysis. Another interesting thing is that in the tensor domain we can talk again about about convolution like we can talk with vectors and matrices and the properties are very, very similar - the convolution between the transfers here is a tensor c with a large dimension. I haven't mentioned the 'Curse of dimensionality' yet but I will. So in many, many aspects of scientific computing we have to evaluate a multi-dimensional function on a grid. So assume that we have a 3d function which is sampled at 1000 points in each dimension so that gives us 1 billion samples. Now if you - 1000 sampling points is not that high, so if you enlarge the number of dimensions to four for example and the number of sampling points by 10 then you have an exponential rise in the number of elements here is 10 to the power of 16. So even a simple four-dimensional function finally sampled becomes impossible to compute on but when you think about 10,000 sampling, but think about the CD quality audio is sampled about 40 kilohertz so each second you have 40,000 samples so it's not that high the resolution we have and still it's almost impossible to deal with it. Now we have to resort to some automatic analysis of data and here we're talking about multi-linear algebra. In very much the same way as in standard linear matrix algebra we can decompose a matrix into a sum of its rank one components which are outer products of vectors ai and bi and these are the eigenvalues so we can do the same with tensors basically, this cube can be split into outer products of vectors ai, bi and cii where r is the rank. So we have the same type of arrangements now that gives us some interesting opportunities because maybe the rank can be very, very high and plenty of those terms can be related to noise. So we could perhaps perform dimensionality reduction by looking carefully into the tensor ranks but this procedure here which is similar to SVD is called the Canonical Polyadic Decomposition, it's the basis of a multilinear algebra. Back to the Lion King, so let's get back at the setting when we have 1000 by 1000 pixels for every frame and 1000 frames over 20 seconds of data so this tensor is of size 1000 by 1000 by 1000 which is one gigapixel of data. If you decompose it using the Canonical Polyadic Decomposition into the sum of those rank 1 terms then here - it's a cartoon obviously so the rank is of course very, very low because the background along different frames is similar; it's either a blue sky or grass or a desert and such like and the frames are not very different from one to another because we are talking about 50 frames per second so typically for this type of problem the rank would be 10. So we are talking about one billion of data we should be represented as 1000 pixels times 1, 2, 3 so 3000 times 10, the rank. So we can convert the raw data of one billion pixels into a Canonical Polyadic Decomposition format which has 30,000 pixels only so you see that the tensor decomposition is performing a super compression of data. You'll see later and the idea is very clear if we can use this type, this principle to deal with say enormous dimensionality of weights in deep learning, so you can have millions of neurons with connectivity which can be very, very dense so think about a graph, if you can convert that using these principles into much more manageable information then we're on to a winner and that all comes without significant degradation in performance. The second thing is that with tensors we can then use physical intuition and physical meaning. So if you're talking about the matrix rank as the number of those independent outer products which built up the matrix and for tensors you add the other outer product with the vector c so you can think about the matrices, outer with vectors. It seems very, very simple but all of us have been to a shop maybe trying to choose colors for decoration of our houses. Think about a color ensemble here and - here we have one, two, three, four, five colors but you can have 10, 000 colors here. So clearly and I intentionally started the presentation with the rgb pictures of tomato and and apple so clearly the base colors here are red green and blue so any of these colors can be obtained as mixture so R,G and B. So although you can have 10,000 different colors and 10,000 times the size of matrices, the structure in data is very, very high and boils down to a linear combination of three base colors. For example for the grey here, the vector c would have equal amounts of red, green and blue. While for the orange there you could have plenty of red, half of that of green, a little bit of blue. So clearly to represent - I can't help, can't calculate by heart now how much data you would have here, all you would need is some known base matrices and very, very simple mixing vectors. OK I'm hoping that now you're convinced about the power of vectors of tensors we can talk about other types of decompositions and maybe splitting data into even smaller chunks. One with this decomposition is so-called Tucker Decomposition which decomposes data into a small core tensoring and outer factor matrices. You can force the elements, it is much like an SVD to be orthogonal in which case we're talking about orthogonal Tucker or you can think about splitting this tensor the cube into concatenation - we'll call it contractional modes of smaller chord tensors. So the outer factors are matrices and the inner factors are tensors. This is called a tensor train and this naive attempt very early on in this area we tried to - because the tents are trained to to represent these as carriages on the train you can see the buffers here. OK, so what is it we can do and I'm talking about low-rank approximation for matrices. We can do the same for tensors so this tensor in a Tucker decomposition can be decomposed into with dimensions i1, i2 and i3 into the outer matrices u1 u2 u3 with the same dimensions i1, 12, i3 r is the rank and the core tensor. Now if you drop the - so if you do a low-rank approximation you drop several of those out of products, reduce the dimensionality here by say 2, there by 2, there by 2 and there by 8, that is the principle of higher order singular value decomposition. With tensors and multi-linear algebra of course they are richer than linear algebra. Linear algebra gives you a flat view of events while tensors are much more natural, you can also do mixing of ranks so this would be decomposition of images in some person identification setting into rank one components in the tensor way. You can see that well - these, basically, matrices carrying no physical meaning. However if you decompose the same thing into rank one, rank three, full rank and so on with vector c mixing these you can see for example this factor here would be responsible maybe for eyebrows, this factor will model maybe the hairline and such like so in these so-called block term decompositions you can enhance physical meaning of your of your data. I'll just finish with a few examples. Tensors thrive on redundancy in data so wherever you have plenty of structuring data you can use tensors to exploit that structure and make things simpler or enhance processing. For example, tensors and Kronecker products go hand in hand, especially when we talk about Kronecker's separability. So this is the original picture of Lena and those of you working in image processing have seen it many, many times that even with 40% of data missing, the tensor reconstruction gives you a pretty good image. If you have 90% of data missing you can still have a pretty good recovery and I'm not going into detail, I'll point you to the work where this was published. The second thing is talking about latent components in data. So with tensors you can connect in a generalised regression way to datasets x and y and you can find which components in x are responsible for some behavior and y, by aligning - so again you have the SVD like type of decomposition - by aligning the latent factors t and u like there, you can see which portion of x is responsible for the behavior of y. Now that's a bit abstract, I know, but this is an experiment we did back in 2003, I've been a frontier researcher in RIKEN, Japan for many years and they had laboratories and monkeys. So the monkey was sat in a chair and the monkey had markers for motion capture system and also an E-COG system with 32 channels. So the idea was to predict our movement from EEG. Now we all know that EEG is responsible for - the brain commands are responsible for our movements but it's only a proportion of EEG brain electrical activity we can record that's responsible for our movement because if I move my arm at the same time I'm looking at the screen talking, thinking what we're going to do next, sensing how hot or cold my room is and such like. So the goal is how to identify the precise  proportion but latent components in EEG which are responsible for arm movement and if you combine the 32 E-COG recordings represented as a time frequency into a cube which is a tensor and for those of you who are more into neuroscience you would know that the Mu rhythm in EEG is about 10 hertz, is the signature of motor command, muscle movement and you can almost see here 10 hertz in the frequency and these are the - this is the tensor produced by the x y and z components for the 4 markers on the hand so 3 by 3, a tensor, time, time, time, series. So in the tensor setting you can perform this regression, no problem although the xyz positions in space are very different from the microvolts in EEG and anyhow the EEG comes in a very, very convoluted way using standard transfer function, standard signal processor to be impossible. You can do this using neural networks obviously but you have a black box model which you couldn't control and the problem with neural nets obviously especially deep nets is not why they work but when they don't work you don't really know why. While clearing a tensor way you maintain the physical meaning associated with data, you train this model and then from the new EEG data you can pretty well, in red predict the original trajectory in urine. So it's time for me to start concluding, so tensors definitely can deal with the cursive dimensionality through tensor network representation. So even an enormously big tensor can be split into smaller sub tensors which can be located even on a different computer and that can be processed sequentially like in the tensor train decomposition at the extreme. So this very, very long vector can be represented as a six dimensional tensor with only two components in each mode, so called QDT: Quantized Transfer Decomposition. If you look at these type of tensor networks they remind you of graphs but the connection is actually the actual connection is not that obvious but also remind you of neural nets and clearly you can have some hierarchical structures and you can perform many operations on a graph rather than using multi-indices in a tensor. For example here we can split the core in red into much smaller chords and attach the outer dimensions to a tensor so clearly we could perform various operations including the the - what's it called - the pre-whitening and such like in a much much easier way. I mentioned that that from the Lion King operation that the number of date in a row tensor is i times j, times k so exponential in the dimension n. The number of elements in the canonical polyadic decomposition representation is linear in the number of components here r which is huge, huge help. You can do many many things in the tensor domain which may not be achievable in the matrix domain like here, you can think about high perspective imaging which is naturally a tensor so many wavelengths of this images at various wavelengths of the same frequency and just the last couple of minutes to connect, to demystify neural net and maybe associate some physical meaning with them. So I started doing neural nets back in late 1990s and have published a book on recurrent neural networks with Wiley 2001 and I always struggled with computational issues and also maybe with physical meaning interpretation but now if you have a huge neural network there's nothing much you can do in terms of its optimisation but you can definitely represent the huge weight tensor as a tensor network and you can optimise tensor networks in any way. So you can super compress it and transfer it back to the neural network domain, in this way you are arriving at the much smaller size, optimised neural network at which you cannot arrive in a standard brute force way and this neural net is hopefully easier to train and run on simple, simple simple computers. Again, so shallow neural nets here deep networks are amenable to CPD decomposition while deep neural networks are very well represented as hierarchical Tucker decomposition or tensor train. Our own work in this area includes the attacker tensor layer for optimising deep networks whereby we were able to perform tensor valued back probe which is interesting by itself and just showing here a synthetic example with images which are either containing horizontal or vertical lines, you can see that the gradients along the modes are either favoring one dimension - one more or the other reflecting basically the nature of data and with several data sets we're able to perform here, say 66 fold compression in the number of parameters while maintaining 94.6 accuracy. The original accuracy was 98 maybe, so clearly this is the way to go. If you think about the graph data on graphs, if you think about a graph at consecutive time intervals you're getting a tensor, so tensors and graphs are related. If you think about any notion which is chronic or separable so you can have dimensions which are represented with outer products of vectors you can have both graph and a tensor. With that I'm just trying to finish, I know I've opened more ideas than given answers our own toolbox for higher tensor decomposition it's called the HOTBOX, it's hot off the press at the Github, you of course have very well established TensorLab Lieven De Lathauwer from Leuven. TensorLy, Tensor Train Toolbox and such like. The upshot is that if you think about the big data paradigm then it's very very natural to first try to perform dimensionality reduction. So the problem of finding the needle in the haystack is basically trimmed down to the haystack and then maybe you'll find the needle. The same applies to big data analytics with tensors or deep networks for graphs and again some of our own work in this area includes this paper in Single Person Magazine which got the Best Paper Award; it's basically a very simple introduction to signal processing on the tensor domain again trying to avoid many, many multi-index equations and replace them with graphical operations. This is more recent, these tensor networks, the two-volume monograph with new publishers and again in foundations and transient machine learning and the bulldog spirit of the Brits is represented here. If you go from a scalar to a tensor you're going to 'beef up' and be much more strong and resilient. And with that I'd like to thank you for your attention, it's really weird to talk to yourself although you know you have audience but you can't see them. I hope that I've convinced you to look to these areas and I'll be very happy to answer any questions that you may have. Thank you very much. Richard Pinch: Thank you, I'd like to take the chairman's privilege, particularly this time I'd like to ask two questions if I may on my own behalf. The first is the general one: there's a lot of interesting and sophisticated mathematics in your work drawn from sort of quite a wide range of mathematical disciplines. Are you finding that the mathematics is there just sort of ready for you to pick up and use? Or is your work feeding back new problems and ideas into graph theory, linear algebra and the other domains that you're touching? Danilo Mandic: That's almost a 'chicken and egg question here.' Of course you can bypass mathematics and you shouldn't, real, real tribal mathematics. The only issue is that mathematicians and mathematics tends to be a bit general and abstract so mathematician doesn't always think about a concrete problem. While an engineer actually is - if they're not solving a complete problem they're not an engineer which is also perfectly OK. Now it's often the case that you need to at least tweak, adjust current algorithms for your problems in hand which means that you do need to understand mathematics in depth but also often you need to design new mathematical approaches. For example I've done lots of work on quaternions which are obviously ordered pairs of complex numbers and if I wanted to do the gradient learning of quaternions, I had a problem because of the only differentiable function in quaternions using the Cauchy–Riemann theory is the the linear function but my neural net is non-linear and also in quaternions we lose ordering so - not ordering - the computativity ordering you losing the complex domain. So a times b is not b times a, so you can't only think about the chain and product rules so that motivated me and my team to introduce the HR calculus for the differentiability, the derivatives of quaternions. For example our cost functions the object is always real, some error squared type of thing so the derivative of a real function of a quaternion variable doesn't exist. So we needed to introduce something. Now of course the new five-phase sometimes little bit of opposition from mathematicians because everything comes from a concrete problem which maybe in order to understand the the new mathematics you need to understand the problem a little bit too. So it is a 'chicken or the egg' problem but many, many interesting disciplines started from a concrete problem and we are not really shying away from that. Richard Pinch: Thank you very much, it sounds as if there's certainly plenty of scope further interaction between mathematics and its applications in this domain. I have - we have a question from William who asked about connectivity: 'You use the word connectivity, is this something to do with the same word 'connectivity' in topology?' Danilo Mandic: Yeah, so I must say that the terminology is not unified across these disciplines and and I'm not an expert in topology so I can't answer. For us, connectivity is basically in a graph represented by say degree matrix or sense of how many connections you have to a node. If you have a neural net then clearly you think about a fully connected net where each node is connected - one layer is connected with each node and all the nodes in the next layer but then in my area, data analytics we talk them about sparsity or sparseness which means that some of those connections are not existing and you like to have a sparse representation so to have as few parameters as possible so that everything you do is maybe more manageable, it has very little at the expense of very little error in processing. So I hope maybe that answers the question.  Richard Pinch: OK so I think we've just got time for one final question from John McWhirter  who asks: 'what do you understand to be the difference between tensors and multi-way data sets and the implication of that in practice?' Danilo Mandic: So of course it's always good to hear from a friend and I would like to say hello to John, I haven't seen him for a long time and John has been this inspiration obviously for my work in quaternions and for those of you who don't know him he's a pioneer of beamforming. So multi-wave versus tensors, so again it's a little bit convoluted so when you think about the notion of multi-wave you think about nodes. So one node can be time, second node frequency, third node maybe value of FFT at that window, the fourth node subject, fifth node trial like in the EEG experiment and so on. Then it all has physical meaning because those data, natural tensor data they are not actually. Now how do you tensorize, some data are natural tensor like RGB images but other data you tensorize by performing maybe some sort of transfer on data enlarging dimension folding somehow so the notion of waves, multi-wave, becomes a little bit blurred. So there's definitely a need to look into that because tensors do thrive on redundancy. If you have redundancy that means that your waves, nodes, can be somehow even mixed. While by removing these redundancy through decompositions to arrive at the core, basically structure in data. So the answer is yes, we use the notion multi-waves even for tensors which are somehow folded from from very few nodes. Richard Pinch: Thank you, I'm going to have to - I'm afraid there are a couple of questions stacked up. I'm afraid we're going to have to foreclose on the questions but before we finish I would firstly like to express thanks to Professor Danilo Mandic for a fascinating talk covering a very wide range of concepts in mathematics and a fascinating range of its applications. Thank you very much. Danilo Mandic: Thank you very much and before we close I was asked by the organisers to show on the two slides about the events of the Institute of Mathematics and what's going to happen in 2021. So it's been a great privilege and honor to talk to you, unfortunately not in person but should you have any questions, anything you would like to ask I would be delighted to answer offline via email and I believe my email is on the front page of my slides. Richard thank you very much for sharing and thanks to John and Maya for enormous help throughout.  Richard Pinch: Well thank you, Danilo, thank you to Mini for giving the previous talk for this session and finally of course thank you to the IMA Conference Team for making sure that the thing actually happened and keeping us technically on the road and I can now draw the conference to a close dead on time, thank you everyone, at 12 o'clock precisely. Thank you very much and good day. 