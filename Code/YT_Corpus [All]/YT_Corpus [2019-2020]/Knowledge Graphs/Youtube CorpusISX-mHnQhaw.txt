 Yeah. Thanks. So I thought that  based on the timing of the talk,   it was better to focus a little bit more  on the implied side around [? a little   bit ?] something a more critical and also  highlight a rapidly increasing gap between   what we theoretically understand about deep  models and where they are heading in practice.   So this is a very real context that we are  actually working on with a larger group. I will highlight that in terms of  understanding how to represent molecules,   how to make predictions based on  molecules, how to create molecules,   entirely novel molecules with desirable  characteristics. So just a bit of   background and feel free to jump in and ask any  questions here, so what are molecules as graphs? We know about the annotated graph. So what's  particular about molecules as graphs? Here's   a particular antibiotic molecule. It has  node labels, so the atoms vary in its node.   You have edge labels. Atoms are bonded  together of different types of bonds. You have substructures that are larger motifs  that are actually important structurally how   these graphs behave as molecules. And you  also have 3D information that the little   wedges there that highlight whether the  thing goes into the board or out of the   board. So why are these type of structures  then be interesting from a machine learning   perspective? Why would we want to work on them in  terms from an algorithms or theory perspective? Well, they are very rich and complex objects. And  their properties actually depend sometimes very   intricately about defined structural details. So  it's a complex real problem that we went to solve. There is quite a bit of data about molecules,  but it's not uniform data. There is small   data. There is big data. And there is a  heterogeneous data of different kinds.   But you have quite a bit of data to learn about  what these objects are and what they should do. There are tons of interesting estimation and  inferential problems in this context. Just to   name a few is the standard machine learning  problem. You have an object. You predict its   properties. That always are challenging  problem with these type of structures. Molecular optimization means that you give me  a molecule. It's known as a lead, so things   that you already know is sort of good, and you  want to somehow automatically turn that into a   molecule that's actually good, say, as a drug.  OK, so that's a molecular optimization problem. So what I'm going to focus here in this talk  is essentially a broader problem of drug,   automated drug design. And how do we solve  this problem algorithmically? And there are   two components to it, trying to understand  properties. If I can predict properties,   I don't know how to modify it, and then how  I actually solve the optimization problem,   turning it into a better version,  turning it into entirely new molecule. So this is not work in isolation. There's actually  a large consortium of pharmaceutical companies,   14 major pharmaceutical companies  and chemistry chemical engineers,   as well as computer science faculty, myself  and Regina Barzilay, that are working on a   range of problems related to molecules  from a machine learning perspective. So you try to lift the chemical literature into  a computational form that can be used for models.   That's an NOP task. This is the molecular property  prediction, molecular optimization part that I   will focus on. You can try to understand reactions  that are a step beyond individual molecules. You have a set of molecules, and you want to  predict what comes out of that reaction. So it's   a set of structures to a set of structures,  a problem. And you want to optimize those   reactions so that particular compound is  more likely to come out of their reactions. And also then, something called a retrosynthesis  problem, you are given a molecule. This is what I   wish to manufacture. How do I find the sequence  of steps to actually get there? So that's often   formulated as a reinforcement learning problem.  It's a type of inverse problem where you have   inverse problem. You're solving is actually  a sequence of structural steps. But today,   I'll just focus on one part of it  and try to highlight that further. So the problem here is automated drug  design. So you're given the molecule,   some design specs that are essentially properties  that you want this molecule to have. You want to   increase the potency against a particular target.  You want to make it less toxic, more soluble,   have some other drug-like characteristics, and so  on. OK. So those are design specs, and then you   need to translate that molecule into a new version  that has those properties. That's the task. Now, there are many things that we need to  understand in order to solve this problem. We   need to understand how to represent the molecules  in the first place, how to make predictions,   effectively based on those representations.  And then also, how do we realize this new   molecule that will be potentially an entirely  novel molecule that nobody has seen before? The molecular space that's still  unexplored is actually quite vast.   The pharmaceutical industry tends  to focus justifiably so on a smaller   piece of it. So there is quite a bit of  potential in these type of approaches. All right. So for my machine learning perspective,  let's focus on the representation and prediction   for a little bit just to highlight a little bit  of the theoretical background here. There is   standard workhorse for machine learning  on graphs, is a graph neural network.   Hopefully, familiar to most of you, it's  a message patting algorithm on graphs. You start from a simple, say, atom representation.  Which atom is in a particular location? How is it   bounded with others? So you have features in  the nodes. You have features on the edges. And   you want to pass messages so as to get  representation, say, for an atom that   actually captures the local molecular context  in which that atom appears in that molecule   and drive that representation towards being  able to predict properties of that molecule. OK, so it's a potentially complicated neural  network model, but it has fused important   pieces as computational problems that will  turn out as limitations as we'll see shortly.   So the first step is that how do I look at my  neighbors, other atoms that are bonded to me?   And how do I aggregate that information  to better update my own representation? And that aggregation step is typically carried  out as a multi-set. OK. So the neighbors in these   algorithms are exchangeable from the point of  view of the algorithm. OK. And that's actually,   in some sense, a strength and a major  weakness in this context. Once you aggregate,   you can combine the node information in  your previous representation and update   to get a better representation  of what that node should capture. The final step is then once you get  these representation for each atom,   how do I aggregate them together to get a  single representation for the graph as a   whole that I can then make predictions about  molecular properties? So I'll focus on just   the two of these a little bit, the aggregate  step and its consequences, and read our step. So there has been quite a few papers  rather recently about understanding   what these models can and cannot do.  So let me try to picturely summarize   now a segment of that work that's  relevant for our purposes here. So here is a set of all node graphs, say, eight  node graphs. One point there is a graph like this,   where the colors now wraps and say the node  labels, the atom properties in that graph. I can   define an equivalence class within that whole  set of the graphs, everything that's locally   isomorphic to this graph, locally, in terms  of the one-dimensional Weisfeiler-Lehman test. OK. So I get a subset of those molecules.  Now, I can sample another molecule within   the same equivalence class. Here it is. Not eight  cycle but two four cycles, and they are colored   in a particular way so that the locally,  from the point of view of each atom here,   its neighborhood looks identical to a neighborhood  in the larger molecule. And not only that,   if you start expanding, say, the computational  graph, the computational tree from how you   would update representation for this node, it  would look the same as a tree in both graphs. So what is the consequence of that? Well, that  means if I apply a graph neural network to both   of these graphs, I get a set of node embeddings,  embedding for the atoms as a result. But they   are identical as multi-sets. So any exchangeable  read-out operation from this set of vectors that I   get through this iterative application will result  in identical representation for the two graphs. But definitely, graph neural networks in the  standard form cannot distinguish these two graphs.   It would give exactly the same answer to these two  graphs, no matter how you chose the parameters of   the model. Well, we can go a step further and  try to enrich this representation a little bit. Recall that the problem was that neighbors are  exchangeable, so this driving this failure here,   so I can define a smaller equivalence class  based on finding something called a consistent   port ordering for the edges in the graph.  So the edges are no longer unnamed. You   have a name for each edge, so I know that I am  receiving message from you. And I can tailor,   accommodating that information in the  algorithm because I know it's coming from you. So unfortunately, even in this sense, defining  an equivalence class in terms of local port   orderings, these two graphs are still part  of the same equivalence class. OK. Even with   the port ordering, you could not distinguish  these graphs because a student of mine recently   actually showed this that, again, they result  in identical. No embedding as a result of the   method. Even with this, enhancement  couldn't distinguish these two graphs. And there are actually a host of graft  properties that none of these algorithms   can distinguish. So that's a problem in our  case since molecular properties actually   depend on these higher order structural  features, so we have to somehow build   them in. So let's take our example just to  illustrate what that means to build them in. So here's a complicated molecule. It's actually  a chain because it's a polymer. So it's actually   built from repeated structural units. But  from the point of view of the graph algorithm,   that would be ran on an atom level. It will  look hugely complicated, and it wouldn't   understand this chain-like structure. And  as you can see, there are lots of cycles   there in these graphs that are particularly  challenging for these type of algorithms. So what we do is we build a vocabulary of  larger units. You can take this as a unit,   the bonded atoms, a simple pair as a  unit, the larger substructure as a unit,   and so on and so forth. So as a result, when you  build this molecule from this larger components,   you see that it has this chain-like  structure as it should since it's a polymer. So you need to go higher up in terms of the  representation to capture the properties of   these molecules. So one problem here  is that there is motif graph here as   we call it. Doesn't actually account for,  it just gives you a symbol for its node of   what's substructure it is and who am I,  in principle, connected to to get this   chain-like structure. But it does not tell  me how intricately these parts fit together. So you get this very fine representation original  molecular graph and a very coarse representation   that's now in terms of motif symbols and  how they are linked together. So you need   something in between to mediate this, and we  introduce another one that actually considers   attachments between these substructure  as an intermediate-level representation. And then you take not only one of them as  the representation but the whole hierarchy   of these as the representation  for the molecule. So just briefly,   what it looks like is that you run GNN on  the atom-level graph. You aggregate those   representations to a node here at the  intermediate level that has information   about what that substructure is but also  how it is attached to other things. So the   vocabulary of symbols here is actually larger  than the symbol at the highest course level. So you have this hierarchy of feet-forward,  integrated neural network architecture. So does   this help at all? Let's see. So here's a simple  example of predicting solubility of a molecule. So you have a simple message passing  neural network. It gets to a certain area   level. We're just getting information about,  say, atom type and the bonds between them.   So a very rudimentary feature representation  of the molecule first and then you run GNN. The second one is you start incorporating  handcrafted features about the local context   in which those atoms appear. So you include  a degree balance, whether it's in the cycle,   whether it's part of an aromatic ring and  so on. So you just add features to an atom   based on, essentially, its substructure  context. OK. You get to a better level. If you know, go back and only use  those very simple features and let   the hierarchy capture the structural  context in which the atoms appear in.   You actually get even better result. So  the hierarchy can learn those handcrafted   features that you would otherwise  have to build into these models. So just an example that they're using these  type of representations is actually already   quite useful. We have demonstrated that in  the context of antibiotic discovery. So once   you have a training set of molecules in their  properties, you can train your algorithm. Once   you have the training methods, you can  scan a much larger library of molecules   in search for one that has a high potency for  the property that you are really interested in. What we did here with collaborators is  take a couple of thousand molecules,   which is a diverse set, and in the lab,   test in a high throughput fashion what their  potency is towards suppressing E. coli growth.   So you wanted to have antibacterial  properties against this particular bug. Then what you do is you have a trade  model. You now scan hundreds million   molecules and rank them in terms of their  potency to kill E. coli. And then you   can take the highest predicted molecules  compounds that have the highest potency,   and you can test them in the lab.  Many of them indeed have that potency. But among them was actually an interesting  molecule that the method found,   it's a kinase inhibitor that's slightly distinct   from a typical antibiotic drug. It  has a different mode of operation,   different mechanism of how it acts as well,  and it was considered for a different purpose. It has high potency against E. coli but  also a high potency against a number   of other very nasty bugs, including those  that, let say, people who are in long-term   hospital care that's very hard to treat.  And our collaborators already tested with   mice that it actually is an effective  treatment against these nasty bugs. So even if you only can learn to  predict properties of these molecules,   you can already make a huge difference  in practice. All right. So that's not   all we want to do. We actually want to  go a step further, actually learn to   generally generate molecules that would have  better properties, entirely new molecules. So we need to talk about how to generate molecules  in the first place and how to optimize them for   the purposes that I wish to use them for. So  the task here is as the following. You have,   say, a set of pairs here of a source  molecule. This is my starting point,   a better version, and then a description  of this is what I wanted to get out of it. And you can change the design specs of the  direction in which you wish to modify these   molecules. And what we're going to do  is translate this task into a task very   akin to machine translation problem in natural  language. You're getting pairs of molecules now,   not pairs of sentences. You get lots of pairs,  and you learn to map from one to the other. So you generate new molecules by translating. So  in a little bit more detail, what this amounts   to is you have a source molecule. That's a  source structure. You encoded hierarchically,   and then you have to somehow use that  hierarchy to unravel a molecule that would   have better properties that translated  molecule from that representation. And you learn these models into end, from  pairs of source and target molecules. So   you have a rigidly structured object.  You need to represent it properly. You   need to generate originally a  structured object at the end. The key challenge here is actually how to learn to  generate these molecules in a meaningful way. So   the encoding of a molecule in these hierarchy,  three-level hierarchy, that I mentioned before   that was actually designed to exactly capture  information that I need for an autoregressive   step-wise generation of adding these larger  substructure units into the molecules. So what that means in terms of the picture is  that at the highest level, I have those larger   substructure and how they are connected together.  So the first decision, when I'm unraveling this   molecule further, is to pick what is the next  substructure that I wish to generate and who   is it connected to. That's done on the basis of  the node representations at the highest level. Then you go down. That's not sufficient. Now I  have to decide which part of this new substructure   is actually connected to the previous part of  the molecule. So that decision now uses the   additional attachment information that I have  in the previous, in the intermediate layer. And finally, I need to decide  how this substructure with that   component detaching somewhere here  exactly what's atoms it is attached   to. And that's done at the lowest  level in terms of the decision. So you get this order in regressive  sequential way of unraveling molecule,   one substructure at a time. Picking the  substructure, which part of it attaches,   and then precisely how it is attached  to the molecule that's being generate. Now, this hierarchical generation actually  carries a number of benefits. Here's one   illustration of it. So what if you train this  model and then you're getting a test molecule,   and you just ask, can I reconstruct it? Can I  encode it? And can I unravel the same molecule? OK. So a simple generative graph  generation model that's based on,   say, atom by atom generation here, starts really  failing as the molecule becomes larger. But if   you generate the molecule in this hierarchical  fashion substructure at a time, you actually   maintain the accuracy of reconstruction regardless  of the size of the molecule that you generate. As this sort of a typical in these type of models,   you need to explicate and connect the information  with very low, short connections in order for   these models to work well. And if you  generate it atom by atom very quickly,   it starts forgetting what the actual context is,  and it's harder to plan to unravel the molecule. So there are other pieces to it, how to model  diversity. You don't want just to generate-- well,   sweats backing for it. All right. So you  want to model the diversity. So from any   particular source, you don't want to  generate just a single output but a   population, a diverse population of candidate  molecules that would all have the desired   properties and that you can incorporate  with their laden variable into this model. Similarly, you incorporate the fact  that the translation from the molecule   to molecule is directed. You direct  it towards a particular property,   so you need to input that direction  somehow. All right. So the challenge   here or one challenge that's not  quite solved yet is really that   training data that you have for these directed  translations is typically of marginal kind. So I have a pair of molecule and its better  property, version, but in terms of a single   property. For another property, I have a  starting point and a better version and so   on. But I have very few examples of going in  there, kind of a combinatorial direction from   the starting molecule. So that's really  an extrapolation that you're trying to   solve here when you are directing it into a yet  unexplored direction and trying to make it work. So just to illustrate a little bit how  this works, I'm just going to make two   quick points about this. So the graph, the bar  in the right is the current model. This here   is a model that was the top performing  model in, say, one to two years ago. And this scale here is the percent success  that when you are taking your molecule and   you're translating it, what is the percent  of the time that the generated new molecule   has the property that you want? So from 3%  to 85% success rate, that just highlights   how fast this area is moving into something  that nobody would want to try into something   that actually companies are now interested  in exploring the molecules that regenerate. So that's an excellent point. So  these are actually with respect   to these results in part synthetic in the  sense that there is a separate predictor   that tries to assess the property,  and you are measuring against that.   The problem here is since you are  generating an entirely new molecule,   to truly assess whether it satisfies the property  would actually have to run it through a lab,   similar to the antibiotic discovery where actually  a small set was run through in vitro studies. So the criteria here was that since typically, in  the pharmaceutical discovery, you have a lead that   you already know is sort of good, so you went  something that's similar to the starting point   and has a better property. So the similarity  here is part of their success probability that   you wanted something that make some structural  changes but not a from-scratch, new molecule. In principle, it's not clear that it  would work as well. But in principle,   the criteria that you're aiming for could be  anything. This is just another example that   highlights the same thing from a useless method to  a useful method in a rapid turnaround. All right. So what's still difficult? This is an illustration  of that is this multi-criteria optimization where   you're going in the direction that you  haven't really explored in the training   data. So the numbers here in terms of going  into new direction, the success rate goes down   considerably as a result. The model hasn't  really been directly trained towards that. But in those low teams, success percentage is  already starting to be useful. You can sample   diverse set of 10 molecules and run them  through the lab and at least get one that   actually would have that property. So it's  [? reaching it ?] but not quite there yet. I'm going to skip this. You can do place-in  studies of figuring out. Actually, the pieces   that I described are all necessary  to get their level of performance. The last thing that I wanted to talk about  is that there are stochastic gradient design,   which is an embarrassingly simple  algorithm that works incredibly well   in practice. There are also additional  incredibly simple things that you can   do to make these models work much better.  And this is one example of such a method. So we've already discussed that it's very hard to  learn to translate from one structure to another.   It's much simpler problem to learn to predict  its properties. So from a smaller training set,   you should be able to learn better  predictor than a translation model. So somehow, that predictor should be able to help  the model to get even better because now, you   essentially have a free assessor of the candidates  that I generate. I can read out those that don't   appear to be good and provide them as additional  targets for the model. So we should be able to   augment additional targets in a semi-supervised  fashion to make these models work even better. So theoretically, this is actually a very simple  setup. A flashback from the 70s, it's just   stochastic version of an EM algorithm, where the  likelihood model is now the property predictor.   It possesses any particular, the probability that  the molecule, any particular molecule, satisfies   the desired specs that I want. The prior here is  the translation model that I trained. It tries to   take an input, the source molecule, and generate  valid targets that would have those property. We're just trying to maximize the probability  that your generated candidates would satisfy   their design criteria according to the  property predictor that you trained. Do you have to classify that tells you  if it's going to have a proper name? Yes. So you give me a data set that has,  essentially, molecules and their properties.   I can arrange that data set into pairs of a  molecule and a better molecule. That's the   training data for their translation  model. I can also, at the same time,   take that data and train a property predictor  for any molecule predict its properties. Now,   the question is, how do I combine  the two to make it work even better? Do you get gradings and  backdrops for [? a living? ?] Well, only in a heuristic way since the summation  here is over all possible molecular graphs that I   could generate. So I can't really sum through  it. So this is just sort of essentially for   illustration of what I'm aiming for. Maybe  you mentioned this I came late for your talk,   but it is, in some sense, an inverse problem.  The likely I want to invert the likelihood model,   which is the property predictor, and  find instances of molecules that score   high relative to that, that inverse  problem is actually quite challenging. In principle, I couldn't train the model  in this way because I can draw posterior   samples. The model itself, the translation model,   the prior is highly complicated. I don't  understand it fully. I don't understand   how it intersects with the likelihood model,  so drawing those samples is incredibly hard. What saves us here actually is the fact  that the prior model here is already   quite good based on the previous results. It  actually, with relatively high probability,   generates valid molecules that have the  property that I wanted, not all the time,   but some of the time. So you can actually solve  this with a simple rejection sampling scheme. You generate targets for any molecule that  you have. You generate valid targets. You   read out with the property predictor,  those that don't appear to be good.   That gives you an additional valid  targets for your translation model,   and you can retrain the translation model. So  that algorithm is nothing but an EM algorithm   but cast in a stochastic fashion. And the only  reason I can run it is because the translation   model is already good. Otherwise, it would be  rather hopeless to try to get the posterior. So this is just an illustration of generating a  target and beating out some based on the property   predictor. So how well does this combination  work? So here's a comparison. Here's the   previous model that I talked about, already  quite good in terms of different properties. When you just add this simple  stochastic EM component,   you actually cut the error rate that is  the probability that when I generate a   candidate molecule that it would not have  the properties that I want. You cut the   error rate by three-fold. So it's quite an  improvement with a very, very simple idea. And perhaps surprisingly, the effect is very,  very robust. So if you make the property   predictor simpler and simpler, so it's very  easy to estimate from the data that I have,   more easier, the simpler it is. But its error  rate as a predictor goes down. And you maintain   the gain in terms of improving the translation  model almost to the point where the error   level of the property predictor is on the same  level as the target range that I'm aiming for. When I really truly, information  directly lose the signal. At that point,   the gain starts going away That just highlights  how challenging the prediction of a new structure   is. So this is sending analogous to talk about  over [? primerizes ?] some in neural networks. Some simple things work in a new context, whereas  this type of approach would not have been used   previously in the simple classification setting  in a semi-supervised context. It wouldn't work   nearly as well. All right. So the point  here that I'm trying to get across is that   these molecules and structured objects, they  actually embody many of the challenges that   we have in terms of predicting properties,  generating, manipulating complex objects. And they are actually becoming  already used. Our collaborators   in pharmaceutical companies are already using  these methods, including the generation part,   but there are lots of things that are  unsolved in this context. You would want   to incorporate physical features that you know  actually generalize to new chemical spaces. That's a very hard problem. How do  I train the method from a subset of   molecules and then have some guarantees that  it actually works in an entirely new part of   the chemical space? OK. The only way truly  generalized is to have a physics model. So   how do I do something intermediate that have  some in variance that help me make that jump. There is a lot of theoretically  motivated work that you can do   to make this multi-criteria optimization  much more solid and well founded in this   space. There are issues over [INAUDIBLE],  explaining, drawing chemical insights from   these models that's in entirely new  area of research. So that's about it. Hopefully, what I tried to highlight is that  there is actually quite a bit of need for   theory here in terms of understanding how  to make use of structured objects. And the   only way to solve it is to have a framework to  formalize the problem, as well as the method   and what the algorithms can and cannot use based  on those structures. So with that, I will stop. [CLAPPING] 