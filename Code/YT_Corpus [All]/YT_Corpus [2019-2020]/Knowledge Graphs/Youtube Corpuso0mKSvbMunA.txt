 today we're going to be coming CSPs but possibly just as importantly you're all halfway through the quarter so congratulations I think six more weeks to go so keep it up also happy Halloween hope you all have fun but today we're going to talk about CSPs so this is continuing these topics that we've been covering since Monday which is about this kind of setting right so you have variables in this case we have 3 X 1 X 2 and X 3 and each variable represents some kind of discrete object that can take on one of several values and so the set of values that a single variable can take on is called its domain and just to continue reviewing these variables have factors which are functions that take as arguments one or more variable and the factors basically say ok how much do I like this assignment of my variables so for example a factor two looks at X 1 and X 2 and it says okay X 1 has been assigned some value X 2 has been assigned some value how much do I like that assignment do I really like it if so you give it a high value do I not like it if so you zero it out and so importantly we call the arguments so all the variables that we give to a factor that's the scope of that factor so we had this example that we talked about on Monday and I'm just gonna revisit it so in this case we have variables correspond to people and there's three people and we know that first of all the first two people person one person two they have to agree to each other and then person two in person three they sometimes agree with each other but not all the time and so first what we say is with our first factor we say that person one is definitely blue so we say this year by saying these are the values that the variable can take on red or blue and this is how much we like that value and what this first factor is saying is that it doesn't like read all it's a zero and it likes blue it's a one and then we have the second factor this encodes the fact that they must agree and with that saying is that every time both the first variable which is X 1 and the second variable which is X 2 and every time they agree they're either both red or both blue we give it a 1 otherwise we give it a 0 and then we have this factor which says that they tend to agree and we're encoding that by saying well if they're if both of its arguments are the same if they're both red or they're both blue then we give it a slightly higher number than if they differ but we're not going to zero it out and then last we say okay well the last last person kind of prefers to go red but you know it's not a hard constraint nothing like that and so then again so like we talked about last time assignments which is like if you have all your variables and you have all the values that you've assigned to them they have what's called a weight and a weight is basically just plugging in all the values for your variables into your factors and then multiplying them all up to get that product and that's the weight and our goal across all of these problems in this whole unit is to find assignments for all of our variables that will give us the maximum weight when we multiply up all of our factors yeah yeah so the question was why is the weight a product and not a son and the answer for that is because remember these are constraint satisfaction problems and so if you look back at this example we wanted to encode the constraint that they must agree and we did that by putting in a zero and since we're multiplying it up if you have a single factor that gives us zero it's like a veto and so that veto power is actually really critical and we leveraged it on Monday which I'll talk about soon so on Monday we talked about an one algorithm for solving these kinds of problems we called it backtracking search and it's it's a kind of an exhaustive you could think of as like a depth-first search of all the possibilities so we have this example from last week where we were coloring provinces in Australia right so we have I actually don't know the problem since of Australia but we have W a and we have V and we have T I think T is Tanzania Tasmania I'll work on my geography but we color them all red right and so what we decide to do was beside to say will color cue red let's do it and then we say hey if we call her cue red let's go with green here for n T and then we pick blue for sa and then green for NSW and we've completed that tree we've gone as far as we can we found a coloring that works we say oh wait what if we backtracked up to this point and try to sub in blue in for NT instead of green which is what we had before and then we can do the same thing we go down that branch of the tree or we could try different colors for Q so for example what if we try Q over here we take that tree down oh but that gives us something that is is not a size five ball so for example here and si no matter what color we give it according to our factors which say like you can't have two neighboring colors it would be not allowed and you fill up the tree this way and so this this is the algorithm we covered and it works because it always gives you a good version but it's not working because it's super slow it's exponential time so there's n nodes and each node has a and so your your it's like for each value of here I can draw it out so like for example if we had to fact if we had two variables x1 and x2 and they both took on three values so these are our vet our variables and these are the values they can take on and these are how much our factors like them then you would have to say for each value in x1 for each value in x2 so it's this exponential blow-up which is just very slow and so we learned some kind of like heuristic ways to speed things up and prune off that tree so we did forward checking which is where like once you decide my value for one variable I'm going to go ahead and propagate that decision as far as I can that shrunk it a little bit we looked at dynamic ordering which is like okay which variable and I'm going to choose to work next and then once I've chosen that variable I'm gonna choose my value a little more intelligently I'm gonna pick the thing that has the least wiggle room cuz maybe that'll help us uh one second and that helped us a little bit but at the end of the day these constraints they helped us prune the tree but they can only work for constraints only if a factor gives us a value of zero does it work because that's when it has veto power and we use the vetoes to say this branch of the tree will never be useful so we can never go down it so if we have factors that are going to be nonzero we can't use any of these things yeah we're on the example how you got the number so like 2 to the N minus 1 yeah so for this I guess this example what it would look like as a graph I guess it wasn't the best example but and actually later in lecture today we're gonna talk about exactly how you could go about this more smartly but let's say you just had two variables leech had unary factors if you were running binary like backtracking search we would still say we would try all different combinations of both variables which is a very dumb thing to do yes so that's the backtracking search interesting yeah so we'll discover a smarter ways to get around that yeah so that's backtracking slow but it gives you the optimal solution every time so maybe a mix back okay so I'm gonna lead into this is a running example that we're gonna be talking about the whole lecture object tracking so with object tracking you have sensors that are selling you like oh my object is here no it's down here oh wait it's over here and what you want to do is you want to take that noisy observation and and run it through your CSP to infer a more realistic estimate of where the object actually is so I will also draw this on the board so so what this looks like is if we have this is time so we'll call it t for time and this is position so we'll call it P for position and what this is going to say is we have sensors that are giving us estimates like noisy estimates for where this thing is at every point in time so for example maybe at time step 1 we get an observation down here at time step 2 it's up here and at time step 3 it's up here but in reality like we want maybe something more like that and so that's gonna be our goal with this running example so this is how we code it into a factor graph in in this case we have variables where the variable is our guests for the real location for that object we have two kinds of factors we have unary observation factors that say how close is our guests to the observation to what our sensor said and then we have transition factors which basically tell us you can't change your guests by too much from time step to time steps so on this graph what it would look like is if these are our observations this is like o 1 o 2 and O 3 then maybe our first guess would be here so we have X 1 down here we have X 2 down here and then maybe like X 3 up here and so our observation factors is going to look at this distance and that would what this is going to say is okay how far is our estimate from the observation and they want that to be close and then our transition factors are going to look are you going to look at this distance between one one guess in the next guess and say well our guesses shouldn't be moving too much yeah so the observation is what this sensor gives us and the estimate is is us saying thank you for the thank you sensor now I think the person is actually right here because the sensor is noisy we can't trust it yeah okay so that's how we're gonna set up this problem and I think so there's an and there's this really cool kind of like Java applet that you can all play around with on your own time and I will briefly walk you through it so what's going on here is basically so there's a lot of documentation that you can read but basically tapping here is we're just creating these variables we have three variables and we're allowing them to go in three positions so in position zero one or two and then this is a little function that's basically encoding the fact that if things are nearby then we want it then we like that so we have two variables a and B and if they're in the same position then we return to we really like that if they're only one away from each other we return one so it's okay we'll take it and if they're farther away then one from each other then it's we zero it out that's a hard constraint we don't like that and then we have this observed function which is kind of a higher-level thing and it kind of um I guess you could say it kind of like preloads our nearby function with with a variable we're going to create our factors and then this is what it looks like so we have we have three variables we have X 1 X 2 X 3 and then we have our observation factors are unary remember that say okay you have to be close to the sensor and then our transitional factors are binary and that says you can't move too much between time steps and you can run it and there's actually a lot of output here and I'm gonna ignore most of this for now I think the thing that is important here is that we ran backtracking search and we found the optimal assignment so in this case it's 4x 1 2 4 X 2 & 3 4 X 3 which gives a final weight of 8 so on our little drawing basically what that's saying is is it saying something like this is optimal where we we put a we say thank you sensor for these estimates but we think the person is actually here at time step 1 here at 2 and here at 3 that's the solution that backtracking gave us yeah - so you know those are the weights or the time steps as you're saying it's like 1 than 2 so X 1 1 is a time step 1 in the next 2 is the time step 2 but X 3 is also time step 2 yeah so the question was what do you mean by here at 3 and what I mean is basically so X I X I is an is our estimate for the position of this object and so at time step 3 our estimate for the position is at position 2 and yep so X 2 and X 3 so that's why there's X's at 2 for both time visas like time step 1 2 & 3 yeah yeah ok I understand like what you're talking about with the scope of the factors but how exactly is there a constraint being yeah so the constraint here is so the fact that if we look at our nearby function and if a and B are farther than 1 away from each other then it returns 0 and that's a constraint because when we're calculating the weight of a factor graph we multiplied together all the factors and so if there's a zero in there then the whole weight goes to 0 yeah yeah you could say that yeah great okay so that's our setup and we're gonna be returning to that a few times awesome okay so moving on on backtracking search very slow let's try to speed it up beam search faster yay beam search so backtracking search if you we have that tree analogy right and backtracking search exhaustively searches the entire tree gets us to that solution but it's very slow and so one way to avoid this kind of like exhaustive search is greedy search which is where you greedily it's like for each variable you greedily select the value that gives it the highest weight so it's right here you look at the values you can take on and you just choose whatever variable whatever value is best and you never look back you just keep on running through it and you go through the whole tree this way until you end up at a complete solution so the benefits is it's very fast right it's linear but the con is that it's a very narrow window like you don't see a lot of the state space you don't explore a lot and so you can often miss the global max so for people who prefer this kind of notation what we're doing is we basically say we loop through all the variables and we try out every value and we just take whatever value has the highest weight yeah so beam search is kind of like an in-between backtracking and greedy beam search is very cool so one way to think yeah yeah so the question was explained agree again and so with greedy what we're doing is as we say so we have we have a partial assignment right and we pick we want to extend our partial assignment so we pick a new variable and we try out every single value that that variable can take on and then we take the value that that that gives it the highest weight so all the factors touching that variable are the most happy with that value and we pick that and then we never look back and then we pick a new variable yeah so the so that's a good observation we can end up at inconsistent solutions and that's totally true so you can actually you can enduring a greedy search you can actually kind of like find your way in kind of a hole where it's like oh damn you know I can't go any further and it's a big problem with greedy search yeah so with backtracking beam searches is kind of like a heuristic way to maybe get around that so with beam search so remember again in in so for greedy search we had one partial assignment right and we were choosing one variable and choosing an extension of that one variable with beam search what you do is instead of one partial assignment you maintain a list of K partial assignments in this case K is 4 and then what you do is on every step is for each of your partial assignments you pick a new variable you try out all the ones and then what you do is you so you have york a partial assignments and you tried to extend every partial assignment you test out all the values for every partial assignment and then you sort those partial assignments based on their weight and then you just take the top k so it's like you have your partial assignments you extend them into all the possible successors you sort them based on weight and then you take the top k so in this case if we have four partials assignments then we try extending them all in the two directions they can go and then we sorted them and then we took the four you can see there's four things that are filled in that have the highest weight and we continue this procedure so we say so what we would do in this case is we would say okay for each of these four solid things we're going to try out we're going to extending each of those partial assignments and then we sort all the extensions and select the top K so yeah yeah exactly so the question was up to the part K right and the quite and answer is yes yeah so for example here for example one is less in case we sent it to two still less than K so we can extend completely so in in notation yeah so we say for each for each variable we we try to extend each of our partial assignments and then we prune out everything but the K largest like best K waits so beep search is also not guaranteed to find the best weight but what's cool about it is that it gives you kind of like a knob that you can control between being greedy and exploring a lot so if K is very wide then you explore more more of the tree and if K is actually infinity I think this is on a slide soon yeah so if K is infinity then that's actually like akin to a breadth-first search of the entire tree yeah on that graph or on the picture we had like the solid in the shade about once yeah for that shaded out great ones we would actually never explore those yeah so so this picture I don't think it's the best picture because so for example okay so I'm looking like I'm looking right here and in reality you would never extend this because it was never selected explore it and then find that it's not yeah so up here so at this point we do consider it because we extend down but then we decided not to select it yeah awesome run time so four okay so four beam search you're selecting a single node and then for for each partial assignment you're trying out every value in the domain so if B is the size of your domains and then you're trying out B things for every partial assignment and you have K partial assignments right so those are the number of extensions you have is KB and then to you sort them and to sort it take if you have a list of length n sorting it as n log N and so you sort your list of KB so that becomes KB log K B and then there's n nodes that you need to the height this trees n so if you do this n times so like I said beam search of the K gives you this really cool knob which mean do you want to explore everything or do you want to focus in on on you know being fast and greedy okay so everything until now what we've covered is is extending partial assignments so we have we're giving like a blank slate picture of Australia with no colors and we say color me Australia like build up this house from the foundations and now what we're going to talk about is okay given like a map of Australia that's already filled with covers colors how do we make changes to it in order to improve it and that's local search so the first algorithm is called iterated conditional modes ICM and what ICM is doing is it says okay we pick one variable and then we ask how what is a new setting that we could choose for this variable that would improve the overall weight so in this case we have one variable it's x2 and we try out all the different values that can take on which is 0 1 or 2 and then for each of those values we go through and recompute the weight and then we pick whatever value is best so we started with 0 0 1 and here it looks like one is a better choice so we go with it and and from this now on we would say X 2 is equal to 1 so something cool about ICM is that when you're evaluating a new value for a variable you only need to consider factors that touch that variable that's all you really need to recompute because everything else is constant with respect to it and so that gives you big big big time savings in practice one last thing is so the name iterated conditional made so iterated comes to the fact that you could solve the whole CSP this way if you just iterate over which variable you're selecting conditional means that once you select a variable you're clamping down the values of everything else and then modes is saying once you select your variable you you try out every single value for that variable yeah yeah so the question is if you kept so if you have your three variables and you keep on going through them and and choosing one clamping the others and then choosing the best value for it would you wrap it in a flow solution and the answer is no so and Wilson will see that in practice yeah yeah so again this that's this is just to give you in the ocean that we iterate through these variables and for each variable we pick a weight that improves it so we have this in the demo so in this case what's going on here is we're saying okay right now we've selected we're looking at x1 and we start with a random initialization we start we're looking at x1 and and these the different values that x1 could take on and then we go through and calculate the weight and we say ok 1 is the best weight for x1 right here so we choose one to be x1 and we step again we're looking at x2 now and oh it looks like actually a value of 1 is better for x2 and so from now on we choose the value of 1 for X 2 and we iterate again now we're looking at x3 and it looks like we choose the value of one and you just keep on iterating for this until you hit some kind of local optimum and I'm saying this is important that it's a local optimum because right now it's converged so I can keep on pressing step and it's not going to change but the weight is four and if you remember during the other thing when we were in backtracking we actually found an assignment with a weight of eight so it does it can fall into these local optimum one way around this is a second algorithm called Gibbs sampling with Gibbs sampling what we do is we inject some randomness into the process to try to like bump us out bump us out of those local optimum into something that can maybe get us into a better area so basically Gibbs sampling is super similar to ICM the only difference is that instead of so you you try out all the values and instead of selecting the value that gives you the biggest weight you sample the value according with probability that's proportional to its weight so for this example we say setting of zero would give us one value of one would give us weight of two valve two would give us a weight of 2 and then to get the probabilities we take the weight and we sum but we divide by the total which would in this case would be five we saw them all up and divide by that that gives us a probably point two so here we say it's 2 divided by 5 which is 1 plus 2 plus 2 and then we use that probability distribution to sample a new value for X 2 instead of just choosing it's like oh you're the best so this is the demo so in this case we're looking at X 1 and we're trying out different probabilities for it and we have weights and then that gives us a probability distribution and we sample from this probability nutrient so in this case it looks like we sampled and we chose 0 for X 1 and then we keep a record of how many times we've ended up with a certain assignment so if we step again now we're looking at x2 and it looks like x1 so a value of 1 is the only thing that works so we choose that value and we add it to the counts and you can just keep on running this process and over time you build up this kind of like probability distribution so this is actually an unlikely simple so I had a point it had 20 percent chance of choosing 1 and 80 percent chance of choosing 2 but it still chose 1 and that's a way that it can kind of like break out of these local Optimum's but in any case if you look at this table then what you find is that over time if you run this thousands millions of times then in practice settings with very high weights will occur very often and actually when we get into problems like graphical models which person we'll talk about soon you could actually say that the global optimum will be the most frequent and good sampling in the limit which is I think pretty cool yeah okay so just to just to show an example of what can go wrong with this algorithm it's still flawed so if we have X 1 and we have X 2 and let's say let's say it's two people and they're trying to decide where to go to dinner so we could have let's say they're deciding between vegetarian and going to a steak house so we have V for vegetarian and s for steak house and they really both want to go to vegetarian and they want to eat together and they'll eat steak but they're not super crazy about it so if you're in this state then even if you're doing Gibbs sampling it's really hard to bounce over here because you have these two kind of like transitionary settings in between and in order to make it to both vegetarian they're going to have one is going to to make the decision to go to a different restaurant and so on since this is so low priority it's it's going to be very difficult for these two people to go over to vegetarian they're both kind of like oh I want to do what you want to do you want to do steak house right I want to eat together you want to eat together let's do steak house then and they they both don't really know that they would be much happier overall if they both hate vegetarian and then this this would really be even worse if if you had so for example if you had like zero is here then there's actually no way for them to get there because there's no probability there yeah whatever you because you're initialization is to be very important because if you initialize to the state where they're both vegetarian then you'd never want to leave and you'd be happy to relieve it but if you neutralize to blitz steak then you're in trouble so in general in optimization in any kind of optimization area where where you can have this problem of falling into some kind of local optimum that's worse than the global optimum it's very dependent on your initialization yeah because if you're just over here then you'd fall somewhere lower just by chance yeah yes where you've initialized to a state where it was zero would you have an equal chance so the question was what if you initialized into a state that had zero probability so what you would do is you would say is you would select a variable you would select s1 and then you would try out different values for this you would say okay I'm going to either choose s or V holding x2 constant and in that case transitioning to V would have a probability of 1 so you would do that almost deterministically so for Gibbs sampling these are probabilities you turn them into probabilities yeah yeah so just to clarify get sampling is not guaranteed to find the best assignment it is not guaranteed yeah but we mentioned something like in the limit it is yes but that's so that's not like a that's kind of a theoretical point it's not really practical yeah it's so in the limit doesn't mean like a guarantee I guess yeah I guess you could interpret it that way yeah so I guess this is directly about your question so if you were to compare these questions which ones are guaranteed to give you the maximum weight assignment and the only one is backtracking search because greedy is too narrow beam search is maybe too narrow ICM is is too myopic and Gibbs sampling is you it's not a guarantee it's likely but it's not guarantee yeah SR can you say or this example even in the limit yeah so the question was in this example even in the limit you wouldn't converge and that's true so if you if you initialized right here there's no way for you to get over there yeah so you yeah I think it's just in certain conditions yeah so that idea I wasn't my intent wasn't to confuse you guys so if that's a it was a theoretical point for like some subsets some csps yeah yeah like you said if we suppose initialized at stake state it there's no point that we can go to like vegetarians and stay in right because in the middle transition probabilities are 0 so is it like safe to say that when you are modeling a problem with give sampling you should never like even if you encounter property 0 just give them some epsilon probability so that there might exist some chance to yeah so the question was is it worthwhile to add some like plus point zero zero zero one to these just to give it some probability and I my intuition says that's it sounds like a pretty good idea it sounds kind of like adding tiny little Epsilon to avoid division by zero but I think it would depend on the problem that you're solving well you can imagine some cases where you would really want those zeros okay so just to summarize so far we've learned two ways of extending partial assignments in these graphs so one is backtracking search which we learned last time which is like a full search of that tree gives you the exact solution every time super slow beam search is approximate and it gives you this cool little knob to trade-off speed and I guess you could say like success or exactness and then second we learn ways to give in a assignment to improve it to modify it one was ICM which was approximate and then the other is get something which is also approximate but it uses some probability some randomization and now we're gonna look at is we're gonna look at two ways to solve these kinds of problems by actually changing the structure of the graph itself so our motivation comes from Australia Tasmania that is that is the motivation so Tasmania if you remember was completely disconnected right so I think we colored it red in the previous example but it doesn't really matter what color we gave to it right we give it anything we wanted and so what we want is we want to kind of leverage this property and more than leverage it we want to we want to kind of like inject this property into graphs that don't exhibit it we want to like build this probability and this property out of graphs so first we want like what is this it's called independence and it can speed things up so just like I said before so is it still there my old oh it's partially erased so if we write this down again so this is the same examples before we have two variables each with a unary factor on it with backtracking search I told you what happened right you do something really dumb which is you do it's it's like two for loops right you try out every combination of them which is exponential where's what you could really do is just say okay for each thing I'm gonna choose the value that maximizes that makes my factor most happy and that's linear time and so that would give us a more efficient algorithm no no okay and we call that property independence so a so in this case X 1 and X 2 are independent and the reason that they're independent is their there's no factor connecting them there's no paths between them and there's no edges between them and so we call that independence and that's the same thing with Tasmania in the rest of Australia so I don't know how independent it is like culturally politically but it certainly is in terms of like math coloring graph theory miss and in simplest so we use this we use this kind of cool-looking pipe thing and that denotes independence so yeah so like we said tasmania's independent what about cases like this so it's not quite independent but it's almost independent you know only if X 1 didn't exist then the rest none would be independent and so that this is where we introduced this idea of conditioning and conditioning is a way to rip nodes out of a graph so and we do that by by saying okay in in this example let me draw it up so there's X 1 and X 2 and then it's 1 7 3 2 1 7 3 2 so 1 7 3 2 red red red blue blue 2 three seven one okay so we're saying X 1 and X 2 are connected to each other with a factor right but what if I say X 2 is definitely blue yeah yeah so the question was can constraint satisfaction problems be written as graphs and as all of them yes yeah because fat because variables are nodes and factors are edges yes so it's actually a really elegant way to think about it because then you can bring in all this graph theory stuff can you also see the board okay I'll try to write big but okay so if I say X 2 is definitely blue like trust me it's blue then what's that let me do it lets me cross out all the rows where X 2 is not blue so here X 2 is red so I can be like you are never gonna happen I know you are not the case you are not true and for you same thing you are not happen and now once I fixed X 2 I don't all of X 2's values are the same so this doesn't really add information to this table and I can just drop that data from the table and what that gives me is it gives me a reduced table which is just a unity factor of X 1 and it can take on red or blue and then I have a value of 7 if I choose red and 2 if I choose blue and graphically what that looks like is I'm deleting X 2 and I just have one factor now X 1 and it has a unary factor now the price to pay for that was I had to assume that X 2 was blue I had to conditioned on X 2 being blue so notationally or maybe like programmatically well I think a good way to think about it is it's kind of like you're taking this the factor that touches x2 and you're kind of like pre loading it with a value for one of the arguments and then the rest of is untouched so you choose a value for a variable you remove the value and the variable from the graph and then you you stick in that value you preload the associated factors with that value so for exhibit an example if we were to condition on these then I'm saying si is green and Q is blue eyes red trust me and then what that does is it rips those out of the graph and everything touching everything touching those conditioned variables turns into some kind of a stump and you preload the value for those variables in the only edge that gets removed here is the one that connects Si and Q and that's because you have a function that takes two arguments but you've pre-loaded both of those arguments already so the it's done there's nothing to do there might as well not exist I'm just an example of the new factors that go in so NT for example that would this factor which used to say NT and Q can't have the same color it it turns into this little thing which just says NT can't be red and you know that because you are assuming that Q you're conditioning on the fact that Q is red yeah yeah so the question was is independence no edge is independence mean is there no edge between them or is there no path between them and we will talk about that so for independence you would need both true but then we're gonna cover an another form of independence called conditional independence which is just a ladder yeah sending a partial assignment because I actually like starting with a partial assignment on the graph and they kind of exploring or optimizing the rest of the notes based on a partial assignment yeah so the question was how is this different from extending partial diamonds and it's not so that's actually a good point I think so conditioning might actually be a case of building up partial assignments this so what I think the main thing to think about is now we've kind of like moved on so partials I'm extending partial assignments and modifying existing partial assignments that was kind of like our old world which is where like this graph structure is very perfect and we can't touch it or change it now we're in a new world where we're allowed to change the structure of the graph and so we've kind of like left that way of thinking yeah yeah so like setting interior signature in what drives the decision process of choosing these values we will get into that yeah yeah yeah so the question was why did we choose to condition these so in this example it's arbitrary but soon we're gonna cover ways of choosing them what how is this different from for checking so in Ford checking what we were doing is is we were propagating the decision forward to reduce the domain of existing factors whereas in this one we are we're changing the structure of the graph itself and we're we're literally removing variables and inserting new factors okay so to move on graphically in general what this looks like is if you have a variable that you want to condition you rip it out of the graph and then everything that touches it turns into some kind of a stub with the value pre-loaded in there this is just a picture of the same idea yeah so I guess you could say this is the mean this is the big difference is that whereas in for propagation we were keeping the existing graph and we're just propagating our decisions to reduce domains whereas now we're we're literally removing this variable and the factors with it and then we're adding new factors that are pre-loaded okay so we are going to now go to what you were asking about which is conditional dependence so if you have three if you have three variables a V and C so that's like we have a C and B then you would say that a and B are conditionally independent because once you condition on C once you pick a value for C remember these turn into stubs or stumps or whatever you want to call them geez and so these are now independent they used to not be independent because there's there's a they could access each other through C but now they're independent there is no edge connecting them and there's no way to reach each other and so this this is just a way of formalizing that this is how we write it so we conditioned on C and then a and B become independent yeah so equivalent you say every path from A to B goes through C and then that means if you remove C then there's no path connecting them so in this example you have s a and Q if you condition on them then you rip them out of the graph and then it looks like western and eastern Australia are now independent of each other because the they've turned into islands and that's writing it mathematically so there's another notion of the Markov blanket which is basically saying okay I went to make I've chosen some subset of the graph in this case it's V and I want to make it independent what is what what are the variables I have to destroy in order to make my variable in island so in this case it would be essay in NSW if you condition on these then V becomes an island and it's independent if you wanted to make this subset independent then you and condition on Q and si and this set of nodes and that you have to condition on is called the Markov blanket of the set of nodes that you want to be independent so it's like if you have some part of the graph a and you want to make it independent of A Part B then C is kind of like the Markov blanket of a if when you delete it it becomes independent of C this thing this is kind of like a set notation thing it's a set difference and this is just a way of writing it more mathematically so we can use these ideas to to like create independent structures now of our data so we had this example before where it was almost independent but not quite now what we can do is we can condition on it so we condition this to be read and then we can find the maximum weight assignment of the rest of it which we showed was easy before right with with this example it's just linear like what's the best for you what's the best for you so once we condition on this thing that's making it non independent then it beat the poem becomes very easy so what we do to solve this is we just condition repeatedly we say okay I have picked my node now for red green and blue solve it condition and solve condition and solve condition and solve and this becomes very quick and then you can read off the maximum weight very easy you just say oh it's green because that was the maximum weight I found once after conditioning yeah so the product so when it way the question was want to talk about weights what does that mean and this is it's kind of loosey-goosey it's not very formally defined but in this example I would say it's the weight of the whole graph so your condition on this and then you find the best values for all of these variables and then you take the products of all of those unary factors and that gives you the weight this is just an example yeah the numbers are arbitrary yeah yeah our GM D doesn't act the same as doing like all possible combinations anyways like computation right yeah it is so so it is so you cover every solution that backtracking would give you this would give you - this is much faster because what you're doing is you're taking a very complicated problem and you're breaking it into easily solvable pieces you're taking an exponential problem and you're breaking it into a linear number of linearly solvable pieces and in practice it's much faster yes yeah so the question was instead of doing instead of exhaust Li searching all variables you're exhausted searching just a couple variables and yeah that's an interesting way of thinking about it I think in both in both ways if you think about it you're it's like even if you're conditioning for every single variable you're eventually going to consider all of the values it can take on but you're you're you're reordering things in a way that's much smarter and that lets you take care of like take advantage of this independent structure in order to do it better yeah any more questions on this yeah so it so it is faster than backtracking and you could see it here so for example if we did backtracking here it would be what there's three colors and there's seven nodes so it would be this huge exponential blow-up whereas if we condition here then it becomes much smaller okay so just to summarize independence is when we have a and B and they can't there's no way to get from A to B conditioning is when we we take a vow we take a variable plug in its value rip it out of the graph and then like preload all the factors that touched it yeah okay well I'll finish this first a tional dependence is when you have two blocks in your graph and if you condition on one part of your graph then if you rip that out then these two become separated and then a Markov blanket is saying okay I want to make my variable an island what nodes do I have to destroy it to make it an island yeah whoa is your question I guess I'm like unsure why it's computationally cheaper because you try a few condition on X one for each of the colors you see I have to like like you're basically you sought to come up with three sets of unary factors for each of the below variables you have to write over each of their domains yeah it also seems exponential so so in this case so doing backtracking so if we did backtracking so if we did backtracking that would be what 7 to the third right is that true 3 to 7 yeah and then if we did conditioning so there'll be 3 for the first condition right and then every time we condition we do 7 times 3 right 6 times 6 times 3 yeah you're right 6 times 3 so this is is 6 squared right what thanks to six times it depends I don't I don't I don't know exactly what these factors are saying this is kind of an arbitrary example but I think I think the point the point here is that this is smaller right so this is this is three to the fourth or whatever so it's faster yeah yeah down here yeah yeah so this was saying this is saying that there's three colors for x1 and then every time I choose a color for x1 I'm saying that there's three options for each of the six other things so once I once I've once I've set x1 it turns into this situation where now I'm saying what's the best value for x1 okay so I look at three things and choose the best now what's the best value for x2 and I look at three things and choose the best and so there's now there's six of these and each one for each item I have to consider three different possibilities yeah I don't it you know it would probably be different depending on what these what the actual factors are and what they're calculating but it's just an arbitrary example okay so now we're gonna do elimination elimination is very cool I think so commissioning is saying I'm gonna rip I'm gonna rip my variable out of the graph and then I'm going to plug in whatever value I conditioned on into all the neighboring factors well elimination says is is okay I'm gonna rip my variable on the graph but instead of instead of plugging in a single value across every single factor I'm going to plug in a different value per factor in order to individually optimize each decision so I think I think it's best showing through example so again this is the thing we have two variables and they're connected by a factor and they have these weights if we condition then we I think this is this example right so if we condition we get what we got before and then if we eliminate here I'll do the elimination now so I'm going to redraw this table so we have X 1 and X 2 we've red red red blue blue red and blue blue they have weights of 1 7 3 & 2 okay so what elimination is going to do is is we we condition it like internally optimizes the value that we don't want to condition on based on the other arguments in that factor so we would say okay we're trying we're trying to eliminate X 2 right now and we do that by looking at each value X 1 could take on and then for each value we dynamically choose the best value for X 2 so first we look at red so X 1 is red what's the best value for X 2 in this case it would be blue so we cross out this row we say if X 1 is blue then X 2 is going to be red because that gives us the biggest and then it just like before now for for any value of x 1 the value of x 2 is already set it's fixed it's decided and so that means that we can drop this variable out of the graph because we've already like internally optimized into this function what its value would be and so this gives us again a new table where we have X 1 we have the values it can take on red and blue and then we have the weights associated with that which is now 7 and 3 so in math what that looks like is is where we used to have this binary factor we've now ripped x2 out of the graph and we we have this new factor there's a unary factor now which internally optimizes over x2 it says give me my x1 give me an x1 and then as soon as you get my x1 I'm going to spin through all my values of x2 and give you the best one that would the best match for it so what this would look like in pictorially I guess is every time you remove of a factor you take all the factors that touch it and you wrote them all together and you merge them and into one big factor and then internally what the factor is doing is its optimizing over whatever variable you just removed so for example if we have so for example we have this kind of coloring problem let's say we want x1 to be red and x4 to be red what's the best value of x3 that we could give so over here we say x1 is red x3 is red that gives us Volvo four and then three to four red red gives us value 1 so we have a 1 in here the other value that x3 could take on would be blue so now we go from red to blue which gives us a value of 1 and then blue to red which gives us a weight of 2 and so we multiply those together we 1 times 2 which is 2 and then internally what this factor is going to do is it's going to maximize over those and choose the value for the deleted variable that maximizes that internally optimizes it for this local problem which in this case is red so just another example if we did red blue so x1 is red x4 is blue there's two options for x3 first it would be red so if we go from red to red that's four and then from red to blue is to say about four times two here and then the next thing would be if x3 is blue so we go from red to blue which is one and then blue to blue which is 1 and that's 1 times 1 and then we would say oh well in this case the best value of x3 would be red so it's kind of like internally optimizing in general what this looks like is you rip the node out of the graph you take all the factors that used to touch it and you tie them all together into one big factor and again that's kind of the mathematical notation there's another way of interpreting this that might be helpful to some people basically it's like you it's like I pick my value and then I'm going to look at all the all the variables in its Markov blanket and I'm going to repeatedly condition for every on every assignment that blanket can take on and then for each of those conditionings what's the best value of my selected variable that's kind of what's going on behind the scenes yeah it's to like faster execution times well just rip out all there was except for one and then it becomes a unary factor in a unary yeah so the question was if if it's faster to rip out variables why don't we just do it for the whole graph and we totally will yeah you'll see it's in a couple slides that's the algorithm so like we have this strategy for ripping out variables and then the next up is to use it to solve the CSB which is what we're about to do yeah okay so there's this question which i think is cool so if we have some kind of a star shaped graph so if we have a setup let's say we have a setup like this which which is a bunch of factors going into our hub and we have we have this variable we'll call it s do we want to run elimination or conditioning on s I hear some whispers but oh yeah yeah conditioning and the reason for that is that if we if we condition then all these turn into unary factors and if we eliminate then it turns into one giant factor which which is harder to solve okay so like you were saying this is this is the algorithm that you get out of it so basically what you do is you you loop through all your variables and you eliminate them all in turn and then at the end of the day you're going to have one variable to rule them all and that variable will just hold the best answer yeah to a case where if we don't have a yeah yeah so the question was wait a second doesn't ordering still matter like can't we still end up somewhere that's not good and that's totally true so towards the end of this lecture is a discussion on exactly that topic variable ordering does matter and it's actually hard it's an np-complete problem is to decide the best variable ordering but so I think I'm gonna do one more example of I'm gonna run elimination on a whole graph which I think will be helpful so our graph so we have three variables we have X we have Y and we have Z and they're all connected through a single factor I'm going to write out the whole table for this so we have X we have Y and we have Z and let's say they have two values they could be a or b so we have a a a a a B I'm gonna run out of space a B be a B a B a a b a b b b a and b b b is everything one two three four five six seven eight yeah okay so hopefully this is readable but and they help weights right so we can just arbitrarily say there one two three four five six seven eight can people make this out seems like maybe so I will go with it okay so what do we do let's say we want to choose Z first we want to eliminate Z so will we do an elimination is we say for all the variables in Z's Markov blanket we're going to repeatedly condition on the values that they can take on and then we're going to dynamically choose the value of Z that would best match that so x and y could either be a and a so this is a a and in this case the best value of Z would be B right so we cross this out next x and y could be a B in which case the best value Z is a let's we cross this out it could be be a which would be B or it could be be B and the best value of Z in this case would again be B so again like we've seen a couple of times before now for any value of x and y the value of Z is already decided it's set it's pre computed so we can just drop this variable from our table which is equivalent to dropping it from our graph ok so now we're gonna work on Y again we do the same thing we say for each for each value in Y is Markov blanket which is X we repeatedly condition on all the possible values it could take on and then dynamically choose the best of that value based on that so if X is a so now we're comparing so now we're comparing these two so if X is a what's the best value of y its B so we can cross this out now if X is B what's the best value of y it's again B so we can cross this out and now again for every value of x we've decided B and so we can drop this from the graph and now what we have is now we have one variable in our graph with a unary factor so we have X and X can be a or it can be B and if X is a and then it has a value of 4 and if X is B then it has a value of 8 and now what we can do is we can just say okay we're going to choose X to be 8 and then in kind of implementation all you need to do is is kind of like look back you could store some kind of back pointers or store your own tables and you can recover your solution from the whole graph from this endpoint so that is that is variable elimination yeah why do we go in the opposite order from engine one instead of lunch at them oh that's so that that's basically that was what I that was um that kind of is a mathematical way of saying what I just said about the back pointers so it's like now we've gone forward and we've eliminated and now we can go backward to say okay X is B now that we've decided X to be be like we look at our old table and be like what was the best value of y that gave us that decision so it's kind of like once you go forward to eliminate you get your solution and then you go back and like read off the values as you go yeah okay so in terms of runtime basically what you could say is so for each variable it's going to have domain to the so every variable is going to have domain to the era T the factory touching so the factors touching like the factor you create from eliminating a variable is going to have this many rows in it and then there's n variables that you have to eliminate and with that why the reason why it's max arity is because if you have let's say you had some kind of graph that looks like this the this factor would be d squared the Des Moines squared and then this factor would be d3 and so like eyes are going as you're eliminating going through this graph like this is just gonna dominate d2 so its eyes you're going through one alumina graph the arity of the biggest factor you've created is going to bound you four minutes and that's where variable reading gets in so what was it yeah yeah so the first one I'm not actually sure I thought about I looked I looked around and I I wasn't able to explain it so yeah okay so but as you're going order matters because if you looked at this there's two ways to do it right so if you went from the leaves and eliminated each leaf variable first then all the factors you create would have era t1 it'd be super easy but if you eliminated x1 first then you get this huge giant factor that gives you this big table of this era t6 would you be ever so so the order really matters in general it makes sense to eliminate variables with the fewest neighbors first it's a pretty like sensible heuristic that in practice works well and we define this term called true width which is saying if you had a factor graph and you're going through doing elimination the back so emeriti of a factor that you create along the way using the best ordering is the truth of that glass so in general a truth is is very very hard to compute but for a few cases you can kind of Reason about it so for example if you had a chain let's say we had a chain and if I was going through eliminating this the biggest factor out create is just a unary just that little guy right so true worth of one if I had a tree so it's just like our example before so if you go from the leaves again you can create these little factors these little unary factors and so you can get away with one if you had a cycle I'll make my psyche a little bigger but if you had a cycle the max emeriti is gonna be too because even if even if you kind of work away from the sides you're eventually going to end up as something that just connects the last things before you delete them so it's like you have your cycle and you chew way at both ends and you're gonna end up with something with just those two nodes relating remaining and then if you had a n by n grid if you have an N by n grid so let's say this just goes on it's the smaller of the two and the reason for that is because if you let's say you go through and you're eliminating these things to like that and like that you're gonna end up with something some situation like this and once you delete this you're gonna get a factor that ropes them all together yeah so that's true with super important hard to compute yeah oh yeah yeah oh yeah you're right so it's not the first - it's the it's not the last - it's the first - so when you're let's say this is my cycle the first factor I create after I delete that is gonna be a binary factor yeah so thank you for that yeah so just to summarize we learned different ways of solving csps add a beam search which is kind of like a souped up version of backtracking which doesn't always give you the answer but it's faster local search you get entire things and then you improve them a little bit at a time conditioning break up the graph into smaller pieces that are easier to solve unknown elimination which is where it's kind of like in conditioning except instead of choosing one value for all your factors you kind of like dynamically choose the value based on the Markov blanket and that's CSPs so we will see you on Wednesday 