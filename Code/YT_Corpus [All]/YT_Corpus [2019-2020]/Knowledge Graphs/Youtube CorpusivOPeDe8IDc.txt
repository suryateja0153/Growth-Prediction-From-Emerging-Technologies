 ignition sequence star six five four three two one zero all engine running liftoff we have a liftoff [Applause] [Music] we turned out of the banking bombshell causing shock waves around the world um good morning good afternoon good evening around the world and welcome to nodes 2020. my name is emily from and i'm one of the co-founders and the ceo of neo4j and i'm going to be your host for the next hour or so as we kick off nodes 2020. i think we have an absolutely amazing 10 hours ahead of us so let's get started i'm really excited that notes is happening again it's been almost exactly a year since we last had our first huge online virtual conference node 2019 and of course it is bigger and better already than last year we have over 10 000 people registered to attend and watch over 70 talks by close to 90 speakers and it's all moderated by over 30 community partners from a wide variety of backgrounds cultures all over the world a wide discrete range of technologies who help us spread both the love of graphs and the word of graphs and attract a larger and much more diverse audience so thanks everyone for chiming in and making this an amazing event in addition to that our sponsors made this event possible so please make sure you drop by their virtual booths for those of you who used big marker before you probably know that there is a a specific feature where you can attend all the each and every one of the sponsors different booths it's like a separate stream just for that sponsor so please drop by and say hi learn about their technology and you can win some some cool prizes there's also a lounge area which is basically a big chat room for all of us attending the conference please participate there try to connect virtually it's obviously nowhere near as good as being able to do it in in real life um but at neo4j and in the graph world we're all about relationships to try to strike up those virtual relationships uh in that lounge area over the next 10 hours i'm gonna be there as much uh as possible so please stop by and and say hi cool so before we get started um a few words about neo4j uh the company most of this will talk about neo4j the product but i'll take a minute or two and just introduce uh the company so we're neo4j the company behind neo4j the product which of course is the leading graph database platform uh in the world uh today there are over 400 global enterprise customers which business rely on neo4j every day and every single month there are over 10 no over 100 000 developers who use neo4j to build applications or explore and make sense of data neo4j and graph databases are today used absolutely horizontally across a wide variety of verticals and industries and use cases and as you can see here on the slide it's used by some of the biggest brands in the world there's a wide range of use cases you have a few of them here on this slide at least 60 or so percent of the the commercial use cases that we see of neo4j fall within the top six use cases but there's a long tail of use cases outside of these top six because graph technology is a is a fundamentally horizontal uh phenomenon you can use neo4j neo4j in a wide shape of form factors you can use it on-prem just go to neo4j.com download and download neo4j community and install it locally in your data center you know on a cloud service you can go to any of the cloud marketplaces and consume it in in that form factor or of course you can use a database as a service specifically in neo4j aura there are a wide variety of ways that you can get access to to neo4j so 2020 has of course been a tough year for all of us i hope everyone here on this on this uh conference is doing well and that you have taken care of yourself and your loved ones um here at neo4j we are fortunate to have a sustainable long-term focus so we've used this opportunity to double down on product that's been the key area where we've spent most of our calories over the past 12 months so i'm excited to tell you about a few of those things that have been happening since since nodes 2019. first and foremost let's talk about the database we've launched neo4j 4.0 in 4.1 and overall the 4.x series is just mind-blowingly good there's so much stuff in there to talk about at nodes 2019 i think there was a 15 or maybe even 20-minute segment just touching on a few of the the features in just one of those releases 4.0 so it's very hard to pick out uh you know you know select areas to talk about but let me highlight three of them um the first one is scalability 4.x includes the multi-database feature which has been one of the top three requested features of all time so very excited about that it also includes sharding sharding through neo4j fabric which of course gives this horizontal scale out both read and write load architecture that so many has talked about in in the graph world so very excited about that the second big area is around security uh where we introduced fine grained role-based access control which means that depending on your role you can access certain properties or not access certain properties it's down on that level of granularity property level both for reads and for rights and then finally like the third area that that i want to highlight is around developer productivity and specifically the reactive driver architecture that was introduced in the 4.x series broadly the neo4j4 series has been adopted already so far in 2020 by our largest customers and it's enabling new levels of robustness and scalability for applications built on neo4j so that's the first one the second one uh you know second but not least in any way we launched aura at notes 2019 that was before we had even unveiled the name it was pre-announced to the attendees of notes 2019 and of course it's our self-serve fully automated always on and always updated graph database as a service it is a mind-blowing piece of technology that we're going to spend more time to talk about later in this in this session and then the third one that i want to call out is that we launched gdsl which is the graph data science library which really rounds out our platforms for support for graph data science that platform as you can see here on this slide has three components to it the center piece is the neo4j database the native architecture for for storing connected data storing and querying connected data using using cipher and then to the left in the slide you see the gdsl the graph data science library with a rich set of graph algorithms and then to the right you see bloom which is used to visually explore your data and make sense of the graph in an optical uh visual way those three components form our platform for graph data science and that's also an area that we're going to spend more time on later on in this call stepping outside of product we've doubled down on developers in 2020 and there are several examples of this i'm going to call out a few the first one is that we've redesigned the neo4j developer training documentation and knowledge base from scratch which i hope will provide a much more seamless way to navigate on on our web properties we've improved improved the first use experience of both a neo4j desktop and neo4j browser by including a first use graph experience where it comes already provisioned and pre-installed with a with a small prototype graph to make it easier for you to get started in the same spirit we've improved onboarding and getting started experience in aura which makes it a lot easier to get started you can also easily export data from desktop a local installation of neo4j and import it into into aura and then last but not least we made bloom available completely for free for everyone locally you know using neo4j desktop on aura and on sandbox which you can access from neo4j.com it launches a small little neo4j instance uh completely in in the cloud from neo4j.com and in all of those form factors you can use uh bloom uh for free um many of you may know that we have this initiative that we're very proud of and that we're spending a lot of time on called graphs for good and that's the umbrella under which for example the panama papers happened where we we try to make neo4j available for high impact activities and projects in the world completely uh for free and the panama papers is a good example of that there's today over 20 independent projects using neo4j to find the cure for cancer it's being used in diabetes research and in a wide range of projects to further uh for for the good of of mankind in the beginning of the pandemic right so in the earlier part of the year there are a lot of countries and organizations and individuals that reached out to us and they started to introduce covid related projects based on neo4j and there were things like scientific analysis like covid graph ranging all the way to contact tracing for some countries to finding misinformation like project domino and we've tried to support that as as much as possible under the grass for good initiative and i'm super proud to see how the entire neo4j community has risen to the challenge of this global pandemic and really come together and pulled up in all kinds of creative ways to try to use connected data to try to uh do our part in in figuring out how to solve this uh the cova 19 pandemic in the similar spirit of graphs for good just last month the icij which is the organization behind the panama papers they published another investigation and this time it's of suspicious financial transactions they're called sar filings or sar filings which total over two trillion dollars u.s dollars right that's pretty significant uh chunks of uh of chains that change that that these journalists have analyzed and verified and with a lot of data analysis tools like in neo4j and link curious and they basically built out an aml solution an anti-money laundering solution to track how money is being trafficked from offshore accounts in one location all the way through the global banking system it seems to me looking at the new cycle that it's this investigation is already overshadowed by all the other news that is going on in in 2020 but we actually believe that this on some level is even more important than the panama papers and if you find this interesting you can already explore the published data in neo4j there's an online sandbox for that so if you go to nfj.com you click on sandbox you can choose the fincen files data set you get access to it you can start playing around and see if you can uncover any patterns that haven't yet been written about in in the press so that concludes our year in review it's been of course a tough year on many levels but also a very exciting year in the neo4j and broader graph community and now we're on to nodes 2020. we're going to start by looking forward to the future in particular since it's the year 2020 it's the beginning of a decade i thought that it was appropriate to see that what what do we believe is in store for the next 10 years in graphs i believe that there are four major pillars that are going to shape how the graph database category will evolve in this decade the first pillar is the shift to the cloud the second one is putting developers at the center of everything that we do the third pillar is the rise of the data scientist in general and specifically graph data science and then finally i believe this is the decade where when the property graph model will realize its full potential so let's have a look at each one of these pillars in order and first of all the shift to the cloud so this should hardly come as a surprise to anyone listening in you know our industry is obviously part of this massive secular shift towards cloud delivery and consumption of software um a little bit more subtle is probably that this is actually happening at a different pace for each layer in the stack it first happened to the infrastructure layer specifically for vms if you recall the first ever cloud service was ec2 elastic cloud compute from from amazon and that is really the vm layer of the stack and then it also happened concurrently in the application layer of the stack so think your sales force or your gmail or your hotmail right and actually the data layer of the stack it took a while for it to happen there it was held back for a variety of different reasons primarily data gravity and then compliance and regulatory reasons but nonetheless it is very clearly happening right now and it really hit the elbow the the the shift of the cloud of the database layer into the into the public cloud it really hit an elbow in the curve in 2017 and in 18 and it's only accelerating uh from here so the question then is why is this happening i think there are two drivers why people want to consume databases through the public cloud the first one is simplicity basically you can focus on building amazing applications not on managing infrastructure right the second one is elasticity you can write your application and then your backend can grow and shrink automatic automatically and you pay only for what you need if there's burst one day you it grows to to that size handles that load and then it shrinks back down and you paid only for that peak load during that one day or a couple of days or whatever it was so i think those are the two big drivers so let's review now what's happening in the world of cloud database services specifically for the neo4j community first and foremost we have neo4j aura professional we mentioned this already you know it was launched at this conference notes 2019 exactly a year ago it's a fully managed fully automated completely self-serve always up-to-date always-on graph database service it's been up and running now successfully for an entire year hundreds of customers and it's actually handling over two billion queries per month which is a pretty staggering number in fact just a few weeks ago there's actually a particular week where there was over 1.2 billion queries in that week and if you just spread that out evenly that's actually over 2 000 requests per second but of course that's not actually how it happens it will actually follow most likely a poisson distribution for those of you who remember your statistics classes from from from college so it's actually even more a sustained rate of more than two thousand queries uh per second neo4j aura is a rock solid product and it's super easy to get started nine cents an hour it will scale up to support your development and your production needs so for those of you who are curious i highly encourage you to check it out today there's a link at the bottom of the slide i think it will be concurrently posted into the lounge area chat room so you don't have to sit here and try to transcribe it off of big marker visually so you can just go click there and sign up today the second of all we launched neo4j aura professional on gcp on the google cloud platform and we're really excited about our deep and strategic partnership with google which as some of you will recall was announced in early 2019 at the google next conference and oron gcp is already available in eight regions around the world it's integrated with the gcp marketplace so you can pay with your gcp credits or your pre-committed spend and the pay-as-you-go pricing is predictable if you find this interesting there's an equivalent link at the bottom of the slide so please sign up to that hopefully it's going to be posted in the big marker lounge area right now as well and then finally we heard loud and clear in this year that a fully managed neo4j in the cloud is exactly what you want but on the flip side there are certain requirements that the world's largest organizations have such as security integrations be it maybe with active directory or kerberos compliance like maybe sock compliance or hipaa compliance and we've worked very closely with a handful of large enterprises to build that into a new cloud offering and that's neo4j aura enterprise and we're launching it in early 2021 on gcp and aws and then asher is going to follow later on in 2021. if you find this interesting stay tuned pay attention to your neo4j.com and you're going to be able to find out more so in summary in this decade on-prem will still remain important and hybrid cloud deployments will remain important and in particular your local desktop environment will for sure remain important for that rapid iteration on in a local environment as you build your applications or you're trying out your machine learning models so that's all going to remain important but neo4j throughout this decade is going to shift into a cloud first company so that's the first pillar on to the second pillar putting the developer at the center of everything that we do so if we take a step back broadly speaking there are two ways of growing a product company like neo4j one is top down where you walk into the executive office either the the cio for infrastructure technology like us or into a line of business executive with beautiful powerpoint slides and you blow them away with just how amazing your product is in those slides you sell it to that cio or that line of business executive and then they push it down into their organization that's top down right or you can go bottom up where you build a product that is so amazing that the actual people the practitioners who use it on an everyday basis they might be the the support desk people right if it's if it's that type of a software or if it's a crm it might be the sales people and for of course for a database it's developers right you build an amazing product that the people who use it every day that they love and then they bring it into their organization they tell their management and then that's how you grow and become adopted inside of these big companies and i'm not going to argue that either one of these is the is the the better approach to building a company but for me the one that makes sense to me is the bottom up approach and there are two reasons why i find this one particularly enticing and the first one is actually ethical and i take a very kind of simplified pseudo-utilitarian view of this and the way that i think about it is that imagine that the world is a video game which by the way it may actually be and that everyone that walk around in this planet have a happiness or meter uh you know in you know on top of their head right look at link here from zelda on on the slide um and every interaction that people have with my company either with the product with the people inside of neo4j that happiness ometer is either going to go up or it is going to go down and the way that i think about it in a very simplified manner is that when i go to bed every day i want the sum of those happiness of meters to be positive right and if you think about these two approaches if you do the top-down approach the cio or the the the executive is going to be very happy because you know it checked all the boxes that they need and they they're one person but the tens of people or hundreds of people or sometimes thousands of people in that organization that use it on an everyday basis the software is not built for them it was built to sell into the into the executive right which means that the usability can be good but most frequently it sucks right and if you add that all up that's a really negative impact on the world right if you flip it to the bottom up one you have to build a product that makes all those tens or hundreds of thousands of daily practitioners to make them happy right otherwise they won't adopt it which then leads to the next reason why i like the bottom up one right in both of these approaches it is easier it's better if you have a good product right but with the bottom up approach it requires that you build great products it just won't work without that it requires you to have a product that the people actually use it that they love and for a database that means building a product that developers love so the question then is how do we do that right and this is an area that we've spent a lot of time thinking about here at neo4j because it's very central to how we operate and i believe that it all starts with a mindset and here's how we think about every feature that we build at neo4j we look at what developer expectations are we look for what's most missing the most critical gaps so for example back in the days we had cipher right we had just invented cipher prior to that was just an embedded java api and people loved cipher but you could only embed it in your programming language in drivers and wouldn't it be great if you could just execute cypher interactively from your web browser well at that time that was impossible right there was no way to do that this impossible we call that ground zero in our usability hierarchy it's not even possible to do something right and the first step then is to make it possible right and in order to solve this problem for us and make it possible we created neo4j browser right and most of you on this call have probably used neo4j uh browser probably all of you who have ever used neo4j you use neo4j browser but possible is of course not sufficient right we want to also make it usable we want to make it consistent inside of neo4j reliable so that it works the same way over and over again again and for an fj browser we want to send a query and get a result back that's very nice but you also want to actually make it usable so that you can save your work you can come back later and pick up where you left off and keep working on your queries so that's what we meant with usable right so that's very great and that's that's useful but it's even better if it's also consistent with all of your other daily developer tools right if we don't just make it useful but we make it normal right so for your cypher queries that means that you cannot just save and load them from your browser cache which is what how it initially was with with browser you should also be able to save and load them from your local file system right so it can be picked up by all your other tools you can check it into github you can open it up with your ide and so on and so forth right so that's amazing i think but we don't want to stop there we also want to ultimately make it magical we want to turn it into an experience where it's this invisible force multiplier where it's such a pleasant experience that you want to come back like you don't you don't need to go back to it you want to come back to it where the technology itself really disappears it diffuses into the background and with the mechanism of how the technology works is hidden right it's like clockwork inside of a watch we neo4j inc the producers of the database like we worry about how to make things work so that you the users of the product can go back to worrying about what you want to do and i believe that when technology fades into the background that allows you to get into the zone right into the state of flow and hyper focus on the problem at hand right and we're not there yet for everything that we build but we're not going to stop until we are so that's the second pillar now on to the third pillar that i believe will shape this decade of graphs i talked a lot about developers in the previous pillar but data scientists are an equally important constituent for us here at neo4j in this decade we will see the rise of data scientists in general and graph data science in particular gartner recently said and this to me is a quite extraordinary quote finding relationships in combinations of diverse data using graph technicians graph techniques at scale will form the foundation of modern data and modern analytics just think about that if your manager or your manager's manager ever doubt the wisdom of you investing in in and investigating into graphs then just show them this quote it's the foundation for modern data it's the foundation for modern analytics as per as per gardner a few months after they they released that statement in the report they were issued a separate report where they'd surveyed companies globally for existing or planned usage of various ai and machine learning techniques and the mind-blowing 92 percent said that they plan to deploy graph techniques within five years and there's also a surge of academic research focused on this right now where there's been over 28 000 peer-reviewed scientific publications about graph data science in recent years and that pace is accelerating if you chart it out it's a very clear exponential curve where the entire research community around machine learning has this as one of their top areas of focus in order to advance the field of ai and and machine learning graph data science actually has a long and storied past and i'm not going to go through it all in detail it it goes all the way back to the 1700s and leonard euler which of course invented graph theory as a as a discipline of mathematics ranging you know all the way up until when google used page rank which really was the first time that it got used at scale in in the software industry or in the internet industry neo4j of course here we invented the modern graph database in the in the mid 2000s and then fast forward to today i think we've made some significant strides in terms of advancing the state of the art of graph data science for the enterprise and here to tell us more about that is none other than alicia frame who is the lead product manager here at neo4j for data science so over to alicia to tell us more about this awesome thank you for that great introduction emma uh and so emil has brought us from you know the 1700s to gds 1.0 and talking about the rise of the data scientists and i'm going to talk about what we've been doing from 1.0 to today so the last four months and i'm really excited about about the title of this section you know the rise of the graph data scientist i joined neo about 18 months ago to build a data science product and i think the funniest thing was when i joined everyone said to me you know at our in-person events number four times you know all of a sudden half the people there were data scientists and we don't understand why and the thing is data scientists know graph we know it's useful we know it's super cool we just didn't have a way to do it at scale uh and i joined neo to really build an enterprise library to do graph data science so i'm super excited about the progress we've made so i kind of like to level set when i present on this we hear data science deep learning ai those terms mean different things to different people so whenever i talk about graph data science i like to start off with definitions what do i mean when i say we're doing graph data science so when i think of data science i think of answering questions with data when i think of graph data science what i mean is i want to answer questions not just with my data points but with the relationships between them and there's kind of a toolbox of different things you can do right so this venn diagram shows kind of the universe of approaches in graph data science so we have graph queries this is your your cipher queries i have my connected data stored in a graph i have questions i want to answer show me how many accounts are there that have committed fraud three hops out from this node of interest right i know what i'm looking for i can write a query i can find that pattern in my graph on top of that we have graph algorithms in our toolbox what this is is different unsupervised and supervised machine learning techniques that can look at that graph holistically and inside of finding local patterns find global trends basically identify important features of the structure of your network and add new data to your data set so that's figuring out which nodes are most important with page rank or doing something like community detection and figuring out which parts of the graph are more connected to each other versus the rest of the graph and so we go from pattern matching to with queries to algorithms to find these global trends and then uniting these is how do i explain what i've done to someone who's not a data scientist how do i explore what i've done interactively and we have graph visualization so just like most data scientists know you know matplotlib seaborn okay inside and out neo4j offers tools for graph visualization things like bloom so that you can interact with that and say does this make sense what have i found why and communicate the value to your team and you can kind of put these together to do everything from analytics where you're just answering a question with graph data i have a whole bunch of customers i want to segment them into different user groups i'm going to run the vein or you can layer those algorithms together those embedding techniques and generate features for traditional model building uh in your kind of standard machine learning setting uh and then visualization wraps it all together but really when we talk about graph data science it's just extracting knowledge from the relationships between your data points to make better decisions uh and one thing i like to like make sure that we we address like up front is why are we doing graph data science um this isn't just me or emel or neo4j inventing a new use of the graph database right the idea with graph data science is it lets you make better more accurate predictions with the data you already have so you know i've been a professional data scientist for most of my career one of the things data scientists always say is like if i could just get a little bit more data i could make better predictions uh but the truth is you already have more data you're just not using it so if you think about your data in your pandas data frame uh your excel spreadsheet you've got rows and columns you have data points you have descriptors but in in the real world there are relationships between those data points right maybe we're talking about fraud and you have individuals making transactions with each other there are relationships there maybe we're talking about drug discovery and you have genes and chemicals and diseases and there are relationships between those data points so when we talk about graph data science what we're doing is we're bringing those relationships to the forefront and we're using queries algorithms and embeddings to reason about them and bringing that into our machine learning pipeline so i'm not saying throw out everything you've done i'm saying that you can make it better with graph-based features so the idea is you load your data your connected data into a graph you extract graph-based features things like betweenness centrality communities pagerank and then you can bring that back to your tabular data world as new columns in your spreadsheet or in your data frame that you're making predictions from and when relationships are important what you'll find is that those relationship-based features you've created improve your predictive accuracy they help you make better more accurate predictions because you understand these global patterns this extended topology of your network and so the reason we want you to do graph data science isn't just because neo4j is cool it's because graphs help you make better predictions so emel hit on this one earlier in the deck and i like to kind of circle back on what does neo4j do for data scientists i joined neo to build a product specifically for data scientists i've been using neo4j for maybe the last decade on and off and at the heart of it is the graph database right neo4j was invented as the labeled property graph we give you native graph creation and persistence you store your graph shaped data in a graph database where you can understand store query those relationships what we've built over the last couple years though is the tooling for data scientists beyond just data storage so we have the graph data science library which is what i work on which is kind of our scalable graph algorithms and analytics workspace it's a library that works with your database to reshape your data and let you run queries and algorithms at the other end of the spectrum we have neo4j bloom which lets you do that visual graph exploration and prototyping and if you think back to that venn diagram a few slides ago right we have queries with the database uh algorithms with the library and visualization with bloom so we now provide the full toolbox to do graph data science i said earlier you know what i work on is the graph data science library i think it's super cool what we've done here is this is the the plugin for the database that lets you do graph data science at scale so when we talk about the library the first thing most people think of are the algorithms there are over 50 algorithms in the data in the library um as well as other machine learning techniques and we kind of break these down into six categories we have pathfinding and search so is there a route between node a and node b in my graph what's the best route what's the cheapest route kind of standard graph operations that you're probably familiar with we have the centrality algorithms which say what's important which node in my graph has the highest pagerank has the highest centrality score we have community detection which helps you break your graph down into connected subgraphs so if you've done k-means clustering before on your tabular data you might think about doing label propagation on your connected data we have link prediction that tells you where there should be an edge that you haven't seen one before we want to predict where they're missing relationships in our database we have the similarity algorithms which allow you to say given two nodes measure how similar they are based on their neighbors their properties their relationships and then finally uh in what we're bringing to you in this release is graph embeddings uh and these are super exciting i sometimes call them one algorithm to rule them all um but what they do is they they are a class of machine learning techniques that learn from the structure of your graph to make predictions to solve your problem and they generate unique representations of each node that you can use for feature engineering and i'll demo this later on but the secret sauce that's maybe less front of mind to most people is actually the analytics workspace so we talk about neo4j the database storing all of your transactional data when you want to do data science maybe you don't want to run it on the full graph maybe you want to run it on a different shape of graph than you've got loaded what you can do is you can use the analytics workspace the graph loaders to subset reshape layer that underline kitchen sync graph where all your data is stored into the right slice and the right shape for the problem you're trying to reason about so the reason i'm talking to you all today is because we're super excited to bring you gds 1.4 the fourth release since april of our graph data science library and what's in here is pretty amazing so i mentioned graph embeddings the one algorithm to rule them all what these are is they let you use matrix math and neural networks to learn about the structure of your graph so a lot of people have heard terms like graph convolutional neural networks graph native learning in gds 1.4 we offer three techniques to let you actually do graph native learning at scale to make this possible we also have introduced the concept of the model catalog and what this is is when you train a model we persist that model so that you can apply it to your data so i train a graph convolutional neural network to learn a representation of my graph that i can use for feature engineering and then i have that trained model that i can apply to my data as i get in new data or as i reshape my graph so that i can fit my predictive models within the database embeddings on their own aren't super informative they are a bunch of numbers they're machine readable they're not human readable so we've also added machine learning algorithms like k-nearest neighbors to let you leverage those embeddings once you've calculated them once you've trained your model predicted your embeddings then you can use things like k-nearest neighbors in the graph to build up more information to make discoveries and predictions about the patterns that belong in your graph and then the fourth piece that we want to highlight is one thing that's come up since we've released the database is hey wouldn't it be cool if i could write my own algorithm or i really like what you've done here but i want to change it in this way or i want to leverage the model catalog i want to leverage the graph catalog but i want to make it a little different so in 1.4 we have the pringle api that lets you build your own algorithms leveraging our infrastructure so our super scalable and memory graph our model catalog our apis you can use all of that with our prego api that lets you just kind of put building blocks together to build custom code and so with that you know this wouldn't be a developer summit without a live demo uh so what i have loaded up is uh in neo4j browser uh basically a retail graph that i use for a lot of different demos it's pretty easy to understand uh the first thing i'm going to do is just call up the schema so you can see what i'm talking about so what i've got loaded in my neo4j database is i have items that belong in categories they're purchased by customers and transactions and they the customers are from different countries and i use this graph because it's easy to understand we all have have made purchases before and so what i want to do is i want to demonstrate using graph sage and graph embeddings to first of all learn the structure of the graph predict that embedding and then build a nearest neighbor's graph so the first thing i have to do is i want to go in and i want to load in the data that i'm interested in so in order to do that i'm going to go ahead and i talked about that analytics workspace i'm going to create an in-memory graph containing just items because i want to create an items embedding i'm going to tell it to load in some information about those items i'm going to load in price the number of items sold to the stock code the page rank of each item and the category that it belongs in and for relationships i'm going to load in a similar relationship that i created before this demo based on co-purchasing so these items were bought together and this is what i'm going to load into memory to train my graph embedding on so i'm just going to click run super fast i've got my in-memory graph now i'm going to train graph sage and when we talked about graph stage that was that convolutional neural network where we sample data from the graph we aggregate information about neighbors then we learn a function that can make predictions for your embeddings so all i'm going to do is just call the name of the technique and i'm going to tell it to create a new model called graph stage demo since this is a demo i'm going to tell it in addition to using the items graph that i loaded that it can use a bunch of properties that i've loaded in category sold price stock code and there's some hyper parameters that i can tune basically telling it how do you learn the structure of these items and so i'm going to go ahead and now that that's the code is there i'm going to run it and what this is doing is it's training a graph sage model and so you can see right here i've got graph stage demo and it's a graph sage model and i can see all the configuration about that model that i just trained and so what this has been what it did is it stores this information in my model catalog and so the model catalog is where i store all the different models that i've trained so i can use them to apply to my graph so now i can see that i've got a graph sage model and now i want to use it so the first thing i want to do is to show you all if you're not familiar with embeddings what do those look like um and so i'm going to just stream the results and graph embeddings are really powerful but they're machine readable what they are is i've trained a deep learning model to learn the structure of each node is a bunch of numbers that uniquely represents each node and can be used to predict these nodes are close together in the graph these nodes are far away these nodes are structurally similar right you can think of it like super powerful feature engineering but to a person it just looks like a whole bunch of numbers so every node in the graph gets this embedding super cool um not useful so what i want to do is save that to my database so i'm just going to write the results back and go ahead store that information and once those properties have been written back they're available for me to use for other algorithms so i'm going to go ahead and i'm going to build the nearest neighbors graph using these the new embedding property that i just wrote back i'm going to create a new relationship that tells me which items are most similar to each other based on their embeddings i'm going to kick that off to run and so what i've done is i've gone from raw data trained my graph sage model applied it to my graph and then used the output of my graph stage model to construct a k nearest neighbor's graph now to kind of finish off that venn diagram i'm going to go over to bloom to visualize what i've done and so i've got neo4j bloom open in my browser and i've stored a few search terms ahead of time so the first thing i want to do is i want to find a node where it has nodes that are similar to it in the co-purchasing graph that base graph i started with um as well as nodes that are similar to it in my embedding similarity where there's not overlap so it has some nodes that are similar based on code purchasing some nodes that are similar based on the embeddings but those are different nodes and maybe that'll make a little more sense when i pull that up and run it so i'm going to have similar embeddings and this is going to pull up an example of that data and it'll load in just a second and what i can see here is i'm using bloom's new hierarchical layout so it's pretty easy to look at and i can see my red spot gift bag this is that central item where i'm looking at what's similar and what's dissimilar on the top i can see the items that are similar based on co-purchasing so these items were brought together this is that base graph i loaded in to do my training and at the bottom with the orange relationships i see the the items that were found to be similar based on their graph embedding so the topology of the graph for these nodes looks similar to the gift bag whereas these nodes up here were just bought with the gift bag and if i just kind of take a look at what i see in bloom i can see well it looks like the gift bag was co-purchased with a lot of other you know i don't know wrapping paper gift wrap items um whereas on the bottom it seems like i've got a much broader kind of cross-section of items so what i'm going to do is i'm going to go in and i want to add the categories that these items belong in so i'm going to use a new search phrase in bloom and what this is going to add in for me is it's going to link these items to the categories that they belong in and so this will take just one second to pull back and what we can see is it zoomed out to get everything in the frame but my my intuition on the co-purchased items was right right they're all seasonal but if we look at the similar embeddings what we see here is these belong to a bunch of different categories we've got quite a few in home decor we've got bathroom we've got garden so what i can see just from this super quick you know five ten minute demo is the embeddings are learning something different than the co-purchasing graph the embeddings are learning topologically my red spot gift bag looks like my vanilla candles maybe that's because we're buying candles to put in gift bags maybe it's because both of those are items that get added on to larger purchases but i'm able to unveil new information from my graph and using the embeddings and then reason about it with bloom so let's go back to the slides so putting this all together hopefully what you guys have seen is we started from pointers right we have queries to answer questions with connected data in my example you could think of it as when i created that item graph that i loaded into memory then we can use patterns we can use graph algorithms to uncover trends and patterns in our data we can start reasoning about this is in this cluster this is in another cluster and then what we've introduced in 1.4 is the ability to make predictions so if you think of this as like the road to you know the rise of graph data science what we're bringing you now is graph native machine learning to learn the topology of your graph to uncover new facts and if you think about bringing this back to the you know hierarchy of needs this is really kind of when we get to that awesome level right going from possible to pleasurable to amazing for me predictions are pretty amazing um just to wrap up if you guys are curious about any of this we have other talks at nodes you should totally check them out including a deep dive into graph sage um we have developer guides and we also have books that you guys can download for free to learn more and with that i'm going to hand it back to emil to talk about the future of the property graph so thank you so much for your time thank you alicia that was fantastic i am so excited to see what graph data science can do to the world of ai and machine learning as this decade is unfolding so that was the third pillar and now on to the last of the four pillars which is the property graph model and i believe this is the decade when the property graph model will truly live up to its full potential before we talk about that though i thought we'd take a little bit of a step back and look at what we've collectively achieved in the last decade if you recall we walked into the last decade with no sequel just having formed right so nosql as a term was coined in 2009 we loved that we saw that as a movement of non-relational databases that rejected the notion that you put all data into one singular type of database and it recognized that we entered into a world of plurality of polyglot backends and we really agreed with that we rejected it as a product category we felt that these were not interchangeable products but we worked really hard to establish graph databases as a distinct category in the broader umbrella of innovation in in data and it worked really well if you look at this chart here you'll see this is from a site called db engines i'm sure many of you are familiar with it it measures a number of signals around buzz and awareness of a certain you know certain databases so for example number of google searches number of twitter mentions number of job posts number of skills on linkedin and so on and so forth and computes a score but it does that per individual database but even more interestingly per database category and as you can see here for most of the last decade graph databases was the fastest growing category in all of data right and we started out we coined the term graph database and so initially we were the only one right now i just looked yesterday there are 32 other graph data 31 other graph databases a total of 32 graph databases um in tracked by db engines so there's this amazing choice of different types of graph databases which truly makes it into a real category not just a one-off thing so that's fantastic and if you look at then where the depth of adoption is for graph databases and what we truly achieved in the in the last uh decade you can zoom in and you see a number of sweet spot use cases right these are the ones that we actually talked about in the at the beginning of of this session right we talk about personalization real real-time recommendations master data management and so on and so forth and what they all have in common is that they're operating on connected data but in particular they have low latency read requirements on multiple hops of data operations right so that's kind of the the joint thing between all of these use cases and this of course is situation or is a situation where a graph database have orders of magnitude better performance compared to a relational database or really any other data model out there so that's collectively what we achieved us here at neo4j for sure you here on this call the broader neo4j and graph community alongside with all the other graph vendors as well in the in the previous decade taking a little bit of a step back from that and a little bit of a detour and then i'll turn it back into the property graph model when you look at most domain models out there my belief is actually that for most applications out there most domain models are actually inherently connected just for fun i looked at i looked up the term domain model on wikipedia and lo and behold there was a it was an example of a domain model which you can see on the slide it seems to be from a u.s healthcare model and you can look at that it is very very connected right it has a bunch of boxes providers healthcare supplier and so on and so forth and they're all connected in various ways i also went to google and did an image search for domain model in an incognito google chrome window and ended up with this page right which is just a page full of domain models every single one of them is highly connected as you can see and i bet that if you think back on all or at least most of the applications that you built in the past 5 10 15 20 years you're going to look at those domain models and you're going to realize that wait they actually look like the ones in this slide they're connected and so let's take a little bit of a look at that domain model from wikipedia the us healthcare one we can take that domain model and we can store it in a relational database no problem like we know this game we've done this before third normal form er modeling like you name it basically wherever there's line in that at the main model we break it out into foreign keys possibly join tables depending on the cardinality we know how to fit this right some of those domain objects will end up in one table some in multiple tables it's doable it's a little bit awkward it takes some time but as an industry we know how to do it we've worked through this many many times before we can also model it in a document model right and here it's actually easier to represent some of the stuff than in the relational model which is one of the reasons why not the only but one of the reasons why the document database model has been so popular in recent years but actually the connections between all these various entities are even harder to model than in the relational database you have to encode them into your document you have to say that you know this provider over here is called id one two three and if it's connected to this individual over here somehow then you have to have a value remember that it's one two three and if this one gets deleted there's no automatic way of maintaining that referential integrity so it's actually weaker at maintaining those links in all of those domain models that we saw before and so then we move over to the property graph model and it's the most natural fit for this domain because exactly like all the domains on that the google image search slide it's a connected data model so it fits hand in glove with the property graph model there's no transformation needed every single element in that domain model can be directly represented in the property graph model it's the best fit of all the data models that i know of for most applications and i'm not saying that you can't represent this domain model in the other data models we just walked through how to do that in the relational model or in the document model these data models are all isomorphic which is a fancy way of saying that you can transform data from one model to the other without loss of data right so it's not interesting to figure out can you store this data in one of the models you can in all of these models they are the equivalent of touring complete but for for for data the interesting thing is it a good fit does it fit hand in glove right and i'm not also arguing that this is necessarily true of the software that these products are trapped in today right in some cases even if you have a highly connected small domain model it's actually easier to put it in a relational database today right because there's just more mature tooling around and so on and so forth but if you look purely at the data model and then if you break down that domain model into its atoms they can all be one two one represented in the property graph model without any transformation without any awkward contortions and that is not true of any other data model that exists that at least i know of and if we are also able to expose that beautiful data model in an equally beautiful dare i say magical product surface then i believe that the property graph model will up will live up to its full potential and it can it can become the default the first choice for most new software projects so that gets us to the end of this opening keynote and so i want to take stock of where we are and what we've said so far but i want to start out by just reminding ourselves about that usability hierarchy from before we start out at ground zero right where if we want to build something it's initially impossible and then we make it possible and then we move from one to two right from possible to usable to normal and so on all the way up to to to magical and if we then look we zoom all the way out and we look at our collective achievements in the previous decade and we apply that hierarchy to the entire graph database category we really went from zero to one the way that i look at it at least in 2010 right in 2010 when we ga'ed neo4j 1.0 this made it possible to operate on graph data at scale in a real way embodied in a generalized product that could be deployed into multiple use cases and that was huge right a second milestone was when we incorporated cipher into the product and this gave us a shared language for thinking about and working with graphs it made graphs usable from any programming language so that was a big milestone for us here at neo4j but we've actually never seen our job solely as advancing neo4j the project neo4j the product or even neo4j the company we want to advance the entire concept of graph databases the graph database category we believe that the idea of that the idea of looking at relationships in data figuring out how things fit together that idea is bigger than we are and so a few years later we grouped together with several other graph database companies and we launched gql gql to make graphs industry normal right and this is the first time in the history of databases that a new query language was adopted side by side with sql the first time in the history of databases so it's a milestone achievement and it really catalyzed this broad mind share across the industry accepting graphs as a new standard so that brings us to now where we are now in october in 2020 and in so many ways i feel like the preceding decade and you know while i'm so proud of what we've all accomplished as a community was just laying the foundation right the groundwork has been put in place the idea of graphs inception right in this in this slide if you will a way to talk about graphs initially cipher and then an entire industry grown together around graphs and that all gets us to normal this decade then the 2020s 2020s is about making graphs magical and how do we get there we follow these paths forward the four themes that will shape the graph database category in the decade to come the four themes that we've talked about obviously a shift to the cloud which is more than a place it's a connected approach from your laptop to your data center to the edges of the world developer productivity graphs always at your fingertips easy to use effortless new audiences like graph data scientists so that the entire world of ai is powered by relationships in data and then finally and ultimately to bring it all together the property graph model which on some level is the data supermodel that can abstract away many if not all of the other data models over time for those who have been with the neo4j community for a while you've certainly seen that one of our taglines or our motto if you will is graphs are everywhere and it's the observation that graphs is like i mentioned an idea bigger than what we are it exists everywhere in life it exists in data for sure it exists physically in our bodies our the human brain is biologically wired as a graph with neurons connected to other neurons through synapses if you if you double click and zoom all the way in on your body your those cells you know consist of protein networks how things fit together in a connected data structure structure is something universal right and so we observed that by saying graphs are everywhere right i believe that the next decade in the 2020 through 2030 is not graphs are everywhere though that of course still remains true but it really is the decades where graphs are for everyone so that concludes this keynote i took liberties of looking out very very far most of the rest of this conference will be very practical and pragmatic and it will be about the today but i hope you enjoyed this view to the future of at least how we hear internally at neo4j look at how the category is going to evolve there's 10 hours of amazing content ahead of you so i hope all of you are having a fantastic time listening into the sessions i always talk at all the conferences where i am is that we are graph people right and what ultimately defines us though if you peel away all of the craft and all of the noise the one thing that sets us apart compared to other communities is relationships it's much harder to strike relationships in the virtual world than compared to into the physical world and i wish this could have been a physical event event so we could meet face to face but in lieu of that log on to the chat areas the lounge in in big marker tweet about this use the neo4j hashtag but by and large beyond everything make sure that you connect have a fantastic day thank you very much 