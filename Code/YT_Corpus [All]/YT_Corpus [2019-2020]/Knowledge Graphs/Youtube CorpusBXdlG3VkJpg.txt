 by way of introduction one of our team members is not here Miguel he had a family emergency Omar's from our financial services division and I work in our national advisory Division all right so why are graphs getting popular so if the graph is a data fabric and I can store all my data as an in-memory object then really what I need is a lot of memory and what one of the major drivers of the adoption of graphs for enterprise workloads is the fact that memory and also compute or just getting cheaper and cheaper and cheaper let's go next slide and so last year I guess it was two years ago now AWS started releasing virtual machines that had four terabytes of memory on them and this year we go to the next slide Microsoft on Azure is releasing machines that will be up to 12 terabytes of memory we were actually just looking at some of their six terabyte VMs recently so what this really means is that you can actually take a huge amount of data and stick it on a single virtual machine what are the benefits of that well if you think about a scale-up paradigm versus a scale out paradigm when you scale up you're actually getting the fastest possible i/o scale out if I'm using data distributed over a bunch of commodity servers my main bottleneck is actually network speed so it's actually a better solution if I can just get really big VMs and stick my data on them and I predict that you know within the next five years you're gonna start to see you know hundred terabyte machines and potentially maybe within the decade you know petabyte scale VMs I mean there's this huge hardware revolution that's going on right now and graphs are particularly in memory graphs like neo4j are perfectly positioned to take advantage of what's happening let's go to the next slide so what our graph nodes relationships and if you look around if you think about your business there's lots of different kinds of graphs that are out there so anything that looks like a complex network is a graph that has dependencies anything that has complex processes and hierarchy associated to it so here I have a ship I have containers in those containers or boxes those containers are going to get loaded on trucks and they're going to go to cities and in those cities or warehouses from those warehouses they go to stores and so forth great graph problem anything involving componentry so we saw Preston's talk earlier on on total cost visibility right all of the all of the component hierarchies so there's a number of graph graphs that are out there around components these are typically called digital twins and so you can literally have in a graph database a digital representation of every single major asset that you have and all of its specific components with each of the serial numbers of every single one of those components all tied together and of course every single sequel database is a graph the very first thing that you do when you design a data warehouse or a database is you produce your concept diagram and that concept diagram is in fact a graph and so if you have any kind of structured data as laying around in a bunch of different sequel databases and most companies will have hundreds if not thousands of sequel databases you can ingest all of those databases into your graph you can bring over the relationships and instantiate those in the graph so with that I'll turn it over to Omar and and we'll take a deep dive into some financial network analysis and what I will point out is a lot of the graphs that you've been seeing so far are our detailed node relationship ground we're going to be talking about subgraphs and there and how subgraphs fit into larger scale macro graphs and so if at any point you need you need a reminder on what exactly we're talking about here feel free to chime in and say okay guys I'm not following and I will break it down for you yeah Dave cycle so there's a number of graph use cases for an analytics perspective that makes graph very complimentary to how you would already perform some an ellipse that exists right so recommendation engines anyone familiar with collaborative filtering or our recommendation engines work and recommend products or recommend most similar persons right within a graph kalyra filtering the mathematical contract of it is implicitly built-in because all your customers will be linked together by the products they've already bought so that mathematical operation doesn't need to happen when you try and do recommendation engines through a graph supply chain network optimization this is a you know a network graph is a network right so you put your supply chain in there like that fraud networks you know various different fraud circles etc community tection social network analysis and then what we'll talk about is anomaly detection so background as Michael mentioned we started on this about a year ago with a financial source client of ours and the problem statement was right so am L right anti money laundering is currently it's done as a very rule-based type system when you really think about it money laundering is a network problem you're trying to understand the flow of funds of how it goes from one intermediate to another intermediate to another intermarry you can't really model that business problem or even that domain in a relational structure you actually to bring it into a network side problem and solve it as a network or a graph construct right and so that's where we started trying to tackle this problem and what we're showing there now is the framework that we've thinking about and the framing that we've kind of built and have started maturing a bit and now we're looking into ways of utilizing this as a generalized anomaly detection framework for graphs that can be used more than just for anomaly detection but a few other use cases right and we think of anomalous behavior or anomaly detection normally is for something somewhat bad and some bad connotations right so AML fraud that type of anomaly but an anomaly can also just mean something else it could also be an opportunity right so if you're a retail bank and analysis behavior girls are just really signal a life change in one of your customers a new job or moving to a new state means that you are now behaving differently than what you were before and so your new behavior if I were to look at at the lens of what I previously viewed you as it would be somewhat anomalous right and so this is kind of the thinking that we started doing is can we utilize this framework for customer behavior so if you've got a new job or a new house we got married significant changes and your behavior these could be potential opportunities for customer growth either offering you new products or understanding what your needs are and now let's see what transaction networks at scale what we learned when we're looking at this from an anti modeling perspective is that it's really a problem of modeling efficient flow of funds versus insufficient efficient right and is efficiency in how people or institutions or accounts transfer money around right is that correlated to the type of behavior that they're trying to perform and so how do we think of as structuring as a graph problem so the first step obviously you know everyone's really familiar builder customers do succeed database right so you realize near something like neo4j to actually get down to ok my customers my accounts their transactions build your customer 36 we solved that problem once you've got that what you'll see is you've got essentially a transaction network right but then you've also got this concept of scale within your graph so as you build your enterprise and as press was also showing he was showing the hierarchy between tanks and the guns and the bullets and then else is it up to you like your troops and your battalions etc there's a whole scaling in your large knowledge graph right so if we start with a retail bank at the micro level you would probably just be looking at say an account an account would have its own relative subgraph transactions It's Made cetera go up a scale and now you've got the full customer right now customers got multiple accounts mole transactions they're all part of the same graph but you can look at them at different scales I can look at it from an account perspective and expect different behaviors I can look at a customer's perspective and expect different behaves and different graph structures go in further and now I'm looking at potentially a household right a husband and wife then I want joint accounts they might have some kids etc take a little further now you're looking at maybe lines of businesses etc so there's a whole scaling aspect when you look at and consider your graph from an analytics perspective and that's where you should think ok how do I actually break down my graph right because the question is if I'm trying to define behavior I had to first define what is normal behavior and to find what is normal behavior I have to understand ok what am I actually trying to understand and so you would have to break it down into the different scales so you can do it scales by collection or nodes so what I just showed you a collection of accounts or a collection of customers or a collection of households right and these are generally defined by your business domain or domain expertise and then there's also scaling by collection of edges this can generally be done whether you know like a time side scaling okay this quarter next quarter or a year or a day or a week right the scaling and how you look at your graph from an analytics perspective is very important because different scales will give you completely different views and that's how you also then you know build up the analytics use case and look at it so in our view from a finer specs that perspective you know you've got your account of micro-level all the way up to the firm at the macro and it's not you know there can be clustering between skills as well so using a toy model here right to kind of explain what I mean by scale and then what's inside those if you explore that right so on the micro level of my accounts if I'm just focusing and performing analytics on my accounts you'll see that my certain products will cluster together because I expect the behavior to be similar the credit cards will behave generally the same way checking accounts to generate behaves the same way and so they'll be if you were to say segment them which most finds firms already do they were generally clustered together in the same somewhat segments go into the customers similar customers that spend the same their graph structure if you run them through various living graph similarity algorithms right definitely construct it and then also just similarity and segmentation algorithm that my banks already performed they'll cluster together as well in their own domain same thing with households if you do households by income you will see that day's segments together in close together you'll see that happen in if you're measuring just similarity from a graph structure perspective as well and so just to chime in here real quick so we're gonna be using these blue circles for the next set groups of slides here and so the point each one of those is a graph or a or a sub graph within a particular scale all of those nodes can all be sitting inside the same master graph but if you take different slices at different scale you have there's a graph of accounts there's a graph of users there's a graph of households they can be all constructed from the same data and so when we're talking about the blue circles we're really talking about different levels of scale within the graph so hang on to that concept yeah because that allows us to set the context right the relative contact for change in one entity relative to the other entities that we expect it to be so in the reddit or example right so to show you the scaling and what exactly what michael mentioned if we have say three accounts one is a college student and two parents weave in that customer scale their peers we're going to look at them separately we move up a level to the household they all get joined into one sub graph and we're looking at that specific sub graph right so this is a context under which we will then be performing our analysis and checking for you know how normal is the sub graph structure here and that kind of then gets to thinking okay so they're all clearly linked right the three accounts that are clearly linked together and they make the bigger household sub graph if there's a change in one in one scale what does the expected change that I should see in the next scale all right and what income you can actually compute that can be modeling right can i essentially modeled a fusion of movement from one to the next and you should be able to write if say the household the two parents decide to move to another state but they still have the same job same income now they'll difficult to figure out how to do remote work and moved from New York to Colorado still being paid the same you would expect the individual customers to potentially move in their domain right they're now relatively wealthy really look so they're their transactions have probably changed and you'll notice that the household is also going to move inside its own scale as well to be with households are similar to them likewise if the college student graduates and moves back another parents or gets a job take note millenials yeah you'll notice somewhat of a shift in the household as well because the one of the underlying nodes in it that makes it up has changed and it's changing behavior so the households changing behavior as well but it's not necessarily that they're gonna change in the same manner right so change in one scale is going to impact changes in to changing the other scales learn some how much right and that's kind of one of the things we're trying to measure obviously the two parents moving to Colorado is a bigger impact on the movement in the household then say the student graduating college potentially depends I guess it depends on what they majored in yes and so the idea is can we now model this as expected change across our graph data models right so what I mean graph data models is you've always got your massive one enterprise graph or parent graph and then those individual blue circles we showed you would be the different graph data models or scales right that you would look into and so it is what are the types of small changes from one data model can impact the next right the example I used household movement or you know a newfound wealth how does that change that change in one data model what's the expected change in the next and so by framing the problem in that way right the similar graph data models can give us a context for dependent way of expressing the behavioral change and so with that we think we can compute it but how do you actually compute that and this is where you know machine learning really comes into play because be change names gonna be a really hard problem for this type of thing in here you sense you let the data and it's if you turn into a big data problem you can let the Machine essentially learn what are all the different expectations right and so expressing behavioral change it's now deeply connected to expressing the structural change on similar graph data models that are supported by the same underlying graph right so that just means essentially if your sub graph is changing in one graph data model I should expect some sort of change in the other graph data models as we've been mentioning and then to make it a little more complex obviously I haven't brought in the time varying aspect of this yet right so for each graph datum I'm also measuring the temporal aspect and I mentioned earlier the other dimension of scale which is time later do I want to look at this from a weak perspective do I want to look at a monthly perspective do I look at our quarterly perspective this type is actually what requires a lot of investigative work right depending on your use case in does a week make sense for fire for fire services probably not does a month make sense you're getting there it's probably probably around a quarter the six-month time range is what scale you want to look at for a customer behavior right it gets some of that seasonality in there between Christmas and tax season and summer and so we've got two different things that we're looking at one is obviously the graph data model which is OK at the customer level account level household level what our scale level I think is appropriate and then also now the sequence the sequential level the time variant level and this gives me context for behavioral change because now I can monitor the changes at each craft data model through time and so now I bring in the concept of a time sequence graph data models right and so this is now the actual framework what I described just now was actually building the graph and laying out how you can start tackling this one analytics perspective which and of itself is also very hard because one thing that we're trying to do here is remove a lot of the feature engineering that takes up a signal amount of time and machine learning but there's no free lunch all that fils training time spent feature engineering is now spent designing the actual graph data model right because now we want to take advantage of some of the latest stuff coming out like like graph convolutional networks or other deep learning methods because they'll essentially try and these are very complex structures and so we want to let the algorithm Senshi learn it without giving it much of a any sort of priorities so the first thing is that intuitive edges are semantically compatible with the parent knowledge graph all that really means is you know if I'm looking at the edges and nodes and one graph datum all are essentially the same thing and the other ones right of the notion of an account is the same across there the notion of a household is the same across everyone the notion of a transaction is the same across all my models home base information theory concerns about information propagation let's get that a little bit use unsupervised architecture so this is what I mean by taking advantage of some of stuff happening in deep learning and machine learning we are trying to remove a lot of the feature engineering because there's just so much going on in trying to model human financial behavior or even modeling human behavior that we think that we should let this be a big data problem and let algorithms essentially try and learn much of this and let us know what it thinks is anomalous right so we're not going to try and the architecture learns how we should be describing behavioral change not the other way around we don't want to make any priorities or assumptions about what we think are normal behaviors we'll just let the data speak right and you know people's financial transactions are generally very easy to model they all generally for a very normal distribution and then so user sizzle distributions learned to identify outliers the specific reason that we're learning are essentially the distributions of your graph right euro as an individual and and then your financial network the accounts that you transact with we're essentially modeling that graph through time and we're letting the out learning algorithms that you learn what are all the different types of transformations that happen at the different scales and then we try and create outliers so the process is you know you've got your parents knowledge graph here in green you've got your time varying graph data models and blue again you'll then try and essentially you might run some memory constraints so you'll say ok I'll do a time-varying of say six months or so and you try and learn a distribution of that the main concept being that if I can a distribution on it can i reconstruct your graph right so can i reconstruct all hearted it too for an algorithm to reconstruct your financial behavior right and if it's very hard it'll give me a large error but normally what we have found is that the error is very low in modeling most financial behavior in transactions if I took a log a cumulative log error it would Faro power law which is what I would expect one things that you all see in a lot of graph analytics is that graphs and networks all generally fall power laws right we're seeing the same thing with modeling financial transaction financial behaviors and so the idea then the key takeaway is that I'll try and reconstruct the actual real transactions we've had near transaction Network and if there is an outlier in that distribution then that is something that can warrant potentially looking further into as an anomalous behavior you do this a couple times you build a couple champion models because you want to try and build different champion models for different time frames and then the overlap in there what I mean by that is let's say you build one for the household you build one for accounts to build one for customer and then you try and rank okay let's try and reconstruct the sub graphs that have happened the the overlap in the errors those would be the ones that you really want to focus on and that's what we tried doing for the anti-money laundering case and which no speak too much about that but so essentially the key takeaway is you know financial behavior is hard to replicate in this framework because we're essentially everything I just talked about was setting up the graph so you could let a very complex learning algorithm learn a huge variation in structural distribution and if it can't reconstruct what you are doing then there is something off about what you are doing right so a financial behavior is hard to replicate in this framework the more likely to be your can be considered somewhat of an anomaly yes so maybe I can give a little bit of a transition example because we're gonna we're gonna go in and look at a specific use case so in this scenario imagine that you're a bank and you have a hundred thousand accounts and each of those accounts you put into a graph and you've created a concept of an account month okay and the account month has all of the transactions that are occurring within that and within that account as a subgraph and then I have a whole series of sub graphs for every month that that account has been active then if I compute the statistics across the entire population of account months then I'll be able to see whose transactions vary the most from month to month and so in the case of a money-laundering example what I would expect to see is maybe massive influxes of funds coming into an account and massive outflows of funds going into that same account but no actual change and the net value of the account and that's what's known as pasture of money laundering and so and then we can compute that not only at the account scale but at any higher-order scale and the vast majority of accounts will basically have the same sets of transactions think about when you pay your bills you basically pay the same bills every month now your point buy a car every couple of years you might buy a house once or twice in your entire lifetime but generally speaking your financial behavior from account month to account month transactions subgraph is going to be basically the same so therefore when we break it down in this kind of a framework we can easily compute the distribution statistics and then quickly identify who are the outliers in the other they also want to add you also when your financial transactions are generally meant to try and be as efficient as possible right if you're gonna get a mortgage you're gonna go straight to the bank or a mortgage servicer you're not gonna go to an intermediate another intermedia and then finally to the mortgage so if you also look at it from a number to that point it needs to be efficient right and so in efficiencies in flow of funds is also something that'll turn up as anomalous so as an example right this is generally when I say scale this could be anything from accounts to financial institutions the middle could be the Federal Reserve right and it would all look very similar like this if they are efficient transaction networks so you think about the way a payroll drops right so a company pays its payroll every month and so there's a big drop of funds into all of its employees as employees turn around they pay all their bills a bunch of retailers right get paid right it's all very direct from a centralized source of funds out to the final recipients of those funds and then the example of something and this is very easy for a model to learn and replicate right something like this right somewhat inefficient and you've got a higher connectedness than normal this might be a little harder for a model to randomly generate based on the distribution it's learned from all your data and so these types of changes is what our framework is trying to capture and we're now looking into you know how do we transform this to the customer behavior modeling not necessarily trying to you know potentially bad behavior but maybe potentially behaviors at signalling hey there's a change in someone's life or a life event I think all right and you guys are working on a paper on this right yes eventually once yeah all right okay so you want to write everything that you've heard today it will be published in some form or another by our data science group advanced analytics group in FSO and I just wanted to close out the talk I realize that this is a fairly abstract concept but what it really boils down to is that your graph contains information that isn't known to any other system when you put your data in a graph those relationships themselves because very very critical data points and can drive a whole new portfolio of analytics that's basically been not available to you in the past and so this is one of the reasons why we're very excited about graphs more generally at ey we've done a number of very large graph projects and this is probably a little bit of a nice eye strain slide here but I'll just read these off real quickly most of the graphs that we've been building are in the billions of node and relationship scales most of them are designed to solve some type of analytical problem as Omar has described but some of the things that we've done recently is for a fortune 100 tech company we took their entire global b2b account contact and marketing touch data set and put that into a graph that had about 500 million nodes in it and two billion relationships for a for a very large footwork company we unified every one of their e-commerce transactions with every one of their retail register transactions including line items and produced a graph that touched 30 million customers combining brick and mortar with online which is a classic marketing problem total product ownership across multiple channels for a cruise line I can actually mention this one this is Royal Caribbean we were in production with a recommendation engine that takes popular activities that passengers have done and recommends them to two passengers so when you go on the cruise you can open up the mobile app it's available on iTunes and it'll tell you what you ought to do based on your buying behavior and other passengers that look like you and that's a compilation of thousands of cruises worth of data we've done some very interesting work in AML which Omar has just reviewed and then also recently we did some really interesting work in the b2b space about recommending the next best event or webinar cross channel recommendations based on multiple types of audiences business decision makers developers and CXO level titles with specific recommendations - for each title and all of these are unified by the notion that all the data that was required was sitting in multiple systems pulling all the data together into a graph allowed us to do deep analytics and then with neo4j x' you know in memory architecture we can easily wire that up to digital applications and deliver an answer in typically under 200 milliseconds so it's very powerful and with that we'll close out if you have any questions thank you [Applause] 