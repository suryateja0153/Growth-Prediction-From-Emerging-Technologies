 hello everybody welcome to the near 4j online meetup we've got a special special version on a Monday for you today so this is the founder of graph theory Leonard Oilers birthday so that's why we're doing doing a meet-up today and this is also the online version of the global graph celebration day which is quite remember which way around those those go so if it's early enough in the day that may well be another in-person meetup for you to attempt I'll paste the link for that into the chart just a bit of housekeeping before I introduce our speaker if you're watching this live you can ask us questions on the right-hand side and the YouTube chat and I will get the questions and ask them to speak up you'll also probably want to make sure that your resolution is set to 720p or higher if you're watching this afterwards I'll I'll put a link to the near future community forum so you can ask questions there so I guess that leads us to our speaker sits a returned speaker so we've got today we have Christophe the limps in this CTO of gravity a partner and he's going to be talking to us about content-based recommendations using knowledge cross the Kristin I guess I'll hand over to you and let you take us away thank you Mark all right is it okay yeah cool good all right hi everybody happy birthday be alert so thanks for joining this meetup about content-based recommendations using knowledge graphs so I'm Christoph CTL offer I'm using you know for Jason's seven years now Atwater since four years and a half so we do daily neo4j based projects and we have a specialization in recommendations natural language processing and knowledge graphs in terms of outline for today I will first explain the problem that we gonna try to solve with knowledge graphs actually so why it can be useful or making better recommendations then so widened the knowledge graphs then speaking about harmonizing data silos so we will see that we will use different sources of data in order to improve the recommendations I will speak throughout the whole demonstration about some natural language processing and deep learning concepts I will not go too indepe because it can take days to explain all of this but it will give you a high-level overview of how NLP and deep learning can be useful in such scenarios and then we will conclude with a small summary so first the problem and to introduce the problem I have the best forum in the world which is the neo4j community forum so there are questions asked on this forum and replies and those questions actually sometimes are not answered but also sometimes are sad and when I'm inside is that this page looks sad why I will explain why the sad is that there is a whole panel on the right that is unused here why we could actually recommend the user a proposed solution based on historical data on the community from or we could we have this box in the bottom which I could provide suggestion about our recommended topics the problem is that because this question was tagged with GWAR arguments which is not really the section why you should tag your question so you took about full text index etc the problem is that we can see that this course which is a platform used under the hood well provide suggested based on the category this question is in again the suggestion here are quite poor because of the wrong category so this is the problem and we're gonna try to make this experience better so provide better suggestion of recommended weeds based on the historical data and other sources of data because there is another aspect is that this suggestion topics provide and suggest only content from the same platform so only you the questions that have been posted on the discourse community platform but never j has a pretty amazing community especially people writing content continuously so we can see here this is mark Needham or horse today this is the statistics about his publications since 10 years already this is the same for Box de marzi you can see that there is a lot of contents actually on the internet that can be relevant to that particular question again there is also the new for a YouTube channel why did this video those videos can be transformed to text with the with the help of transcripts and so we could actually use it to also make recommendations so as a summary of the problem is that a wrong choice from the user would actually lead to poor recommendations there is no category and tag suggestion to recommend the user for a better tagging so actually if we could propose directly the user to choose another category for his question it would lead to much better suggestion as well and there is no real additions from sources outside of the community platform so of course here only took three but there is there is a plenty of them there is my car hangars block there for Jays own blog the graph for a blog etc etc and we're gonna solve this with what we call knowledge graphs but I want to explain maybe first one and why using knowledge graphs well first what is the knowledge graph well at the end is just a graph right so but a graph of knowledge so here we know that we have a source of data which can be more didn't block the community questions the community forum that produce articles those articles have topics and in the text there are mentions of maybe epic procedures DB internal procedures some features and those articles can be similar to other articles so our problem is that we cannot provide good similar to relationships but also we cannot we don't have features right now to extract a procedural names and internal DB procedural names from the text so uh knowledge graphs actually describe the problem that you are trying to solve so here our problem is that we are trying to solve similar relationships between articles and general knowledge words also show you the the path to the solution because if we can actually extract up our procedure names DB internal procedure names feature names from the articles but also credo relationship is similar to better topics etc well this is what will lead to our solution now harmonizing data silos well as I said there is a couple of data source that we can use first of all of course is this neo4j community forum but we're going to use as well so the mark Needham's blog maximize this blog and then your JS YouTube cello I will not go into the full injection process I will just give some hints about where to find the sources for importing this data so here there is a new Fuji community from that provides a RSS feed and a road of right back a white back subscriber to this RSS feed that you can use it's open source and get ups on your fridge a disk or slack so the first goal of this consists of a quick yes could you feedback candy as if is impossible we make your slides on the full screen like on the play mode yes system of the text okay okay so our this application a while back for actually getting the data from the from the magic ability from in real time so you you subscribe to the RSS feed and you get even so you can use this application you can also for example filter out categories that you're not too interested in it so the first purpose of this application was actually to notify on all slack when a new question was posted on the community from but this can be used for other purposes you can just extend the application and use it for your own purpose for mark Needham's blog maximus is blog etc while there is no science you have to parse so we have small Python scripts that use crappy for parsing the blogs and in the data so here in the example you see that it first is just into elastic but then we because we hold the data so we can reuse the data as we want and at least feed it into a new project so if I would check now the graph that we have I can check that a community topic but those are currently topic questions that are from the Bahji blog if you want to take this question you can just say where right so this is the question this is the information we have about the questions or the categories both are correct and we have the text of the question here and the title of the question right and we can all the source that we have in the database okay so aha unity topic max de marzi Magnum and YouTube channel so the goal will be to start from a question and see how using the other sources of data can improve and actually give some recommendations the first thing we're gonna do is while extracting the topics from the questions themself so on the community question forum there is topics but also tags so here there are no tags if I would choose maybe another question here or attacks check here we can see demo video and presentation tags so I'm gonna consider the tags and the categories as a topic so it can be used together for the recommendation so to do this we're gonna extract and split so the other tags are stored in an array so we can just unwind them and create a tag look at the topic notes so here I find all the community topic questions I don't want the categories I create the topic and emerge the question to the topic all right I will the same for the tax alright so now we can do tower graph so the pink nodes are the question the blue nodes are the topics so we can see already that we start to make connections between the articles the questions or the articles themselves via particular platform knowledge base quark well etc so categories and tag so in terms of our knowledge graph what did we do so the only thing we could do is take a source of data put the article from community in AFRICOM and extract the tags and categories into their own topics and relate the articles between between them there is however one small issues so the if we have in the graph is mainly coming from structure information that means that we could only use with the data that we have not the data that we think we could have so by data means that we know that they are mentions of a book and such procedures into the text but we cannot really extract them as it is because it's it's quite complicated I think secondly if I take an articles like from Maxim RT of or mark Needham's blog the promise that there are categories so if I go to mark leader mark medium well over read the categories that are here if they are sub will not match with the categories that are available on the community forum or they are non existent or they will not match so that means that we cannot recommend based on the cut on the topics content outside of the the committee platform so this is why what we're gonna try to improve as well so let's start with the first so no distinction for about a POC and procedures so how can we actually extract those informations from the text well we're going to use natural language processing tools and mainly if we think about natural language processing and its implication in knowledge graphs we can say that actually natural language processing helps for creating the relationships needed for a particular solution what I mean by that is that if we look at over here we know that one of our solution for creating recommendations is to create link between an article and specific notes of a procedure specific notes of DB procedures specific notes of features etc so natural language processing will actually help us to create those relationships into our knowledge graphs and if you know go after you know that we provide natural language processing extension a top neo4j so I will not go too much into detail but we have created actually the ability to do almost all natural language processing steps with the help of neo4j cipher procedure so here for example what I can do is I can take an article like or on article and I can say to annotate the text by the annotation what does that mean is that it will extract the sentences and the tokens from the text and apply some labels to those tokens well for example the token is recognized as a neighbor procedure or is recognized as a person or is recognized as a location or its sentiment is positive fixed like this so we're gonna run this here alright so now I will show the graph so to know that natural language processing tools are very very heavy based on linguistic features so for example if the word is a verb or if the word start with a capital letter etc so here we could extract named entities so face that are not just a word but I actually representing something in the real world and we can analyze those entities so this is or text the entry points to the NLP NLP graph and our sentence and tree named and it is recognized if we take here the first entity you can see that it is recognized as a entity miscellaneous so which means for us nothing actually will discard them so it's just that the linguistic parser recognize this is not just a word it can be some trick relevant but we don't know what same for Ebola and interestingly same for dollar categories it is recognized as money why because it's one token and it starts with a dollar so you can see that using out-of-the-box natural language processing tools January is not useful you don't have to use it in P just for the sake to use NLP you you have to know why NLP sometimes is not working or how to make also the natural language processing better so how can we actually improve this step is by training the natural language processing engine to speak the same language as we want as we do so the language we speak is a POC receiver DB procedure features articles topics etc the problem is that generic generic natural language processing engines will recognize generic entities we speak about persons locations money dates organization names which in our case is not very useful but it is because they have been trained on generic data set such as Wikipedia etc so how can we train our system well the training is a very heavy process so the first thing to have is a label data set that means that you need to take a dozen examples for example of text documents that are relevant to your domain so here we could take for example 100,000 articles from the community from and then start to manually annotated so if I take what this process is done I can take this question here again in a tool that helps for annotations create a method annotation tasks now can start to annotate the text so for example here I can say that full text is a feature I can say that unwind is a cipher clause I can say that DB dot index that full text query notes here is a procedure I can say that char along classical section is an exemption and I would do this for thousands or more than thousand documents and what's is done it creates me actually an example that I can use to train the natural language processing engine in order to recognize them in the text so I will not do it for thousands because I already did it actually so here actually I have more data sources from the Fuji where actually we annotated already this data and what the output is of this process is a huge file that represents the thousands of documents that you have so where each word is actually on one line with at the end a label here when it is o it means there is no the identity and here you can see there is a POC that means that it is recognized as a a POC entity etc so they will become actually distinct notes I mean an 11 into our graph so you can imagine the size of the text for 3,000 documents but this is not uncommon to have a million documents for example so a January especially with machine learning tools more data if you can have it's it's better so and we can reuse it into our graph so in order to understand how it is represented afterwards we can take an article and find actually some articles here I will maybe zoom out a bit so the green nodes are procedures so here I can change to value and the that's it precedence so you can say here right so those are sentences while there is a procedure inside them that means that we can already start to collect articles between them that use the same procedure names so if we would take actually our current example article I could take my article the ID 1 2 9 1 find the whole path so this is a bit worse but this is where we need it because we hold a lot of context of the words in the sentences themself so I find the the occurrences of procedure of everything that is relevant so procedure Apoc etc and are which are fine again that's a recommendation wire there is the same mention of the same procedure in their sentences so if we run this query we can see that for the question as an example this is the first recommendation and similar procedure that are returned so here I could add a distinct actually all right but just extracting namely entity from the text we can see that we can actually already return relevant similar articles that the user can read now I want to mention something about named identity so under the hood each use machine learning algorithms such as CR CRF for example the problem is that sometimes it's not useful to use machine learning based argument so why especially I mean the best example I can give is that you want to return only the names of the current and BA players while with the problem of machine learning I mean it's not a promise a good thing but is that it tries to generalize so that means that if you annotate even if that document for example unwinds put text it will try to recognize other words that could mean the same thing and would map it to the same concept so if we take the examples of the NBA players that means that it really recognize other persons in the text with sufficient data it will generalize and recognize other person that are not NBA players it just because natural language processing is based on linguistic features so in that case and probably in the interprocedural names for example we would be better off to use a regex based parser for example for extracting automatically the entities from the text so however for the concepts full text we could generalize etc so yeah this is something to think about when you do natural language processing it sometimes machine learning is not the solution ok so if we take back again or graph here what could we we could do the source connected to an article the topics and the connections to epic procedures DB procedures and features however one thing we couldn't do yet is connecting Mack Needham blog together with topic because there is no such topic into into the content so here again we're going to use some natural language processing tools and a bit more complex but kind of easy to use with our plugins it's called document embeddings so if you're familiar with water bag of water repairing if you know how how it works is actually vector a word that contains a vector embedding meanings about this word so for example if you visualize on a on a vector space model the word king you could see that it's very similar to Queen the word man is very similar to woman such as Apple is related fruit and apple with a big a is related to company or to iPhone for example but this is great for words the problem is that if you compare just words together there is no meaning about sentence or during world ordering in a sentence etc and this is where you can use document embeddings so you will actually pass the whole sentence where each word in a neural network will feed its own water-bearing so this has a huge advantage in the fact that for complete sentence you can find similarities between texts that don't speak the same language to give you an example because this use word and bearing the sentence the president flew to California last night will be very similar to the lull Trump arrived is on Francisco just because it used water beddings and those words carry out meaning with them and they will be able to to recognized sentences even if they don't speak the same language and it crowd it works as well across languages so that means that you could find a sentence and find a similar article in another language for example because modern bearings are provided in multiple languages so how are we going to use document and bearings to do this well the first thing is that we have to create a training a train model for the topics I will show quickly here we're going to create a model that will take first every article once again community question risk topic sorry that's it I will take every article will related to a topic so discounts because we only have this relationship for the committee questions we have one question and one or multiple topics then we will return the text of the question and the topic name as the label or the class for our training model so at the end of this output we will have a model where for each label it contains all the document embeddings of the text that were related to that label so and once it will be built we will be able to request to that model with new text while this in which topic this text should be classified so what we will do will create a model based on the community questions then we will take the articles from the mark Needham's blog ask the model predict me in which topic this should be classified and we will create that relationship so let's first create the model it will it will take a bit so approximately 50 minutes I'm kidding so yeah let's see we have to wait a bit so maybe start four questions or if they are yes someone was just asking what is the tool to your sharing so I guess maybe you can share the Hume Hume page yeah so this is a new maps so you mr. commercial software where actually we add interface atop or plugins and also some enterprise features of up against but especially the labs helps teams to annotate documents for creating models specific to that domain so when I speak about teams is that we we have use cases where annotation centers for example hundred thousands of editors and they create squats four hundred eighteen different concept so it can be and it can be entities but also relationships between between entities and if it generally they will assign the same documents to some squats so they can see the conflicts or some conversions between the understanding of the same text cetera because language is is very very complex sometimes the same word doesn't mean the same thing to the same person so this is why there is the importance to have overlap between the documents annotations between different person so managers I mean the reviewers of the annotations can detect conflicts and and do feedback retrain their team based on the language I mean this is very sometimes very complex so yeah so this is this is on absorb got three more questions now I represent articles is a series of word embeddings or a single document compelling by Zack Zack asks that question so what the text of one article is one document America but the classifier so the model train at the end it's not based on the its base path class right so one topic will have one be compelling for all the documents sorry so you can actually because they are vectors we can even store those vectors I mean I have a some left over test or buy on my database but for example you can see here I take a random article there is also the doctor vac which is the name of the other when we store the vector so this is the document and burying a vector and and because they are vectors you can use for example Cassini severity to compute the similarity between two two documents based on the vectors which is which is by I was just wondering if you use the similarity analysis in the process I guess not look at so I will come later to the similarity but we don't use it here yet so I'm waiting the model to finish but it's a tradition so I'm free normally it was six minutes effective but I've some some chorus from Rodrigo do you have an integration with chatbots with human we do it depends what type of chat but it for example we don't generate response so that means that the architecture of the database has to be where we have a very open dated way of storing data or chatbots but we use natural language processing for chatbots as well but you have to know that shut but is not easy there is no out-of-the-box solution that can do chant pots on your knowledge graph without having to to fine-tune a bit so what we do we have capabilities for intent detection for content retrieve them based on the question etcetera so if you look actually on YouTube or maybe on YouTube you can see a couple of a presentation we did about using chat BOTS with foj and FP and also I showed during our tour last year an integration with Alexa for example for voice based questions to the garage so yeah we do but it's I mean in the in the real world it's not so easy how it there is there is functioning to do but yes we okay next question since I think you shared so far you've shared doing annotation NLP and attention do you do you have any other methods that what about what about annotation or vote oh yeah what the question is was annotation the only NLP method is implemented or are there other methods oh no there are plenty of methods so if I mean I cannot show everything today yeah but if you go to let me show you for a maybe I can also share this it's linked to people go to new for jnlp you will find in the autumn in in the open-source version a couple of features so we have keyword extraction we have sentiment analysis part of speech stemming topic modeling we fancy a we have talked to back we have k-means okay table lot of stuff so yeah okay oh it's really it's done okay yes are you creating a classification prediction model EG new on there on top of the doctor back embeddings re doing a matrix factorization into the topic space SPD now from Zack again it's using purely the the prediction based on doctormick active so yeah so we don't do additional stuff actually the library we're using it's pretty pretty well furnished so there is all the filters for for documented bearings or so k-means over those and barracks etc so it's pretty it's particularly so and we actually the good thing we've documented biddings is that even when you have small amount of data per class so small amount of examples you you can stand still have low but good accuracy while if you're using much more model such are for example Facebook provide as well for text classifiers with some small amount data set they will provide zero fix the it will even corner because there is not enough examples for example so it's pretty great to to use doctor back in in the real world actually because having dozens of examples it's not always the case to all the people so sorry so the model is finished so now we can we can actually get the topic prediction so I will take some some blogs from MacKinnon a website I will especially constrain to containing neo4j for this example curry why because mark wrote Cavanaugh I mean a couple of a good amount of articles that are not related to especially tuna fillet as well and we take five and then I will ask the classifier to return me the label and the score so let's check here so here it is the title of marks Needham blogpost this is the level to which it was classified and this is the score same here analyzing CSV fight this cabinets a crustacean coughing my name etc so of course the projection are we can look world but again it depends on the content that was posted in each of those categories on the community farm so the model is the accuracy of the model will depend on the accuracy of the classification used manually by the people on the community forum so if a lot of those are wrong or there is too much actually content going into community content and blocks category etc this might actually influence the quality of your model right so now let's do this for all so I'm gonna use a Bock here so I'm gonna take all my items were post find actually two possible labels and get the prediction and then I will store that prediction how are we store it I will just create a relationship between marked items low note in topic ml I will make a distinct relationship type because it's useful actually to to actually decrease the score if a recommendation comes from that side for example I mean whatever logic you want to that topic all right so it started and I prepared something to show while it was running so here there are already some question I mean because we're using a polka dots iterate the transaction are already committing in the background so we can actually already check some results so here we can see my rhythms color driver it will be classified in this part topic from the committee from and this is also related a related question or article on the community from that is related to that marked items block so now if we we can take other things color federal de beyond logic is related to break them yeah so it again it's a may be clustering it depends on the content it depends also again about because it's yeah it's just that this appears to be Antonio JDeveloper block okay for example so then if we want to visualize this much more at the graph we can see here so this is here this is a community topic anyway no it is not okay this is a topic this is a community question or content and this is mark Needham block alright so we can see that with the help of an opinion different we could actually make those relationship possible alright so other questions before I continue yeah okay so I'm waiting impatiently it stops okay let's check quickly the status March and that work okay Amazon all right it's 1391 I know it's a Muslim what we gonna do the same with box de marzi but there is less data so it's gonna be good my juice is average price size is probably longer yeah you you wrote it more than ten years ago so yeah I used to write I used to write a lot okay hope he's finished okay so no it's running for max but it's gonna be fast so let's take quickly again the quarry for you here okay so let's maybe here fault works let's tickets to work and that text contains foj and okay so you can see here the rising CSV actually that category is leading to some strange what is it doing out there now posts of mark against pastes and max all still here no max is finished so now I'm gonna actually take some content for from max the same as you so you can see here this is community topic a topic this is max blog post alright so you can see the relationship so we can maybe return 25 or hundred of them all right so this is box post postmark bus poster we know the maximum Jonah right so so this is the topic now if you want we can see the connections between your blog post and max book costs for example alright so here what we're gonna do is we're gonna take your topic classified your blocks that specified in the topic and same for max I just gonna filter out some to generic categories actually because that it leads toward stuff so we can see here's the topic name your blog and max know Mars is broke per we can see here Java JDBC driver on yours case flight search with neo4j Java Java flight search we know that max looks to write with the Java API so so generally everything Java will need to do max stuff but again your blog about JDBC or Java about spark Python etc so this is the connections between between you and if we want to actually see for example with the articles we took here if we would say ok in that box we want to recommend also articles coming from the outside of the platform we could do it now easily because we can use well the classifier okay we're actually based on that question this is max blog post that would be returned based on the classification so here in a bleak full-text search finding - pestering graph based search solution to slice your own data as you saw which is quite relevant because it's about full text search and yeah and similarly we can use so we can use it to one way we can use so that way we can use the named entities recognized way so here this is another article recommended and why and that's it we can also I want to show so about those similarities so let me check in my story yeah okay start here so let's say I want to take two articles so let me do this I will return the ID and title ID okay so this ID we go for example I will return that one so I take the vector of the question I take the vector of the recommendation back I return gravity's between those two our music for that the array property that we saw earlier return similarity but we can see it sooner add 25 person so you can do this actually for all the I think so you can store the most similar articles as well between the notes so I could store and store the similarity other questions got a couple okay so first one is do you have any experience assessing or signing truthfulness the claims of accuracy / legitimacy to opinions Hey so it is a this what fake news discovery yeah I guess yeah we do again is that I mean the structure of your graph is different that what we see here so if you buy storing a lot of information so as I say the graph is pretty verbose now we can actually matter realize the garage so now if I want to say I will create a load person I will create a node oppa so we'll just deliver a book and I will create a relationship between a article and a POC and on the relationship we saw actually all the facts so all the articles that make this relationship possible then you can you can start to evaluate the accuracy or the truth of some some facts so also if they are contradiction between facts sometimes but to be honest this requires also relationship extraction generally because you need to recognize the relationship in the text so let's say for example I saw John writing and you can make a relationship because there is a linguistic relationship John drives Ferrari for example and then you would have another fact that say John doesn't have a driving license so you have to combine machine learning with some human base forms actually then fully much fully automated is quite complicated but yes it's it's a it's a use case we have okay one more question that's there right now is that the doctor next thing you said is that in the near for jnlp pocket or is that only in this it is in you but talk to whack Java is available open source I mean you and so if people want to like use some of the stuff that is in human I what's the approach to doing that they can ask fort while we can send our limited times like I think I said humanoid overcome is that the best place to go is there a better place uh the best pastry would say send an email to info okay good yes I guess we're pretty I mean we can start to I would just make one will cry the only doing it if you've enjoyed this and I guess by the comments a lot of people have enjoyed watching this don't forget to like the videos that other people other people can confine them Christoph's Kristoff's working and we've included links to the repository and to the hue tool that Chris has been demonstrating so don't forget to to go and visit those if you haven't looked up in the chat so you can try it out on your own data well I Kristin writes a lot of stuff as well so we can certainly link to some of some of that as well so what's our last term and then yeah I just again we if you look at the chorus before sorry I will take the other one yep this is the query that we we know get those this is the results that know we are with NDP and knowledge was enabled so we actually is pretty relevant based on the question and this is the results that we have by using actually only the structure features that are available in the data where actually went is this wrong wrong features we get wrong the results I could so this is what I wanted to point out as a mask right cool cool all right thanks mark thank you taking the time and thanks everybody for watching we'll see you next time Thanks all right 