 Hello, all aspiring and actual roboticists to R cubed! Today we want to investigate how mobile robots are able to navigate through crowded places. The field of autonomous driving is gaining more and more important in today’s research and several companies have already achieved enormous success in developing cars that can drive without any human conduction. But not only road traffic will benefit from autonomous vehicles. Autonomous driving also finds its way into our everyday life, since robots are increasingly populating our human ecosystem. With robots entering our daily tasks, their ability to perceive, understand, and act in a socially conform way is becoming a key requirement for many application scenarios. Just imagine how we could benefit from vehicles such as autonomous self-driving mobility scooters or even autonomous wheelchairs for patients or disabled people, which are even able to drive through urban environments. But how can we teach such vehicles to drive safely and avoid bumping into objects and people? Just as drivers follow certain rules by considering traffic lights and priority rules, most pedestrians do so as well when walking through the city center: Avoiding collision with other people or pass to the left if the person in front walks to slow. These are ingrained social norms - an unspoken set of rules that pedestrians follow. And while these rules might be simple to us, robots usually don’t have the social awareness necessary to follow these social norms. For a robot to move at all, it must be able to do four things: first, it must be able to perceive its environment, then figure out its location, to plan the route it wants to take to its goal, and finally actually move down that path. Engineers at the Massachusetts Institute of Technology (MIT) focused on the planning step, by figuring out a pathfinding algorithm that enables the robot to navigate through a crowded environment. Furthermore, they used off-the-shelf sensors such as webcams, a depth sensor, and a high-resolution Light detection and ranging (LiDAR) sensors, as well as open-source algorithms to determine the robot’s position. The research team considered Reinforcement Learning, which is an area of Machine Learning, to teach the robot social norms. In Reinforcement Learning the robot is modeled as an agent that can act in and perceive its environment. When the agent takes an action, such as driving in a specific direction, it receives a user-defined reward that could both be positive or negative. Obviously, the agent aims to maximize its reward on its way to the goal. During the training phase, it will try to find the optimal policy, being the steps to be taken that maximize the accumulated reward, by consequently receiving rewards and adapting the previous strategy appropriately. After training, the robot now has learned an optimal policy for its task. In our case, the final policy provides minimal travel time of the robot, and at the same time respects the constraints of the robot’s kinematics and the fact that the robot shouldn’t collide with other agents. For the new algorithm from MIT, negative rewards are given for certain unwanted behaviors. For instance, usually, pedestrians overtake others on the left, thus states in which the robot would pass by a pedestrian from the right are penalized. In this way, the unstructured and unspoken norms of pedestrian traffic don’t need to be defined explicitly for robot navigation, which is very useful because of their complexity. To address the problem statement of a multi-agent environment, the movements of the surrounding pedestrians have to be taken into account for modeling the path of the robot. MIT decided to approach this with the use of a neural network. This figure showcases the architecture of a neural network for four agents whose state representatives are used as the input values. The blue input blocks illustrate the three nearby agents of our robot. All connections from the previous to the subsequent layer share the same weights, meaning, the states of the three agents can be swapped without affecting the output value. This is an important constraint for the multiagent system’s symmetrical structure. After two of these symmetrical layers that capture the state of the agents, one Max-Pooling layer is inserted. This specific type of layer is able to select the most important features of the system in order to remove noise and have an aggregated representation of the states. Finally, two fully-connected layers are used to collapse the high dimensional feature vector into a one-dimensional scalar value that determines the policy. As activation functions in the hidden layers, the researchers decided to use Rectified Linear Units. Additionally, two experience sets were used to train the algorithm to distinguish between trajectories that lead to the goal perfectly and those that ended in a collision. Thereby, the learning can be improved by focusing on the scenarios that fared poorly for the current policy. If you want to know more about how an artificial neural network works in more detail, let us know in the comment section down below or check out the links in the description box. The mobile MIT robot is able to navigate through the sprawling hallways of MIT’s Stata Center in Cambridge while maintaining a speed of 1.2 meters per second which is similar to the humans’ pace. The robot traveled to a goal over 50 meters away, with an average of approximately 10 people coming within 2 meters of the robot every minute. Tests showed that it was able to keep a safe distance from pedestrians and generally respect social norms. A safety driver was walking approximately 5 meters behind the robot, but their help was never needed. This innovation clearly offers a promising avenue for further research and experiments on autonomous vehicles in crowded environments, which could then help for example paralysed people in wheelchairs. All you’d have to do is get into the chair, point where you want to go and it knows how to drive you autonomously and safely without the need of a person to push you around. We here at Roboy have also been working on getting Roboy to drive around a university campus! Today, Roboy Team created history! Because for the first time in the worlds history full scale humanoid robot was driving a rickshaw! My name is Lennart and I am the team lead of the autonomous driving team at Roboy. In order to navigate around obstacles we need to create a map - for this we’re using simultaneous localization and mapping! Once we have the path on the map we need to control the bike to stay on the path. For this we’re using PID controllers to control the bike and the steering angle. The Roboy project is an open source project with the goal to build a robot as good as the human body. We created an open platform for robot development that unites researchers, companies, students, and artists from a broad spectrum of disciplines. Roboy means full-stack robotics: from mechanics to electronics, to control, to cognitive systems. Artificial intelligence is everywhere, and we work on all levels – to make Roboy truly intelligent. The Roboy team aims to build better, more useful robots by adopting principles from biology and to help humans with musculoskeletal limitations through better prosthetics, better training, and exoskeletons. So, if you are interested and want to know more, check out our website: roboy.org or the GitHub account github.com/roboy. Thanks for watching another episode of R Cube (R3). See you soon and stay curious! 