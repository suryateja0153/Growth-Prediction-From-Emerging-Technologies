  SIDDHARTHA SRINIVASA: Hello, everyone. Welcome to Robotics Colloquium. This is a colloquium where we meet weekly to hear talks by distinguished speakers. Today it's a real pleasure to introduce Caelan. Caelan is a PhD student at MIT, working with Tomás Lozano-Pérez and Leslie Kaelbling. Caelan's does some really exciting work in multi-modal planning, which is sometimes called task in motion planning, a term that, we both agreed, we dislike extensively, so we'll call it multi-modal planning. But just the idea of being able to deal with discrete and continuous spaces at the same time. As soon as your robot picks up a coffee mug, that coffee mug is picked, and now the entire world changes for it. So being able to deal with these really discrete changes and how the world behaves when you interact with it is something that Caelan's been working on for quite some time now. I think the first time I heard of Caelan was Peter [INAUDIBLE],, Leslie, and I had a large argument at an NSF program meeting about who would actually win the lottery to be Caelan's PhD advisor, and MIT won. I hope Natalie. But Caelan's done really remarkable work, even before he started his PhD, and has only continued along at an exponential trajectory. So it's a great pleasure to have Caelan. Caelan will be with us until the end of the day, so if you have questions-- CAELAN GARRETT: And tomorrow. SIDDHARTHA SRINIVASA: And tomorrow, too, so if you have any questions or want to meet with him, please do talk to him after this talk and feel free to reach out to him yourself. So without further ado, Caelan. CAELAN GARRETT: Thank you. All right, thank you all for having me here. Like he said, I'm Caelan. I work with Tómas Lozano-Pérez and Leslie Kaelbling. And today, I'm going to be talking about my work with them, as well as with Dieter Fox and [INAUDIBLE],, and it's all going to be about robotic planning. So we're interested in actually enabling our robots to autonomously reason and act in very large unstructured, or semi-structured spaces. So these may be things like household environments, where you might know maybe the geometry of the actual kitchen and the types of objects we might interact with, but the actual initial positions of them, as well as the task goal, may change. And warehouse fulfillment environments, so trying to then flexibly deliver packages of different sizes and shapes and best allocate robots for doing various manipulations that are required. For a food preparation course, making sushi, I think, in this little example, but we're very far away from that, of course. And even construction. So here, we have two ABB arms that are mounted to the ceiling, and they're trying to assemble this large wooden structure without causing it to fall down or colliding with each other. And so the way that we actually frame all these problems is as a task in motion putting problem. As Sid said the name I think is a little bit misleading. So don't worry about it too much. Really, what we mean is we're going to be planning in a hybrid system with both continuous and discrete variables and control actions that's often very high dimensional. And so the type of variables we'll see here are configurations of the robot, so the joint angles of the robot's arm, and maybe actually the 2D pose of its base; poses of objects in the world; the joint positions of doors and drawers. And some of the more discrete notions we'll have as state variables are whether the robot's attached to a particular object or not, whether something might be cooked, something that's a little bit more like an abstraction over the state of a pot, for instance. And also relationships that are derived from inherently continuous quantities, like we can actually formalize the notion that something is on something else if it's at a pose that's known to be stable, essentially, for it. And we'd like to be able to develop algorithms that can actually react to a wide variety of different low level manipulation primitives. So this includes moving, picking, and placing, of course, but also more dynamic skills, like pushing, stirring, pouring, observation actions, like detecting, and potentially further actions down the road. Here's an example of us solving a typical task with our PR2 robot. Here, the task is to cook the broccoli and serve it to a human. If you couldn't tell, the broccoli is the green block. So the stove is on the right and the plate's right there in the middle. And if you notice, the robot actually initially uses its right arm to pick up the broccoli and places it at an inconvenient location. And that's because the left arm can't actually reach all the way to green block's initial pose, and so the robot is forced to actually perform a handoff to actually achieve this task. And the key thing to note is that the robot was not told at all that it had to do this nor was it told explicitly which values would be acceptable for doing a handoff. These are all things that it just covers automatically via this knowledge about its capabilities, essentially. This is another example of the robot actually stacking. So here, this one's a little more simple, but still, the robot needs to actually plan motions to safely perform the stack that don't cause it to topple over while during execution. And of course, I said that we'd like to be able to go beyond picking and placing, and so we're starting to look at trying to integrate with pouring and scooping skills, to prepare coffee, in this case. And so here, we have our coffee in our blue cup, and we're going to get some sugar from the bowl and dump it. And then, of course, to ensure that it's really nice, we have to stir afterwards. The stirring's a little hacked, but the rest is actually pretty good. And then we have coffee. And there are essentially constraints when actually manipulating the spoons and cups, and that the orientations must remain in a certain set to prevent spillage during carry. And so these constraints actually need to be accounted for when actually planning the motions of the robot itself. I've also been doing some work with some collaborators in MIT's architecture department that are interested in using robots for 3D printing. And so on the right is a KUKA arm, essentially, with what is effectively a hot glue gun that extrudes plastic, and the goal is to make the structure. And these problems are quite challenging, but for different reasons. So in the previous problems, the robot was not given the all the exact state samples it needed to actually achieve the problem. Well, here we have exactly the structure. However, here, the planning horizon is much longer. This structure requires 306 printing extrusions to actually complete. Additionally, in addition to geometric and kinematic constraints, there are stability and stiffness constraints in the structure itself, that what can easily happen is that if you build it in a strange ordering, it might deform during printing, and the resulting product will be garbage, essentially. And so this is another example of a robotic application that has a variety of interesting constraints that are all very affected by decisions you make. And more recently, been trying to make algorithms that are super fast, that can actually solve the full trajectory within around 10 minutes, and can print cool shapes like the [INAUDIBLE] bottle, actually. And one of our hypotheses is that we can actually do a lot of the same planning using the same algorithmic tools. So we don't want to reinvent the wheel for every new robotic application that we discover. So to give us some background on task and motion planning, unsurprisingly, it is drawn from task planning and motion planning. In the AI world, people call task planning classical planning. And really, the key idea is that we want to plan in a large, discrete, and finite state space that's composed of many different variables that each can take on some values, essentially. And so oftentimes, these problems are represented using a planning language, such as strips or PDL, Planning Language Description, or Planning Domain Description Language. But the syntax is actually not as important as the idea of just planning very large state spaces. That's the Cartesian product of values of each individual variable. And so for the more scripts-like representations, typically state variables are Boolean facts. That can be true or false, like whether something is on something else, in this example, or whether what's holding a block, whether a block is clear, which means that there's nothing on top of it. And we write down the dynamics using these action descriptions, where we have action parameters. So for a stack, the parameters might be the block they'd like to stack and the bottom block you like to place it on. We have preconditions that actually must hold for the action to be executable. Here, you have to be holding the block 1 in order to stack it and block 2 must be clear again. And we have effects that actually modify the values these state variables. And so again, one of the examples is doing blocks world, but note that here, there's no geometry at all. It's all completely discrete and relational. However, the AI planning community has actually done a lot of good work that actually is able to synthesize greedy best first search characteristics that can enable algorithms to much more quickly solve these problems than like a vanilla breadth first search, for instance. And so really, having this structure and having the factoring of different state variables and action preconditions and effects enables you to do a lot more, in terms of discrete search. Of course, the second part is sample-based motion planning, which is quite different in that you're searching through an entirely continuous space, essentially. And really, the key challenge can be actually scaling to high dimensional systems that actually may have like seven to 13 degrees of freedom, for instance. The most relevant part of motion planning that we'll actually build on is sample-based motion planning. In particular, we'll make some comparisons to PRMs and lazy PRMs. And the high level strategy is this, is how you take a planning problem, where you have an initial configuration and a goal configuration, and a set of obstacles that you can't collide with. Oftentimes, algorithms first discretize the space, either deterministically or randomly, by sampling a bunch of configurations. Typically, then algorithms connect up configurations that are close together and admit safe paths between them. And then, basically, you have a finite graph and you can perform any graph search you want. And there are, of course, very many algorithms that have been developed for this, even many algorithms that are more recent than 2011. So task in motion planning is very tricky, first because it inherents the difficulties of both classical planning and motion planning, that we have this high dimensional continuous space, and also the state space itself grows exponentially in the number of variables, that every time you add a new object, then it may be in a couple places at a given point in time and the number of states that's actually in your descretized state space can grow to be a very challenging thing to search. Additionally, all of these tasks are often very long horizon tasks, where we want to do not just one or two motions or manipulations, but we like to actually do a sequence of activities that actually cooks a meal, for instance. Something that's actually a bit more specific to task and motion planning is this integration between the underlying geometric states, or continuous state, in general, and the high level strategies you can do to actually solve the problem, that you might have a problem like this one, where there is a cup that you'd like to serve to the human, but there might be a movable block in the way. And so because the robot must actually grasp the cup with a side grasp in order successfully pour, and the cup is near the edge of the table, outside of the robot's effective workspace, it needs to move the green block out of the way to some other post that's safe before actually picking up the blue block. And we like our algorithms to be able to solve for these kinds of constraints automatically with no human input. But these constraints can be much more exotic than geometric and kinematic constraints. Again, stiffness and stability, when you start to think about observations, visibility is actually critical. Torque limits, joint limits. And so really, just anything that involves interacting both the actual control family that you're taking with the underlying state is something that's very specific to this integration of discrete and continuous planning. And so here's an example of the robot solving this task. It's also supposed to actually pour the material in the cup into the bowl when served to the human. But as you can see, it successfully finds a pose for the green object and is able to do the pour. And the robot also plans this very nice [INAUDIBLE] trajectory that goes underneath the table, but that's OK. AUDIENCE: How long did this one take? CAELAN GARRETT: 15 seconds. But sometimes these kinds of constraints can be quite subtle. Sorry, one second. There we go. OK. So here, the task is to stow the SPAM in the top left cabinet, and it starts in the drawer on the bottom. And so of course, in order to do that, the robot has to infer that it must open the door of the cabinet. But it actually does something very subtle right here. It performs a re-grasp. In this setup, it's very difficult to grasp the object with a side grasp in the drawer, but it's also not kinematically feasible to do a top grasps up above. And so just from the notion that it's able to sample values that are on different services, it's able to find one in which it can do this switch and successfully complete the task. Additionally, because the kitchen we like to keep tidy, this problem's actually non-monotonic, in that it has to open the door first in order to do this insertion and then redo that action. And so these kinds of puzzle-like activities can kind of come out pretty frequently because of many robots' limited capabilities, in general. And again, we'd like to be able to have algorithms that can actually take advantage of their ability to do these re-grasps all on their own. So there's been a lot of work in this area, especially recently, but it's usually stemmed either from purely the motion planning community or purely the AI planning community. For motion planning, there's been a lot work in multi-modal motion planning, which is like extending motion planning to be roughly a sequence of motion planning problems, each defined by a mode, which defines the effective configuration space of the full system. These algorithms are really quite general and good, but they lack any factoring, that really, most of these, actually all of these algorithms represent the state as one big entity that can't be broken apart. And so you can't think about individual pieces of the state, such as the state variable for the laser pointer, independently from each other. And because of that, really, all you can do is just a brute force search through this space. And because the branching factor is huge, very quickly you end up reasoning out a bunch of things that are completely irrelevant in a common manipulation environment. From the AI planning side, there have been some works that extend discrete AI planning by allowing something called a semantic attachment, which is basically a test for whether something is feasible. And so one could make a test to see whether there is a trajectory that goes between moving to an object from one place to another place. However, all this work entirely assumes that the state space is finite. And so the robot itself can actually choose the placements, the grasps, the configurations it would like to stand at in order to solve the task. And so it's very limited in its applicability in robotics environments. There've been some work to actually try to extend semantic attachments to deal with a samplers that actually can produce values for arbitrarily long and actually solve very general tasks. However, a lot of these approaches, including one of my own prior works, are pretty inflexible to new domains. If you were a user and you had a new robot or a new manipulation task, you'd basically have to rewrite the whole algorithm in order to actually apply it, which is obviously not ideal. We'd like to be able to have some sort representation for a problem that we can pass an algorithm and then have some properties about it that will guarantee that it will find a solution, at least if one exists. And that's really the first contribution that I'd like to bring up, is that we really want to try to produce one of these representations that will enable us to make general purpose algorithms. And so we call our representation PDDLStream. It extends playing domain description language, and I'll get into why that's interesting at all in a second. But to start with, I think one thing that's undervalued in the robotics community that's valued a ton in AI planning is that it's really cool to make descriptions of problems that are extremely general purpose. But PDDL itself doesn't require planning for a robot or an autonomous system, or even just one single entity, that you can actually formulate a ton of different control problems for a system, all using the specification. It does happen that people oftentimes care about playing for robots, but it is itself entirely generic. And so we'd like to be able to have the same type of generality when actually developing our own representations and algorithms. The key contribution we'll provide representationally is to extend PDDL by allowing the specification of sampling procedures. And these are useful because, first, they allow us to actually model domains with infinitely many actions, such as continuous domains. So we can start to think about generating many different placements of objects that are relevant for some sort of pick and place task. Then additionally, we actually provide, again, as promised, generic algorithms that can operate on this representation and find solutions, all while treating the actual implementation of the samplers as a black box, so with no insight, in terms of what is actually going on inside of it, other than a very small declarative specification that I'll get into in a second. And so as a user, all you have to do is specify the samplers and the actions that the system can take, and the planners do all the rest the work. And we can show that we have some nice theoretical guarantees, such as probabilistic completeness, if the samplers that we provide cover the right constraints and densely sample values in each constraint. So getting back to why we chose to extend PDDL. So again, one reason that's very straightforward is the fact that the AI planning community has put in a lot of work to make very efficient search algorithms that operate on representations that have a factored state and/or factored action preconditions and effects. And so as soon as we extend this, we can directly use all their algorithms, which is good. But really, the key thing that's more critical to robotics is understanding how these representations actually enable the parametrization of actions. So when you write down an action in one of these action languages, really, you're not encoding a single edge in a state space graph. You're encoding the difference between two states, essentially, that if I were to pick up this object, you could use this action, potentially, in a state in which this thing is over here versus over here versus over there, all with the same control parameters. And additionally, a lot of times, most of the state doesn't change as the agent interacts with it in many of our applications, that again, a robot only can usually manipulate one to two objects at a time. And so a lot of the state, if we assume things are quasi-static, remains the same. And so what these languages allow you to expose, by modeling this delta between states, is the fact that these changes can be represented with very few parameters. And so in our blocks [INAUDIBLE] the environment, if we're trying to stack an object, we only need the names of the blocks that are involved in the stacking. Everything else is irrelevant to that actual action itself. And by being able to represent these types of transitions using few parameters, we'll be able to sample values for them that apply in many different contexts, in many different states as the robot interacts with the world. And we actually can visualize this under the lens of constraint networks here. So here is an example where we have just a pick and place task. And we have five action types, Move, Pick, Move a Holding, Place, and then Move. And here, we're visualizing the different constraints on each state as it evolves. And the key thing to notice is that each constraint itself only really mentions a couple of these variables at a time. Like there's no one joint constraint everywhere. Everything's very factored and sparse. And additionally, there are only really seven free parameters in this whole problem. Most things don't change. Like if you're not planning to pick up object B, then it's always going to be tied by equality to the same value. And so by exposing this decomposition and understanding when different values are entirely constrained by equality, we can start to expose the true dimensionality of the action space and effectively sample relevant actions. So throughout this talk, we're going to use the very simple pick and place example as a motivation. And so here, the goal is to place the blue block in the green region. And the key thing here is that the red block is obstructing any valid grasp of the blue block and must be removed. And we're actually going to tackle this problem mostly in 2D to start with, and then we're going to look at how this actually applies to 3D. But the key insight is that it's about the same, really. The dimensionality of the inherent qualities doesn't matter as much. It's really about the same relational form, and the only thing that changes is the samplers you specify, that instead of doing trivial inverse kinematics, you use an optimization-based algorithm to find kinematics solutions. And so this is what our 2D example is going to look like. Here, the goal is for block A, the red block, to be within the red region, and the B block obstructs the placement of A. And the robot is effectively a vacuum gripper that can do top grasps. So one of many solutions to this problem, of course, is to pick up B, place it somewhere else, pick up A. And one thing to note is that there are infinitely many solutions here, that we could have placed the B object here, here, here, here. Actually maybe not here, where the A object started. But really, the robot itself is what's coming up with these values to solve this problem. And so we can try to actually model this domain using just as much of existing PDDL as possible to see exactly where we need to extend it. And so starting off, we can declare facts about the world, so things like types, such as A is a block, red is a region. This numpy array, negative 7.55, is a configuration of the robot. And really, the only difference from standard AI representations is that these are programming objects. They're no longer just strings. But otherwise, the same stuff. We're just describing a mathematical formulation of something. We can also define the fluent facts in the world, and these are basically the changing state variables, such as the configuration of the robot and the poses of the blocks and the state of attachment of the gripper itself. And finally, we can describe the condition for which this is a successful execution, and that is that it succeeds if there exists a pose that A is currently at and that pose is known to be contained within the region red. So so far, so good. And the last thing is we just need to define the actions. And again, these look very, very similar to the blocks [INAUDIBLE] ones, except for the key distinction that the values that are the parameters to these actions may be continuous objects themselves. But we can do the same thing. We can make a move action that considers the initial configuration of the robot Q1, a trajectory that it takes, and Q2, the resulting configuration. And you can move, if you start out in the perfect configuration, and then once you move you're no longer at the previous one and you're at the new one. And we can describe pick. And so here, pick takes on the name of the block that we're going to actually attach to, the initial placement of the block, the resulting grasp, and the configuration of the robot. And we can perform it if the robot has that configuration, if the block is at the perfect pose, and the hand's empty, of course. Yup, so it's all the same stuff so far. The only thing that we haven't really talked about is, first, what do these predicates mean, motion and kin? And where do we actually produce values of them, essentially? And so if we're just magically given a bunch of appropriate values in the world and we know that they satisfy these relationships, it turns out that the problem is basically done, essentially. The problem's entirely finite here and we can just do any regular search algorithm here. And so we can take our fluent state, and I'm only visualizing the true fluent facts, so the state variables, effectively. We can think about a movement action that moves from the robot's initial configuration to 0, 2.5. We can search moving another way. We can even get back to the first place we tried to move. And we can start to pick and place things. And really, again, once you have all these values, it's just a standard graph that you can search. Really, the tricky part of all of what I'm going to talk about is getting these values and actually understanding what constraints they satisfy. If you remember, so the robot's only given the top value, so its initial configuration, the initial placements of the objects. And here, give it two grasps, but these things could also be generated from some sort of sampler. Yes? AUDIENCE: Is there objective functions [INAUDIBLE] robot, or is it just finding something that satisfies these constraints? CAELAN GARRETT: I'm going to talk about cost-insensitive planning for most of the talk, but yeah, you can actually specify a cost function for this formulation. And when we talk about trying to plan and execute, it will be actually critical to find plans that are both not costly and are likely to succeed. Otherwise, the robot will be aimlessly doing things about the world. But for simplicity, I don't mention cost here. So in order to use the same high level plan that we had before, that means that the algorithm must find a placement of object A that's in the red region, a collision-free placement of object B, with respect to the one you choose for A, four kinematics solutions that allow the robot to each pick and place, and four trajectories to move between them. As I mentioned, the way that we're going to try to actually produce these values is via sampling. And so one can ask now, what samplers do we need? And this story already is a little bit complicated. First, it's because, for some of these constraints, they inherently are on lower dimensional surfaces of the ambient space itself. Like if we think about stable placements of an object on like a stovetop, that's in 3D, a 3D manifold within CE3. And so we already have to know a little bit more about the geometry of the world in order to actually be able to, with any positive probability, produce a sample on that surface. But this is OK because we may actually understand surfaces and producing points or placements that are normal to them. And maybe we can even do that and generate, arbitrarily, many samples that actually lie on that surface itself. And we could do that by any number of infinite sequence. But then the tricky part is, what if you have multiple constraints that have lower dimensional impacts on the problem, like kinematics? How do you actually satisfy both at the same time? And this is where we actually introduce the notion of a conditional sampler, so something that takes in, as inputs, existing values, and then can solve to find appropriate output values that satisfy the given constraint. And intuitively, if you're in a robotic manipulation domain, one interesting conditional sampler is an inverse kinematics solver, taking in, here, the placement of an object in grasp, or more generally, in effect, a pose, and producing configurations that actually satisfy that. And the cool thing is that under certain conditions, you can actually compose these conditional samplers, in the form of a directed acyclic graph, and actually generate values that satisfy all the constraints conjunctively. You can't do this entirely generally, but in many cases, this is possible if you understand the geometry of these surfaces. And so our key representational contribution is to actually formalize this notion of a conditional sampler, and this is what we call a string, essentially. And really, you can think of it as this picture right here, where it's some Python procedure that takes in some inputs and generates a sequence of output values that might be infinitely long. And if you want a very concrete example of what that could look like using Python syntax, it's just a function that basically, using yield, returns a generator, essentially. So we need one more thing, though, to be actually useful. So if someone just gives you a Python program without telling you at all what goes in and what goes out, there's not much you can do with it that makes sense, right? You won't know what types to actually pass in. And so we need to actually understand a tiny bit more to be able to use this while actually planning. And so this is where we introduce a declarative specification of the domain and the certified facts of a stream. Domain facts are basically like types of valid inputs. Like if you're calling a motion planner, you better pass in configurations and not strings to the motion planner. So that's pretty self-explanatory. But really, the key thing is this notion of a certified fact. This is an assertion that all the outputs of this sampler are guaranteed to satisfy a given constraint, such as from our placement sampler, all poses that we produce are guaranteed to be within the region that they were sampled from. And so this is really nice because once you get these new values, you have some sort of understanding about how they actually relate to other entities in the world, and you can start to plan with them. And so now we can actually express all the samplers we showed before using the specification. And if we're trying to, again, make a sampler that produces poses of a bowl on top of a stovetop, we basically need to ensure that what comes in is the name of a block that you'd like to sample for and the region that you'd like to sample on top of. And then as a certified fact, we know that the new value is opposed for that block, and additionally, that that pose is known to be contained within the region. And so this allows us to then think about, what placements could actually accomplish our end goal task? And here's a very simple example of implementing the actual Python function that goes into this sampler for our 2D domain, and it can be quite simple for some of these lower dimensional systems. And of course, we can extend this to model our inverse kinematics conditional sampler, taking in poses and grasps and producing configurations that are known to satisfy a kinematic constraint. And so in 2D, this is trivial because it's a functional relation, but in general, it might be a little more tricky. It might actually require doing analytical inverse kinematics to find the full set of solutions that could actually commit a pick or a place. And likewise, we do the same thing with motion planning. So to conclude this part of the talk, so the key representational contribution is this notion of a stream, which is a conditional sampler, essentially. And if you, as a software engineer, want to use the system, all you have to do is create a domain file which specifies the actions that are involved, a stream file that specifies the properties of the samplers, and then pass in the initial state, goal condition, and the Python implementation of your generators. And the guarantee that we provide is that our planners will consume this representation, and if the samplers you specify can produce values that support a solution, they're guaranteed to actually produce them, as well as a certificate that it's actually a quick solution. All right, so we don't know if this works so far, but it's not quite useful until we can describe some algorithms to actually solve these problems. And so in this talk, we'll talk about two algorithms. I've actually subsequently identified a bunch of other algorithms that are variations on this, and also quite different ones, so I think there's a lot of room for actually trying to build on these algorithms. But again, the key thing is that the algorithms we specify will decide which streams to query and which order and how they're actually used in the search. Both algorithms will alternate between a searching and sampling phase. Really, the searching phase is taking in a finite PDDL problem and then doing a discrete search to find a plan. And the sampling phase, essentially, is used to modify this problem in response to the solution that's obtained by the discrete planner. Again, one bonus for operating in PDDL is that we can use an off the shelf planner as our discrete search algorithm. And so in this, we use Fast Downward, which is a very efficient planner that is open sourced. And again, and we can drive some guarantees, depending on the properties of the samplers. The first algorithm is pretty simple, but it's a good starter point. I call it the incremental algorithm. And really, the intuition is that one way of solving these problems is to take a very brute force strategy. For instance, what you could do is you could possibly try to query all possible input combinations to samplers, and then periodically just check whether the finite set of values you've produced actually admits a solution. If not, you have to keep going, but if so, then you're done, essentially. And so it's this very simple little state machine. And we can look how this would actually play out on our simple domain. So when we start off, you know a couple values. And on the first iteration of calling our samplers, we can produce some robot configurations to pick and place the objects at their initial poses. We can produce some more placements of objects that are in both regions. And we can make one degenerate motion plan to move to the same configuration. So now we have some new samples, and we can pass these to our discrete search algorithm. And it will report infeasible. There aren't nearly enough values to actually solve this problem. That's OK. We didn't expect, necessarily, to be able to do it on the first iteration. So we can return and make some more values, for instance, making now new configurations of the robot that can do each pick or place, making more placements, because we might need more. And now we can start to actually make trajectories that move between pairs of configurations. Again, we pass these samples toward a discrete search algorithm, and it still fails. And that's because it still isn't able to get to a place to put down the orange block, in particular. And we could repeat this process, potentially, for two more times, but eventually we'll find a plan. This is great because now we had at least one algorithm that is able to choose these continuous values and solve the problem, all automatically. Of course, this seems like a lot of wasted effort, if this is all we can do, because this is a very brute force algorithm that did a lot of unnecessary work, in particular, that we had to evaluate 182 sampler calls to solve this very simple problem. And so this is challenging, or this is detrimental, because first, many times in robotics, these samplers are quite expensive, like calling a motion planner might require spending one second of computation. IK is kind of difficult to satisfy many applications. And additionally, when you have many variables, then you're just increasing the size of your finite search problem, making Fast Downward's problem, or job, harder, essentially. So this motivates our second algorithm, which tries to incorporate laziness in order to drastically reduce the number of samplers that are actually called. And the key idea is let's only call the streams or samplers that we've identified as being useful for supporting a plan, namely, what we're going to do is we're going to plan not with actual outputs of the samplers, but with hypothetical outputs that represent an optimistic thing that could be an input to another sampler. And so essentially, what we do is we create, in Python, a first class object that represents a potential output of each sampler. And so down here in our little example, we could make an output of a sampling placement stream that is for block A in the red region. We can make a kinematics solution, Q0, for picking up A at its initial configuration. And we also can recursively apply these values into future samplers. So we can make a hypothetical configuration that could pick up A at the hypothetical placement p0. So again, the key thing here is that we haven't done any real sampling, we're just reasoning about, very optimistically, the capabilities that we could have. And this is the basis of our focused algorithm, which tries to lazily plan using these values before actually calling any of the associated samplers. Namely, what we can do is we can start off with our problem and we can take in all of our streams and generate these optimistic samples. Once we get a plan, if the plan contains only real values, we're done. There's nothing optimistic. There's nothing that's like an approximation at all. However, if we have some values that are unbound, we have to actually call the associated samplers. And that's where we actually move over to this region and we extract the samplers that are associated with each optimistic object, query some values, and then, essentially, we have a new problem. We've now actually generated some more facts. And there's a little bit of subtlety in order to enforce that the algorithm on the next iteration doesn't do the exact same thing, but we can achieve that by essentially disabling the ability to use certain samplers that we've tried before. We'll walk through an example. So here is a simplified version of the domain I showed you before, where there's no more block B. And so what we can do is we can create these optimistic values only for the robot and block A. And the first optimistic plan we get involves moving and picking A and placing it in the region. Once we have that plan, we can extract out the set of constraints that are involved. And so two kinematics solutions and two motion plans in one stable placement in region A. We can associate those with the samplers that achieve these constraints. And then we have our directed acyclic graph, and we can just call the associated samplers. And if each sampler succeeds, then we're done. We basically have found a valid binding for each optimistic object that we used. Of course, we want to not just be able to solve problems in which you can do this all in one swoop. We want to be able to deal with the fact that our optimistic assumptions are actually way too optimistic. And so if we go back to the original example, where now block B is in the red region, when we start off, we arrive at the exact same optimistic plan. So again, just four actions, moving object A. And we result in the same set of constraints, with the exception of one additional constraint, which is the C-free constraint, which means that A at placement p0, is collision-free, with respect to B's initial placement. But again, we can extract out the DAG that's associated with these constraints and try to actually sample values. But the key thing here, actually, is that by design of this problem, there is no placement of A in this region that is not colliding with B at its initial placement. And so this second sampler, effectively, returns no values and says False, that you can't do this. So now what, of course? We'd likely not want to just give up. We'd like to explore other candid options, but how do we enforce this? Again, the key thing is that we can prevent the search algorithm from using the exact same samplers they just tried to force it to explore a new plan. And that's what it does, essentially. Now it has to actually move B somewhere to even possibly be able to satisfy the constraints. And as a result, it gets this different side of collision-free constraints. And so here, where the first one previously failed-- 8.21 was in collision with B's initial placement-- 8.21 might optimistically not be in collision with another placement of B. It might also still be inclusion, but it has the potential to not be. And so this allows us to get a different sampling DAG, and we now actually have another avenue towards actually binding the values on this plan. And we've done some scaling experiments and we've been able to show that this, indeed, is much, much faster in many like real PR2 manipulation settings. OK, I'm going to switch gears a little bit and talk about, where do we get some these samplers? AUDIENCE: [INAUDIBLE] CAELAN GARRETT: Oh, yeah. 20 seconds for the focused algorithm. AUDIENCE: [INAUDIBLE] CAELAN GARRETT: Yeah. I timed that at two minutes for this one. Yeah, the incremental one can get really, really bad very quickly. And this is planning for a long horizon, actually. I think over there, it has to do 32 picks and places together, top left. OK, so one question could be, where do these samplers come from? And there are some things, again, like sampling placements, IK, motion planning, that we actually have a good grasp on and we don't need to learn. But there are some things that are more dynamic, like scooping and pouring, that are a little bit more difficult to specify. And so I've been working with a colleague, Z. Wang, in trying to actually use Gaussian processes to learn the set of values that actually satisfy these more dynamic constraints. Namely, given a context of the world, so maybe the dimensions of the objects that are involved, such as the radius and height of the bowl and cup, if we choose Control Parameters, that actually generates successful pours. And once we have that learner, we can then incorporate that into the preconditions of our actions in order to ensure that we actually apply a high quality pours when actually solving tasks. And the key thing that we do to actually learn these skills without many samples is to use Gaussian processes because of their uncertainty quantification, that we can get variances out of their predictions and understand where we are quite uncertain. On a more systems level, one of the cool things we've been able to do, actually, is to use my planners when trying to train these learners, that we can actually set up an environment with two scales that measure the mass of the particles in the cup and bowl. And on every iteration, we sample a pair of objects to manipulate. The learner chooses a control parameter it wants to try. And then the planner observes the world and plans a full robot motion to actually solve that problem and apply that pour. And so we can actually use planning to actually better enable data collection. Additionally, we can actually start to use active learning that tries to select regions of control space that are both very uncertain, and also known to be kind of on the borderline between good and bad, in order to explore the space of good pours. And as a result, we can get a learner that's trained on 100 samples, in the real world, that's able to pretty successfully pour over a variety of new test objects of varying dimensions. All right. Finally, over the summer, I spent some time at Dieter's lab, right down the street. And the key thing that we wanted to do was to take advantage of the cool kitchen environment that he has, where there are all sorts of sources of both stochasticity and occlusion due to it just being a normal kitchen, essentially. And so that required actually extending our formulation to be able to handle stochastic planning problems and partially observable planning problems. So the way that we are tackling the first part, the MDP component, is to actually do online planning, essentially, to react to the current state of the world, and then compute a new plan. However, still, one might worry that all the planning I've done so far is assumed that actions are deterministic, right? So we have to be able to incorporate a little bit of the likelihood of an action succeeding in order to be able to successfully act in these domains. But one thing you can do is that you can actually perform a cost transformation on the underlying problem that incorporates both the reward, the expected reward you get from traversing, or taking an action, and the likelihood of that outcome. In some cases, that can be actually a very good model of what's going on in the underlying world. And this allows us to then do cost-sensitive and deterministic planning and still make reasonable probabilistic choices. And here's a very simple MDP, actually, where truly, there is a deterministic planning transformation, such that deterministic planning can find the optimal policy. And really, the key thing, intuitively, is it's where the unexpected outcomes of your actions are quite local. In this case, there's a self loop that transitions the same state. Again, we have to also address partial observability, and there are many sources of that in this kitchen environment. And really, we want to ensure that our robot can take observation actions to gather more information about the world. And so what people have termed planning the space as is belief space planning, essentially. And the key idea here is we're no longer planning over individual values, so not over just the single pose an object, but distributions over their possible poses. And we'd like you all to actually work in geometric worlds, where there are often multiple modalities to our underlying belief about the state of the world. In this example there is a Cheez-It box and a sugar box on the table, including this green block from the connect that's up in the top right. And we'd like to be able to say that, after observing this world, we'd much more likely think that this green block is at a placement behind one of these objects than it is on the sides or in between them, essentially. And then on top of that, we'd like to be able to then plan actions that manipulate these movable objects in order to attempt to receive an observation of what's behind it. And this is where operating over particles, rather than just multivariate Gaussians, is actually quite useful, because they're able to actually represent very interesting multi-modal distributions. So here's an example of our system solving this problem. Its goal is to cook the SPAM. And initially, the camera's over here. And it can't see what's behind these objects, and so it tries to actually relate the sugar and cannot detect anything behind it. And so now it updates its belief that the SPAM is most likely not behind the sugar. Here, it moves the Cheez-It box and it's able to get a partial detection. And now it can actually update its belief about the world. In this case, it still has a decent amount of variance in the actual literal pose of the SPAM, and so it moves it once more. But now we have a good enough pose estimate to do an open loop pick. And we can pick it and store it on the stove, and do a nice little cook by turning the stove on and off and waiting zero seconds in between. AUDIENCE: [INAUDIBLE] CAELAN GARRETT: Exactly, exactly. Yeah. AUDIENCE: [INAUDIBLE] reasoning about occlusion geometry? CAELAN GARRETT: Yes, exactly. Yeah. That's one of the advantages of working with particles, is that you can kind of operate on each particle independently, as you would normally do with any sample, and understand upon an object being in another place, would this particle be visible? And even assigned it an observation model, if it's stochastic, if you actually understand that quite well. Yeah? AUDIENCE: Is that incorporated into the motion planning aspect? Domino sugar was moved behind the Cheez-It. Did it know that there's only three possible objects [INAUDIBLE] at a specific height to not hit the SPAM, if it's there? CAELAN GARRETT: Right. That's a really good point. So actually, currently, we are assuming that we know the all the objects that are in the world. There's no open worldness of something that we can actually detect. But I think that's critical in any real system, and that's one thing that I'm looking to actually extend. And that's where the more occupancy-like representations can be quite useful, to understand what parts of space you inspected and can certify as being actually safe to traverse. But to get into how we actually model this using our planning formulation, it actually doesn't really require any modification at all. It just requires modeling it appropriately. Really, here, again, our state variables are now probability distributions over values. And we need to take observation actions, so let's specify an action that can detect something. And so really, what this is saying is that if we're trying to detect object O and it starts off at oppose belief, oppose distribution pb1, and we know that our observation is, with prior probability, visible, given all the other object placements, then if we hypothesize getting a particular observation, we get the resulting new belief state of that object. And again, we can incur a cost that associates both the underlying control effort to observe, which isn't as significant here, but more importantly, the actual probability of actually getting our hypothesized observation. And so all that's remaining is to actually implement the samplers that are used to actually produce these values. And again, we can specify streams. And so here, we can specify a stream that basically samples observations from a prior, essentially. So if we have this current belief, and we might think that upon moving something we can then detect how to predict its pose, we can perform a deterministic belief update, given any observation itself. So just completing the posterior. And we can also, then again, score the likelihood of receiving that observation [INAUDIBLE] to include within the cost of detecting the object. We've been able to actually do a lot of really cool partially observable tasks in this domain. For the first one, the goal is for the SPAM to be in the top drawer. And the prior for this task in the second video is exactly the same. But this one, the robot looks out, and the SPAM starts in that bottom drawer. And so once it opens it, it's able to get a detection and then actually concentrate most of its belief in that drawer, and then be quite certain that it's achieved the task. While the second one, the task and the prior are the same, but the latent state is quite different. The Spam actually starts in the top drawer here. And so it tries to inspect, and then now becomes very not confident that it's in the bottom drawer. It actually closes the bottom drawer first, because it can't actually open both of them at the same time, due to the Franka's end effector width. It picks up the Spam, places it for a re-grasp, closes the top drawer, and then opens the bottom drawer again to stow it. AUDIENCE: [INAUDIBLE] that particular [INAUDIBLE],, is it closing the bottom drawer because of the kinematic infeasibility via motion planning? CAELAN GARRETT: Yeah, the constrained motion planning. Yeah, actually, we compute a pulling trajectory, and the Franka's gripper basically collides with the top of the drawer. And so we would get a collision if we tried to open the top drawer when the bottom drawer was also open, by basically just-- let's see if I can-- like that. Yeah. AUDIENCE: [INAUDIBLE] It's just that if you can have seven positions in the bottom drawer, and there were four of those that would not collide with the front [INAUDIBLE],, do you have to specify all of them, or would it-- CAELAN GARRETT: I think to be truly complete, you have to sample from the side, which you can totally do. There is maybe some gray room, in terms of how frequently do you want actually introduce them? It might be the case that only one's necessary, while you might need to produce a million because it's very, very constrained. And that's really just a part of the problem itself, and depending on your sampling strategy, will actually influence how quickly you're able to actually solve it, given the number of feasible samples. It's another one of these things that comes up all the time in sample-based algorithms, where problems are easy if there are many good samples, essentially. So if there aren't many good samples, it's going to be quite difficult, but you still can retain completeness guarantees in the limit. So some takeaways of the talk are that we introduced a language for expressing task emotion planning problems, and really just hybrid planning problems, in general, that extends PDDL by incorporating sampling procedures. We have some algorithms that are able to efficiently solve these problems by, in particular, operating lazily on all the samplers we provided. We've been able to actually apply these to real world settings, where we do have to deal with stochasticity. Again, we perform an approximation, but it's quite actually appropriate for a lot of non-adversarial manipulation domains. And finally, the planning formulation itself can be directly applied to operate over distributions, instead of just single entities, and now we can reason about observation actions in order to solve challenging problems in occluded environments. All right, that's the end of my talk. Here are some outtakes/bloopers bloopers when we had the wrong joint limits for the PR2. The PR2, for those that don't know, has something called a soft joint limits, which fails silently, and they pull you away from the hard joint limits. And so we were planning trajectories that the robot couldn't physically execute, but it would return success, that it had gotten close enough to the set point as possible. And so we had a lot of really weird pours to start out. And actually, on the Franka at Dieter's lab, I had the wrong joint limit for one of the joints for a long time and was unable to open the cabinets from one side, essentially. That was only in a couple of ERDFs. Some of them had it and some of them don't. So check your joint limits. That's the moral of the story. All right, any questions? [APPLAUSE] AUDIENCE: Thanks for the talk. Rehashing some of the things that were talked about [INAUDIBLE],, there is laziness in the way you're searching the space, but there's also efficiencies that you can gain by the fact that large parts of the C space that you're checking haven't really changed much. And so here, your atomic motion plans are not sharing information across different steps of the motion plans. What are your thoughts on addressing that? CAELAN GARRETT: Yeah, so I think there are three ways of addressing it. One is to use learning, essentially, to learn over distribution environments. I'm currently not doing that, but other people in my lab are doing it, and of course, people here as well. There is trying to implement each motion planner to use some sort of shared structure. Like you can very simply say that instead of calling a separate RT indication, we'll call it a multi-query motion planner, and actually check to see, given the current state of the world, whether each transition is valid. I think the most subtle, but the most useful one, again, is the formulation of the problem itself. Like if you are actually a robot point operating in 2D, essentially, and if you can write down the road map that the point robot can go through and express that each transition is valid, as long as it doesn't collide with each object currently, then you can reuse that structure, all without creating more actions, essentially. And that's really what you're doing, is you're trying to, instead of creating a new parametrized action conditioned on the whole world, you're making a condition that must hold for a single controlled parameter. So there, the idea is to have fewer actions actually specified, but make it such that they have to obey certain conditions for them to be illegal in a given state. And so I think the latter is something that isn't actually that clear in a lot of robotic planning, because if you think of the whole world as one entity, you can't possibly break it apart and think about different pieces and how the interplay within whatever you're doing. AUDIENCE: I guess just correlating [INAUDIBLE].. I mean, oftentimes, planning time is dominated by motion planning, which is exclusively my fault. CAELAN GARRETT: [LAUGHS] AUDIENCE: One thing I could think of is if you think of certain arrangements that you are making right now that might be feasible and the right thing to do, but actually are going to cause your motion timing time to increase, you're inadvertently creating these mazes that are going to affect future Caelan. I wonder if there's ways in which you can also do a back shading, or some way by which you can signal to the current Caelan that their future tasks are going to be affected by the current motion. CAELAN GARRETT: Yeah. So I guess there's two things there. So one is just for a single planning query, essentially. One thing you can do is you can actually incorporate the expected computational effort to call a sampler when using the lazy algorithm essentially. And instead of trying to minimize costs, minimize the sum of that effort, which can bias you towards decisions that could actually be very quickly realizable. And so if you had the choice between basically pressing a button or something versus like a hairy, expensive motion punting problem that you'd have to solve, but the latter is actually cheaper, you can bias the algorithm to more quickly find the suboptimal solution, which is actually quite good. And I think that one can start to make those kinds of meta-inferences also while planning and executing. Like for all these tasks, the planner is actually much quicker than the robot executing the same operations. And many times, there are things that are towards the end of the plan that are just can be completely re-computed anyways via re-planning. And so it doesn't make sense to think about the millionth motion you'll make today, essentially, right now. I think, really, the key insight that I've had when doing planning execution is that you want to do as little of work as possible, such that you do something, because you have to do something, and you can certify that the action is actually going to progress towards the goal, essentially. And so as soon as you can convince yourself of that, you can throw everything out. Like if you know you're always be able to solve the motion planning problem, then don't do it now. Just wait, essentially. However, if it's the case that you can't actually reach an object, then that might really influence the first thing you do, essentially. AUDIENCE: Back shading, so just the second part of the question. CAELAN GARRETT: OK, back shading. OK. So to start with, so you can use any search algorithm for the discrete search phase. There's been a debate for a long time in AI planning about forward versus backward search, essentially. Leslie and Tómas love backward search, but I don't like it. I mean, the trade-offs are that if you go forward, then you're always exploring states that are reachable from the start, obviously. And if your going backwards, you're always doing things that could achieve the goal. I found that in many domains, given the set of things [AUDIO OUT],, it's much more easy to actually arrive at states that are never legal. Like there's some sort of internal constraint violation by going backwards because there isn't necessarily this ability to kind of understand that there's some sort of inconsistency and prune it from the search. While going forward, many times the limitations on the actual actions you can do enforce you to do, at least search over things that could be relevant, in general. But actually, one counter example to that is in the construction application I showed before, it turns out that back chaining is actually way more effective than a forward search because the goal specification is entirely given. There's a single goal state. And when you go backwards, essentially what you're doing is you're removing elements from the structure. And every time you remove an element, then the collision-free configuration space weekly increases, essentially. AUDIENCE: [INAUDIBLE] CAELAN GARRETT: Yeah. AUDIENCE: That was the example I was going to [INAUDIBLE],, like It only gets better. That's been [INAUDIBLE]. CAELAN GARRETT: Yeah. However, it makes solving for stiffness instability harder. So there's always this interplay that's forced by the constraints that are involved, and it can be quite specific to the domain. Good news is that everything I presented can leverage either, essentially. AUDIENCE: And maybe the beauty of this technique is both of them are very general. You can apply many domains, and then if you want to go into this domain, which is more than the domain. So positive limitation seems to be that you're committing to these specific symbols and you're committing to specific ways of searching them, if you want things like that. And in the end, you still have to solve, let's say, the rounding problem. Like how do you use your perceptions to fill in all this central information that you need? Now, as we know, all deep learning, there's a lot of work going on that goes kind of the other way, where they try to come from the perception space and then maybe learn the appropriate symbols and [INAUDIBLE] limitations. What's your thought on what the strings are? Or where will be the limitations? CAELAN GARRETT: This is something that I'd love to work on in the future is that it's perfectly OK to plan with incomplete models of objects. Like if you see a new object in the world, you can kind of roughly estimate different inferences of it, and also maybe it's too much of impact. And you totally can plan on that approximate version and actually rule out things that may be unsafe. And so you can start to actually integrate a lot of approximate knowledge into these reasoning systems. I think the part about the symbolic structure is maybe the most difficult. Despite the efforts we have for learning these samplers, we still assume that we know the parametrization of useful skills, essentially, like the structural form. And I think recently, there's been a lot of work of trying to actually search over discrete structures and actually find good ones that model domain, but it still seems like a very open problem, in general. And a lot of the learned skills I've seen are very bad models of the actual domain itself. So I think that's the one thing that we truly cannot address currently that I think is a big avenue for future research. SIDDHARTHA SRINIVASA: I think we are out of time, unfortunately, but thank you so much. [APPLAUSE] 