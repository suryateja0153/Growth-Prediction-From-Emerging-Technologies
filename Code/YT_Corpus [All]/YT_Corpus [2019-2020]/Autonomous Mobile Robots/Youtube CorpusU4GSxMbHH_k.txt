 everything's still good you guys all see it mm-hmm okay cool so um I have a whole talk and I have a whole spiel but I want to make sure that this is interactive and even though I talk loud and keep moving forward just interrupt me at any point if you have a question if you want to you know interject an idea I'm super happy to stop any point and discuss and with with that I'm gonna take it away all right so as many of you guys here I love robotics I think robotics is super cool and throughout my PhD I've had the opportunity to work on a variety of different robots so the very first robot I have worked on was actually with Dylan and my my first year my PhD and I worked on this robotic manipulator that unka has in her lab and after that I worked for a few years on aerial vehicles specifically these quad copters that we have in Claire's lab and more recently I've been working on ground vehicles now what's exciting about being a roboticist right now is that even though all of these examples I've shown you are in the lab we are having the amazing opportunity to see these robots in the wild through companies so you can see here just a sampling of companies that are pushing each of these robots from ground vehicles like Lemos autonomous driving cars to manipulators like canola and TR eyes assistive robots that are actually interacting with the real world now one thing you'll notice across all of these domains is that there's one common theme it's that robots are operating in close physical proximity with people and when this happens robots need to change how they plan their motions and behave around people whether that be to avoid them or to collaborate with them and so a key component of doing this efficiently for a robot is to have a model of human behavior now to date it's been quite difficult to figure out human behavioral models but we've had some advances in this domain thanks to again the availability of more data so here are just a few examples I took from both industry labs and also from academic labs where people have started to try to take real data of people that are operating the wild and extract some kind of model that can help us predict you behavior and put those predictive models into the robot motion planner so on the top right you see an example from CMU so this is a top-down perspective on on a office building and these little points that you see that are highlighted show salient locations that people like to go towards or on the lower left-hand side this is a picture from uber and they sort of are using all their large amounts of traffic data to try to extract models of if people are going to be braking or speeding up or taking turns and despite all these successes though I also want to highlight a few things which are that no model is really ever going to be complete and perfect no matter how much data we have there are still bizarre things that happen in the wild when it comes to people and here are just two examples I found online from YouTube dashcam videos dashcam videos are perfect ways to find examples of people behaving in strange ways so on the left hand side this is a person just driving down the street and there's a person who's playing soccer not in the soccer field but on the side of the street their ball starts you know rolling into the street and the car has to quickly stop so it doesn't hit the hit the person who's playing soccer again it's not unreasonable to realize that if I was treating an autonomous system and designing a model of human behavior I might never have actually experienced people playing soccer on the streets and so this might be an unexpected behavior that I can experience as a robot there's even more strange behavior that also exists which is a lot less explainable so this is an example of a person who just decides to stop in front of the car I as a human don't even know how to explain this to you I don't really understand why they did this but it's another one of these like tail and corner cases that might be really difficult for us to model but pose important concerns for our autonomous systems that are deployed in the wild now there's several things we can do as engineers to try to solve this one perspective is to say let me use this data as an engineer collect this put it into my training set and offline do some kind of model improvement and then the next time I deploy my system it's gonna be potentially more robust and this is definitely something that is being done and for sure should be done because all of these example I'm showing you here that the robot experience is online our ways are useful pieces of information we should leverage offline as well but there's another question that a robot needs to answer which is online what should it do so online the robot needs to somehow realize that what it is experiencing the behavior it is seeing doesn't make any sense and it somehow make a safe decision so that for example the passenger inside of the vehicle or even at passing the the human being outside doesn't get compromised in terms of safety now this online question is one that's particularly fascinating to me and that's what I focused on for most of my PhD which is how can a robot even know that the data that's coming in is not actually well modeled by its by its human models and then how does it actually automatically make a safe decision and so this is going to be the key thing I'm going to focus on in this talk which is this notion of robots that can introspect about the validity of their models by using data that they're collecting online and once they introspect about the validity of their models they should automatically be able to come up and generate these safe decisions in light of uncertainty so I'm going to focus on this during my talk and the way I'm going to dive into this and make it a little bit more concrete it's through two projects that I've worked on during my PhD the first is on designing introspective human motion predictors that then automatically generate safe motion plans from a robot's perspective and the second is some a very new work-in-progress that I just want to share with you guys where we can actually start to more formally analyze these predictors that get to change online based on data now I'm going to start with this first we'll explore body of work that I've spent a lot of time on but before I move on any questions all right cool so let's dive into it now what is this robot planning a prediction problem that I'm alluding to now in this running example I'm gonna be showing here I'm looking at this aerial vehicle it's state as denoted by X R at time step 0 and we have a single person in the environment their state as denoted by X H of x 0 and the robots trying to move towards a school that's shown in blue now the robot in order to make a good decision needs to figure out where the person is going to go in order to do this they need to run some kind of prediction scheme but ultimately it's going to produce a set of states shown here in red and it's going to try to avoid so here the robots solving for example some kind of constrained optimization problem where it's trying to minimize some overall cost subjects of dynamical systems constraints and subject to the constraint that it never has any of its if its physical state intersect with these future states the human might occupy by solving this optimization it's considering all these candidate trajectories and then the goal is to pick a candidate trajectory that makes sure that it avoids the human at all time and can make it safe now let's zoom in just for one moment I'm specifically the prediction side of things so where do these human predictive human predictions and human models come from one common thing that people like to do is they write down a dynamics model for the human - just like we did for the robot that lets us capture the state evolution over time of the humans however one important question comes into play which is how does the person choose their actions so we we might be able to maybe learn an a reasonable F or even write it down well it's a little bit harder for me to understand how people are gonna choose their input so one perspective this is the robust safety critical perspective on this problem it's to be very conservative at this model of human actions and so what this actually means is now what we're gonna try to do is think about a very large set of possible things the person could ever do and make sure that our robot protects against all of them to make this idea more concrete let's look at a really simple planar model of a human so here the discrete time dynamics shown in the xy-plane so hy x or the are the states of the human the human has a fixed walking speed B and they get to control their heading so you H is gonna be some angle that lives from minus PI to PI this big set u this conservative set of all actions the person could take is going to be any of these angles now if I use this kind of model and I want to protect against any of these possible angles the person could ever choose and all the states that could evolve as a result of them choosing any of these things I can formalize this idea by again looking back into robust control literature I'm saying given this set of all control inputs what are all the possible human trajectories I could see and what I'm essentially doing by this computation is computing something that's more formally known as a forward reachable set so this equation here this FRS shown in red for a particular state XH for some future time T says this is the set of all states X such that there exists a sequence of control inputs you that get me to up to that state now for the simple example I written here it's this really simple model which actually ends up in a really simple closed form for reachable set which is just shown as these concentric red circles that keep growing outwards and so these actually will now form the basis of our predictions the robots now going to have two collision check against each of these concentric circles that emanate out from the human forward in time and only get larger as we predict more and more longer horizon predictions now this is surely safe because we've written down quite a conservative model of people's behavior and the resulting predictions as you see are gonna start to occupy a lot of the space and this is good from a safety perspective but really bad from an efficiency perspective so here I'm gonna show you what this kind of predictor looks like in practice on the right hand side you see a top-down view in simulation and Arvest of the same environment that you see on the left hand side which is our experimental test but instead of using a quadcopter here I'm using this ground turtle but just for demonstration purposes now on the right hand side the important things to focus on are emanating gray circles forward in time these are the forward reachable sets six seconds into the future under our model and the robot is visualized as a green point and it's a motion plan that it's constantly replanting it's shown in green when the robot uses this kind of conservative motion predictor it definitely will not collide with this human being my lab mate Ellis but in order to avoid the human it has to leave the environment it has to leave the work space to always avoid all these concentric circles and this is unfortunate from the perspective of efficiency the want of robots to be useful so we can return back to that problem I said earlier we know what our dynamic systems model is but maybe there's a better way to model how people make their decisions and this is where this this idea of using offline data comes into play I mentioned it earlier on but we can leverage again all of this new data that we're generating to extract a model how people make decisions in a little bit more structured way so here you see that same top-down perspective of an indoor environment and these salient points are gonna be things like doors tables chairs in the environment which based on the data tend to be places that people more likely like to move towards they're not randomly moving in any direction they tend to be moving towards one of these locations to encode this one approach is to write down these parametric models of human decision-making so here we have this distribution or how people choose their actions so the probability of a person taking an action you H given they're at a particular state xh is parametrized by this value theta which is gonna encode one of these goal locations in the environment in general there are many ways that you could come up with this distribution what's nice is just that you've had this learned parameter theta that you can now encode all these different ways that people trade off their decisions but one of the models that I focused on in my PhD is a popular popular model in the cognitive science and econometrics and now more widely in robotics model which is called the noisily rational Boltzmann model now what's important to notice here is how this model is encoding human decision-making so first notice here this Q function so q function which is again derived from dynamic programming principles is encoding this the overall value of the human taking an action UT from XT under a particular objective now this is going to let us understand how people might make decisions so if I plug in a particular theta let's say I plug in the door it's gonna say that people are gonna be more likely to choose actions that take them towards the door because if they move in that direction it gets them as close as possible to the door as fast as possible the other important parameter to note about these models is this variance or otherwise known as temperature parameter which essentially scales how optimally the person appears to choose their actions this ends up resulting in this distribution of actions I've shown here in red we're more saturated larger arrows represent more likely actions and smaller arrows that are more dark represent lower probability actions so here you see that if I plug in theta one or the door as an objective that the person cares about the action that takes them towards the door is assigned more probability under this model and this is nicely again captured through this q function that is inside of this model the last component that people consider in these in these kind of frameworks is that even though I have extracted a model of how people make decisions this data I might not know a priori which of these betas the person is optimizing for for this reason people place a distribution or a belief over these model parameters which can encode some kind of prior again on how people make decisions maybe I have a high prior on people moving towards doors but it's gonna let me actually leverage data online to improve my model and better make my model align with the measurements I'm seeing online so this means that when the person actually moves not towards the door but towards a table we're gonna apply a simple Bayesian update and we're gonna now shift our probability mass away from the door as a likely hypothesis for what the person is optimizing for and now move it towards the table if you put all of these components together the last thing that the robot needs to do is now actually predict and because this decision-making model we have here at stochastic we're gonna end up with distributions over States forward in time because that stochasticity has pushed forward through the dynamics this means that now we have these distributions I'm showing here in these colored circles forward in time for for example a first time step forward in time second time step third times time step so on and so forth until the end of my prediction horizon here what's important to note about the coloring is that the the pink parts of the color mean high probability that the person is going to be at that state at that future time and blue means lower probability if we return back to the planning problem though the last thing I want to mention is that originally I posed this problem as a deterministic obstacle that the human that the robot is avoiding this kft is this deterministic sight and so one thing that practitioners also like to do is you have to somehow convert these state distributions into these deterministic optical obstacles and the way that we can do this is essentially by asking what states forward in time are assigned sufficiently likely probability epsilon is a thing that governs how sufficiently likely and then those are going to be the states that make up the set K so these are going to be the states that we actually avoid because under our model these states are assigned sufficient enough probability this means that when the robot plants it's going to safeguard against these forward of all being larger sets and now it gets to choose its action in a way that sort of takes into account how people make decisions and ideally isn't as conservative as as that full-forward reachable set any questions all right yeah good is that like okay set some like confidence set around like where your model predicted the human would be yes so yeah basically you're asking find me the set of states that make up 95% of my probability distribution saying you don't have like the 95% confident state set of states and that's the thing you avoid and you can be more and more strict depending on your choice of epsilon but if you choose a non 0 epsilon you're gonna start to see interesting behaviors where you're actually restricting the set of states based on how the model is is predicting okay so if you miss a possible goal for the human and that's what they're going for then you're just kind of hosed right that's exactly what I'm gonna address fantastic question yeah yeah so perfect you know exactly where we're going so let's before me before we address Daniels point which we will and the that will be doubly well we're going to focus on for the rest of the talk let's first see it working because sometimes this model actually does work and when does it work well it works when people behave according to our model so here you're gonna see actually Sylvia who's on this call she is our canonical human and all these experiments and she is shown on the left hand side with our example quadcopter that's going to be flying around it's shown in blue and we have this modeled goal it's shown in red on the right hand side in that top-down environment again theta1 is what we're representing it by and this is actually I think just a trash can in the environment or maybe it's a door that she's walking towards what you're gonna see is these predictions forward in time again red or pink means high probability and blue means low probability and Sylvia is gonna be walking perfectly and optimally exactly towards the door when she does this the robot essentially perfectly can take it's straight line trajectory its optimal path towards the goal it doesn't climb with her everything works out ok this is quite different from that full forward we triple set the robot never had to deviate I knew exactly that she was just gonna be going towards the door now one of the things that could go wrong is for sure an on modeled goal will actually look at that example a little bit later but another type of failure that you can have is an unweld obstacle that actually forces sylvia to no longer take the optimal straight-line path towards that theta one and she has to deviate significantly from what the model predicts so this is what we're showing here to this coffee spill that's shown in right and also that's shown on the ground through teeth on the left hand side video when Sylvia does this this predictions over where she's going to go are going to be consistently incorrect the robot is gonna keep thinking that she's just gonna move immediately and straight towards the door when this happens the robot collides with her and we see we see our failure come to light now if we if we see this failure what should we do so if I looked at this example one more time and I recall that full Ford reachable set in that scenario it would have been nice if I could just revert to the full Ford reachable set and have the robot react and move away from her because actually that conservative behavior was desirable at that point when she started deviating and so it'd be really nice to have her have that full form reachable set protect against a Sun model obstacle but the question is how do you automatically know to revert to this full Ford reachable set how should the robot just magic this this decision off and so one perspective could be well maybe i can pre-program some kind of switch so maybe it'd be nice if you get these measurements of how the person is moving over time and maybe you have some pre specified heuristic of how you want to switch from at first using your intense driven model but then quickly switching to this conservative full form reachable set this is one approach but in general we can do something that's even better where we can actually not just have the robot be conservative at some point some switching point but we can actually have the robot try to automatically interpolate between that full ford reachable set and the goal driven model this means that when the human is walking around at first it might use this intense driven model but over time it might start relaxing that intense driven model all the way until it reaches that full ford reachable set well then as soon as she starts walking back towards the door we'd like it to automatically scale back and trust that intense driven model that's the best of both worlds that we're really trying to get at the question is how do we do this and so to answer this question we're gonna look back at our at our human model that we were looking at now we want to encode this this blending between a full Ford reachable set and this a gold driven model automatically and what we're gonna do is we're gonna ask yourself what is this model saying well this model is saying that people take noisily optimal actions in pursuit of a modeled objective why is it noisy its noisy because of this beta parameter that we set here that's that variance parameter and the distribution that essentially starts to place probability and that's more or less on the optimal action that the person can take towards a particular objective interestingly though so even though this parameter is typically fixed what we're gonna do is we're gonna say what if we actually change the value of this parameter and let this value actually automatically scale us from Fulford reachable set to purely goal driven let's see that let's see that through an example so imagine I fix the parameter so I say the only thing that my model ever knows about is that people like to move towards doors so if they don't one is the only parameter one choice that I can that I can encode of my various parameter is zero if I plug in zero for my beta parameter I'm gonna get a uniform distribution over the actions what's interesting is if I use this kind of model for prediction the resulting set of actions that are are basically equally distributed right and this means that when I actually push this through my state's prediction I'm gonna get this nice uniform distribution and physical state space over the future location as a person could go to so this is a prediction one second forward in time it's visualized in 2d on the left hand side with the HX hy but it's also visualized in 3d so you can actually see the height of any individual grid cell in the environment which shows the probability of the person being at that grid so at that future time one second in the future what's nice here is when we look at this this starts to look like the forward reachable set it's it's taking up you know this this circular region around the human and if the robot was to use this - for collision checking it would start to avoid a lot more set of states be a lot more conservative around the person nicely though this beta parameter also can code super optimality so if I chose a really high value of beta let's I chose 10 or 100 or even higher essentially all the probability mass is placed on exactly the action that takes the person towards the goal this means that when we look at the state predictions similarly we're seeing essentially all probability mass in the direction number of the goal so now what we said to ourselves was we're gonna use this built-in parameter and instead of fixing it to be low or some medium or some like high number let's interpret it actually as a measure of our model confidence it tells us how well our model is performing and so instead of pre specifying this built-in parameter let's do the same kind of Bayesian inference technique to estimate this model confidence online based on how the human behaves in order to estimate this we're gonna do the first thing that any Bayesian will do is put a distribution over the parameter so imagine wearing this like simplified scenario we only have three values that beta can take zero so that's the the random model one which is reasonably optimal and then 10 which is like very optimal according to our model objective as you can see now I've also added beta as an explicit parameter of our distribution and now what we're gonna do is we're gonna say what happens if the person walks directly away toward from the goal this is the worst case sort of unmodeled behavior where the person is just not at all walking towards their goal now in order to understand how this inference is gonna let us capture this automatic scaling from goal driven model to forward reachable set let's see how that Bayesian update will actually work when we observe this measurement to see that in action well let's consider what the Bayesian updates gonna do so the Bayesian update is gonna plug in for each possible value of beta it's gonna say how likely is this observation that I just got under each of these possible models that's what that that's what that likelihood model here in the right-hand side of the Bayesian update is doing you're plugging in different values of beta you're saying how likely is each of these under that data so if we look at these models and just for my simple cartoon applied example you can see that the models that start placing a lot of emphasis on the modeled objective on the model objective that says that people like to move towards doors they're shifting all that probability mess away from that backward action which actually means that the uniform distribution the one where beta equals zero is going to assign the most probability to this backwards action now intuitively what this Bayesian update is capturing is it saying that if the human data is suboptimal under the modeled goal then the robot has the wrong model it's essentially no good it's not explained anything but the beauty of the bayesian update is it's going to start placing that probability mass in the direction of that beta equals zero that completely uniform distribution and this has automatically given us a way for the robot to shift its model confidence around so so far let's recall our original my original claim that we wanted to solve we found a way for the robot to introspect about the validity of its model by using that data online but what about the second part what about automatically making a safe decision well what's really nice is that this method of estimating data actually does both for us automatically you don't actually have to change anything about our motion planner it is just the predictions themselves they're going to change that automatically produce these really nice safer plans to see that in action let's take a look at that very first set up now on the right hand side also visualizing that that belief that our initial belief over the over the beta parameter and maybe we start out with a belief that is like relatively high on a goal driven model so you can see what the most probability of the science-u beta equals 1 and the predictions will look something like this they'll say people like 2 are gonna move towards towards the door now the robot will take one step and the human will - when we observe this measurement as you can see our posterior distribution over beta has shifted and placed more probability mass on beta equals 0 so suddenly this measurement has told us our model isn't actually that good and when we push that through the state predictions we get those full forward reachable set looking predictions this means that when we're collision checking again we're going to be collision checking and guessed your volume of states and the robots going to take a wider berth around the person automatically let's see this in action and make sure that Silvia remains safe against all these robots that are using misspecified models before we see the predictions and actions just let's just focus on the behavior just qualitatively on the left hand side you're gonna see just two different perspectives the same exact setup where Silvia is avoiding this unmodeled coffee spill in the environment and the robot here is high-lead at the top in a red circle and at the bottom it's again in the same location as you saw the fort as I play this you can see that she's walking around the coffee spill and the robot doesn't actually end up hitting her it deviates out of the way and safely avoids her now the more interesting thing to look at here is the predictions so what actually happened on the prediction side of thing and how did that affect the plan we're gonna look at the same example but now we're going to notice something that I Sylvia starts to deviate around this unbundle obstacle those distributions start spreading out a lot more the robot automatically starts drifting more to the left and then it avoid sir but there was this acute part in between where actually after Silvia finishes deviating and she starts moving back towards that modeled goal the predictions start to tighten up again they start to become more confident because she's abiding by our model after that point now the same thing actually applies for a variety of other examples we tested out one more that is exactly getting to daniel's point it's an unmodeled goal so they're these two goals that we know about that are shown in red and there's a third unmodeled goal the person's moving to which is shown in this red dotted line here now on the left hand side we're gonna see what happens if you don't actually have your robot reason about its own models validity so what's gonna happen is as long as the person is moving towards one of these goals everything looks good the predictions make sense but as soon as they deviate this these predictions don't make any sense they keep thinking that the humans just gonna turn around and run towards one of these goals and in fact a near-collision sort of occurs here it doesn't actually happen but they get you know worryingly close now in the Bayesian confidence this is this model confidence estimation framework that we proposed we get the same sort of behavior as long as the persons behaving according to one of these goals but as soon as the person steps awake and starts moving in an unmodified way triple set state distribution and when you see the resulting the resulting plans they take a lot wider berth around the human questions of course so one is about dynamic obstacles so have you guys considered dynamic obstacles and I'd imagine that would make the Bayesian inference part a bit like a little bit more challenging because the probability of actions of humans will depend on the dynamic obstacles I see so you can you refine a little bit more so human right now right now for example you have the coffee spill for example all right useful is fixed then you have I mean when you are doing your Bayesian inference over beta that that is based on whatever action the person takes and have this probability of actions of humans and because coffee spill is like fixed right you have another like quadrotor running towards you right like coming in like some other thing or some other like thing coming towards you then your Bayesian estimation of beta might not be as as easy so like I was wondering if you have considered those settings or how it does it change yeah so that's a fantastic question so we haven't done any work explicitly on that but I think what makes that particularly difficult is like I think that if you truly want to encode that you need your cue function to be a function not only of the human state and the human action but also this other dynamic obstacles state and potentially action too so this makes computing the cue function really hard and it's so first of all what what's what I'm hiding under the hood here is that right all of these techniques here are relying on dynamic programming nicely because we have a small set of Thetas i can pre-compute all these cue functions give them to the robot and just switches if I have human I think the human model is also avoiding a dynamic obstacle then now I have to pre-compute all like pairwise combinations of the person and the robot and all these different key functions and then store them on the robot and have it use those online or somehow be able to like compute them online so I haven't actually done that I think that's super interesting and and very valuable I mean the cheap way I guess to maybe do that is I wonder if you could somehow encode like through relative state or have like the other moving obstacle being another goal and somehow be able to be clever about your computation so you don't actually have to explicitly pre compute it but I haven't thought it all the way through yeah can they can I see some yeah first Andrea are you gonna talk about the multi human yeah take my next slide okay so we do something like like that but trying to fudge it a little bit we did try with with Zach from Michael Koch endeavors lab we did try actually implementing dorsa your old like nudging the drivers stuff and we found that at least for this particular example we couldn't run it fast enough that it would outperform just this like dumbed down get confused yeah I have one quick question as well so I was wondering because your update over beta include try saying like like a dynamics update where you say the robots action affects the what the humans beta is in some way fantastic question no so as you see and can go back really quick so the Q the the human model and maybe I don't have it here actually yeah here so let's let's take a look again at the Q functions the Q function or like the human model in general is conditioned only on the human state the humans goal and the beta not the robots state or the robots action so the way didn't code that is again put both those things as a component of what you're conditioning on but it only makes the computation now harder because now you have to somehow think about again these reactive schemes and so I think that those I think those models are super interesting and they are very valuable to consider they only get more important in some of the other examples I'm gonna share a little bit later on where like you're doing collaborative tasks or you're trying to explicitly model like the interaction setting for these collision avoidance settings as Sylvia said it's actually in practice been like shockingly at how little you can model and it just is a really useful principle to just revert to the Ford reachable set from a perspective of performance like if you just want your system to not hit the human when they do something weird if you want to really maximize like efficiency information gathering actions if you want to do collaboration this will not necessarily help you out tasks in another group of course okay so there is this framework that switches between reachable set vs. and the kind of was conservative approach I was wondering if you guys have looked at it like over repeated interactions almost to see if you learn something about the goals so you have like this extra goal that you have in models like like if you go to that environment again like now now do you have like that knowledge of the new goal and you incorporate that like yeah it's a great question so I think actually this harkens back to perspective one that I mentioned at the very beginning of my talk well you can you can view actually the you can view the confusion of your human model as self supervision for where you should go back look at your data and retrain so that's one perspective it's like you look at like the performance execution you say at all of these points in time and all these states I got confused let's like retrain and extract new features perspective to is you want to do that learning online and that's something that I am really excited about working on over the summer actually because it's a question I get a lot and I think that I think the honest way to start answering that is you need to start incorporating real sensors and like richer data because as you see right now and all these examples are shown you they're just motion capture tracking so we just are tracking X Y state perfectly but to understand how to like expand your model second subtract your model set if you want to start adding new goals you need to have richer information like from cameras so I think that there's two angles there's like the offline for some online answer that I think both are super cool and neither of which we've done thanks anything else okay cool so actually alluding to all these interesting questions that people had let's actually take a look at how this approach can help us deal with complicated scenarios like multi human multi robot interaction circuit scenarios so here we have again our favorite human Silvia and we also have our other favorite human Jaime who we used to be a PhD student here but now is actually a professor at Princeton and Sylvia and Jaime are two humans that these two quadcopters which are shown here in pink there's one right here and one right here you need to avoid now each of these robots have their own goals one is trying to get towards this blue point the other one is trying to get towards this blue point and one of the challenges about these scenarios is interaction effects if you truly want to model this whole entire scenario you need to model how Sylvia reacts to Jaime how Jaime Rex to Sylvia has Sylvia reacts to the robot how the robot 1 reacts to the robot 2 and like all these different paralyzed like interactions now the issue is that that's really difficult not only computationally but also just from a modeling perspective how do I know how people react to each other or to other robots I don't even know how to come up with good models of that either so maybe we can say okay let's uh let's let's try to just slap like a really easy model where we're gonna say that people don't explicitly reason about each other they're just naively optimizing for their own objective maybe they even think they can move through each other physically because like their own dynamics their internal model doesn't account for collision avoidance with the other human and see how far we can actually get by deploying these models but putting on top of it this model confidence estimation framework and we're gonna see this in action where I'm gonna play the video twice so first let's just focus on the main part of the video where we just see everything working out what we're gonna notice is that the two quad copters lift up and then they're gonna start to move and Sylvia and I may have to cross paths as well as the robots have to cross paths in order to navigate through the environment now what you're gonna notice is time it's kind of dancing that's unmodeled he's Spanish he's listening to some like good lot of music Sylvia is being optimal because she's always optimal drinking your coffee and the robots find their way around if we take a look again at the video but watch this lower right hand video of the prediction of themselves you can notice that Sylvia's prediction so Sylvia is this is this human being here they're only gonna get more confident so notice that we can really predict her pretty well they get a little bit unconfident when she has to move around Jaime but they still become really really confident if I play the video one more time when hi my name is moving he's dancing and being a lot like quite unmodeled as he's gonna move his predictions are always quite noisy when the interaction happens actually gets a little noisier but consistently the robots are always able to avoid both of them and each other using this using this like prediction scheme that let's - lets us blend from four reachable set to the goal driven model so so far I've shown you a lot of examples in the quadcopter domain we've also tested this out also for ground vehicles - so we put this on a Dubin scar model are they car driving on roads and the unmodeled things are on Mulla components here are for example an unmodeled turn this is an unmodeled goal scenario and also a pothole that the robot but the human is sort of going around and we've seen some interesting success there too but some of the more recent work on this has been also extending it to manipulation domain so I want to just quickly touch on how this model confidence and like having robots introspect can be useful in that domain - so in the manipulation context and this is again going back to that first project that I worked on with Dylan in that very first project we were looking at anniversary enforcement study inverse reinforcement learning settings where people get to physically interact with the robot and teach the robot something about its reward parameters and in the very first work I did with Dylan we set up that framework but then more recently we revisited that framework and asked ourselves well what can go wrong what can go wrong in these IRL from physical interaction domains and in this particular problem stuff I'm going to show you we have this home robot and the feature space it knows about is this distance to table like how FAR's and effector is from the table and so maybe it has two parameter values that it can take it can say oh I have to be either really far away from the table or quite close to the table when the robot is interacting on the human though the human can come and try to teach the robot something about its feature space it kind of physically interacting that data unfortunately in this setup that we have here the human is gonna try to teach the robot about a feature that is not in its feature set so the human is actually trying to correct the orientation of the end-effector not the distance to table so that the robot keeps the cup upright what's unfortunate about the scenario is that the data now that the robot is receiving just fundamentally cannot be explained by its hypothesis and if the robot naively tries to learn from this kind of data what we're gonna see is that when the person tries to correct the end effector they're gonna have they're gonna keep correcting but they're also gonna be naively affecting how far away from the table the robot thinks it should stay so you can see it's sort of jerking around just trying to update actually distance to table even though the human isn't giving it a correction about the distance to table so this is kind of unfortunate because again in this IRL domain the robots just simply learning about a feature from the data but it can actually explain the data it's getting because a feature set isn't specified so we can apply so so yeah so here we see this mistake in learning about the table but the human inputs actually about cup so we can apply the same kind of reasoning and say well what if the robot maintains this joint distribution over this feature so that it has these different distances to the table but also model confidence so just like we did before now the robot is going to ask itself for any given measurement any given correction how likely is this correction under distance to table these two different distances from a table but also under a random model beta equals 0 or theta driven a goal driven model and for example maybe when the person tries to correct the end effector what's going to happen is maybe this kind of distribution pops up and the robots gonna say well it's pretty unlikely that this correction makes sense under either the small distance from table or large distance from table but it's the most likely if the person is just like if beta equals 0 if my model confidence is really low when we see this in action now the results are a little bit they're a little bit anticlimactic from a learning perspective but they're actually quite useful so what we see here is that even though the robot still can't the right thing what it's doing is it's not learning incorrectly so when the person is correcting the end-effector trying to make the cup orientation align better with their preferences you can see it as I play this again the robot isn't jerking around a lot it's actually not learning at all it's not learning that distance the table because it says I don't understand this input I'm not going to learn instead because the only way I would learn is if I was learning erroneously so we're definitely not expanding the model set or somehow you know incorporating this this Cup feature but at least the robot isn't mistakenly trying to interpret the data and do something incorrect and unsafe so hopefully now I've shown you all these different application domains I've convinced you that this online learning are through human data is a really valuable thing to apply for a variety of robots so that they can automatically make safer and more intelligence decisions when they're getting data from people and so this this or concludes that first part of the body of work now there's gonna be a separate body of work which is as a note it's very much a work in progress it's quite experimental and still very much in the works so before I dive into this I want to just take a moment and see if there's any discussion that people want to have can you just get philosophical question that sounds great yes how do you pick your Epsilon yeah so there yet so there's a cop not a cop I answer but there's like the engineering perspective it's like okay yeah let's just like tune it there's another angle that to be honest I haven't thought all the way through but I wonder if there's a way to actually determine what Epsilon to choose based on again the real-time performance or like the data itself so maybe you could like I wonder if there's a way either to pre compute what epsilon is right based on some performance criterion so you say like I want to run like I want to run like n simulations of all these simulations I'm gonna have this kind of like performance so my human it never gets hit and I'm like you choose your epsilon which is like an uninformed search kind of thing but I wonder if there's also a way that you can use the data that you get so as the human is moving you can find a way to choose an epsilon that I don't I guess like better incorporates the absurdity that you want like it's also kind of strange because so these all these exponential models that I mentioned here they're very aggressive in terms of how they shift probability mess around it's choosing the epsilon in general is just like a very difficult thing in practice because you're that the the decay of the probability at any given state is like very steep and so if I choose the difference between or if I choose an epsilon that's like 0.1 versus 0.15 but that might suddenly capture or remove huge set of states so I feel like that question needs to sort of be almost answered offline you need to say like if I had this model it's like Kiki this is how it behaviors one of these kind of data what epsilon should I choose that meets some kind of performance criterion like it almost feels like you know do some kind of intelligent optimization I don't know mm-hm I don't have a well-formed answer though as you can tell yeah it does have like a large effect and like how you're choosing it you know you're ready no absolutely agreed yeah and there's so this second part of the work that I'm gonna mention it's an it doesn't have to do with the epsilon per se but I think some of the tools that I've started to think about more recently helped start to get at these questions of when you let things change online like how long is it gonna take you to achieve confidence how conservative unis actually be like can we more can we formalized a little bit more but I don't have a satisfying answer to be completely honest I have a quick question yeah so when you're filling the bottle of finding the probability for each of the voxels that you're showing is that based on taking your estimate of where the person's gonna be at a time step forward and then adding up the probabilities uh yeah okay and then I was why not you use like a Gaussian at each instead of using a like a histogram a multinomial distribution youyou can so it can't depends on what model of the human behavior you choose so here we're not using gaussians and so what ends up happening is you have these other kinds of distributions that come out but but you can you can use other kind of modeling techniques if you happen to use for example a Gaussian type predictor what you would do is instead of maintaining uncertainty over beta you'd maintain uncertainty over your variance of your of your Gaussian and so basically you're just saying like how spread out should my - should be it should it be uniform wear with the state space or really tightly centered around whatever the go should mean is so basically the if you want to think about extending this notion of introspection it's basically just estimating like the the unsexy way of pitching in is just estimate your uncertainty online another thing that I think is interesting is I was trying to like connect this to the prospect theory based work that like we did recently at HR right where we're not using a noisily rational model because there are settings where it is not necessarily that beta parameter alone is not going to capture everything you have like almost like a jump where you're doing it's an optimal actually you're more likely to do it suboptimal so I'm wondering like how like this work could be extended to those settings - that's almost like a like you have now like a hybrid system and you're trying to estimate yeah I mean because it technically this is a hybrid system where each beta describes its own mode its own mode now it's almost like not just a different beta it's like fundamentally different distribution it's like imagine I'm estimating between Boltzmann or Gaussian or Boltzmann and Poisson like whatever I can choose like my n fundamentally different distributions that like can't just be changed based on changing a single parameter it's like the fundamental distribution itself is different it's almost like that it feels like any of them anyways any final questions yeah I had a question about the car example so it seems like once you get that unexpected behavior you kind of lose any notion of like lane following right in terms of prediction yeah yeah like once your beta goes down closer to zero you just don't have any more lane following so if like a car was coming from the other direction you just have to say I'm on the brakes I guess yeah yeah you can import you can encode more like structure in how the the constraints like play into the predictions by so for example here like all of these obstacles like the lanes boundaries are constraints or it's like that I mean they're a negative reward in the Q function of computation so when the predictions happen they don't actually make it they abide by these constraints you could do the same kind of thing which maybe has an additional component of the Q function that rewards the human poor Lane following or for not like crossing that boundary maybe I've like really high negative reward when you when the person swerves and so maybe there's some way of encoding that we haven't actually thought about that but you're very right I think the behavior that would happen right now is like you just slam on the brakes that's it yeah I guess the game would then to be balancing that with also making sure that they can leave their lane if there's an obstacle exactly yeah and there's some interesting work there's some interesting work from Marco Pavone age group with Karen and a former student of Claire's Moe and they try to sort of infuse not necessarily for like these kind of human prediction schemes but they try to infuse some of these like stricter safety notions that both include things like Lane following and and like obstacle constraints when you have a vehicle running next to another vehicle and you're not sure if that vehicle is gonna like enter into your lane it's a different setting but I think it's like kind of relevant in spirit okay yeah thanks I also take a look at that yeah all right any final questions before I get into the more the last small experimental session of my talk I'm also happy to keep having more discussions - all right cool so I'm gonna introduce this next section by revealing something that I haven't been totally truthful with you about during this talk so so far and all of the running examples I've led you to believe that we can go from being confident like this to conservative essentially instantly like within one measurement but in reality we really can't like in reality especially if you have an incorrect prior like the one that I see that I'm showing you here it's gonna take a series of observations and some nonzero time to converge to that pull forward reachable set now why is this important well as we already now know since the robot is relying on these predictions during that entire time period when you are scaling from optimistic and trusting your model to conservative your robots plans may be compromised because again you're relying on predictions for it in time that are just fundamentally incorrect and require more data so what we really want is we want our robots plans to somehow be able to know how long it's gonna take how many measurements it's gonna take for it to become conservative and if the robot knew that a priori maybe you could incorporate them to its motion plan and become overall a lot more robust so as I said before like this this like high-level question is how long what will it take for my robot to go from an incorrect prior to that desired true model confidence estimate now in the current sort of line of thinking I'm investigating there's two scenarios that I've divided this question into the first scenario is what is the best-case time they will take the robot to detect that its model is incorrect now intuitively you can think about this as if the human is acting an on model way like they're running away from the door but they're giving us as a robot the most optimal measurements to build to make us estimate that desired posterior so they're kind of being cooperatively unmodeled and they're gonna help us estimate that lo model confidence as fast as possible the other perspective is what's the worst-case time it will take the robot to detect its models incorrect so this is kind of this deceptive model of the human they really aren't actually going towards the door but they're taking us like roundabout way that kind of starts towards the door and eventually curves backwards and this will be sort of this worst-case time that'll take you to estimate that low model of confidence is everyone on board with this with these high level problem statement all right cool okay so let's focus just on this left-hand side and think about how can we formalize this question into something we can actually compute so if you recall what are the components that we're operating with here we have this human initial State it's visualized as this black point here in the 2d plane we also have this parameter of our human model says people like to walk towards doors and we have this incorrect prior we're starting out with its places very low probability that the human is moving towards doors and very like a lot of probability that human actually does follow our model recall our question it's what is the best case time it's going to take the robot to detect that its model is incorrect now notice that this question incorporates or like relies really heavily on the human input because the human input like their their action is the thing that not only know does their State forward but that's actually the thing that we're gonna plug in to our posterior update to shift our distribution to make our belief move in the right direction so because of this what we really want to do is we want to somehow reason about this joint evolution they want to think about under these different observations I could get these different use actions the person could take where would they go physically in state space but also what would happen to our belief what would happen to our distribution over our model confidence to reason about this joint evolution we're gonna think about the joint state space of both the physical state of the human so she here shown an X and also the belief over our model parameters okay does the reasoning for why we want to reason about both together make sense to everyone if you only have two like if it's a binomial distribution why do you need two parameters we don't we can only just maintain one yeah but so in general you need like the cardinality of your discrete random variable minus one so yeah so we're actually just gonna be looking at a 3d state space here so I'm gonna show you what that three states a space looks like now so we have x and y in the plane but now our z axis is going to be the probability that the human is operating according to our goal so if the B of beta equals one the other thing we can do now is we can actually represent our initial prior over the beta in this joint state space with this point so this point you see here is going to be our initial condition in the joint state space where the person is at minus three three meters in the environment but also the probability of the person being goal-driven is 0.8 it's very likely okay now the next question we need to ask ourselves is well we want to actually achieve a desired posterior we want to reach this point where the probability mass on this low confidence is sufficiently high let's say 0.8 and the probability of the human following our optimal model is really low point two what would it mean for us to encode this concept in our joint state space does anyone have like an idea for how we could translate this into this diagram here okay so a first pass is what if I what if I encoded a new Z desired which is at minus three in the human state space three in the humans Y state space and then zero point two because our desired posterior places zero point two probability on our high confidence model is this is this capturing what I want is this actually what I care about we want like a reachable set with the beta at point two I don't exactly know what you mean by reachable said but we do want a set so what we actually want is we actually want this representation of the of our desired joint state now what does it mean it's saying that I definitely need my third state my belief to be zero point two I want to place low probability on my goal driven model but in terms of where the person is I don't care where they are I want my person in my physical state space to be able to be anywhere in physical state space as long as my posterior is equal to zero point two so we actually take a look at what this looks like in like a math Michel toolbox so this this set here that you see this cube that's sort of flattened out exactly is representing this so what is it saying it's saying that we want low probability mouse on the goal driven model but the human can be anywhere in the state space so this is what that that belief is zero point two and then human can be anywhere here physically we don't care now this set is an encode that that target that ideal distribution we're trying to reach and we now have to ask ourselves how do we actually compute the best-case time it will take if I start out I'm starting out at this initial joint state incorrect prior at the initial time and I'm trying to reach this desired posterior where the person is anywhere in state space at some final time where that final time is encoding that best-case time this is the conceptual problem that we're trying to set up now the beauty of this problem that we've just set up is that we can actually leverage dynamic programming to solve it so if we know that at the final time we want to be at this desired posterior with the person being anywhere in state space let's compute the solution backwards in time let's start out saying I want to have this posterior and move backwards in time until we reach our initial condition this problem can be more formally stated through through this equation this is the hamilton-jacobi bellman equation now I know it's a lot to look at I'm not going to spend too much time discussing it what is important is that you can think of this as just continuous date and time bellman equations that let us represent these steps really nicely and easily through the value function and the last other thing to note here is this formulation is really nice because there's a variety of tools that we can just use off-the-shelf that work for general non-linear systems to actually just plug in this desired question that we have and answer it because a lot of the people in robust control and Systems Theory have spent time and energy developing this tool to exactly answer questions like this but not about distributions but about physical systems like just X dot equals f of X u so what we're gonna do here is we're going to simulate this backwards in time I'm gonna show you what that looks like one of my favorite things about using these tool boxes is seeing these cool funky unintuitive shapes grow in front of my eyes so we're solving backwards in time and we're what we're doing essentially as we're trying to solve this backwards in time until this blue volume that I'm showing you here includes our initial condition this means that we found a sequence of optimal controls or a sequence of measurements that the robot would receive that take us from that initial condition and bring us all the way to that desired posterior so we did it we found it as you can see here in the best case it's gonna take us 20 1.3 seconds to determine that the person does not move according to our model what's also cool is these tools let us extract optimal controls as well and again optimal controls in our case are gonna be these optimal measurements that we could observe so what does this mean it means that if the person is starting out or if the if the robot is starting out with a high prior on the person moving towards doors and a low prior on people moving randomly but then they observe the following optimal sequence of measurements of the person moving directly away from the door which is intuitive to you and me but now mathematically we've like convinced ourselves then we can ask this is the trajectory that will as fast as possible make the robot gain high confidence or more specifically a probability of 0.8 that the human is random they're not actually abiding by our model so this is this framework that I just told you here is a sort of a general way that we can start to answer some interesting questions about these these predictors that can evolve based on measurements and our different kinds of assumptions so so far I've only shown you what this looks like for this best case time you can also set up the same problem for a worst case time by essentially just changing if you're if the system like the human is maximizing or minimizing so if they're being helpful or hurtful to the robot this is easily encoded through the those dynamic programming equations I showed you before but ultimately what the vision is for this work is we'd like to now use these times to develop some interesting contingency planner for the robot where if the robot knows how long it's going to take it to receive measurements and gain confidence in the human behavior maybe you can become conservative for a certain amount of time wait to get those measurements and then confidently plan its behaviors after that point in time but as I said this is still very preliminary work I literally made these figures yesterday so they're very fresh off the press any questions about the setup just a I'm sure I was wondering how would you use this estimate so you're able to estimate like how how much how cooperative versus deceptive they're being yeah so maybe a more maybe an easier example to think about is imagine that instead of us like the beta imagine us estimating which goal a person's going to so they're either going towards a door or a table what I would like to know is that imagine that ground truth person is actually moving towards the door I want to know how long it's gonna take the robot if it starts out maybe with a 50/50 prior to estimate for sure that the person's moving towards the door what the robot can then do is if it knows this time it can say it can use the max of those two times so we can say here's the worst case time will take me to predict that the person is going towards goal one worst case time it takes to go towards goal two and now up until that time like let's say it takes it three seconds for three seconds it's gonna avoid both of these trajectories so both the trajectory moving towards the door and towards the sorry towards the door and towards the chair after that one second it knows it's gonna have certainty after one second in a worst case it's gonna for sure now which of those two goals the person's going to ambiguity will be resolved in the future and then from then on it can plan these contingency plans or basically say at that time if I have high confidence the person's going towards the door I'm gonna plan around the door if the person's going towards the chair I'm gonna move around the human moving to the chair this just means that now they can be throw but can be even less conservative because it knows when ambiguity will be resolved a priori like it can think in the future about this worst-case and then plan accordingly versus being dependent on the data to tell it at a certain time that's what the vision but so that means is a huge it that's how you would use the worst-case estimate right or how would you use the best great question oh there's like a whole punnett square you can write down of like human is acting optimally robots acting optimally and like what each of those mean to be honest I don't think I have a great answer for what each of these measurements mean I think they probably depend on what you're trying to ask in the case of what this safe collision avoidance kind of stuff you care about typically worst case times because you don't want to climb with the person but I think there's something interesting to be said about the best-case times I just don't have that off the top of my head it's a really interesting idea though because you know I think that ideally you'd always want to be able to estimate when your state estimation is gonna consume verge like the beta for example in the case but it's one of those things it's like a hard thing to do yeah yeah I have a couple of small questions the first one is just a fun question what you showed us what the trajectory looks like for the human in the best-case case did you look at all at what the trajectory would look like for the human when they're like purposely trying to be deceptive yeah so I don't have those results here and it's because so there's something's under the hood that I've sort of glanced over let's think about intuitively though worst case so imagine that the person starts out here and we're thinking about like the set of all actions they can take so imagine that they can take anything in the full four three triple set what's the worst case thing they could do well they could just like keep walking towards the door and like and in that case like basically maybe I maybe would take me infinite time to know that they're actually not going towards the door if they just skim it like this or not infinite but maybe a very long time I'm just and I think my little brother's doing the like I'm not touching you thing yeah so when when we're answering questions like worst case time there's an important thing to ask which is how worst case do you want to yeah so for example imagine that the person can take any like like the person could take any action but then we can say well maybe the in the worst case we don't want them to be able to take like this sequence of actions maybe they can just take any action like in this cone like this but not including the door I wouldn't be thought about trying rather than having the human be worst case the human being like actually random yes so here's the issue alright let's go back to the equations and let's think about like why what am i hiding alright so let's look at these equations so all these equations are saying is we're saying that so the way that my value function changes in time as described by this partial V partial T it's going to be determined by this minimization of the inner product between the value function for any given state and the joint dynamics let's not worry about this for right now let's just worry about this man so what were it what we're encoding here is we're saying the human you H gets to choose some action from the set and by minimizing what they're trying to do is essentially give us actions you H that pushes as fast as possible to this desired posterior so min is trying to help us like get here as fast as possible notice though this is a deterministic set so you H lives inside of a big set you each of which inside of there are equally likely to be chosen so here in it like this big circle that I showed you here of all the possible actions a person could take we're saying the person can take any of these they can choose those that will the minimization will help guide which of them they choose based on what problem we're trying to solve in reality though you're right like these are actually supposed to be distributions right like there's exponentially more likely for the person to move towards the door or may be uniformly at random so this is a determine eyes version of like that stochastic problem in order for us to leverage these computational tools naturally you're super right that there's value in asking is there a stochastic equivalent like if I really want the person to be if I if I must I mean the person is choosing their actions according to a different distribution or according to a uniform distribution how long will it take but the tools we have right now to my knowledge aren't amenable to that which is why we didn't do that we just sort of determine eyes it does that answer your question I have some questions but I'll ask them offline okay cool yeah this is very much still in flux question so so so when you are looking at disbelief over beta being won or being zero soar you're considering the deceptive case or the co-operative case so you're kind of like making a decision they're like what if you have like a more complicated like distribution for what beta would be like I would imagine and getting computationally things or not yeah nice there are you just not going there or like we have finds than go under and addressing the computational issues so right now there are some worries I just have about this so right now I'm living in this world I'm wearing about that I do think that the computational issues are very important because as you saw this discrete distribution I'm plugging into the state space is already it's like 3d and we have a variable that takes two values like this is not gonna scale yeah yeah in this great case yeah yeah exactly so now what's nice is that the beta parameter itself you actually don't have to maintain uncertainty and practice about over that many values of beta to still get really good performance so plugging beta into this is really nice but if you want to keep a joint distribution like beta and theta which is what we actually need when we're trying to estimate for more complex models that's gonna blow up like unreasonably really fast so I think interesting questions to ask there are things like sampling based approximations to these to these methods so for example you don't really need to use a level set toolbox to compute this you could also just forward simulate potentially some kind of like control sequences and try to find a way to sample through the space efficiently I'm also talking completely out of experience I haven't actually done this but there's some recent work out of I think actually sir touch Carmen's group at MIT where they taught up try to use like sampling based approximations to reachable sets so we could maybe do an equivalent sort of in this case especially if we don't care about extracting optimal control that might be sufficient just to get like an over approximation of the time and I could help this skill well another expression I'm gonna sneak it out at Molly's so say you compute the worst case in the best base time and you're trying to use the actual time to infer where between those two extreme like those two behaviors you are what would you use it as an interpolation function so I wonder if you could use the data you're getting like the actions of the person's actually taking to try to see if there's like where along that intermediate you're in intermediate you are so if you know maybe that the worst case II so this was the best case sequence of controls maybe there's some worst case sequence of controls and maybe you're getting something in between maybe there's a neat way of finding something in between those two I actually don't know it's a open question so you mentioned I know this isn't exactly what you just showed me you mentioned the example of you have two possible goals and you want to know like what's the longest time you know say they actually have goal one you think they have goal - what's the longest time it could take you to adjust to get to goal one um I'm just I feel like this approach might be a little bit over conservative though because let's say you say alright it could take like three seconds to figure it out but in those three seconds it'd be going towards the goal like going towards goal two so like you don't need to avoid like both goal one and we'll - in that worst case example right um so like I think it depends on so I think it depends on the on the strictness of your safety criterion so imagine it takes me one second to estimate confidently that the person is moving towards goal one but it takes me two seconds to competently estimate they're going towards goal two even if I get measurements of them going towards goal one I haven't reached enough like basically the max of the two I for sure know is gonna be safe because if you safeguard for two seconds you know that no matter which one they are however after one second you'd have achieved high enough confidence that they are going but you don't have high enough I don't know I haven't thought that all the way through I there's probably more intelligent ways of kamon yet so far I just thought of like the most like naive worst-case way to combine those two times it's uh yeah what I was getting at is that it might be worthwhile to think about like not just how long it takes to be confident about it but how far away you are from them like until you are confident actually your question I think relates to another thing that we can easily encode in these formalisms so notice here the target set that I'd wrote down so the person can be anywhere in state space but we have to reach a desired posterior another thing we can easily encode is we can say no actually imagine the person was really going towards a goal right here the target said then would be would have zero point to a probability on beta equals one but it would be just a circle over here and so now when we computer dynamic programming problem we're saying if the person really wants to get to this unmodeled goal here and there trying to be as helpful as possible showing us the posterior what's the minimum time it would take and that way you're actually encoding this like other goal driven behavior which I think will I mean is encoding a different thing but it's another kind of number you can get out to know their sequence of optimal controls that maybe could be a leverage to reduce conservatism but it implies some kind of structure that you know about the human like this is more related to the to coal case I know I have an answer your question all the way it's a great question so if you have multiple goals that your trip that you're trying to go between can you model that like uncertainty over which goal the human is going to deterministically the same way that you have for the other variables in your in this optimization yeah like so would this answer your question like imagine that I just have like imagine i model a person a solving like an lqr problem we're like the goal is their terminal cost they're just trying to like move as fast as possible to lqr essentially that straight line trajectory could be my prediction of their motion so instead of placing like distributions over where they're gonna go based on some kind of like stochastic model the optimal control they choose is just an optimal feedback policy it's like minus K times X that could be like a deterministic way of doing it but but then how do you wait say say you use that path when you you calculate it your answer for each of your goals how do you weight each of those those different solutions yeah exactly so uh I think there's techniques in the literature that try to do things like this to be honest I don't feel well versed enough to claim generally how they do that because I know that for example in an autonomous driving some of the demos I've seen our conferences they talk about having safety backup plans or state like worst-case prediction so they maintain two trajectories one or like the drivers are just going straight following the lanes and the other is maybe like one other trajectory that's kind of wild and like going off and I think they might maintain I know that uber gave a talk where they were discussing this but they maintained I think probability on each of those two being likely to occur and they safeguard against some like whichever one is most likely I don't know how they actually blended because it's uber they won't tell us so that's maybe one way of doing it where you just maintain uncertainty over like just two discrete trajectories one that's worst-case one that's like reasonable and you're safeguarding against a union of them if you're being really conservative or some kind of weighted combination these are great questions though that's all I have basically I was just gonna harken back to this this original statement and this body of work that we've talked at length about now obviously this work doesn't happen on my own robots are hard and you need a lot of people and also really good advisors to support you through these hard times here's a list of all the wonderful students have gotten to work with and my two advisors and with that thank you guys so much for having me thank you I thought I'm unmuted sorry 