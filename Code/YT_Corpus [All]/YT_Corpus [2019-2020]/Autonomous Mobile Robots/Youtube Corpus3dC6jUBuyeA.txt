 back in 2010 class of 1950 alumnus Roger Mudd an extraordinary and well-known journalist in America gave the university a generous gift for a center that would look into important ethical issues at the time back in 2010 Roger Mudd said given the state of ethics in our current culture this seems a fitting time to endow a Center for the study of ethics and my university is it's fitting home that's what Roger Mudd said in 2010 it was true then it's it's definitely true now the center plays an important part in the life of the school by choosing a topic every year a broad topic of ethical importance this year's topic is the ethics of Technology not a day goes by without all of us reading in a newspaper or on the internet something about the technological revolution and about the many issues that that revolution has brought about such things as gene editing altering human DNA artificial intelligence and robotics in the workplace and elsewhere big tech firms like Facebook and Google and the other ones their practices their handling of private information issues relating to cyber security in which in a number of noted situations people have hacked into seemingly secure institutions so we pick that broad topic the ethics of Technology and we began then to put together a schedule of speakers from outside the university and from within some of our own talent and we've come up with a very exciting I think schedule for this year's ethics theme our keynote speaker is a bioethicists lawyer and scholar and researcher into the whole area of gene editing gene modification altering of human DNA and her name is Josephine Johnston she's the head research scholar at an entity called the Hastings Center in the state of New York she has a new book coming out called human flourishing in the age of gene editing so she's bringing a philosophical ethical perspective to some extremely important scientific developments we've all read about the Chinese scientist who some months ago said that he had altered the DNA of to microscopic embryos that later developed into human persons and are now born the question of what are the ethical limits of such practices and Josephine Johnson is going to lead us through the evolution of that question over time and then she's going to pick a specific application connected to Gina she's going to look at the good parent in our culture does the good parent alter the genetic makeup of its children does the good parent look into all of this is the good do anything to make use of sect technologies how should we think about this what ethical values should be part of the cultures consideration of this topic we're very excited about that as the kickoff speaker for this year's series [Music] you you you you you you you good afternoon everyone and welcome it's my privilege to introduce the third guest speaker in this year's Mudd Center series on the ethics of technology as you know each year the center chooses an overarching ethics related theme and invites different guests to explore deeply one facet of the larger topic so the facet under scrutiny today is artificial intelligence and robotics and we are very pleased to welcome Ronald Arkin Regents professor in the College of Computing at the Georgia Institute of Technology professor Arkin is a roboticist and Robo ethicist with research interests that include interaction between humans and robots robot ethics behavior based reactive control and action oriented perception for mobile robots and unmanned aerial vehicles robot survivability bio robotics and much else besides he has over well over 200 technical publications in these areas as particularly known for his book behavior based robotics which was called a comprehensive intellectual history of robots and a thorough compilation of robotic organizational paradigms from reflexes through social interaction now professor Arkin received his undergraduate degree from the University of Michigan a Masters of Science degree from Stevens Institute of Technology and a PhD in computer science from the University of Massachusetts at Amherst he joined the faculty of Georgia Tech in 1987 he is director of the mobile robot laboratory there you really should check out the website of the mobile robot laboratory Georgia Tech check out that website where you'll find a fascinating description of the current projects of that lab now dr. Arkans work has taken him across the globe as guests of the top institutions in his field he was a visiting professor at the Center for autonomous systems at the Royal Institute of Technology in Stockholm he held a sabbatical chair at the Sony intelligence dynamics lab in Tokyo he served as a member of the robotics and artificial intelligence group at the laboratory of analysis and architecture of systems of the National Centre for scientific research in Toulouse France most recently he was a visiting fellow at the school of electrical and computer science Queensland University of Technology I could go on but you get the idea he's been all those places and now Lexington I want to mention as well that he has served on the board of governors of the Institute of Electrical and Electronics Engineers society on social implications of technology his talk today is entitled robots that need to mislead biologically inspired machine deception we're really happy to welcome him to W now won't you join me in welcoming Ronald Arkin to Washington [Applause] thank you very much Brian thank you very kind introduction but actually this is not my first time in Lexington I've been here before Simon recalls that time I spoke about autonomous lethal autonomous weapon systems killer robots as they're often referred to I'm only talking about robots at my today okay so you might feel a little bit better about that but there's lots to talk about in that other topic as well too but not today I've been working in this area with many of my graduate students over the past ten years so what you're going to see is a overview of much of the research I'm not going to go into deep technical details about what we did if you like that visit our website as you mentioned there's plenty of that there but we have ongoing work right now much of it has been funded by the Navy most recently we've been working in Team misdirection for the National Science Foundation and guess what we've graduated to counter deception as well - so we're working in all these issues but you might ask and I might ask why isn't this working because it's not huh why why why why should I do that I don't know if you know this but I bet you do deception is everywhere it's all over it's rampant it's throughout nature animals use it all the time for camouflage they use it primates use it as well - and even a variety of different aspects which I'll talk about in a minute but one of the key factors is especially in humans it may be considered a requirement the ability to deceive for social intelligence for us to get along with each other you can't stand the truth so again let's look at some of the nature ones this is a broken-winged display by some kind of clover I believe it is for the ornithologist out there so give me her not there but you either pronouncing or knowing that better it's a display oh this one's also funny is about pay more attention back this wasn't it let's started on its own is a mother grooming her child who is using tools to crack open nuts ok that's an important tool skill that chimpanzees happen to have defueling is good job of it he's not giving any of those nuts to his mom but his mom is grooming him making him feel good one listing but as chimpanzees have a tit-for-tat kind of behavior quid pro quo I guess should I say appropriate for this day and age that they are doing this and she starts grooming the young and it starts grooming and guess what mom does okay she decides that I want those two of myself and use this distraction for that particular purpose so there's all kinds of different ways that you see this in nature this is but a few [Applause] adversarial sports you cheer for deception in that please picture any of those days a good spanked in a basketball party that guy standing here right Oh what could be better deception okay and this one is also another example it's used in for placebos also used for effect and feeding feeding youngins in this particular case so so the whole notion of using that deception is part of human behavior as well too and sometimes we appreciate it and sometimes we don't and we're going to talk about both of those aspects as you move on and then at the end some of the ethics associated with that as well too and again that little signpost in the corner there says honesty is the best policy unless you want people to like you I always tell my wife hates me saying this but I say it anyway hi if you're out there that she doesn't have to she asked me how do I look and I know the answer before she asked that question she looks wonderful she's beautiful she looks better than she's ever done but it's I'm not deceiving her of course not I may be deceiving you by saying that right now but what's what's the story also an interesting thing is some people I was pointed out to me at one talk I gave on this subject as well too I mean a good way to make people feel better is sometimes to not tell the entire truth so feel free to say after this talk that was the best talk I ever heard okay and I'll be guaranteed to tell you right now this is the best audience I've ever had as well so there's interesting ways in which human beings do these sorts of things but getting back to more of the psychological aspects of this Dan Dennett in particular talked about the notion if you want higher honor intentionality in artificial intelligence the ability to formulate ideas and act upon them you do get the potential for deception as a by-product and keep in mind again for those unfamiliar with the Turing tests which is a classic hallmark test of intelligence it is fundamentally based on deception it is based on a computer pretending to be a human being and if it succeeds it's really smart wow I mean that's what AI is okay pack of lies in in some respects don't Cody don't tell that to your class okay this is an interesting movie as well too I don't know if you've seen it it's getting a little old right now but it really captures to some extent tars which is this bizarre robot over here interesting kinematics for those who know what that is the Matthew McConaughey is talking to it and make sure the volume is up just being honest hey tars what's your honesty parameter ninety percent ninety percent absolute honesty is no is the most diplomatic nor the safest form of communication with emotional beings okay ninety percent is dr. Burton so if you could put a parameter in your iPhone how often you want it to tell you the truth do you really want to know it's gonna rain tomorrow do you really want to know that your son hasn't called in the last three weeks what do you want to know yeah how much truth can you take or do you want to take this is actually a design decision for people that are creating these artifacts and including robot assists as well too we have to determine how these artifacts will relate with you and if your content out there they could never lie they can only tell the truth but then will you use it will you want it in your in your life okay so we there's of course some related work in this particular space a communication signal some of the earliest work was by Dario Florian Oh in Switzerland where he had robots that using genetic algorithms evolved to find food that's a good thing and then lied to the other robots around and about where it was so they wouldn't find that food there was a very selfish interested action this one was a great one rock at yale brian's caselotti did a rock-paper-scissors game they are all familiar with rock paper scissors and by the way don't play rock paper scissors with a Japanese robot because it will beat you every time really it can see your hand before you put determine from your hand what it's gonna do in advance you won't win but here in this particular case Brian ran an experiment with human subjects to basically play rock paper scissors law sometimes and it won sometimes but eventually wet I was cheating again okay paper scissors and it won when it did that so what do you think the outcome of that was people kind of woke up is one thing but people actually attributed more intelligence to that robot than the one that didn't lie not interesting Liars are more intelligent so it would seem and again in physical therapy in scaffolding for students telling the class you're doing well a whole bunch of different aspects that's been used and the military since time immemorial going back to the Old Testament and the Trojan horse warfare is based on deception according to Sun Tzu The Art of War and Machiavelli paraphrasing him he said although deceit is detestable and all other things yet in the conduct of war it is laudable and honorable there are certain aspects of deception in the military which are criminalized by international humanitarian law such as feigning a wound ruses those who referred to but flanking your enemy having ambushes all those things are fair game in the military there's an entire Field Manual on battlefield deception for the you served you may have encountered that at some point okay so what have we done what have we done we've done a lot we've studied human models of cognition from interdependence theory of psychology we have looked at biological models based on the handicap principle and the dishonesty principle we have looked at squirrel cash hoarding we have looked at other oriented deception by that I mean I'm going to lie to you it's for your own good and I'm not lying when I say that it's for your own good this is a good lie like placebos or other things as well - for the benefit of the mark and I will use the phrase mark meaning the recipient of the deception and the work that I'm talking about and the more recent work which I'll just touch on because we're in the middle of it right now is in teen miss direction how to relocate entire groups of agents whether it be robotic or people from one location to the other even though they really didn't want to go there in the first place by tapping into their herding instincts so this is one of the definitions of deception we use in our early work drawn from bond and Robinson's definition you pick a false communication you send it the mark receives it the mark interprets it and it influences what the mark is going to do in a way which is valuable to you or me in this particular case okay so basically it's a false communication that tends to benefit the communicator and what we did in this early work without going into the gory details of it is have a robot be able to do what we call situational awareness or assess a situation figure out what's going on here okay and then there were two components to it one is to determine when to deceive is deception warranted in this particular case because you need to know when to deceive you can't just deceive at random you have to have the right opportunity to be able to do that and then after you do that you have to know how to deceive so these are the two components that we teased out this interestingly came out of work they were doing for trust in the first place we were developing models for a robot to trust the human being why should a robot Trustee human being I mean everyone's wondering why should people trust robots but why should a robot trust a human being was there any good reason for that people aren't trustworthy especially towards robots especially kids I don't know he's seen some of the videos but kids can be very nasty towards robots so you have to be careful in terms of where you put your trust so this notion of conflict independence comes from the interdependence theory that I mentioned before and two of the axes of this model dealt with correspondence and interdependence so there were two things that we needed to recognize one there needed to be a dependency upon the actions of one affecting the other if one's action doesn't affect the other there's no reason to deceive in that particular case why bother in that case and you had to have conflict as well too there had to be something you have that I want or our vice versa or something along those lines so what ended up determining this is we found that we could create yes parameters and we did this before that movie came out by the way at about four years parameters which would determine how much deception would be warranted and when and that would be that area in the upper left it's dependent on k1 and it doesn't show k2 there but there's two parameters there which determined how much and how often I should lie to you how trustworthy so you could set a dial set a dial on your device okay let me go over here now there's an algorithm do you like algorithms I don't know if you do it you don't if you do there's one and you can see the gory details of it embedded showing the models for situational analysis that we used and the relatively simple parameter check that we use to determine when to deceive again technical details are available in the papers associated with this then how part comes into being okay how do you act deceptively you know okay that's important I lie to you now right it's important that I like to you now this might not be a good time to do that but what am I going to tell you and how am I going to tell it to you and how am I going to deliver that to you and so we used a Bayesian belief network in this particular case probabilistically for a robot that was playing hide-and-seek and had the ability to use three different sensors to detect the agents that were nearby and the bottom line is using a game theoretic approach for those unfamiliar with game theory basically what we wanted to do is induce in the mark rewards that actually didn't exist we want it to lie and say that if you play dead right now you're gonna be okay so you get a positive reward if you play dead which means don't move that might not have been the best thing to do when someone is searching for you under these particular circumstances so the real thing that that false information that we did was switch a number basically in the matrix over there and we also evaluated things such as what can we do with different types of sensors one would think that if you have more sensors intuitively you become more immune to deception right the more ways I can see the world the more likely I am to see it as it is well deception fails when you have absolutely no sensors you're literally talking to a rock in that case and you can't change the outcomes so we saw that there was a greater ability to deceive when we had more sensors available and also the more questions we asked to the partner this goes to that partial theory of mind that I think I brought up earlier I need to know what you're going to do if I tell you something so if I ask you these questions about what you're going to do if I tell you this or do it indirectly and then I learned that I can use that against you and a robot can - okay so what we did was basically in this case create a relatively simple experiment that ran like this in a scenario I kind of liken it to what happened at Star Wars 4 which was Star Wars one if you remember the way in which they came out and basically what we did was this relatively simple experiment to validate it because probabilistic in this case and so the tracking agent in this case is good for one shot deception [Music] right [Music] okay [Music] and it worked so what did we say okay what did we say we said that we can find when to deceive and we could use that to effectively deceive in a simple very simple set of circumstances here this is one would think not ground shattering research but it did show a capability so what we said in the paper and that we published in the journal of social robotics international journal of social robotics it said the results do not represent the final word on robots interception they are a preliminary indication that the techniques and algorithms described in this paper can be fruitfully used to produce deceptive behavior in a robot much more psychologically valid evidence will be required to strongly confirm this hypothesis that sounds like a responsible scientist doesn't say okay there's several caveats well let's see what happened researchers at the Georgia Institute of Technology may have made a terrible terrible mistake they've taught robots how to deceive but when machines rise up against humans and the robot apocalypse arrives we're all going to be wishing that Ronald Arkin and Alan Wagner had kept their ideas to themselves oh yeah here we go this is another one Ronald Arkin and Alan Wagner two names likely doomed to live in infamy and a stunning display of hubris the men researchers from Georgia Tech published a paper that detailed their foolhardy experiment to teach two robots how to play hide and seek this was my favorite one of the pretty robots capable of deceiving humans built by crazed boffins okay what do you think are you scared now did I scare you with this capability let's see what else is available all along we've assumed you assume folks that robots were innocent servants until now scientists at Georgia Tech have taught robots how to lie presumably to tell whoever gave them the research grant that their money was well spent these Georgia Tech's are the same brainiacs that last year built a robot that plays jazz now they've created a robot that can lie and say it likes jazz I love Gobert I still do to this day but I was a little concerned so I loved the piece what my funding Gatien might say the next morning when he saw that they said do you want us to continue doing deception and he said I want more I kid you not that's again it remember that central role in terms of military applications so that was surprising but we also got this it's not an invention first of all but it was hailed as something ground shattering and breakthrough this is the only person that article I think that got it right it said any computational code that supports the external behavioral properties of theory of mind and deception can help us understand what's going on in the human mind somebody's paying attention okay that's the trouble with robots people get panicky how many of you said the word robot apocalypse today or killer robots yeah there you go there you go I knew it I knew it's out there you will before the day is done okay now what did we do next we looked at Eastern gray squirrels Eastern gray squirrels get nuts nut golf balls and they hide them and they bury them for the winter okay so this was a natural behavior for eastern gray squirrels and what was found in biology was the interesting revelation that they patrol their caches okay here's my nuts I'll check them there okay I'm gonna go over here this is the evolutionary behavior okay my nuts are good okay let me go back over here this may be time separated nobody's gotten into my nuts that's good that oh oh there's another squirrel over there okay I see a conspecific to be more accurate okay I'm gonna go over here okay then I'm gonna go over here there's no nuts in those places they're empty empty caches as well - the whole goal is to delay pilferage okay to try and keep the other agent from finding those real nuts quicker than they had okay so we thought that was worth checking out and implementing which we did we usually check out our ideas in simulation first and gather much of the scientific data in simulations and then validated on actual robots part of the reason is simulations you can run a whole lot more than the actual robot experiments in a shorter amount of time and basically they did this sort of thing and then eventually this guy discovers it and they change their behavior and this one starts following it and it goes to empty caches in this particular case and then the competitor eventually goes off we did this on our actual robots in our laboratory as well two and one we had to tell the first one we had a fella operated a predator or competitor I guess that's referred to and the other one the other robot was autonomous as well - that's really kind of boring it's just watching them go back and forth but basically validating the concepts that we had in the simulations as well the results looked something like this you can see the delay and pilferage occurring in the presence of deception relative to the without deception and then we reported our results we were wondering oh no what's gonna happen now that we did this well this is what happened everybody loves squirrels okay I used to have chickens or someone they they were not necessarily my favorite animals but squirrels Birds teach robots to deceive Georgia Tech robots learn deceptive behaviors from squirrels animal Bluffs inspire a new breed of deceptive robots what does that say about us okay if we get deception from a model of psychology at the end of the world if we get it from squirrels that's really cool I knew in psychology still escapes me from time to time but that's the way people reacted to this well undeterred we went on to another area of deception aggregate and validated by mo sahabi profiling it definitely thought earlier that oh you got to check out and what happened here the paper makes in that case your comments well what we're going to do in these what these Arabian babblers is try and feign strength one of the questions is the handicap principle says that any agent any signal that threatens the life of the agent or that is sufficiently important must be true so in other words if I'm going to faint strength you better believe I can carry that out okay desert antelope do the same sort of thing as well too when a lion approaches I don't know if you ever seen these antelope jumping up and down when a lion approaches basically it's a signal saying to the effect that look at me I'm so fit you can't catch me you can try if you want but I can get away should it faint it should do that if it's really not feeling so good that day it works some of the time it doesn't work all the time but this is the same kind of principle that's here but the question that we asked is does it always have to be true and another study looked at well before I get to that I'll talk about the dishonesty principle in a minute but let's look at guy okay that peacock whoa that's heavy that weighs me down it's a beautiful display but it can cause it to lose energy because it's tired or it can be caught more easily by virtue of that why on earth does it have that because it shows it's fit and it can deter predators because of that signal that comes out and the analogy I use the dishonesty principle is where this comes into play and let me give you the story of forgive me I can't say a person this is it trying to attract a scarce resource which is a p.m. that's why they have that display to show their fitness well a Rolex I think is a good example of the human equivalent of that a man walks into a bar okay he's wearing a Rolex why is he wearing a Rolex does it tell the time a whole lot better than your watch does I don't have a Rolex by the way I'm pretty nice watch it but not a Rolex well a man walks into a bar and he's quite often trying to attract scarce resources and sometimes it works okay well the next thing is a man walks into a bar and he's wearing a fake Rolex does it work maybe why would there be a market for fake Rolexes as they didn't work and the last story is everyone walks into a bar wearing a Rolex or a fake Rolex nothing works in that particular case so the issue that we were trying to find is basically how many fake Rolexes would work in a barroom situation that's referred to as the dishonesty principle and there's this honesty principle addresses the issue of fake signals that can be used for effect in either attracting resources or rebuking predators and we did it in the context the Predators here this is the simulation and the actual group showing the mobbing behavior and the thing is a robot can is a perfect poker player okay you can't tell it can Bluff wonderfully okay better than any human being can do I would contend and what if it's on its last you've seen your cell phones and the battery's down to like this much okay should I try and find a charger or or should it fake that it's strong or just die in place so to speak so we wanted to study how many what the situations are where this kind of deceit would play a role this bluffing in this case so we studied this again and looked at the strategies it wasn't too unexpected again deception was the best strategy when it pushes the mob size to the minimum number required for the part of their to flee and for small mob sizes in other words if you have like two or three agents and one is unfit that's pretty risky business and you're likely to die under those circumstances but if you have larger ones and there's you know seven or eight others as well - that's enough to increase the harassment rate of the harassment value for the predator to allow you to escape didn't get any publications I know the press didn't pay attention anymore they were tired but now my PhD student just recently completed completed this work on other oriented deception again this is for your good I'm gonna tell you a lie because it's better for you you'll be happy if I tell you this lie isn't that good conference say no utilitarians might say yes we'll talk about those ethical consequences in a bit but this again is - even if it costs the agent something you HRI by the way stands for human robot interaction allows you to produce deceptive effects that produce value in the mark and oddly we looked at extending criminological law as the basis for this deception because it added something to it which we didn't have before and these are instances where you might see this in the case of accidents or a triage sometimes people will lie to the victim helps on the way somebody get help okay you'll be all right not so sure is that okay well what that does is potentially reduce the likelihood of shock and thus is of the benefit to the mark there's a terrible movie and what's called terrible bosses - I don't know if you've seen it I am not recommending it in any way shape or form but it has an interesting seat where this is a spoiler alert and I feel no compunctions about telling you that near the end of the movie there's three guys one of them gets shot he's lying down on the floor and the first one comes up but he says something to the effect of look at you you're gonna die the other one comes up and says you can't tell him that he'll be alright you'll be fine don't tell him that this is look at him he's gonna die he's really in bad shape the other one is saying no it's okay don't listen to him what's the right answer watch other Roma do find out if he's Catholic you can call a priest I suppose but this is a notion what do you do in this particular case so in this study we looked at criminological law and not only when and how we had to have a reason of why in this case that's where motive came into play it's important to have a reason for why you should lie under these circumstances so that was incorporated into it as well and so we had this notion of motive a reason and tension for man attention you still have to have the method the how and you had to have the wind which was called opportunity in this particular language this discipline we're always in robotics looking for other disciplines for ideas you guys have any as well we're happy to hear this okay so there were two types as well two we looked at deception by Commission which is kind of what we've been doing deliberately telling you a falsehood but you can withhold information as well too which is a form of deception both of those counts in terms of deceptive community so we explored both of those in our study we use some human robot interaction studies using a small humanoid with a now studied this with elderly patients or elderly subjects in the context of a pill sorting tasks for those who are elderly you may know what that is and making sure that we could assist in learning how to do pill sorting effectively which if you don't do it correctly has very serious consequences and using three forms gesture recognition as kinesics or body language variety of different forms the facial expression if we use that particular robot and also the spatial separation between that and the individual three different components that we merged into an appropriate response on this study and these show some of the examples where we would give a true action should have been hiding in the other case it's showing positive a variety of different things so basically we know what it should have shown happiness in that particular case or sadness or disgust again there was probabilistic release of these things and then we studied the effects on the human subjects as well and the one thing that leaped out oh and also we use case based reasoning a form of artificial intelligence to analyze the situation that we were in and bring that into play to call from analogical reasoning a case that has happened in the past lawyers know about this a case that has happened in the past and then use that as precedent and modify it accordingly for the current situation a fairly classic artificial intelligence technique and we used that also enabled learning the setting that we did in this case was using a dual task study where we asked them to do some math problems at the same time they were doing pill sorting to stress them a little bit to see if they would still do well a fairly standard technique in these kinds of studies and what's interesting to note is that the one case where significant differences in ROS was in frustration we were able to reduce the frustration in the patient but the client I have to use the right language here by lying to them which would potentially keep them more engaged in the task over longer periods and not abandon it due to the frustration but that's a hypothesis we haven't validated that with a longitudinal study okay all right so we'll get to the ethic stuff in a minute okay bear with me we have one study that's ongoing as I mentioned this is this misdirection task which is dealing with teams of agents working together there's lots of examples of this political misinformation bots strategic lying use of feints Andrews's in sports military deception but we were looking at con artists and shields in particular a shill is a Confederate to someone who's trying to deceive you okay I'm lying to you and you're gonna say that's great why more but you're not gonna say it that way you're just gonna like it if you've ever played or seen three-card Monte or something like that the confidence people do a shill will come up and start to win and then you'll start saying hey I can do that too and you will lose your shirt eventually as well so we're studying actually what's the effect of shills on deception in this case and the problem that we're looking at is moving a team of robots or agents from one location to another and we're exploiting in this case the ability and carrier a city of people if you are in a group let me just say you're milling about outside after the talk and you see people starting to move off in a particular direction with intent love you like where are they going I'm gonna check that out you might follow them that's an underlying assumption we have a threshold for that is it one person enough to drag or drag you off is it 2 is it 10 how many people does it take to mislead you into following them okay that's the question that we're asking and the shells play a significant role in that how many shells does it take to additionally reach that threshold here we have a leader and we have we have push and pull in this particular case we're looking at the pole right now there's a leader and everyone is following them we don't have a robot that can entrace people or other agents at this point in time but if you had confidence that what leader was a true leader even though it was deceiving you and that's part of what a good con man knows is that you have to build trust in the mark before you do the deception if you don't have that trust it's not going to work so we did a several different studies in this looking at we haven't we did implement this in robots I'm sorry we use a new technique called the robot areum at Georgia Tech which enables us to study these different things but in some cases it didn't work but with the addition of shills we were able to drag a good number of folks over and we studied the number and the number of shills and when it would work and this research again is ongoing at this point in time my student actually is presenting a paper in China I know it's this week or next as well too and a robot ethics conference on this particular topic this shows the robot areum that we have there which enables us to run these experiments fairly straightforward with little robots in the same way that we did in the laboratory with different numbers of shills and demonstrations and the like sometimes it splits when we put obstacles in but the real metric is can we get everybody over there and how many shells does it take to be able to do that okay so if you want to be calm people you can be calm people where's the con persons I don't know what gender challenged with come on that okay so there you go this is the push approach this is like sheep herding okay sheepdogs sheepdogs are pushing them it's a fear based approach in this case we use agents that go behind and push the agents in another direction it's still a single agent but one agent may not be enough if we use several shills as well too we can more efficiently move the group in this case we still have a leader but the leader and shills drag all the others around in this particular case and here's some of the videos again boring simple robot experiments but basically showing and testing the ideas that we have and quantifying the amount of shields versus numbers of agents that we can effectively carry this out will be varying the environments even more in time to come ok this is an ethics talk right should robots lie how many think robots should lie how many things they shouldn't lie how many don't care what about your phone should it lie to you what if your phone you set an appointment ok it knew you were perpetually five minutes late to every class of meeting and it set the clock deliberately five minutes ahead of time so that you would be on time would that be ok well it's fear I'm good that's deception for your own benefit as well too so you don't get in trouble now there are many different ways as those of you studied ethics are quite familiar with the different frameworks that exist will first look at consequentialism and deontology if we look at deontology as evidenced by can't what did Khan say taught in every 101 class he said don't fly they will destroy all human communication okay the universalization of that will show that you can't trust the one person you can't trust anybody communication is worthless if you lie okay so robots better not lie then write any deceptive behaviors are morally incorrect according to that particular theory what about consequentialism or taking a utilitarian perspective where you're looking at accrued benefits over a larger group what if I make you happy by lying to you and unhurt me that much doesn't hurt the people around me all the stakeholders are the largely unaffected is it okay makes you feel better right that good well if you're a consequentialist you would say sure why not it's very interesting and that's this virtue ethics as well too and other aspects thank you virtue ethics and other things as well to which bring into question what is the right answer to this question even if it's me lying for my benefit what if I get more benefit and you lose a little it's a war okay what if we win the war because I've carried out a deceptive action lots of question how to count the stakeholders which is a chronic problem with consequentialism and the luck but there's folks that believe is there an inherent right whereby humans should not be lied to do you have the right to fundamentally perceive the world as it is there are philosophers Rob Sparrow for example it's argued this eloquently that even a robot dog like I both which I worked on and Sony as I mentioned earlier is a fundamental deception which it is it's not alive we are creating the illusion of life in these things but it makes you happy if you like it and makes you happy what's wrong with that you are not perceiving the world as it truly is you have a right to do that so there's this notion of opting in or opting out or transparency and the like okay and again this notion of appropriate use once you create the technology will it be used in an effective way and finally what do you think about the people that are doing it okay why are we creating it in the first place oh why what have we done Wagner and Arkans name will live in infamy for forever we're not alone in this particular space and as you probably know deception is rampant on the Internet it's everywhere you're being lied to almost every other thing that you read there whether it's political whether it's social just not the truth but you still keep reading it why do you do that okay so again consequentialism tells us that it's fundamentally wrong utilitarian arguments say that under circumstances it could be right lying would be from a virtue ethics perspective not a good thing right because it doesn't make you a better person per se but if you care about other people maybe it does I don't know about eudaimonia and the like this interesting case of parent management of patients with dementia in Germany they actually you know dementia patients have a tendency to wander off and not be found so what they did at this home was they put a bus stop sign outside so the dementia patients would stand and wait for the bus that never came okay good was it really don't have the right to go and wander off okay apart from that whether a human being should do it or not should a robot do it or not well there's a group a very large group one of the largest professional societies in the world if not the largest and actually I was in charge of this committee the effective computing committee as well too and I was on the executive committee for the I Triple E global initiative on autonomous and intelligent systems which have created guidelines for all sorts of ethical questions regarding autonomous systems some of the recommendations that we made for deception is we need to find the acceptable conditions and it was determined by these were crowd sourced commentary by the way it's not just a couple of people in a room coming up with these recommendations that deception may be acceptable an effective agent that means an effective agent that is expressing emotions not feeling emotions but it's expressing emotions to the point where you think it has emotional state when it's used for the benefit of the person being deceived not for the agent itself deception might be necessary in search-and-rescue operations or for elder or child care one of the things I have patents with Sony and Samsung in robot emotions notice I put them in quotes these emotions are not emotions the robot actually feels they are states and behaviors that the robot exhibits to make you think it cares about you or it's happy or you it loves you we can do that that's incredibly easy to make people feel those sorts of things when you go to a movie screen you see a bunch of dots on a screen now you cry you laugh those people are actors they're great deceivers right actors they're deceivers they're not the people that they pretending to be they're liars each and every one of them a director is managing it all right under your nose and you're still happy about that hmm you pay for the privilege to be deceived suspension of disbelief is something that you believe is a good thing and then finally interception to be used a lot and reasonable justification must be provided by the designer so is it ethical for me to create a robot that you think has emotions that you will care about you will actually care about it you will miss it when it's gone you will treat it if we succeed like a real pet you have feelings towards those I'm not going to go because it's this disillusions people sometimes about what actual animals have in terms of or lack thereof in terms of emotional state but we / we project we project onto artifacts and other things as well - what we want it is OK for me to create robots that support that illusion that lie that illusion of life anyway so that's pretty much what we've been up to if you want more information on that the full report is available on the website we are continuing the work and I think I mentioned the beginning we are beginning to do counter deception as well - to find out how to undermine those shills that are planted in the crowds I remember as Brian mentioned the beginning of the social implications of Technology Society and have served as an officer and most interestingly there's a course that I share with my students called robots in society where we engage in discussions all semester long about this and many many other questions underpinning robotics research and with that I'll conclude and open up the floor for questions thank you [Applause] all right yes I wouldn't refer to that as deception unless it was used in that context altruism is what you're referring to where someone will sacrifice for the benefit of others highly controversial topic in biology as well - and yes we have considered it but we I haven't attracted funding for it was one of the reasons we haven't explored it to date but altruism is an important aspect of these kinds of systems so the notion of sacrifice on behalf of others warrants further investigation it has been studied by some but not so much in the context of robotics to date so there's Neil Wilson the ants guy the famous biologist he said that at one point that altruism doesn't exist anywhere in nature it's an illusion that you think you're sacrificing something but there's really some motive in it for you that you don't understand he recanted that more recently as well - it says actually he does believe it exists so it's it warrants analogy I'm not sure what a robot really can sacrifice in the sense of feeling sacrifice is the other thing as well - we can make it look like it feels something this is all that illusion as well - but we can certainly have the robot walk off a cliff or or do something well there's a wingman project as well to the air forces has developed which potentially supports those kinds of actions as well to to enhance mission capabilities for these kinds of systems so yeah excuse me it warrants further discussion certainly I see a question over here if you could wait for the microphone excuse me so kind of been highlighted in one of your last slides there was sort of I Triple E guidelines and it talked about a designer having to you know have a good valid reason for doing something and most of the research I haven't read the papers so maybe I'm misunderstanding this but they are more of the actor kind of AI things that have been designed to look like they're intelligent or mimicking human behavior but there's a big topic nowadays in the artificial general intelligence world where they're talking about intelligence that doesn't really have a designer and the level of behaviors that it has and then you have to talk about more about the alignment of the interests of those kinds of Agis with humankind have you any thoughts on that research nary I haven't done research and I presented in an AGI conference early on as well too but for those of you who are not familiar with artificial general intelligence it really is concerned with trying to achieve human level performance and intelligence and beyond with the human beings cognition being the hallmark and eventually it will some have argued in the singularity where we reach that point and go beyond it intelligence of these machines will leave us behind and who knows what will happen I have not personally focused on that I have focused more on near and mid-term people have projected the singularity for example where we can be duped easily and led to who knows what particular purpose remember I said higher-order intentionality is often a prerequisite for deception the near-term problems are very daunting even in my case the issues of privacy the issues of intimacy which we haven't talked about the into intimacy's of lethality the issues of deception and self-driving cars which others are talking about as well - there's so much on the near and midterm plate I have not personally focused on the AGI agenda but you're right I mean if they receive that particular level of intelligence they will have achieved the ability to do deception then better than anybody and I have often said that although I'm not particularly concerned about it at this time I'm glad that there are people thinking about it and paying attention to those particular problems Nick Bostrom I had a debate with him a few years ago as well - a few familiar with him any other questions it seems like for the concept of deception to work that the robot must be able to determine if it's deception worked or not okay so is that a part of this and early on you said why would a robot deceive or anyone deceive if it doesn't benefit the deceiver and one answer to that which he partially covered later on was that the robot or person needs to learn if the deception works or not okay so how does that enter in okay in response to the first question that's like battle damage assessment in the military as well to when you've done something it's very important to know how well it has worked the kind of deception which I hope you could pick up on that we looked at was what we refer to as one-shot deception which is the easiest type if it didn't work we're in trouble in any case and it doesn't even want repeating because they will figure out that that's been used and you can't repeat that particular strategy in the same similar circumstance for those particular agents so the next question was related to that which can you refresh that part just just basically the that the robot needs to understand the deception worked or not well if it accrues its benefit like a con man and runs off that's all it needs to know it may run off in either case but does it accrue the benefits that had been allotted to it as part of the game theoretic approach so it's useful to know that there's no doubt about that especially for sustained deception if you it's really hard to maintain deception over long periods of time if you look at people that are having affairs and other things like that as well - it becomes hard to maintain that sustain it over greater periods and rather than just lie once and maybe you got away so we have not studied sustained deception in this particular concept which I would contend warrants it other other than incidentally as I mentioned before if you consider building a robot dog that looks like it's alive and the goal is to make it looks like it's alive indefinitely that sort of thing we have done but that's not quite the same as the the forms of deception I was talking about today ok time for one more oh right there so obviously you can you know stand up here and talk about all these different ethical dilemmas that robots face and you know and they're very important topics to cover but kind of how do you deal with like so for something with self-driving cars right it's very public everyone kind of more or less understands that we need to come to an agreement on it before we can proliferate the technology but for things like like marketing or advertising or you know like Facebook for example right like they could start utilizing these deceptive right mechanistic properties and no one would ever know and you know same with foreign actors and with domestic actors so how do you try and kind of like how do you try and control like is there anything you can do about that have you thought about you know how you control a deception in these closed private areas where there's not really the same amount of insight as a topic like driving yeah that's that's a very good question and the issue is laws controls regulations legislation if it's done in a back room somewhere someone has to shine the light on it whistleblowers and other things as well - or ways in which information could be obtained about inappropriate action the guidelines the I Triple E guidelines and other things as well to provide a basis for thinking about regulation but they're just guidelines right now the I Triple E actually has gone further in this case and they have formed standards about certain past so we've talked about nudging earlier today they have one for nudging which is being crowd-sourced right now as well - in terms of what should be done or what shouldn't be done in that particular case you might want to take a look at how they're trying to change this into a standard and then ultimately I Triple E has lobbying capabilities in Washington and conceivably could and clout within the companies that you mentioned as well - because many of the computer scientists and engineers are members of the society try and inculcate ethical behavior in the individuals but there will always be rogues I mean you have to be realistic about it so the question is to try and find ways to regulate and penalize inappropriate behavior which means it has to be turned into illegal behavior at some point in time if we can find ways to do that that's what's going on with the lethal autonomous weapons systems debate right now which has been going on for six years right now at the United Nations we're still arguing about definitions of what these things happen to be but what's very interesting is six of us were in a meeting at the near MIT for three days locked in a room and we basically came up with people with broadly diverse ideas on what should be done from ranging from the stop killer robots campaign to folks that argue including myself that there are potential humanitarian benefits can accrue yet we came to agreement on a potential roadmap forward which we're hoping to forward to the States for their consideration along those lines so all you can do is speak up I guess that's that's the key factor just don't assume everything is going to go well but don't assume everything is going to go poorly as well - because there are many people that are concerned about the behavior of scientists engineers and companies at this point like guess that's it thank you very much [Applause] 