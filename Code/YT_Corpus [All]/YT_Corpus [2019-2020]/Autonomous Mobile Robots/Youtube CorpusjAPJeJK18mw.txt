 [Melanie Zellinger]: Our next speaker of the afternoon is Sergei Levin from UC Berkeley. His research is in using learning for decision-making and control and he's very well known for his work on deep learning and reinforcement learning, and he's going to talk to us about his results on robots that learn by doing. [Sergey Levine]: All right, so imagine tying your shoelaces, and now imagine that I asked you to explain to me how exactly you tie your shoelaces. If you recently had to teach you a small child to do this, you might actually be able to answer this question, but for a lot of you, you probably can do this very well, but would struggle a little bit to tell me exactly how, and this is a special case of a particular problem that has actually puzzled people for a very, very long time. So Socrates, for example, spent a lot of time going around and asking people who had particular skills - maybe craftsmen, priests, and so on - how is it that you do what you do? What is it that makes you an expert, and how can I become an expert also? And across the board, people didn't give very satisfying answers to this. There are a lot of things that we're very good at, especially physical skills, that we actually really struggle to explain, and if we're going to think about building intelligent machines that can reproduce some of those behaviors, this should perhaps seem like a bit of a problem to us. If we can do things that seem quite complicated and intricate, but can't explain how we do them, what hope do we have of telling our machines how to do them? In fact, the problem is actually even more mysterious than that. You can know how to tie your shoes, but you probably also, without really worrying about it too much, know how to generalize that skill to new situations. In fact, this was true for most of the things that we're able to do. We can distill down the essence of those skills and generalize them pretty much effortlessly. So, if you have a different kind of shoe, even a kind of a peculiar kind of shoe, you're not really going to be thrown off by that. You understand, you have a common sense that allows you to generalize your behaviors, but when we try to systematize that common sense, when we try to pull out the bits that make that work, you think 'well, maybe there's something about objects or inertia or physics that we really need to understand to get that generalization'. Across the board, in real, open world environments, the reality kind of defies our attempts to systematize it. We always are confronted with this kind of endless sea of exceptions and special cases, so that each of these very seemingly elegant concepts aren't, by themselves, quite enough to achieve good generalization. Now, what we could do is we could imagine that maybe the unfortunate reality is that these things really are just that complicated. There isn't, sort of, a simple equation that you could write down that would allow you to perform the skill. You really do need to somehow internalize all the complexity of the world to make that happen, and it's just too much to expect that we'll be able to specify that by hand. In fact, if we look to domains where we've been able to build machines that successfully generalize in open world environments, things like computer vision, speech recognition, natural language processing, etc., we'll actually see that pretty often the recipe that does result in good generalization is to not worry so much about distilling down the underlying principles that underlie, let's say, computer vision or speech, but to build large, high capacity models, that's where the deep learning comes in, but also - crucially - train those models on very large amounts of data, and that second part cannot be ignored, because even though it's easy to say that 'well, big neural networks are responsible for some of the success in computer vision' that's not actually true. It's big neural networks and large amounts of data. So now we could think 'what does this imply for how we can build intelligent agents that actually go out into the world and make decisions, that actually behave rather than just passively perceiving?' Well, we can use large, high-capacity models, that's fine, that's actually reasonably straightforward, but we also need to get the kind of data that will enable that generalization, and that part actually ends up being very difficult. If you imagine an autonomous agent like a robot, for instance, if that robot is going to learn a skill like tying shoelaces and will then generalize, it'll need to actually experience a wide variety of different situations to acquire that generalization, to learn about all those special cases and exceptions, and that's a big puzzle. Where do we get the training sets for this and who will label them? There is also a second puzzle, a second puzzle that's not even resolved in the passive perception world, which is - once you do train your model and you encounter something peculiar, what happens if you don't generalize? How do you fix your mistake? Current methods of computer vision and speech recognition don't have an answer for this, either, but I'm going to claim that for autonomous agents, not only can we actually do better in terms of acquiring large data sets, we can even fix that second problem, using essentially the same approach. So, here's the idea. Autonomous agents are autonomous and they can collect their own data, so they should actually have a big advantage in terms collecting large datasets as compared to the kind of systems we use for passive perception. So they should have an advantage, they should be able to go out and collect their own experience, through that experience gain understanding of the world, and if something goes wrong they can just collect more experience and fix it. So, in the same way that you might imagine that a child plays with things in the world - this is what we heard about in the last talk as well - acquiring an understanding of how that world works, our machines should be able to do the same, and if they're able to do this, they will have large data sets from which they can generalize, but more importantly if they do encounter a situation where their current model fails, all they have to do is just keep going. Just collect a little bit more of that self-supervised data, improve their performance iteratively and get better and better. And I think ultimately that's going to be the recipe to build autonomous agents like robots that can actually generalize in meaningful ways. So what we need is to make it possible for these systems to essentially improve perpetually through their experience, and that's, of course, a lot easier said than done. So the trouble is that the world is full of these exceptions in special cases, like the different kinds of shoes. There's not really any simple, parsimonious policy that will just do everything, so while individual local things can be explained, and I promise Russ I didn't pick this after looking at your title *laughs* unfortunately in many cases simple parsimonious rules are not sufficient, and you really need to deal with all the complexity, and the mistakes are inevitable. So even the best system will fail sometimes, and if you want a system that will actually generalize in a meaningful way, you can't avoid the mistakes. What you could do is you could have intelligent, autonomous agents that keep gathering more experience and keep improving so that when they make the mistakes, they can prevent - they can fix them, they can prevent making those mistakes again. So how can learning-driven agents achieve this kind of perpetual improving? How can they keep getting better and better through their experience? Well, we need to design algorithms, methods, where you can have an agent that collected large amounts of data. It should be able to use all the data that's collected to improve its model, whatever that model might be, whether it's a predictive model or a policy, and occasionally needs to be able to go out and get some more data to fix up those cases, to fix up the mistakes. So this process has to be automated in order to scale, because if you have every part of this loop that improves as you gather more data, the whole agent gets better and better. But if there's any part of that loop that doesn't improve through data, it will eventually become the bottleneck. It might not be the bottleneck initially, it might be a very good initialization, but if there's something in there that doesn't improve and everything else does improve, that bit that doesn't improve will be the bottleneck, and it will be the thing that holds back performance. So the corollary to this is that any manual part of this loop will eventually become the bottleneck. So, then the game that we have to play is to try to imagine how we can build the system so that there is no bottleneck. So that every part can keep getting better and better. So, there are a few things that I might mention just to make this a little more concrete, that are things that sort of seem like a good idea at the time but can become really nasty bottlenecks if you're not careful, and I'll intentionally use examples from my own work because I don't want to sort of call anybody out, but these are a few things that are maybe worth keeping in mind. You can get very far by hand-designing a state representation. You can say 'well I have a robot, my robot interacts with objects that are primarily rigid bodies floating in space, so I will localize rigid bodies I'll use their 3D coordinates and I'll use that as my representation of state, and then I'll do some learning on top of that,' and that's a very good idea because it makes learning easier, but unfortunately not all tasks fit into this representation, and of course once you've gotten far enough, eventually all that other stuff, all those exceptions will become your bottleneck, and they'll be the things that you can't handle. So if you have, for example, a robot that needs to interact with some deformable objects, then of course if you assume the world is made of rigid bodies that's not gonna happen. Okay, that one is kind of an obvious one. Another one - this is actually quite popular in robotic reinforcement learning experiments - instrumented environments. If you want to train your robot to do something, this is an experiment we did a few years back where robots are opening doors. You need some way for the robot to tell if it's doing a good job. You need a reward function or a cost function, and you see that those doors have a little wire taped to them. That wire leads to an accelerometer which is used to measure the angle of the door. That's how the robot knows if it's succeeding. That's very nice because it provides a clear learning signal, but of course it's a huge problem in reality because it limits the number of environments and tasks that the machine can do. So if it trains on these doors, then goes to this building and tries to open a door here and fails, it can't improve because it doesn't know whether it's succeeding or not. Here's a third one, this one's gonna be a little controversial. Using simulation. Simulation is great, because it can let you prototype your algorithms, and simulation can tell you how to get things to work in the real world, but if you're relying on simulation to build the entirety of your controller for your system, that will become the bottleneck. So a few years back, we developed some methods for transfer from simulation to the real world that have since come to be known as domain randomization. It's a really great method for transferring behaviors from simulation to reality. You can actually transfer things to the real world without any real-world data whatsoever. It works quite well, provides excellent initialization, the trouble is that if this is the only thing that you use to build your policy, then eventually your agent will only be as good as your simulator. It's not improving through data, it's not improving through its experience, which means that eventually the simulator becomes the bottleneck. So how do you unblock this data pipeline? How do you enable the robot to just keep getting better and better the more it experiences the world, or whatever other autonomous agent that you have. Essentially, we can think of it as - there's this data set that the robot can collect through its experience, there is the agent, what it actually does, and the data needs to flow, it needs to keep making it better and better with nothing obstructing that pipeline. So in today's talk, I'm going to talk about a few of the recent works that we've done that try to unblock some parts of this pipeline, obviously not all of them, but hopefully making a little bit of headway on that problem. So, I'm gonna talk about three things. The first one is, how we can efficiently define which tasks the machine should practice. So, how can we get away from that instrumentation of the environment so the machine can derive a learning signal from natural sensing that it has onboard? How we can get away from having to define tasks altogether? How a machine can actually invent its own tasks, practice those tasks and use that to learn about the world, and lastly how we can incorporate large amounts of prior data into a single policy, as maybe also related to some of the things discussed in the previous talk about historical data. So let's start with the first one. Many of the reinforcement learning successes that we've seen, especially in the media, are focused on games. Games are great for reinforcement learning, because in games there's a very, very clear signal for what you should be accomplishing. In fact, in some games it's right there at the top of the screen. So in an Atari game, you look at the score, that's your reward function, you just have to make that number big. Unfortunately, in the real world things don't usually work that way. If you want to, you know, get a robot to pour a glass of water, something that any child could do, you have to build an entire perception system, an entire vision system, a very sophisticated one, just to detect how much water there is in the glass. So, you have to essentially solve part of the problem before you can even begin solving it. It'd be really nice if we can acquire the ability to determine success or failure without having to build anything by hand. So we can start thinking about ways to do this. One really simple way to get started is let's say, well, okay maybe somebody can give us a few examples, a few examples of what success and failure looks like. So maybe, if this robot needs to stack some books on a bookshelf, it's told 'here's what a good book placement looks like, here's what a bad book placement looks like,' let's train a classifier on that, and let's use that classifier as a reward function. Fairly simple data-driven way to do it, pretty general, doesn't require programming to specify a new task, just requires a few examples. This really doesn't work. It's a little subtle why this doesn't work. Let me explain it with a little simulated example. Here's a robot doing kind of a silly task, it's pushing that green cylinder to put it on the red circle. This is using a ground truth reward function. The reward is one when the cylinder is on the circle, and zero otherwise. It's kind of tough because the reward is very sparse, so actually if you just run reinforcement learning on that, it doesn't actually work all that well. But that doesn't matter, this is just to show you what the task is. If you train a classifier from images, the image shown in the top right corner of that video, the robot does not succeed at the task. What does the classifier think is going on? That's the graph in the lower right corner. The classifier thinks the robot is doing great. It shoots up to 1.0 probability of success and says 'you're doing it right'. Why does it think that? Well, because you'll notice that the robot takes its fingers and it kind of angles them upwards and points them towards the camera, and that's enough to fool the classifier. Now, you might say 'okay, that's kind of a lousy classifier. Maybe I should have trained a better one.' The problem is that the reinforcement learner is explicitly trying to optimize against the classifiers, essentially trying to discover a natural adversarial example. So even if you use a large amount of training data, this happens all the time and it's a really, really big problem. So, you need something a little bit more. You don't want to ask your reinforced learner just to exploit your classifier. It turns out that we can borrow a few ideas from inverse reinforcement learning, and develop an algorithm that we call variational inverse control with events (VICE) that largely mitigates this problem. The idea behind the algorithm is actually very simple, though the derivation to show that this idea is correct is a little more involved. The idea is that you train your classifier, and then whatever the robot does, you tell that it did it wrong. It's like the worst exam imaginable. Whatever it does, it's just told 'no that's false, that's a failure'. That's added to the training set for the classifier, the classifier is updated and eventually this actually squeezes out all those bad situations. You can also think of this as kind of a variant of a GAN. Okay, so you can use this idea to get some real-world robotic learning, with tasks that are specified just with examples. So, we're going to have a person show the robot a few example successes. Everything that the robot does will be labeled as a negative, the classifier will be iteratively updated. Everything here will be done from images, so the classifier is just a ConvNet that takes in the image and outputs true or false, the policy just takes in the image and outputs the robot's actions and everything is trained end to end. This can work, but we can make it work even faster if you make one small modification, which is to allow the robot to occasionally ask a person whether it did it right or not. So if we allow for a small number of queries where, for those situations where the robot is most unsure, it can actually ask for a true, a correct label from the human, then things will go even faster.  So here's an evaluation on three real-world robotic tasks, all of them learn from images. This method here just does the negatives, and it works some of the time. This method uses just the active queries, and this one does both of them together. Now, of course, I haven't told you what those tasks are, so let me show you some of them in video form. Here is a relatively simple task. The task here is to push this cup to the white coaster. These are a few of the examples that are given to the robot in the beginning, and then here's the training process. You can see at the bottom how many queries the robot has made. So it only queries for a small fraction of its experience, just the thing that it's least sure about, and you can see that after about an hour and fifteen minutes, it can actually do this task and push the cup from any location directly from images. Now, of course, that task is not that interesting because, you know, pushing rigid objects on a table, there are many ways we can do that. So we wanted to test out a task where using image observations was actually really important. We thought - well, let's get the robot to drape cloth over this box so that it actually has to watch the cloth, watch the wrinkles in the cloth to see if it's doing it right. So, here are some of the examples provided the robot in advance, and here's the final policy draping the cloth. Now, we looked at this and we thought 'well, this looks like almost effortlessly easy, so we need to really make sure that it actually needs vision to perform this task.' So we constructed a baseline where the robot was not given the image, and was just told to move its hand to the position where we knew it would be if it draped the cloth successfully, and indeed that doesn't actually work. So this means that vision really is important, it really needs to look at that cloth to make sure it's being draped properly. Okay, now of course there's a lot of related work to this, a lot of the mathematical foundations for this method are based on inverse reinforcement learning, and there are a few citations to that. There's actually surprisingly little work on using classifiers as reward functions, probably because if you do it naively it doesn't work very well, but there are a few references to that on this slide. Okay, let's talk about the second blockage in the pipeline. So, if we can specify tasks just by giving examples of outcomes, that's good because humans can specify lots of tasks pretty easily, but you still need a personal loop to actually specify those tasks. It would be really nice if the machine could practice behaviors on its own without anybody telling it anything about what it should do. So, let's imagine that we want to create a situation more like the one that I showed in the video before, where the machine actually plays with things in the world and continuously improves without being given explicit goals. We still need some way afterwards for a person to communicate the goal to the robot, so we'll say, we'll do the same way as before. After training is completed, a person will show the robot a picture of what they want, and the robot should go in and rearrange the world to match that picture. But during training, that's not gonna be present. So the robot is going to interpret these pictures by using some kind of latent variable model. It needs some way to understand whether what it's seeing now is actually similar or different from what it was asked to perform. So we'll train a latent variable model, there are many choices for this. You can use something like a GAN or VAE, the point is that it needs to embed images in a latent space where distances actually have a meaning, where distances have a meaning in terms of similarity. So we'll use this at test time, so the robot understands whether it's succeeding, but at training time, what we'll do is we'll use that same latent variable model to actually hallucinate potential goals. So we'll actually sample from the model, generate potential images that might correspond to valid states of the world, and ask the robot to go and practice those, to attempt to reach them. Now, this seems like a very simple idea, and some of you might have already realized that there's something a little bit wrong with this idea, because where are we gonna get that latent variable model? Well, presumably you'll get it from whatever the robot already did. So the robot will have done something, maybe it picked up this pointer, it'll use that to fit this latent variable model, to understand which states exists in the world, it'll sample from that model and it will just do the same thing again, so it'll just keep doing the stuff that it knows how to do. We need to somehow force it to diversify, and there's a very simple method that we developed for doing this. The idea is that if, let's say, these circles represent the points that your robot's visited, you'll see that there are some points kind of densely clustered there in the lower left corner, and they become sparser out on the edges. What we want is we want to preferentially set goals that are out on the periphery, out on the tails of the distribution. So what we'll do is we'll reweigh those points based on their local density, use that to fit our goal sampling distribution, our latent variable model, and then sample from that to set new goals, and this will get the robot to go and explore the periphery more than the middle. So it'll get it to actually go into those regions where it's less comfortable to force it to explore. That's the intuition. Mathematically, we can actually show that under idealized conditions, if we apply this algorithm, it will actually eventually get uniform coverage over all valid states. So this is actually one of the few exploration algorithms that does have theoretical guarantees on coverage, and we can use it in the real world. So this is a real robot. It was not told to do anything. It was placed in front of a door and asked to do whatever it wants. Whatever it wants using this goal sampling method, and at zero hours you can see it's not doing something very meaningful, just sort of shaking around randomly but after about 25 hours of training, it figured out that it can open the door. Usually when you watch this thing training, what happens is that once it starts opening the door a little bit, it quickly over samples those door opening images because they have lower density, and then it really starts to go in and play with the door. After 25 hours, it gets pretty proficient at this, so that at test time a person can go in and show it a picture of the door, open to whatever angle you want, and the robot will open it to that angle. So here we're just sweeping through the possible angles to make sure that for every door angle, it knows how to do it, and again this is all done entirely from images. So, a priori, the model doesn't know anything about doors, it doesn't even know anything about rigid objects. All it sees is image pixels. So, you can see it can open pretty well to a variety of angles. Okay, and again, related work. There's been quite a bit of work on exploration, exploration's a very widely explored topic in the reinforcement learning literature. One of the things that makes this method a little bit unique is that it does have a guarantee under some simplifying assumptions that it will get full coverage all over valid states. There's also quite a bit of work, especially very recently, on unsupervised reinforcement learning - learning without goals - so you're welcome to check that out, and then for the third part of this talk I'm going to talk about, talk a lot more about, actually, generalization. So, so far I talked about how we can specify objectives, how we can avoid having to specify objectives, but I haven't actually shown reinforced learning policies generalizing in a very broad way, the way we've seen in computer vision and speech and so on. That's what I want to talk about next, and I think in order to get there we need to have reinforcement learners that can incorporate large amounts of data from their past experience. So, we need to do off-policy reinforcement learning. We can do model-free learning or we can do model-based learning. I'll talk about model-free learning in this talk but there's also some model-based work along these lines. So, off-policy model-free learning, what does that mean? Well, practically speaking, it's basically the picture I showed before. You have a agent that has interacted with the world, it has collected large amounts of data, and then it's going to use that data to train a policy or a value function, ideally for as many iterations as it wants to using the reward for the task, and occasionally if it's not happy with its performance, is going to go back out into the world and collect a little bit more data to fix up those mistakes. And the way that we're going to do this is we're gonna build on a very venerable reinforcement learning algorithm called q learning or fitted Q iteration. Hopefully, many of you are already familiar with this method, but the high-level idea in q learning, your reinforcing objective is to maximize your total reward. A q function tells you if you start in a particular state, and then take a particular action, what will be the reward if you then follow your policy. The Q function is very, very useful, because if you have one you can get a better policy. The way you get a better policy is by taking an action with probability one, if it maximizes your q function, and that'll be better than the policy that you had before. Or, you can skip the middleman. So, this is something called policy iteration. You can also just directly fit a q iteration, which means that you minimize the difference between the left-hand side and the right-hand side of this equation, which will give you the optimal q function. So either of those is fine. We're going to go with the second one, but what we're going to do is we're going to implement this kind of q learning procedure at a very large scale. We're going to essentially try to do it at ImageNet scale. That means that we're going to have a large number of robots all interacting with the world, collecting very large amounts of data, on the order of millions of interactions, and we'll use those to train a very, very large q function. The basic idea is exactly the same, just scaled up massively. So we'll have a collection of robots that are collecting data in real time, we'll have all of our past data from all of our past interactions - in the experiment that I'll show this, about two months worth of data. The data from past interactions is being pushed into a training buffer, data from online interactions is being pushed into a separate buffer, and also saved to disk for later use, and then we have a bunch of these workers that are going to crawl over that buffer and they're going to calculate the right hand side of this equation. We're calling them Bellman updater workers. They're just calculating that max, because it's a continuous action problem that max actually requires a little stochastic optimization of its own, so we have a separate job for that. That gets pushed into a labeled buffer, and then there's a separate training thread that crawls over that and actually performs the training, which is just regression. So this is kind of a systems-level view of large-scale q learning. The task that we're going to solve using the system that we called  QT-Opt is robotic grasping. We're going to do robotic grasping from monocular RGB images, so very, very simple sensing. Just an over-the-shoulder camera, not even a wrist mounted camera in this case. We'll train on about 1,000 training objects, and we're going to execute about 600,000 training grasp attempts total, but this is an off-policy setup so all those pass grasp attempts are being reused. The network here uses about 1.2 million parameters, so it's enormous by robotic standards, tiny by computer vision standards, but somewhere in the middle, and the only grasping-specific feature in this system is the reward function, which is just one if you grasp the object successfully. But you could also imagine combining this kind of approach with the classifier-based rewards I discussed before. Now, to me, the interesting thing about the system is the kind of behaviors it exhibits after training is finished. So, in contrast to robotic grasping methods that are largely geometric in nature, where they try to find a good grasp point and then execute, this is a continuous method. It continuously modulates the commanded position of the arm to correct mistakes, to reposition the grasp. If someone pushes an object out of the way, it immediately reacts. So it's fundamentally using feedback. It's relying on feedback to perform this grasp rather than perceiving the world once and then executing the perfect grasp right away. It can grasp in dense clutter, it can grasp objects that are small, big, transparent, etc., and it does so in this very active way, and this - one of things I find really engaging about this is that if you watch these videos, it really does look like it's paying attention to what's going on in the world. How does it work? How well does it work? Well, it works pretty well. We tested it on a test set of a bunch of objects that it had never seen before. It succeeds 96 percent of the time and actually the 4 percent of the time that it fails, usually what it's failing on is these little rubber octopuses. So, other than the cephalopods, it's actually doing pretty well. And in fact this is, of course, with test objects that it has never seen before. If we allowed it to fine-tune on those objects, presumably it would get better. Okay, now that was a fairly standard q-learning approach just scaled up massively to get it to generalize effectively. We could imagine building reinforcement learning methods that actually do have generalization in mind, in a sense that they have some kind of regularization that encouraged them to generalize better. One of the things we've been working on for the past two years is the framework of maximum entropy reinforcement learning, these are reinforcement learning algorithms that aim for better generalization through a very particular choice of regularizer. In standard q-learning algorithms and actor-critic algorithms, when you learn a value function, you want to get a policy that will maximize that value function, will sit at the peak of the value function. This seems like a very reasonable thing to do. You want to do the thing that gives you the largest reward, but you can consider an alternative approach. Maybe instead of sitting right on that peak, perhaps what you can do is you can cover all the peaks in your value function. If there are two things that look about equally good, maybe you should try to do both of them, and if there's one thing that seems just a little bit worse, maybe you should try that as well, but a little bit less often. This at first seems like a peculiar thing to do, but turns out to work pretty well and I'll explain a little bit of the intuition why shortly. Mathematically, the way that you can do this is instead of maximizing the expected reward under your policy, you minimize the KL divergence between your policy and some positive transformation of the value function, like an exponential transformation. So, you can write out the definition of KL divergence, that's the expectation of the reward under the policy, plus this other term that ends up being the entropy of your policy. So it's saying 'be as random as possible while maximizing reward. That is a very close connection to robustness, and that's what's going to make the policy somewhat resilient to perturbations, and will make it generalize better. Let me show you a few examples for this, and then I'll conclude. So, here's an experiment that we did with Lego block stacking. So this is a relatively simple task. We weren't so interested in stacking Lego blocks, what we wanted to see is whether we would get better robustness out of doing this. So, if you then test this policy that was trained with maximum entropy reinforcement learning, and a mischievous graduate student comes along and perturbs the robot, it actually is a pretty good ability to recover even though it never trained for those recoveries explicitly. It's recovering because of the maximum entropy reinforced learning rule that caused it to inject as much noise into training as possible so as to get it to be robust. Algorithms based on maximum entropy reinforcement can also be used to learn more complex tasks, like this quadrupedal walking task. So this is a four-legged robot, it doesn't know anything about walking in advance. It's turned on and asked to learn to move forward as far as possible. Initially, it just wiggles its legs at random, but after about two hours of training, it can do a pretty passable job of running, and because of the maximum entropy training you can actually generalize decently well to some modest perturbations. So we can put it on a ramp that it's never seen before, and it can actually climb that ramp decently well. We can also put it on some stairs and it will do a pretty decent job of going down stairs. Going up stairs is still a little challenging, so we're working on that, but down stairs works decently well. Okay, so to summarize, I talked about how if we want reinforcement learning agents that actually generalize in a meaningful way, we need to get them to improve perpetually as they collect more data, which means we need to remove all these blockages in the pipeline, because any blockage that's there will eventually become the bottleneck. It will eventually be the thing that most limits performance. If we can remove them, then we can get good generalization. What else is left? There's a whole lot left, but I'm out of time so I'm not going to cover it. Instead, I'll thank you very much for your attention and I'll be happy to take any questions. *applause* [Melanie Zellinger]: We have time for a couple of questions. There's one, by Emma. [Audience Question 1]: Thank you. Nice talk, Sergey. So the grasping thing, I wonder, does that really just work because of the computer vision? So, in particular take the, let's say, latest, like, post net from ZeroFOX and company that seems to be doing really well, combining with raster Drake's MPC motion planning, with some handcrafted simple graphic on camera - would you do anything better than that? [Sergey Levine]: It's a good question. I don't know, but maybe one thing that I would say in response to that is, I do think this task is really really hard. So, people almost never do robotic grasping from monocular RGB images over the shoulder. Now, maybe we made it needlessly hard, but in terms of understanding the performance of reinforcement learning, this is harder than the setup for most robotic grasping systems. The other thing that makes me a little bit optimistic about this one is, if you actually watch what it's doing, which is what I wanted to pull up the video again, it's actually reacting to what it's seeing. So it's really doing visual feedback. It's not just figuring out the perfect grasp, moving in, and succeeding on the first try. So it's not good enough to succeed on the first try, but it's good enough to correct. I think that's really important, and now that said, if you ask me, like, what would be the killer application of reinforcement learning policies that can generalize very well, I probably wouldn't say robotic grasping because I think you're right, I think there are other ways to solve grasping problems, but I do think it's important to appreciate some of the difficulty in the setup here. [Melanie Zellinger]: So maybe one quick question. There's one back there. [Audience Question 2]: Following up on this, where would reinforcement learning do really well? Because the videos that you showed are impressive, but still very controlled environments and the more complex the environment is, the more data you will need. [Sergey Levine]: Yeah, so thank you for the question. I actually, on my last slide, the one I went through in about 50 milliseconds, I did have a video of actually a mobile robot that was driving around in a less controlled environment. But I do think that is actually the big challenge, and I think that one of the big bottlenecks that we don't know how to address yet, but I think it's a huge one, is to get robots to actually learn in uncontrolled settings, because that will eventually be the big blocker for better and better generalization. I think there are huge challenges there involving safety, involving robustness, and so on and those are big, open problems for us to tackle. [Melanie Zellinger]: Okay, with that let's thank Sergey again for a great talk. *applause* 