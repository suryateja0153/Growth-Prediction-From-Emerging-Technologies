 you [Music] all right thank you yeah thank you all for having me here giving me the chance to share something about what I've been working on so we see children in the world can learn to walk and then with experience they learn to walk faster more stable II and in robotics you can program a robot to walk but no matter how much this robot walks back and forth along the field it's not going to get quicker or more stable so in my research I've shown how actually robots can learn from experience and with experience they can learn to become even faster and more stable when walking in fact faster than anyone had been able to program this particular robot before the the general picture of my research though is that I want computers and robots to be able to learn how to take actions to achieve goals so not necessarily just focused on robotics but potentially applicable to many more areas what this means concretely is that the computer or robot what I'll refer to as the agent should be able to learn a policy which will map any given state of the world to an action to allow it to reach a goal so this is the same as the goal of the field of reinforcement learning and I think probably many of you all here are familiar with the many attention-grabbing success stories that reinforcement learning has been putting out that have come out of reinforcement learning the past few years so superhuman performance at video games superhuman performance at the game of Go simulated robots learning to play soccer and then also some industrial industrial success stories such as web marketing some work done here at Microsoft has contributed to some of that control of data centers or control of home thermostat systems but with all of these successes if you look a bit closer of course what we can see is that these learning systems are not very efficient so this Atari video game result involved the agent taking 50 million actions before it had a good policy in the game of Go playing 21 21 straight days of self play millions and millions of games and is one more example these simulated robots a year and a half of compute time to learn all the skills to play soccer so if we want this reinforcement learning to be applied to real-world robotics or healthcare or more data scarce settings there's still a lot of work to be done and I think the question that many people in both industry and academia are asking is can reinforcement learning be data efficient enough for real-world applications so this is the question which is motivated in my research as well but I think before answering this question there's actually another question that we should be asking which is just how can an agent learn to predict the effects of its actions meaning like if I do this then this is the outcome to expect whereas if instead I did that this is what I should expect um so I'm gonna talk about a couple of different ways that we've looked at this prediction question in my research the first one is just going to be given a fixed policy a way of doing a task what is the expected performance that we'll see when we run that if we were to use that policy to accomplish the task and the other thing I'm going to talk about is how an agent can predict how its actions will change the underlying state of the world so I'm going to start with this question of how we evaluate the performance of just a fixed behavior but before jumping into that I'm going to briefly introduce the the reinforcement learning setting if you're already familiar with this this is just like a way for me to kind of get some of my assumptions that I bring to the table out in the open so we're all on the same page so the reinforcement learning world has several components how its usually defines I'm going to introduce these with an example that I'll use throughout the first part of the talk of an autonomous vehicle learning to drive to reach a destination so the first thing you have to define is your state space for the autonomous vehicle this might be its position in the world or where other cars are relative to it this is all the information that it needs to make decisions the vehicle is available to it actions which influence the underlying state of the world such as like steering the wheel braking or accelerating and then there's a notion of reward which captures what it is to be successful at the task so maybe it just gets a small small reward for reaching its destination and if it gets in an accident it could receive a very large negative penalty and then finally it's the policy is what defines the behavior of the agent so for any state in the world the policy tells the agent how it should take an action to achieve its goal interaction in this setting happens sequentially it's not just about choosing one action but the agent will begin in one state it will take an action receive a reward depending on how good that action was in the particular state and then it will transition to a new state and this process will repeat and so one assumption I typically I commonly make in my work is that this interaction will get one for some fixed number of steps and then the agent can like go back to its initial state and try the task again and this whole sequence of interaction is is referred to as a trajectory through state action reward space now in reinforcement learning the problem that most people think about is that of policy improvement we're given some specific tasks let's find the policy which maximizes the amount of reward we can obtain but there's another very important reinforcement learning problem and that is just policy evaluation where we are given a fixed policy we're not going to try to improve it but we just want to know how much reward should I expect to receive if I ran this policy and this is I would argue that this is a more important problem than that of policy improvement first of all it's necessary it's usually a necessary first step if you want to learn a good policy you should first be able to evaluate your current policy and second when you want to deploy systems in the real world you really want to know how well is this going to do before I like put it out there and have it like interacting you know with real people and real objects so let me just go a bit more into depth on the policy evaluation problem okay so imagine this autonomous vehicle has one of two outcomes that's going to see when it drives and I'm definitely greatly simplifying here of course in the real world there's just like an infinite different possibilities that this vehicle could experience but let's just say with very high probability it reaches its destination and with very low probability there's a safety driver that has to intervene to prevent the car from crashing and these outcomes we can quantify with the amount the amount of reward that the vehicle would receive for them so reaching the destination is like a small maybe a small positive event the safety driver happening intervene is a very strong negative event so evaluating the policy is just a matter of evaluating the expected value of what the outcomes will be but where this gets difficult in the reinforcement learning setting is that these outcomes aren't just like kind of one-shot things but they're actually the they're happening in a trajectory so we have like a bunch of individual decisions which all kind of combine together to result in these outcomes and so these probabilities are not themselves known you can't just analytically compute what is going to happen when you run your policy so one setting where this is like really important to consider how we can do policy evaluation well actually sorry just I'm getting ahead of myself I the more formal notion of this instead of moving away from this example is that we're given a target policy which is what we want to evaluate we're going to assume that the target policy is a distribution over actions given the current state and what we want to know is what is the expected total reward that will be received along a trajectory when we run this policy so why do we care about this so imagine that like you were an engineer at a self-driving car company you've got a new policy that you were considering pushing out to users that was going to be controlling their car so one thing that I mean I hope that a self-driving car engineer would do in this situation is they would before pushing that out they would spend some time testing the policy trying to get a very accurate evaluation of what is gonna happen when we push this policy out to real users and then after this testing phase there would be a deployment phase where the policy would actually go out in the real world and during the deployment phase you could expect that there's gonna be a lot less safety mechanisms that are in place if the policy does something really bad so the testing time is where policy evaluation is very necessary and so how I imagine this would probably be done is it would hire a very large number of test drivers you'd give them the pot a car running this policy they would each go collect some trajectories running the policy and report back with how well the policy performed quantifying this of course with the the reward that was received and from this information if you did this you could get an estimate of how well the policy was going to perform so this is a very natural way to evaluate a policy but what I would want to emphasize here is that it's a very passive approach to doing this you're just running the policy and seeing what happens and I'm gonna talk in a second about why that's bad but first I just want to say this is it so there's the general way of doing this is what's known as Monte Carlo evaluation in a reinforcement learning setting repeatedly run the target policy observe state action and reward trajectories and then just average the total reward seen across each trajectory it's a really simple way of doing this the the problem with doing it this way is that there may be very rare outcomes but that they have a very large impact on what your estimate of your policies value is going to be so for instance in this case it's you're very rarely going to see the safety driver have to intervene and so you may miss that or you may if you see it more frequently than its true true probability you may get in a very inaccurate estimate that way so what this would look like is if this showing on the x-axis here how much data is collected and the y-axis what your estimate would be with the blue dashed line being the true value that you're trying to estimate you might be like you know overestimating your policy's value and then when you get this like a single rare incident your values gonna kind of swing around and it's not going to be until you've collected a very large amount of data that just by the law of large numbers you would converge to getting a good estimate of your policies value so what would actually be better to do in this setting is to take a more active approach in how you collect data in other words what we would want to do here is we would want to increase the probability that the safety driver needs to intervene and if we did that we can get a better quantification of the impact of that event on our estimated reward so if your I did say that policy evaluation is somewhat motivated by like applications of safety so I am now advocating that we should increase the probability of a very bad event so if that sounds strange it is exactly thinking about these kind of settings where we can possibly put safety mechanisms in place like here a safety driver of being there to intervene so that we can just get a very good estimate of the policies value before we're in a setting where there are no controls and whatever happens is going to happen and maybe cause you know cause larger damages or something so if you agree with me that being more active in how we do policy evaluation increasing the probability of these significant negative events can be a good thing there's still technical challenges which remain to be able to do this so the first of these technical challenges is once we take a more active approach to data collection where now we've shifted the distribution of the outcomes that we're going to see and because we've shifted this distribution we can no longer just average the rewards we observe like we do in the passive approach and then the second thing is that because we're generating trajectories you need a particular policy to be able to to figure out how to generate these significant rare events so we need to know what is the right data collection policy to be able to explore so it's in this this second technical challenge where my work has made its contribution I'm going to first talk a little bit about how we can address this shifting data distribution problem as this is necessary for the second part but before continuing I would like to just to get some terminology straight when I say the target policy I mean the policy we wish to evaluate and when I say the behavior policy I mean the policy that we're going to you going to deploy to collecting data so first with this first issue how do we handle a shifting data distribution when we change the policy we've run it so imagine here that we're now running a behavior policy which is with 50% probability reach the destination and with the other 50 cent probability is going to cause the safety driver to need to intervene so looking at just the first outcome you can now see that we're going to be reaching the destination about half as often so a very simple trick to correct for this is to just multiply the outcome you see by by around a factor of two and so this is going to adjust for the fact that we're now seeing we're not seeing that event as much as we would have otherwise the the general way of doing this is a technique called important sampling and it's a widely used technique I'm gonna just briefly introduce it how it's used in the RL setting we first repeatedly run the behavior policy we then add up all of the rewards received along each trajectory just as we would in the passive approach but then the key thing here is that we re wait the reward totals and you don't need to understand exactly how this is done but basically you're adding up the rewards as you would in the the passive approach and then multiplying by a relative likelihood factor so this factor just says if this outcome was more likely under my target policy than the behavior policy increase the weight on it if it was less likely than let's decrease the weight on it once we've reweighed it it we can use our observed data as if it had come from our policy of interest so this makes important sampling a very general technique so that and it allows us to address the first technical challenge that we have here but we still don't know what the right behavior policy is and so that's what I'm going to talk about next so myself and collaborators looking at this problem we're asking the question of what if we could only send out a you'll test driver in the setting that we've described so we want to know we have just uh yes it's bothering me so suppose I think about the services suppose there is a state where the safety driver needs to be and I came out roughly like reaching the state so in terms of biking but a policy that gets me to that state stage half the time and your example for instance I can have that goal in my mind but then coming up with that policy seems roughly as hard as solving the reinforcement learning problem because I'm thinking about now the ability to actually maximize this toward which is getting to that state in the MVP so so so so I guess that it doesn't consider a party that I'm having is that in order like is the problem of trying to figure out good data collection policy going to be basically as hard as the reinforcement learning problem or is it some is there some structure that makes it easier yeah so that's a great question first of all I guess we're mainly focused on the problem of policy evaluation here so I think the you know the main question we're asking is is just running that policy using the target policy to generate data is that more efficient than adapting your policy and using that to generate data and and so the answer to skip ahead and bid in the talk is yes it is more efficient to do that in terms of the policy improvements question like could you adapt things we're doing here to that setting the answer again is yes in principle you could there's some there's some other challenges there as well um I guess also just intuitively what we are going to do is reinforcement learning for finding a better policy the data collection policy so we started with this question of you know what if we could just send out collect a single trajectory could we get a perfect evaluation of our poll see with just a single sending out a single test driver here so what this this is again is the important sampling estimate you don't need to follow all of the math but we're basically saying with a single trajectory this is what our estimate would be and can we make this exactly equal to the true value of the policy so it turned out that that yes you can do that however there's for some while it's like theoretically you can show it exist you can't actually analytically compete what that best policy is so we're gonna do what what Alec is suggesting here and we're just going to search we're gonna learn what a better data collection policy is all right so we write down an objective is this done you know pretty standard in machine learning you don't need to follow all of the exactly what this is but this is the mean squared error so we basically look at the true value and an hour estimate of that value given some data and we want to know like how far off are we from a squared error perspective so you can't actually estimate this quality because it it depends on knowing the true value of the policy and if you knew that you would already be done with the evaluation problem however what we showed in our work is that you can estimate the gradient of this objective with respect to the behavior policy so this now gives us a mechanism for adapting the behavior policy towards one which will give us more more accurate evaluation what this looks like graphically is if you can imagine the space of all possible policies compressed along the x-axis here we might start with the target policy and if we use that to collect data we could expect to have a certain amount of error on average we then estimate our gradient take a step along that gradient and we end up at a behavior policy which on average will give us better data for policy evaluation so if you're wondering how this kind of connects to the talk of increasing significant rare events it turns out that the expression we derive for the gradient tells us exactly how to increase the probability of significant rare events that might be encountered so again you don't need to follow exactly what this expression says but we have one factor which says for these actions increase their probability and another factor which captures the significant kind of how rare and significant the particular event is and overall this expression is saying if an outcome is like very rare and has a big impact that increase the probability of seeing it so now we now have a mechanism that allows us to learn a better behavior policy so what this looks like in this setting is we would collect a small amount of data using the the initial target policy anytime we we discover an event which is going to have a big impact on our estimate we increase its probability and this can alter it this can be repeated until you have a very good data collection strategy and really the key thing here is that you can always like you can use all of the data you're collecting as you learn to also evaluate your policy so there's no like wastefulness here this is like is always going to improve over at our age just passive way of collecting data the generalist strategy is that we're going to collect a small number of trajectories with the target policy locally improve the behavior policy and then collect more trajectories and then we can use all of the data to evaluate it and optionally repeat this as many times as desired and so with this contribution we now know what the right behavior policy is and we can use this for more accurate policy evaluation so I'm going to show briefly show some empirical results these are on some reinforcement learning benchmark problems the y-axis is mean squared error so lower is better and the x-axis is how much data has been collected so this is the Monte Carlo approach it will get lower just because it's a consistent estimator of the true value but if you're simultaneously adapting the behavior policy you can get better more accuracy faster and these are it's important to note that this on a log-log scale so these our results are saying between 30 and 70 percents more accurate policy evaluation for any thick amount of data that you could collect so this is this wraps up the first part of the talk just the main takeaways from this is that first I would want to emphasize that policy evaluation is critical for safe and reliable deployment of autonomous systems and that I think active data collection is a really important step to doing this reliably in practice this work also connects to a larger question of counterfactual reasoning which is very important for autonomous agents to have and so my work has also made several contributions in this these wider questions one of our most recent results has again been looking at this important important sampling type methods which to rely on computing a certain ratio so we just presented some work showing that if you take the the denominator of this expression which is a the true probability you were sampling data from and replace it with its empirical estimate that you can improve the quality of important sampling so that's some of the most recent work that we've done on this topic for I guess I was kind of curious in particular for the car for example what would be like how did I think about target policy like is it a very good policy that's running a lot of time in the near upright position or is it a bad policy that's most of the time like not getting there because that is going to influence also the effect of policy adaptation right so could you yeah yeah so in that case that policy was a partially optimized if somewhere in between the two extremes we did have looked so the amount of improvement you get is going to be dependent on how much variance is in the target policy return so if you're really is like one like extreme if you're running a target policy that always sees the same thing there's no advantage to doing this we did always you're always going to see some improvement if there is variance in the target policies outcomes yeah so actually I'm about to Anna I switch gears and talk and talk more about like a different part of the work that I've done so if there's any questions on the first part of the talk I could pause here and take a couple male yes what Steve what's the exact context in liquid sea largest things the state dimension that you actually expect yeah yeah so I would say what we did the basic method that we've implemented is essentially the reinforced algorithm if you're familiar with this for Policy gradient methods so we didn't take this beyond implementing this and like a kind of simple reinforced approach in terms of so we haven't really tried to scale it up beyond those much but I would say you know like so reinforce was like a policy gradient method introduced in the early 90s there's been a lot of literature showing how to scale that and you can do a lot of those same techniques here so yes so those the ones I just showed I think we're both horizon of two hundred time steps yeah so I mean I guess I don't know if you like your questions kind of getting at some of like important sampling and long horizon issues so I think it's you know intuitively what's happening is that we're we're learning a policy which is gonna be kind of close to the target policy but if there's a way to get to these more extreme like rare events it's going to increase the probability of those actions so you don't really have some of the same long horizon issues that would normally plague important sampling when your data is just given to you from some arbitrary behavior policy in fact here we're kind of using like the variance of the weights to counteract the variance of the rid returns there so we didn't find it like too hard in practice but yeah like the one way this would is if you were trying to like use that gradient with a very poor estimate then that wouldn't ya okay so now moving on to the second part of the talk I also think it's very important that agents are able to predict how their actions influence the underlying state of the world even in a somewhat task agnostic way so what I mean exactly by this you can connect to model-based reinforcement learning you know again going back to this autonomous vehicle example we know that if this vehicle was to accelerate from standing still that it would begin moving forward at a high velocity but to an agent which is just beginning to explore what its controls do to change the state this isn't at all obvious so concretely what I mean is that given any state and an action the agent may choose to take in it we want to know what would be the predicted next state which is going to result or possibly a distribution over next States and I think this is really key to scaling up autonomy because if we could do this perfectly like the policy evaluation problem we could do very data efficiently instead of like collecting any test trajectories we would just simulate all the different outcomes and we could know what any policy was going to do similarly for policy improvement instead of you know instead of generating real experience we could learn from synthetic or simulated experience in that way so this part of the talk and the work we've done here is very much motivated by the gap we see in robotics between simulated and real-world domains so in particular this is a bit of experience that I have working in the RoboCup 3d simulation domain so what is I think interesting to note here is just how smoothly the robots move they're able to like you know have these long kicks and move quickly to the ball and the reason for this is that you can apply reinforcement learning in a domain like this and you can heavily optimize all of the skills that the robots use and so that you get something which is much much better than you could possibly hope to hand-tuned so let's contrast this instead to what we see on real robotics so this is from the the RoboCup standard platform League competition this is a robotics platform that I've worked on throughout my time as a PhD student and what you should notice here is that the robots are moving much slower they're kicking the ball less far and less accurately and there's no hope that you could really do reinforcement learning directly on this platform the robots would break actually I guess I saw that you all have a now robot on the third floor like just trying to imagine applying a reinforcement learning algorithm on it you would need like a lot more a lot more robots so one hope is that we can just apply the same learning that we're doing in simulation and if we do it in simulation then we can just put it on the physical robot and it will all work out and we won't need to actually do learning on the robot so you have this setting where you have an initial skill in simulation that maybe doesn't isn't working that well like a slow walk you can have the same situation on the physical robot where it's can it can wall but maybe not as fast as you would like apply a policy optimization algorithm in simulation and what you get out is a much faster walk at the end so then maybe you're done you just take this faster walk you put it on the physical robot but unfortunately what you typically see when you do that is that the the policy finds something in the simulated environment it over fits to it and it tries to exploit that in simulations get high reward and that ends up failing in the physical world so this is conjecture but like what it seems to have happened in the video I just showed is that the robot learns in simulation it can walk very quickly by just sliding its feet along the ground and that doesn't work when you you have a you're trying to walk on carpet like this no so I'm going to show how we can actually learn in simulation and get it to transfer onto a physical robot and introduce an algorithm for doing that just to make this a bit more concrete this is on the problem of robot learning to walk so in the the general MVP setup for this we have a state-space which is the position and velocity of all the robot's body parts there may be other variables that we can't measure well the action as the robot is going to choose commands for each of its joints and we're gonna reward it based on its forward velocity that it's moving we just care about moving in a straight line and so we want to learn a policy which tells us how to map the current positions of the joints to commands which are going to to make it walk fast and so you can think of this in the trajectory interaction is happening in a loop where the agent is sending its state to the policy and then the policy is responding with an action so what we want to now do is replace instead of interacting with the physical robot we're gonna interact with a simulated version of the robot and learn from the experience which is generated through this loop them there's a general methodology that we're gonna build on in this part of the talk this is known as grounded simulation learning so what this is is that we assume we have a policy we can execute in the real world we can use this policy to generate an initial set of state action trajectories these trajectories tell us what are the effects of our actions on the state of the world it gives us some data that we can use to see how what the effects of our actions are in the physical world we didn't have a step of grounding the simulator which means trying to make the simulator give us trajectories similar to what we observed in the physical world once the simulator is grounded we then apply reinforcement learning on it and we can now expect our improved policy to transfer back to the physical robot because we had this grounding step so returning to this problem I'm just gonna make this a bit more concrete and that what we want to do is we're going to receive the current joint positions of the robot which can be obtained from its sensors and then the policy is going to compute commands to sins there and so when we talk about grounding the simulation what this means from the perspective of the policy is that somewhere between sending those joint commands to the simulation and getting the new joint positions back we need to change something so that the resulting next joint positions it receives are similar to what it would had it taken the same commands in the real world and so what we do in our work is we focus on modifying the joint commands that the robot sends to the simulation and by doing this we're gonna call it we're going to change the commands in a way that makes the simulation give us back joint positions like what we would have seen in the physical world our approach to doing this is that we augment the simulation with what we call a grounding module so when the robot sends its joint commands they first get passed through a grounding module which is going to transform them and then into a modified joint commands which is going to cause the simulation to respond differently than it would have had we just passed in the original commands and we can construct this grounding module through two supervised learning problems so the first step of this is that we're going to observe our data and we're going to learn how to predict what the real-world effects would be based on the data that we've observes so this is a supervised learning problem and that we have real-world data and you can chop up this trajectory and get a set of state action resulting next state at least for some components of the of the state features and so now we can just train a model which tells us what is going to be the real-world effect given some actions we then need to just choose an action which is going to cause the same effect within the simulated environment this is again can be done with supervised learning we can collect state action trajectories in simulation and learn an inverse model which tells us for any any transition what action will cause it basically how to reproduce the same effect in the simulated environment so once we have this grounding module we just incorporate this into the reinforcement learning loop here and from the perspective of a policy it's going to look as if it was interacting with an environment more similar to the real world so I'm going to discuss or show you a one application of this to the task of robot walking this is again learning on the now robot that we use in the RoboCup competitions we're going to do five minutes of walking on this robot to collect the real world experience and then we're gonna apply a reinforcement learning algorithm in simulation which requires 5,000 minutes of walking this would be a completely intractable amount of experience to collect on the physical robot so this is the initial walk that we start with I mean one one important thing to note here is that this is this is a walk engine which was developed for the RoboCup competitions so there's a lot of people throughout the world trying to make this robot walk very fast and this is as far as we know one of the fastest walks that people had developed for this robot so then we can put the same walk directly in the simulation this is it just kind of moving normally along before we've done any grounding we then applied the grounding algorithm that I just suggested and you're gonna see that the robot is going to begin having a bit more trouble to walk it's going to become a bit more unstable so this is more like what we see when the robot is walking in the physical world then when it's walking in the simulation environment so once we then apply reinforcement learning in the grounded simulation I'm just doing a head of local policy improvement on the policy we end up with a wok which is 40% faster than than the previous wok we started with so this is now and as far as we know this is as fast as anyone has gotten this robot to go so to conclude this part of the talk I again just want to emphasize I do think that better action modeling is like a very important step towards long-term autonomous agents they were able to do a lot of things in the in the physical world and that in a lot of a lot of real-world problems not just robotics although certainly robotics is a great example there are existing domain simulations and that when these simulations are available if we can leverage them then it's going to go a long way towards efficient reinforcement learning there are of course also environments where we don't have good domain modeling and I think being able to model a domain from scratch will also be an important capability for autonomous agents we've also done some other work relating to how you can use domain simulations to try to put confidence intervals around the performance of policies when you're interested in things such as safety of deploying a policy so this uh this wraps up the part of the talk just there I wanted to introduce some of the work that I've done throughout my PhD before concluding I'm just going to talk a little bit about some of the the next directions that I'm really interested in exploring towards making reinforcement learning data efficient enough for the real world it's just a general comment before getting into the specific research directions I think that like applying studying reinforcement learning both from my more theoretical side or a more algorithmic side as well as trying to apply RL to problems where it just falls flat on its face at the current moment is a really important way of doing research so one of the problems that I plan on continuing working on long term is this problem of robot soccer it would be just really intractable right now to imagine learning this task from scratch and so I think you know this is a really good test bed for our ell algorithms I think it's also really important to work on multiple application domains as you don't want to necessarily produce technique at least for me it's important to not just produce algorithms which are only specific to one test area so throughout my PhD I've also been doing work on traffic systems for autonomous vehicles not necessarily just learning although we are also looking at reinforcement learning in this context and you know moving forward I'm hoping that just you know by working across different application areas there's the potential to develop algorithms which are applicable to many different areas so in terms of the the research directions which I think are really promising to explore for achieving this I think looking at things such as state and action abstractions is important for scaling our ell understanding the theoretical properties of methods that we propose are important for having reliable methods that we can trust in practice and finally although I focus on prediction I think that it's also there's still like gonna be some steps left from connecting prediction to better policy improvement in terms of state and action abstraction the the basic idea here is you can think of that like an agent has different levels at which it can make decisions so for an autonomous vehicle maybe it has an action drive home which itself is constituted by the different directional turns that it has to make along the way and each of those is made up of even like very lower level controls beneath the hood there so in RL reasoning at the right level of temporal abstraction is really is a big impact on how effective your has a big impact on how effective your algorithm can be if you're trying to do if you're imagining trying to get a vehicle to learn to drive home and it's reasoning about this like very low level of actions it's a very difficult task compared to trying to reason at a higher level of abstraction one problem that I'm particularly interested in in the space is the ability to learn in very low fidelity simulations so if you can imagine in the game of soccer you could be simulating at the level of the robots actual the control of each of its individual joint motors you could also imagine simulating at the level of just circles passing a ball around one of these problems should be significantly easier to learn in not necessarily easy but easier to learn in for things such as strategy and so you know what is the right level of trade-off what can you learn in a very abstract representation that then and can then be transferred back into the the real task of interest I also think with introducing abstraction you're introducing approximation which can hurt the guarantees that you can put on methods I think it's it's really important that we establish the properties of methods that we're developing so for instance statistical properties such as just what is the confidence that I can have in my estimator or you know is a method consistent meaning do I eventually get the right answer with just enough data so I think being able to like establish methods like this are very important for practitioners to be able to trust what we do similarly it's also important to be able to show that your methods are scalable in the sense that if a practitioner can't use it it doesn't really matter how good the guarantees are and also explainable so that you know why why your predictions are what they are so finally most of what I do focuses on prediction but I think there's still even once we can do perfect prediction in the space of modeling the effects of actions or evaluating a fixed policy they're still going to be some questions open to connect this to efficient policy improvement so I'm very interested in looking at how do we search the space of all possible policies defines the best one for a task or once we learn on one task how can we transfer what we learn to a new task and so I think once we can do and of all of this we're finally going to be able to answer like the big question and say that reinforcement learning can be data efficient enough for real-world applications and so that's what I'm looking forward to to continuing on to do just before wrapping up really quick of course there's a lot of people we've contributed to this research my adviser Peter stone has been involved with all of this the first part of the talk was done in collaboration with Scott nickim at UT Austin and Phil Thomas at UMass Amherst and then the work I've done in the space of traffic systems for autonomous vehicles that I unfortunately couldn't spend time on today has had like a very wide number of collaborators as well so with that thank you all for coming to the talk today and giving me your attention um and I'd now be happy to take more questions yeah your trick is learning to grow right and so you know with that right there you basically memorize what the deltas are and you keep on iterating on until you come up with a policy that acts in the real world like you would expect it yeah and usually control people started doing that because they they found that for complicated any system modeling is just hard right so they're like so if you treat of learning control is nothing but fancy words for I'm going to bypass modeling I just learn a nominal model and I'm going to learn the deltas now there's this like you know one way it is you learn an explicit model and find the deltas in the model the residuals in the model that matter in the spaces that you care about the other way is I'm going to just completely bypass modeling and learn a good policy right it's unless I missed some details it seems you are taking the route of like I'm going to bypass models right yeah what kind of both and that we are we're leveraging a simulation which itself is a model of the world and so but within within that simulation we are learning in a model freeway where we are just trying to directly get a good policy there yeah right so would you comment on what your thoughts are like is there any advantage to see in that specific domain let's say with the now there's a superhot model by the way and can we do approximate models and then learn inject that habit in the simulator and then learn the like you know that does that you would need to learn yeah I think I think it's a really good question um so I think what we're doing is actually somewhat similar to that in that we are inherently and of locally modifying the simulation so back a few slides new is kind of showing this process it's a loop that's because it's like when you you try to modify the simulation in a I guess a data dependent way it's going to be inherently bounds what that current distribution of data you get from your foreign policy is there's a um and so in that sense we're you know correcting the policy we're correcting the simulation local to that particular policy optimizing there but then we have to go back to the real world and get more data to continue that process so it's it still has this like same at least like high level like iterative learning approach yeah so I'm curious what extent you can correct the simulation just by changing the action space like often a lot of the errors in the simulation could come from the physics that it's you said they carpet no there's no proper real life and you know just look to your foot higher isn't it so I cover that in all situations yeah that's a good question to the I mean I guess like some part of it is that we've so we've done this I was talking some about walking we've looked at some other tests with the now robot and found that we can correct things I could you know imagine situations where construct correcting the actions isn't sufficient to do it um but it does so in like something even in like we're not necess you can even even though that is like a physics parameter that could potentially be adjusted as well you can't actually like kind of use I think see things getting captured and like how it changes its emotions like if if the foot is getting on the carpet that in simulation like instead of like letting your foot and move as freely as it would then by changing the actions you can make it appear as if it was getting stuck more on the carpet so I think like you can like kind of capture some of these things by just modifying the actions as well I think there's definitely some things that it wouldn't be sufficient to do as well though it's not that question as well the kinematics of the actual little robot I mean these robots are notoriously bad at having like correct joint positions right so how much how much hmm I guess did you run experiments to see how much better your algorithm gives this kind of like transfer to real life versus just injecting random noise to the motors because if you would take random RAM random permutations of the motors like how much better does your simulator approximate that instead of just like tweaking all of your little motors continuously to try and inject that kind of real-world problem into that yeah today agent know it's a good question so we did look at that baseline as an approach and so I can what we did gave ended up with like faster walking performance I think the other interesting like kind of trade-off you see what those methods is that like by like injecting noise you can learn more robust policies so I guess going back to the notion of local improvements of the policy if you inject the noise like you we could learn for longer and you can still get like some stable policies that would transfer to the robot but then you kind of gave up some of the like exploiting the real world so like not as fast but still but still stable if it makes yeah but I mean at the same time like your algorithm that tries to approximate the real world might have some air and you're still translating air to the robot yeah no no yeah exactly I mean they're still error I guess what yeah what we saw like kind of empirically was that I guess less error than just kind of adding the way some into things so just cool to see the result of so is that like truly the documented fastest vlog for that little guy so we took I mean this is like I guess it's mainly based on so the RoboCup competitions or like there's like a lot of people trying to get the the fastest walk on these nails I mean we use this in the the competition last year and there weren't there weren't teams using a faster walk than this as far as we know like reported speeds it is as well so I feel like that needs to be like the subtask in the Nano competition yeah so we've there is some I mean like there's some work on like trying to get them to go just like incredibly fast and you don't care if the robot falls over that so in that case that's not exactly what we're trying to compare with here because we're can't we want to get something which is actually useful for like the task of RoboCup so this is like both fast and stable whereas you can't get them to go faster if you're willing to have your robot fall over like at the end of its run or so but yeah yeah all right well that's time desire okay [Applause] 