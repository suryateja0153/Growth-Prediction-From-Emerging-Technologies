 In my lab we have a few exciting projects which are related to robotics, autonomous vehicles, and surgical robotics... We as humans use proprioception and haptic feedback in order to determine where our body is located, incorporating the sense of touch, you know, tactile sensation of objects will help determine the placement of objects without using just vision -- sometimes vision isn't always the most reliable source of determining where things are placed, and so incorporating in these other sensory feedbacks into robotic systems can make them more robust for manipulation and grasping... We are looking at optimizing clearance especially within constrained environments, so if you are inside, for example, the chest, you have moving soft tissues in there. Our research is more on trying to find the optimal placement for the task and as an example here we have a moving boundary. So as you see as the boundary moves, the task also moves... where the task is where the optimal placement is... The exciting part of this research is that we use concepts and reasonings from various fields including statistical physics, mechanics, control theory and for example we help NASA in their work on optimization on traffic of the airport... So currently my research is about modeling queue dynamics... currently in a lot of major airports a lot of planes have been waiting on the runway or at the gate to take off because of air traffic congestion. Before airplanes depart they go through a lot of different queues, so we hope that our research will help ground air traffic congestion. In this lab we use ePuck robots to simulate unmanned aerial vehicles, anything that might have uncertainty associated with it, collision avoidance, obstacle avoidance... for instance you have a multi-agent system which is composed of unmanned aerial vehicles, which have to collaboratively communicate to each other in order to achieve an end goal... The common theme with all of this work is how to control robots or autonomous vehicles in the presence of environmental uncertainty. My research is on autonomous vehicles, in particular on the anticipating of uncertainty of the vehicle's perception -- so imagine you're driving on a curvy road in the night and you can't see too far ahead so you have to anticipate for this uncertainty in the orientation of the road... and that's what I try to do with the sequence of these gates or roadways in this simulation. So this is the Cosmo robot, the robot that I've been working on, pretty much, it can navigate through the streets... so that's what I want to implement. With the camera, so far, I have lane detection application based off the images that the robot is seeing; I want the robot to navigate the streets and stop at the stop signs and have optimized movement with the pathfinding, knowing where it's location is at and reacting to it accordingly. It's not always that you can work on a theory and then be able to demonstrate on a small scale without further investment in building something large, so the exciting part is really that we can close the loop in our lab and then show to others how they can use this on a larger scale, with real-scale devices. 