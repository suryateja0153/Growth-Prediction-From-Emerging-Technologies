 please give a warm welcome to Aleksandr Faust deep learning for robot navigation [Applause] but she was born with mobility and every step that she took in her life is painful she was never able to leave in growing up I had to do other things and so long and then reminded all right I never minded but it always made me wonder there's something that could be done better for her fast forward in time right now in the United States there's 11 million people who stick to us then is combined population in the entire state of Connecticut never lose autonomous robots service troubles going on almost litigating bring packages food medicine and so on you make tremendous opportunity to help improve the lives of these people so we need navigation at scale on real robots in real world big words what does it mean let's look at the video this is a pet robot going running in a micro kitchen just like one I don't know if you guys saw here today going from the same start position to that sink and we just let the camera roll for I know about 10 minutes I want you to pay attention is how much things change around now there's a person there to block the way there's another person coming in that's real world this is what the robots need to deal with check out the next one now there is a janitor car we did not make this up by the way this is a foot this is a fully learned system there are no tricks and the rest of the talk is about how we got there the same fetch robot is 250 pounds and maximum speed of 3 miles an hour colliding with it it's kind of coolant of football player walking into you're running into you you don't want that so if this is important right second thing is robots come in all shapes sizes right they have their sensors every robot has a different sensor and experience is the world in different way they have different dynamics how they move and they have different bodies right they can fit through different places so the little guy here next to fetch is the same actually robot without its head it's 100 pounds lighter but it's also twice as fast the so that's something that we need to keep in mind and if we're looking at navigation at scale we want safe infeasible motion in the dynamic world that these robots can do the standard solution which consists of okay we perceive the world her perception tells us where things are then goes to the motion planning layer that says okay now that I know where the objects are I'm gonna plan my path and then that path is being sent to the controller that then goes and executes the path doesn't scale and the reason doesn't scale is because each layer doesn't know about the limitations of the layer above and every time we put the robot in a new environment we need to hand-tuned and fix bugs and correct these assumptions between the layers there needs to be a human in the loop to fix that so how do we go around that let's look at the biological systems we learn curricular skills everything we do we experience the world with our sensors and we express ourselves with motion speech or however interface is the same regardless while what I'm doing so can we take inspiration from biological systems and do something similar let's go take a step further so evolution determines our abilities evolution determines the system that we have that enables us to learn certain level of tasks next to that then we interact with the world this is my daughter learning to navigate and learn how to learn but learn through trial and error and after we have some skills we go and reflect on them how good I am at this speaking business where can I improve how I'm good at navigation there was a break here I was trying to get to the stage some people are better than others right but we have some self awareness of our abilities so if we take that inspiration the way evolution reinforcement learning or learning skills and then self-reflection or into building intuition what's easy and what's not what would be the most basic navigation skill what we just saw it was basically getting to the sink or getting here to the stage you just need to kind of get your destination and you need to deal with whatever is in your way to deal with it right it connects directly sensors with acting with controls and the output is a sequence of feasible actions to reach a nearby goal that's what we call point-to-point policy and for dynamical systems if you have background in math it's actually there is only solution for very limited number of systems otherwise its numeric it's np-hard problem and there you need to fall back to the numerical systems another Mortman to numerical solutions so let's use reinforcement learning for that we have our robot we want to learn a policy that's familiar with reinforced learning generally what it does okay the so we need to take our observations what are the observations it's what is robot observes and this is the noisy Leiter observation you can read white here is there is room in front of me black there is a wall and this is basically 220 degrees folded out in two and the great things that's nice because there's always noise we need to feed several last observations to kinda have the estimate of the motion in the environment we train it in static environment this is a map of yet another micro kitchen I don't know why were obsessed here with the micro kitchens but they static obstacles it's a small place and you just will let it go and roam around in that space the question is what is a reward here what is objective objective is to go and reach the goal right good you put reinforcement learning to that it comes in tractable it needs to explain forever before it finds the goal so that it's not quite gonna work the alternative is to set the rewards and then you hope for the best in tuning the rewards turns out to be very labor intensive process true story had an intern last summer that spent a month going through 25 different generations trying to find different rewards and did not work the e and then we figure it so the second question is what is the neural network architecture for this policy and this is art as well we're guessing how many layers how deep they are well what activation functions and all that sort of stuff we're guessing just for example why rebar tuning is hard this is the policy that has our best hand engineered policy so to speak it gets to where it is but it's very suboptimal motion also known as a drunken robot so what do we do we automate the reinforcement learning we combine the ideas from genetic algorithms with the evolution and we reinforcement learning and due to lay two steps in first one we involved a reward to learn the reward that is able to learn the task and then a second layer we with a fixed reward we go and learn the neural network architecture so we start the population of reinforcement learning agents each one of them has a slightly different reward out of the rewards are given is a parametrized or the whole macro decision process is really given as a parameter ization then we go after they're trained and we evaluate them against the task objective get to the sink or get to the to wherever it needs to go we're randomizing the goal in the environment after the training within mutate the reward find the next one select the best policy once we're through the training we run total of thousand agents hundred agents in parallel processing at a time we have the reward we fix the reward and then we repeat the same process with a fixed reward varying the neural network architecture but this time we're actually evaluating against the cumulative reward because the reward is fixed now we're comparing apples to apples right and at the end we have our neural network architecture the whole process takes about a week to Train it's not cheap by any means but is generalizable we can change this is our training simulation environment raised floors and it goes and runs around hits the walls it's all good no real people there indeed and then he directly transfers to the robot this is my dog he had the best date lots of treats that day but here in this way I'm blocking its way it's trying to go behind me and notice how it turns around and then once I move around blocks and finds its way back that's very cool that's a transferable skill right we invested time and training but it's promising right we can use that in many as a building block this is the freight travel basically running the similar thing just much faster comparing to the classic approaches classic approaches produce very smooth trajectories the moment you start adding sensors no sensor in localization all that noise for them they start breaking down the REO algorithms vanilla without - into rewards they still get to where the needed they're coping with with a noise but they're not optimal we saw that there in the loading so the question now is how well this generalizes so we did the benchmarking for several Motor Co environments the first one was done on GDP g4 with Spurs go now we're looking PPO and soft actor critic and doing the simple so in these Motor Co environments say for humanoid to really go the task that we want to do is make it go as far as it can maximize the distance travel but somebody spent time engineering the rewards right and we consider that as a complex objective so we're going to evaluate it against that first question we asked is how well this works complicated graph when I say orange is the outdoor L blue baseline is the hyper parameter student and the green baseline is the baselines that are from the opening item the basically the more complex task is the author of improves you who helps you much better and there is the video of the humanoid running we're actually running the other methods are walking the second question is does it make sense to spend time and engineer multi objective or can I just spend time since this is really what they care about here are some ideas what the reward should contain and let it run so out of all the graphs I'm going to let you focus on this one here the light read is optimized over the standard objective this is the multi objective the complex reward that's given with the Monaco environment the red one is the one that's trained and optimized over simple distance traveled and then even though it's optimized over that we're evaluating over the complex standard reward and that's the gap in the performance so it's probably not worth if you if you can afford computational resource it's probably not worth it time to go and hand engineer the solutions third question is if I have a fixed computational budget should I go hyper parameter tuning or the reward search and basically this graph tells us that you're more likely to get better policy faster tuning the rewards in certain rewards then finding the hyper parameters with a caveat that these hyper parameters are reasonable if you have not reasonable hyper parameters then you're out of luck so Tyrell in general evolution determines internal rewards that enable agent to learn the task and then reinforcement learning learns that tasks given the abilities of the agent and we get better policies with less engineering okay so these were the basic skills how do we move on to more complex skills you guys all made it here to this room right along your way here you made some decisions you had some weight points that you're then kind of you get there and you deal however you're going to get there so the long-range navigation tasks we can think of it as a sequence of feasible waypoints that lead to the goal and the question then is how do we find the sequence of the goals so one naive solution is now we have our skill we have our outer L agent that we trained we invested time in training it and we say we want to put a robot in one these buildings and we're giving the map the ideas from the sampling base planning anybody familiar with sampling base planning okay here's like a 30 15 second overview you take a map the environment you sample the points randomly you take any sample says hey is this a valid sample yes you keep it now we discard it now that you have a sample you need to connect them into a world graph or road map and the way you do that you take for every sample you find its neighbors and then you roll out this trajectory and in this case we use our Tyrell policy to roll them roll it out and since it's elastic we roll it out multiple times and we only keep edges that can be now that that agent can navigate consistently once we have the roadmap now we know that these waypoints are traversable by that agent and they tune nicely and we can use it and to go and search the graph to find a solution so here is an example of one of them so just notice here that for example here the road map is not connected and if you zoom it in there is actually a pole like a pillar in the middle and that particular robot is noisy enough that can navigate that so that edge is not there putting that on a robot we actually the results are for the success rates are the same in simulation and in reality so we close this seem to real gap which is good but the paths are not optimal and it's now computational expense yeah for building a road map like this we need to evaluate something in the order of a hundred thousand edge connects and that takes time and we end up with the zigzag path but we can do a long-range and it's a very immature skill think of it as an immature complex skill so how we can make the connections step more tractable so if we focus on finding water the most relevant this is a search problem right we have a sample we have options find the most relevant samples that are ahead of us and then connect question now is can we practice the skill reflect on it and get better so for example we practice our outer rail policy in a completely new environment run the trajectory observe the sensors what the sensors were at the beginning and observe the metric over the entire trajectory that we're interested in and note that for example if I'm interested is how long is going to take me to get out of the door given that my sensor observations everybody's sitting so it's probably easy and if my sensor observation is like everybody's walking around it's a break time it's probably more complicated and but I know using that and exposing agents to these experiences we collect the data set then then we can train the predictor we can train a predictor that takes sensor observation that metric and can tell me how likely that I'm going to get to the door in time and then we apply that when we're planning in new environments to reduce the search time and particular metric that we're interested here are feasibility safety and importance of the samples so the first one first metric I'm going to talk about three metrics is time to reach so this is exactly what I said getting to the door during the break or not during the break or if I'm driving a car if I ask you to go ten meters ahead it says okay this is easy but if there is a wall in front of you you can say no that's not easy that's going to be hard so we observe the agent and then we build a map on the right is basically it's a heat map the goal is in the middle and it's the estimates how difficult or what is the time to reach to the center of the map on the left side is trained there is difference it's conservative but around the goal it has a good estimate what is feasible and what's not feasible then we use it to build a tree the blue tree used the standard method in green is this our LRT algorithm as we saw it it's much smoother path the two reasons why it's smoother one is because we're using outer rail and this system does not have the steering function that they mentioned two boundary problem and outer rail learns it and the second one is that the search is more focused so this is what the building a tree looks like trying to get to the goal and it's kind of biasing the search towards the goal until it gets there this is what looks on the robot on the left side is the ziggy-zag naive approach and after learning the feasibility and kind of focusing on more important or the easier samples we get smoother pass the second metric is what we call sweat volume basically the idea here is looking at the efficiency of the motion saying that my I shouldn't be doing this when I'm walking right I should minimize my volume through through the trajectory so we can learn that and the one on the left is the Baxter robot basically minimizing the volume of the motion whereas the one on the right is not third one in the last example is let me give you an intuition if I need to go to Seattle from here what would be my first Waypoint that door right I think here what's happening afterwards right but there are like three door four doors in this room and there are points of interest stage or in the middle of the hallway or the cameraman they're not points of interest and the idea here is that there are important waypoints that are locally recognizable that don't depend on the larger picture that we can learn so how do we do that we have the naive long-range planner that goes through our waypoints and finds we go in traverse points and then we pay attention and say okay for each Waypoint that I traversed how important it is how many times I traversed it and if I remove it is it still it does the the connectivity of the space breaks it's called between a centrality so if both conditions hold this is an important point and in that case we want to know that so then we train the predictor that says okay given my neighborhood given my local picture what they can observe in a point how important do we think this point is for this particular robot and then in the new environment when we start building a road map we say hey okay this sample if this sample is important I want to connect it to the other important stuff and these local less important stuff I should just connect locally so we end up with the hub's so to speak I want to connect doors to doors hallways to hold that kind of stuff without explicitly saying that this is a door this is a hallway so here's a picture this is our training data set if you've been to Mountain View this is the Googleplex the four buildings down there we traversed along the way and turns out that there are four buildings that have been - like four hallways walkways that are connected guess what they are important right it learns that narrow passages doorways hallways entries to the hallways are important once we train and apply so we say ok this is the small patch of the environment the local in our several samples there how important they are well this one is not very because there is nothing much that you can go here but these bigger ones this kind of leads to lots of places that's important this is applied training set never seen before and we create roadmaps that are about hundred times more efficient so this is naive planning with kind of these extra waypoints around a little bit who's there not suited ok comes to that go around and so on and the next one is the smoother the cure article roadmap with critical samples already there pretty much goes new sub optimality here okay so we talked about connecting sensing and acting without RL and learns better policies with less engineering and bakes into Robo dynamics with sensors directly then we practice a skill and we observe what we learned and we learn to estimate feasibility efficiency in importance with respect to the ability of that particular policy and then we fold that back in the planning and we have faster and efficient higher level planning within robots or policies abilities genetic algorithms reinforcement learning and supervise working learning working together and each if you can improve any of these dimensions whether Jake algorithms better reinforcement learning or self or supervise the whole system can improve and get better of time over time and with more experience our navigation improves on real robots in real world now that's a first step towards larger this is a research project we're now building there is a lot more work to do there are lots of interesting questions over what are the right metrics what is the right curriculum of skills how we put them together and so on one interesting way to combine different things to get to scale in navigation and hopefully have helped lots of people in the process and I want to thank my collaborators are some references and thank you guys spending afternoon you you 