 I'm gonna be talking today about my thesis work on algorithms to robust autonomous navigation in human environments. So if we look across the field of robotics over the past ten years or so, there's some pretty incredible things going on. Lots of new types of robots that are coming out and we can really categorize these different types of vehicles and robots in terms of what they're designed to do. So over on the left, we have these drones that are really "human entertaining" robots. In the middle, we have these "human supporting" robots helping people do different tasks in warehouses, for example. And on the right, we have robots that are potentially going to be saving human lives, whether it's robotic surgeons or robotic cars. And so clearly as a society we keep building these robots that are designed for humans, but if you look closer in a lot of these videos and across the board there's really oftentimes isolation between the robots that we're building and the humans that we're building them for. So there's sort of the one-off cases where there's a car that needs to slam on the brakes in front of a jaywalker, but for the most part we aren't seeing robots all around us all the time yet. So the big picture goal of my thesis is really to bring these robots to be embedded in our society on a day-to-day basis. And the limitation why we don't have those today is a lack of fully autonomous navigation algorithms that can handle the real-world environments that humans often operate in. So if we could solve these types of challenges we'd be able to do some pretty cool things, like on the bottom we have these like last mile delivery robots that can drive your pizza or whatever actually to your front doorstep. Down in the middle it'd be really exciting to have robots moving things around in hospitals whether that's patients or supplies. And then in the longer term it'd be great to have robots in our houses doing chores so that we can go off and do stuff we want to do rather than vacuuming. And really the technical challenges that prevent these robots from existing are listed right here. The first is being able to have algorithms that can operate in previously unseen environments when you don't have a map ahead of time. A second important challenge is coming up with algorithms that can reason about other agents in the environment like other robots or people. And then finally all of these systems that we build need to be safe and we need to guarantee that they're safe but they can't be so conservative that they aren't able to accomplish their true purpose. So these are really perception, planning, and safety challenges, and those are kind of key themes throughout autonomous systems and throughout my thesis that you can think about it's sort of binning these different challenges. So if you follow like the tech blogs or robot blogs out there you probably think a lot of these robots exist already. But I have some bad news -- these delivery robots are often driven by a person sitting in front of a screen (at least a some percentage of the time) so there's kind of a gap between these autonomous systems and the things that are out there in a lot of cases. There's definitely robots in hospitals but if you zoom in on this one you see this little sign that says "please do not enter the elevator" with the robot and to me that's a that's a huge failure of the third point which is this super overly conservative behavior. It's not quite where we want to be. And then lastly, these robot vacuums are kind of only robot vacuums today and they're really just cleaning up skittles and things like that, not really doing these general-purpose chores around the house. So my thesis tries to address these problems through this concept of learning and it's learning throughout this stack of autonomy. So the three problems I'm going to discuss today start with this first one, which is: if you give a robot a task of say go to the kitchen, the problem becomes to reason about what things are in the environment and how those things correspond to getting towards the goal. The second problem comes up when we think about when there's moving objects in the world and now the questions are really where are those things and how do they correspond to the robot's actions. And then the third problem comes in when you actually try and deploy these types of learned systems on real robots, there's going to be some uncertainty just based on the fact that we don't have perfect sensors, and so understanding how that uncertainty maps to the decision and the actions that the robots take is a very big challenge as well. And so these high-level questions are really perception, planning, and safety challenges like I alluded to before, and those are the three problems that I'm going to be going to be discussing today. So this gives a little bit of an overview of the techniques that I'm going to be discussing. I'll start off talking about how we can deploy robots in new environments, then I'll talk about how we can deploy robots in environments with pedestrians (which are examples in moving objects), and then I'll show some algorithmic and theoretical ideas for how we can protect these learning algorithms from the uncertainties inherent in the real world. Alright, so let's dive into this first topic and this is all about context- guided exploration. And what that means is really: the high-level goal is we'd like to give robots instructions that are human interpretable, so things like "go to the front door" as their objective. And the the application or use case we've been talking a lot about throughout this project has been looking at this last 100-meter delivery problem, which is even more specific than the last-mile delivery where say you drop a robot off down here at somebody's curb and you wanted to find a way to get to the front door of somebody's house. Some of the technical challenges  associated with this problem are that: you aren't going to have a detailed prior map of every single navigation environment that you may be operating in, and furthermore a goal like the "front door" doesn't really have a geometric meaning on its own. So there's no position or coordinate in space that your robot is trying to plan towards. So to make this solution possible, what we're gonna do is use prior knowledge to put the visible data into context. And what I mean by that is your robot is probably only gonna be seeing something like this from its camera view, but the fact that a lot of humans environments are heavily structured, and a lot of houses look quite similar. Once you've been to one house you should have a pretty good prior on how to plan at other houses as well. And that's what we're going to leverage in terms of context. So the more technical problem statement is: imagine you have some semantic map of the world, s, which is denoted in this cartoon as this red circle is where your robot is located, and there's all these different types of objects in the world. All these different rectangles and their colors correspond to what type of object they are, so maybe this is your house and the different colors objects are your refrigerator and your table -- things like that. There is some actual goal coordinate that you'd like your robot to reach but we're only going to provide the type of object that needs to reach -- not the position. Furthermore, the robot or the agent is not going to have access to the full map ahead of time. It's just going to have this partial region of the map that it's built up with its on-board sensing so far. So we can write this notationally as: there's some part of the state space that is observable, like the agent's position the map it's built so far, and the type of object that is looking for, and there's some part of the state space that's hidden, like the goal coordinates, and the entire map. The objective that we're trying to solve then is to come up with this planning policy, pi, which minimizes the expected time to goal subject to different constraints. And one of the ways that we can make this more doable is by supplying this set of training maps which come from the same distribution as the map that we're operating in right now. But we haven't seen that particular example before so that's where we can kind of pull out this context of similar types of environments. So there's quite a bit of work in this general area of doing context-guided exploration and I can categorize it by these different types of groups but really what we want out of a solution is to get the interpretability of these classical graphical model-based approaches, along with the scalability of the more recent deep learning based approaches. So what I want to do is leverage as many of the robotics tools as we can because they've gotten us pretty far, while introducing just a little bit of learning where it's most needed. And so that kind of contrasts the view that's going on a lot in the learning community which is the goal would be to go from your sensor data stream and learn through some really high dimensional policy policy space to just choose an action. What I propose to do instead is to leverage a lot of these robotics tools that have been around for a while to do things like mapping through SLAM to build up occupancy grid representations of the world as one way to compress all that sensor data. Then if we had access to the cost-to-go of our map, we could just employ optimal planning and choose an action quite easily. So really there's what I see is a much smaller learning problem that needs to be accomplished which is just going from these occupancy grids to these cost-to-go's. And I'll get into a little bit more of what those things mean going forward. So the proposed algorithm is called Deep Cost- To-Go and the approach is based on this idea of image-image translation. So building on the the type of architecture that I described on the previous slide, we can use standard robotics tools through our sensor data to build up maps of the environment, and in this case this is a aerial view of somebody's front yard, and you can kind of get a sense of what the different colors mean by yellow is out on the street, blue is their driveway, cyan right here is the walkway towards their door, so their door is really located right around here, if you can see that little red dot I'm pointing at. So we're gonna take that map, which is an image, pass it through this deep neural network and it will output another image. And the second image is called the cost-to-go estimate, which is essentially a heat map, where lighter regions like here correspond to areas that are close to the goal, and darker regions out here like out in the street are further from the goal. So if you think about getting to the front door this map if you didn't know where the door was would be pretty helpful for your motion planner to kind of choose which way it should explore and push its frontiers forward based on which areas are likely to lead to the goal. So that's sort of the perception piece and the scene understanding and this example video is going to show how we can put that into a planner. So we took a top-down view of a real delivery-type environment, like a aerial view of an actual house, and semantically labeled different parts of the map. so this is basically our scene and there we're simulating the agent sensors so it starts off with no idea basically of the map ahead of time and it's going to build up this map as it goes and we're gonna be predicting the cost to go over here in this this image right here so I'll let this play and what you can see is the the agent is moving around in the world building up its knowledge by making a map and then at every time step it's predicting what's the cost to go of all these different places in the map and that informs which positions it should try and go explore next and at this point had seen this walkway and it knows that that's highly correlated with reaching the goal and it's able to reach this red area which is the front door in this case quite efficiently and really the takeaway here is there's very little wasted exploration in terms of this path this is a house that has never been seen before only other houses have been seen and yet there isn't a lot of waste of time exploring over here and these kind of unlikely to contain the goal areas so this gives the sense of how learning the context helps in a particular place what I'd like to now show is is how this generalizes to a larger set of environments and that's what this plot does here so we look at the path efficiency of our proposed algorithm deep cost to go in green versus the more the seminal work in this area of frontier based exploration which is unaware of any of the context or learning ahead of time and what we see is this path efficiency which is the ratio between an optimal path if you knew the map ahead of time versus the time it took the agent to reach the goal without having the map ahead of time we see much better performance from our algorithm in green versus the other approach we compared against and this isn't categorized by houses that were trained on houses in the same neighborhood different neighborhood and basically showing the ability to generalize beyond just the houses that have been the neighborhoods that have been seen before so this is comparing against sort of an older algorithm but there's more comparisons in the thesis that compared to this RL implementation I think doggie furs work on helping make that comparison possible so the the big idea of this plot is that we've shown across a large number of different house environments that this context helps in these grid worlds in 2d but at the end of the day the real world is 3d and so to see if this algorithm work in 3d we put together this simulation of a high fidelity simulation of a delivery type scenario so the robot is right now building up a map of the environment using a forward facing camera and that is being built up online it's a semantic map block right here the cost goes being predicted just like before in the in the 2d cases but now it's based on this map that's made from these forward facing sensing and what I think is cool about this is not only are these houses that haven't been seen before but it's never even seen anything in 3d before everything's been trained from satellite views of houses that were collected just areally and we're able to leverage these robotics tools to kind of bridge this gap between 2d and 3d and so once you get to about this point and you're kind of on the walkway the the problem is easy and you can see the door and kind of just plan to it so the video is just kind of fade out at that point so just to wrap up this part of the talk the contributions of this context sky to exploration work are a new formulation of this problem which is really a scene understanding problem and we're formulating it as a image translation problem we provide an algorithm to Train this cost to go estimator and we show how you can learn in a world where you don't need a realistic simulator but you can evaluate in a realistic simulator the algorithm is able to reach the goal faster than an approach that doesn't have context awareness and a rol approach that does and the key idea is we're planning without needing to have a prior map so this is kind of a specific use case that we've been talking about in terms of package delivery but really I think the idea of this work is it's really a framework for deploying robots in environments where you don't have a prior map and you don't have gold coordinates of what you're exactly trying to look for and I think when you think about it that way you see much more general purpose applications that this work could be extended to so a lot of these scenes down here on the bottom right our places that we actually don't have Maps ahead of time and maybe we're looking for a type of thing but we don't know exactly its location and that's where I think bring bringing this work into the future would be cool to try and tackle some of these robotic problems as well and expand the types of environments that robots can operate in alright so that wraps up the first part of the talk that's really the first contribution that I've been talking about and now I want to switch gears into this second topic which is trying to understand this problem of when you deploy a robot in an environment when there's a lot of moving objects like how do you plan collision free motions and if you look across the internet you'll see a whole bunch of these types of robots coming out whereas just different types of systems that are being deployed right alongside humans they're really embedded in our everyday environments and I'd like to talk about the challenges associated with doing this and how we can do it even better going forward so what makes autonomy in these pedestrian rich environments hard is that pedestrians are really dynamic and decision making obstacles that need to be avoided and what I mean by that is that there's a at every time step the person is really whether they're thinking about it directly or not they're making decisions about how they should try and get to their goal and how they should avoid other things and it's pretty tricky to do this on a robot because there's no sensor that tells you exactly where people are going you have to infer this from where they've been walking in the past and no two people are exactly the same so there's some people that are more cooperative and some people are paying more attention and really reasoning about this distribution is important so we can kind of distill all of these high dimensional concepts and challenges into more simple simulations or models like is being shown down here on the bottom right where you could imagine just a bunch of circles which could represent people or robots or any mix of them that are trying to get to some bowl position in the world and just try not to crash into each other pretty much so when you think about the world at this level this type of model you can even generalize this a little bit more and not worry so much about people and robots but it reminds me of this clip here from Finding Nemo that people may have may recognize where essentially you have a bunch of fish in this animation that are trying to solve this multi-agent collision avoidance problem as well and what's going on is all these fish if you haven't seen the movie they're just trying to make these different formations to give some visual instructions to other fish and what they're what I think they're doing as somebody who spends way too much time thinking about collision avoidance is trying to get from one point in space or in the water and swim to some other place in this formation and each fish needs to get to some place as fast as possible while using its fish sensors to not crash into everyone else so that's supposed to be just kind of a cartoon example of an idea that's that really just generalizes this problem statement but we could be a little bit more technical about it which is which is shown on this slide here so you have a robot in the environment which is this black robot right here and there's some other agents in the world which are all these different types of objects and all these objects and agents have gold positions but only the agent itself that can sense that knows its own goal has that information available so notationally this looks like again some part of the state space is observable some part of it is unobservable so in particular we're going to consider the other agents positions velocities and sizes as observable and other things like their goals their preferred speed and they're heading as unobservable so the goal is once again to come up with a planning policy and that's written down here as this policy pi should be the one that minimizes the expected time to goal but now we're much more worried about this collision avoidance constraint and I'd like to introduce just kind of to give a sense of where this approach that we were talking about came from so the prior work was led by Steven in which John alluded to before and this work is really kind of shows how you can learn to do this closure to Borden's problem purely in simulation and so what's going on is these agents are just moving around trying to estimate how long they'll it takes to get to their goal positions just by trying out different actions and this is based on this idea that's popular in reinforcement learning called self play where you really get the training data for free and so the way that we control this type of learn collision avoidance is through the reward function which is specified right here so you do something you know you can tune these parameters but you give some positive reward for reaching the goal some negative penalty for colliding with other agents and really through that you can control the ability to have this emergent behavior of collision avoidance and so this is the project that I got involved with with Steven leading the charge at the end of my Master's and what we were able to do is take this idea that was trained to simulation and deploy it on these different types of Hardware scenarios so up here is a case where Steven and I are basically walked around in our old lab with a couple of these robots moving around and we're just adding things to the environment that these are scenarios that weren't specifically trained for but the the policy that was learned was pretty general and able to avoid these different types of things and then the video below it is showing the robot moving around in the Stata Center at MIT doing collision avoidance in a more public setting so was kind of where we started with this and the reason I call out this work in particular is because this had a lot of advantages over the the his the literature that came before in the sense that a lot of the expensive computation was offloaded to this training step and that allowed us to exceed this day there are quite a bit my thesis goes into detail about how we can really scale up this prior work and one of the things that I'm not going to spend too much time in this defense talking about is really this the way that it was formulated before as splitting up doing collision avoidance and estimating the time to goal has some limitations and so we go into the nuances of that in the thesis the other piece that we've focused on that I'll talk about more today is this challenge that comes up when you try and deploy these systems in larger and larger environments where the number of agents in the world varies over time so your robot may be operating in one hallway and there's three people nearby and then it turns the corner and all of a sudden there's 15 people nearby you know you really need a collision avoidance system that can be flexible in the number of agents that it sees at any particular time and a lot of the like one of the technical challenges that comes up is the fact that these neural network policies that encode that behavior that was learned in simulation are often feed-forward deep neural networks and what I mean by that is you apply some fixed size vector which consists of your agent state and the observable parts and other agent states and you really have to define how big this vector is ahead of time in order to feed it through all these different layers and come up with a collision avoidance policy or a velocity and a heading angle for your robot to take and so the work before required because it had to be a fixed size vector you can only choose a certain number of your nearest neighbors to do collision avoidance around and so you have to be a little bit strategic and who you pick and then you're naturally going to ignore some of the agents so my thesis proposes this idea of bringing in this this concept of recurrence or this idea of a long short memory module which allows you to feed in a sequence of observations at a particular time step and so all these different blocks here represent different other agents positions velocities and sizes at a particular time and those are fed in sequentially in a way that outputs a fixed size representation of everything that's been seen before and then you can just feed that through the rest of the network like we did before but now you're more aware of everything else everything in the environment rather than just a fixed sized number of them so this Ellis team through space is a different way of using Ellis TM than than is typically done in sort of like sentence translation or sequence modeling now we're worried about different are sequences different things throughout the world and that allows us to process a variable number of agents so between those two things that I've described as kind of the innovations in this part of the work we're able to scale up to larger and larger numbers of agents and that's shown in these animations down on the bottom with the new algorithm so we see these six and twenty agent scenarios the algorithm was actually only trained on up to ten agents and what we're able to generalize to these larger types of cases by employing this new flexible architecture so there's some more specific numerical results in the thesis that compare these algorithms on random to randomly generated cases and we compared against a model based a prior learning based approach and another groups learning based approach with our new algorithm and show a reduced number of failures and pretty much the same time to goal which is a net positive for our algorithm so there's more of these types of results in the thesis that I won't get into too much detail here in the defense because I want to spend some time showing the videos of how this actually maps to the real world so in this video we took the this policy that was trained in simulation and we deployed it on this mobile robot and in this case the density of people is quite a bit higher than the types of things we were doing before so now there's you know five or six people walking around here getting much closer to the robot and we're able to use this flexible architecture to account for as people as they come in and out of the field of view of the robot and really just have one policy that can adapt as the robot just moves back and forth across our lab space and it's still going at the same speed this about human walking speed as the system before but this is really the thing that we've been showing to visitors in the lab for the past couple of years now so the other cool thing that we were able to do with the same technology is put it on other types of vehicles and in this video I don't know how easy it is to tell from this angle but these are for UAVs or quad rotors actually they're hex rotors that are flying in our lab space and each of them is employed with our collision avoidance policy that was learned and so what we've done is we've given each vehicle a goal position in this space and they need to fly to that goal and get there as quickly as possible while avoiding collisions but if they just went straight they would crash into one of the other vehicles so this highlights the fact that what's been learned is really a skill of collision avoidance that can generalize to different types of robots with different types of sensors in this case they're getting information from a motion capture system about where everyone else is and these are scenarios that we didn't explicitly train for and so it just highlights the that something that was learned in simulation can map to a wide general set of things and then the last thing I want to show on this topic is tying back to that Finding Nemo clip if you give these different agents goal positions that correspond to specific places that correspond to letters you can start to spell out words or you know words as a composed of a bunch of letters so in this case we're spelling out the name of the algorithm cadr L or collision avoidance with deep reinforcement learning and the agents are just following our policy trying to get there as quickly possible they weren't trained on these specific examples but the fact that they were trained in a bunch of random cases gives them this ability to generalize to these types of formation type settings so there's probably a way more efficient way that all the agents could get to their goals if somebody was assigning them they could kind of just shift from one point in a letter to the next point but we purposely randomized the assignment of where everybody needed to go just to highlight the fact that we're doing collision avoidance let's make sure we actually have to do collision avoidance all right so that wraps up what I wanted to say about the contribution the second part of the thesis which is autonomy embedded around pedestrians so really the key things that we introduced are what I didn't talk about a whole lot which is relaxing the assumption of what we're saying other agents are going to do that was a key way to improve our ability to scale to large numbers of agents as well as this new LS TM based strategy to address the fact that the number of agents could change over time so there's numerical results that I highlighted here as well as more details in the thesis and then we showed the aerial and ground robot experiments to give a sense of the deploy ability so there is some code associated with this project so the first QR code over there on the left is Ross code of a pre trained network if you're interested in just integrating this in your robot today and then if you're interested in in training your own closure one's policy we provide our environment as well as recently written documentation which is the first time I've ever done that and hopefully we'll make it easier for people to use that code going forward and kind of establish them benchmarks as we're thinking about what are the the challenges and collision-avoidance environments so that kind of wraps up what I wanted to say for this topic but when I was at stop and chop a couple weeks ago I saw this cool-looking robot it looks like it's doing collision avoidance in pedestrian rich environments which I thought was pretty cool so I had to take a video and then I was talking with some people in our lab about it and somebody pointed out that one of these robots recently got stuck basically so it turns out their goal is to their objective is to find clean ups or spills and alert the management and this robot just got stuck here for 15 minutes because a tissue was on the ground and it was trying to call for help and this is to me ties back to that one of those early slides of this is a totally safe behavior and maybe it's the right thing but it's so conservative just based on this tiny little uncertainty in the environment that basically threw off everything else that it was programmed to do so that's kind of a good segue into what I want to talk about next which is what happens when you deploy these systems in particular systems that involve learning in real-world environments and things like that are going to happen there's going to be small perturbations to the world that you didn't account for you know how do you make sense of that so a Wells a fairly well studied or a recently studied failure mode of deep learning is this idea of adversarial examples and so deep learning is really the the technology underlying a lot of things that I've been talking about today which is these computational representations allow you to get this state-of-the-art behavior in many robotics tasks but they fail on these very small perturbations to the observations and that's this idea of an adversarial example where in computer vision people often talent times talk about you know I trained a neural network to take a picture of a cat and say that's a cat but if you add so little noise that a person can't even see it that can be enough to make the system think it's a rabbit or something else so that exists and that's out there but what concerns me a lot more is the fact that you can go out today and buy an autonomous vehicle that is subject to the same problem so this group at $0.10 research realized that you can spray paint a tiny little white dot on the road and causing a autonomous vehicle deep neural network classifier to think it's in the wrong lane and swerve into what could be oncoming traffic so luckily this video is just on a test track but really what concerns me is that we haven't been developing methods that are actually robust to these uncertainties and furthermore the defense's that people are starting to come up with often oftentimes don't come with the theoretical guarantees to actually prevent these types of disasters so that became the the really motivating example of this last third of my thesis that I was working on with Bureau and another student in our group and the problem statement to be a little bit more specific about what we're trying to solve is this idea of an adversarial observation and reinforcement learning so the ideas say you have a DBQ network which is this representation of how much reward you expect to receive in the future for taking specific actions in specific places and assume that that Kunik network was trained with perfect measurements of the world then you take that q network and you introduce it into a test environment and that test environment consists of an adversary or some noise process that didn't exist in training and what the noise our adversaries allowed to do is at each time step it observes the true system state so this s naught dot right here and it's allowed to choose some new state somewhere inside one of these balls these different colored regions correspond to these different LP balls and so the noise or the adversary can choose some point in here and say that is the RL agents new observation so the RL agent doesn't actually observe the true state and system that just guesses perturbed version of it so you could imagine this model is captures a lot of sensors if you know something about your sensor specs you can kind of define what epsilon values you should use and then if you have knowledge of the Q Network if some adversary really knows what you've been trained on you can model a worst-case noise process that really tries to attack your weaknesses so in this problem there's a lot of work solving a related problem that I want to distinguish versus what we're doing so the related work is oftentimes trying to solve this very education problem which is returns a yes-or-no answer which is could a perturbation change my neural network decision for this particular observation or measurement and so there's ways of doing this exactly there's ways of relaxing the problem to make it more computable in real-time but I think it's solving a different problem than what we really care about in controls or robotics which is what happens if you do this and the nominal action your robot was planning to take is deemed to be not robust your system still needs to select an action so that means just saying that your action you're thinking about doing is sensitive is doesn't really solve the whole problem so what I'd like to do is use some of the cool mathematical machinery from these that are solving a different problem and solve the robust decision-making problem instead and so I'll kind of draw this cartoon on the next this is the next animation to give a sense of what I mean by that so say you have this two state system which is you know a couple of states right here which are passed through this learned deep neural network that's already been trained and that's trained to output a Q value for action - and action one so the Q value just means here's how much reward I would expect to receive from that state if I took this action so nominally in deep learning you take a particular observation of the world and you pass that through deep neural network and find out okay I know the Q value fraction one I know the Q value for action - if then point lies above this hyperplane or the line of slope one it means that action two is somehow better than action one you take action - but in our case we actually have a whole set of states that we need to consider because we know the true state could lie somewhere in this set that's in pink over on the left and so what we want to do is map every point in that set to some point in this Q space and that provides us with this adversarial polytope that's essentially every single Q value that could be true if our state could be anywhere inside some state set it turns out computing that polytope exactly is np-complete so very difficult to do quickly for large now so this is where some of those tools come in where we can relax the problem a little bit and compute a certified outer bound for this so all of the verification problems are solving this problem which is does one of these sets cross this hyperplane and what why that makes sense is if you think about one of the points in this set maps to a point up here above the hyperplane that means action two is better if a point down here maps to a point on the other side of the hyperplane that means action one is better and there therefore is some uncertainty about which action would be taken if you don't know your state exactly so what we propose to do instead is let's consider the lower bounds on this whole set which is for action one we can compute one lower bound of our Q values and action to we can compute a different lower bound on our Q values and that allows us to reason about all the possible outcomes at once and choose between our different actions and so that's where I what I described is the robust decision-making problem which is reasoning about the worst case and coming up with an action that considers all possibilities and so I'll just write that mathematically here what I just said which is the RL approaches typically will be just solving this Arg max of the exact state that's been provided to it so this state came from the adversary you just look up what is this Q value and choose the action which maximizes that what we'd really like to do is consider these worst-case possibilities where we now know that our state s isn't just a particular state but it lies in this ball and the problem is that that is the np-complete thing so to compute this we can leverage these tools to do convex deep neural network verification just to approximate Q and that allows us to figure out what's a lower bound and an upper bound for our Q function as long as this set this state s lies in some ball so by using those bounds we can introduce what we threw some a paper with Bjorn and I about certified adversarial robustness we can provide an action rule which uses the lower bound on our cue function and chooses actions based on that so in words what that means is at each time step your system should take the best action for the worst possible reality and we're able to solve this in real time by leveraging some of these tools typically minimax problems you can't really do that quickly but doing convex relaxation is allow you to make the problem a little bit easier to solve with sacrificing a little bit of accuracy so when I describe it as certified what I really mean is there's these two things that we can provide us guarantees one is that by using this lower bound our robust action will consider outcomes that are at least as bad as the worst possible outcome and the other thing we can provide is a regret bound such that we know that that the action that we recommend will actually be within some value of the optimal action at our true state even though we don't know the true state and we don't know the optimal action so we look numerically at a collision avoidance type in scenario to evaluate this algorithm and see if the theory that we're coming up with actually mapped to something in sort of a RL context that is interesting to us and so in this case we looked at collision avoidance and we ran a bunch of simulations of agents trying to avoid each other and what we saw is as we plotted how many collisions occur in the world over on the left as a function of on the x axis our parameter of how robust our system should be we essentially consider increasing the size of the epsilon ball that we're considering and the number of collisions drops quite a bit so that gives the sense that we're we're introducing robustness that we can control in an environment that we weren't specifically training on and even though some adversary is changing what observations the agent is receiving we're able to counteract that quite a bit now there's no free lunch here if you add in too much robustness your agent is going to be so scared of crashing into the other agent that it's not try and get to its goal it's just gonna try and stay away from everything and so over here on the right we see the reward which accounts for both collision avoidance and reaching the goal Peaks out at some point for a particular epsilon value and then drops off because the system becomes too too robust in some sense really just overly conservative so that's one particular adversary that we show it against it turns out if you look at different strength adversaries as the adversary gets weaker and weaker the performance also shows the same types of trends so the takeaway from this plot is that our system is able to reduce the number of collisions in this type of example improve the performance in terms of reward even when there's imperfect sensing that was not part of the training environment so this is really an add-on tool that we can can stitch on to a trained dqn policy so to wrap up the the this part of the talk the contributions of this work are the first formulation of robustness certification in the deep reinforcement learning literature and we did that by bringing in some tools from verification to solve this other problem that is more relevant to the controller robotics domain so in the thesis there's some demonstrations much like I shown with this adversary where we compare against different types of noise models and different tasks but there's also this application that I think is cool which is another type of adversary which is if this agent which is in blue is more like a bully or a behavioral adversary we show how this framework can defend against that to some extent as well just by accounting for things that were not exactly modeled ahead of time we can still be robust and avoid an agent that's behaving differently than the types of agents that it was trained against so a big picture the takeaway of this is that we've addressed a primary issue in bringing these systems that involve deep reinforcement learning into safety critical systems especially ones that may involve humans and we can model and account for things that weren't modeled in the training phase and kind of add on after-the-fact so that wraps up the the stuff that I've done in the thesis that I want to talk about today and there's definitely a lot of future directions to take this work and I've just these are pretty specific in terms of what I've talked about today but these are some ways that I think we could extend these types of ideas so in terms of the cost to go estimation work I think it'd be cool to think through different ways of modeling the fact that there's not always one path to get to the goal and that could be something that you kind of constrain when you're learning this image translation thing when we look at the collision avoidance work I think we went sort of on one extreme which is learning everything and I think a lot of other groups are looking at trying to use physical or probabilistic models of everything I think there's some cool sweet spot there where you can use physical models for robots which we know how to describe robot motion pretty well while introducing some learned models of human behaviors and kind of leveraged this idea of MPC and learning together to do motion planning so I think that exists in some fields but in the collision avoidance context I think that could really have a lot of promise and then lastly for the certification work I think thinking generally about learned policies it'd be cool to have a way to calibrate what you're certifying and how robust your system should be online even if you don't have a good model of how big these different epsilon balls should be if you don't know much about your sensor or what attacks might be coming in so with that I think I'll wrap things up I can just leave this thesis contributions slide up here to kind of summarize what I've talked about and it also points you to different papers if you're interested in some of the more nuances of what I've talked about otherwise I think we can open up the questions thank you 