 hello everyone welcome back to robotics today this ongoing experiment or and build wall seminars in robotics today we welcome Scott endorsement who is coming from Boston Dynamics to talk about humanoid and as well as Sanka Kim who is our invited guest panelist specialist waiting to to ask those spicy questions Scott has spent most of his adult career and working with dynamic machines and washing machines and pushing on them just to take joy to see how they struggle to stay balanced all the way from when he was at you must working with you but then as a postdoc being the lead of the controls team the MIT DARPA Robotics Challenge and and then later now more recently moved to Harvard to Harvard where he also did research and humanoid robotics and more recently as most of you might know transition a transition that I'm sure was equal parts exciting and uncomplicated to be a research scientist at Boston Dynamics where he leads a team of engineers that how to put it they I guess they have fun I getting Atlas to do crazy things and Scott were all eager to to hear more and see more of what's what's up with Atlas thanks for joining us today thank you very much over now so let me get the screen share on how does that look great alright so yeah thanks very much I'm excited to have the opportunity to talk to all of you today and share some of the works that we're doing on the Atlas project at Boston Dynamics but before diving into that I wanted to give just a few slides about all the things that are going on at the company and who we are as a company more broadly so Boston Dynamics is a company that's been around for quite a while over 25 years and throughout that time its activities have evolved quite a bit but one thing is sort of remain constant and this overarching long-term goal of trying to create robot technology that meets or exceeds the performance of humans and other animals and this is an along many dimensions right so building robots that have you know mastery of their own body dynamics that allows them to fluidly and robustly and efficiently move through their environments as well as abilities to make intelligent decisions and perceive their environments and ways that enable to do manipulation and affect their environments in useful ways so obviously this is a really big goal and we're not close to satisfying it yet but we're all sort of motivated and excited to try to play a part in bringing this this technology to reality so over the past few years in the company has been a particularly exciting time because there's been a concerted focus on turning a lot of the robotics to technology it's been developed in the company over the years into sort of bonafide robot products that people can buy and that would ideally add value to you know their own projects and businesses and so what that meant is that we've had to sort of find ways to balance our sort of core mission of identifying and solving hard robotics problems that you know make significant jumps in the state-of-the-art along with you know doing all the things that are necessary to really develop and deploy products you know robustness testing certifications and support infrastructures and all that stuff so that's been that's meant that we sort of had to grow pretty rapidly so now we're up to about 290 employees along with that increasing headcount we have an increasing real estate footprint as well so we're about to move into a new headquarters still stationed in Waltham Massachusetts but you know it could be a much larger facility all shiny and brand-new and lots of great facilities for doing cool cool robot development we also have another office which is all the way in the other side of the country in Mountain View California and so they're also you know just moved into a new facility and growing rapidly and that's sort of the heart of a lot of the work that's happening on the sort of logistics side of the business in terms of a robot lineup if you will so I'm sure you guys have all seen most of these robots before in various media outlets or and/or YouTube so spot is sort of our general-purpose super-famous quadrupedal bot that's meant to you you know do lots of things and do them well we have some technologies that are being a little bit more focused and trying to address certain problems in logistics that we think would be a high-value for robots to solve and then Atlas is our pure R&D effort that doesn't have an immediate product horizon but is a sort of vehicle for us to explore innovation and hardware and software and try to solve problems that we think will be really important for our company you know years into the future so just a couple slides and the robots they aren't atlas so you know spot as I says they're kind of go anywhere do anything robot and you know not being part of this project myself but being able to be inside the company and observing how this is you know made the transition from what was originally a research robot to you know a product that is in people's hands all over the world has just been super cool and a really unique opportunity you know and part of part of the thing that's exciting here is that this is a technology that didn't really exist right so there wasn't really you know quadrupedal robots that could kind of go into challenging environments and actually do useful work and do useful inspection and so part of the the fun has been identifying what the markets actually are for a technology like this and so it's been cool to be able to see a spot in people's hands doing things as very busy kind of going underground and inspecting you know mines and powerful you're doing you know remote interviews and things like that for permitted medical applications and in particular in response to the recent coded epidemic - you know Dancing with Cirque de Soleil performers right and you know what a time to be alive if you didn't catch the news last week we just launched an e-commerce site so if you have a credit card that's burning a hole in your pocket you can literally go on a website and order yourself your very own spot which is pretty cool so on the logistic side of our product efforts we've got a couple projects that are ongoing so handle which you've all seen in various incarnations on YouTube is this really our sort of obviously a mobile robots meant to go to where the work needs to be done and is geared towards doing things like alt eyes and deep palletizing packing unpacking trucks and trailers and so it has to solve lots of really hard problems about you know being able to accurately and quickly perceive objects in you know very very lighting situations as well as with sort of heterogeneity and the items that it needs to deal with and you know be able to move dynamically quickly and also in highly constrained environments like the backs of trailer trucks or shipping containers things like that pick is sort of the robot you see on the right is really a product that's a 3d vision solution that is aimed at quickly and reliably palletizing Andy palletizing with mix q pallets and it's something it sort of can be integrated with with off-the-shelf arms so that's a just a very quick summary of all the other things that are going on in the company so for us of Tom's gonna focus mostly on what we're doing and particular on the Atlas project and so as I said Atlas is really our you know R&D system it's it's the thing that we're using - you ask really big and hard questions and and see how much progress we can make and how quickly we can do it so you know the large part of what makes Atlas special is you know what's built into the hardware and I'm not going to talk a lot about that but I want to give it at least one slide that kind of gives you a summary of some of the key technologies that have been innovated within the company over the past several years that make Atlas two really special robot that it is and so you know this robot would call it out list but it's really you know maybe I'm going to count it like a fourth incarnation of humanoid robot at lost dynamics those you've probably seen the previous versions that were used as part of the ERC this is a very very different robot despite sharing the same name so some of the key technologies are you know really high power high-density batteries you know hydraulic power units that capture all the important pieces of a hydraulic control power system in a very small form factor that can fit you know inside the volume roughly of a human torso custom valve technologies that a lot that support really high bandwidth and high efficiency control using techniques like structure optimization and leveraging modern metal 3d printing to build components for the robots that are both really strong and very lightweight and all these things were to feed together into producing a robot that is both high strength and low weight which is really the ingredients that you need to do sort of exciting athletic behavior so as a sort of overarching mission for what we're trying to do on Atlas is broadly we're trying to create breakthroughs and dynamic whole body control so that we can eventually say we're at a point where we're a meeting or exceeding you know typical human performance now obviously that's a tall order and it's and it's a long-term goal but we feel like we're able to make steady progress towards that goal and it's something we think that is attainable so this is a concept animation that gives you a little bit of an idea of the kind of things we're targeting and it's interesting because it contains a lot of elements of things that you're not typically used to seeing a humanoid robot doing right so moving very quickly running no jumping coordinating its whole body doing out of axis rotation the world with things that aren't its feet for the purposes of mobility right so there's lots of stuff going on here that's a little like outside of the typical humanoid box and so this is really the kind of thing that we're after and what we're doing and as an organizing activity as a project we've been focused a lot on parkour over the past couple years and so you might reasonably ask why why parkour and so parkour is interesting in that it has a few features to it that raise interesting technical questions I would say so the first is that it's characterized by you know high energy athletic behaviors that would presumably put us up against the physical limits of the robot and that's a good place to be for us because it forces Hardware innovation first of all and second of all it forces software innovation because we have to be able to have control techniques that can capture the relevant constraints of the robot in order to behave robustly and physically it's explicitly responsive to the environment so our core is about sort of looking at what's going on around the robot and then choosing dynamic behaviors in response to what it's seeing okay it's not it's very different than the activity of you know maybe building a very reliable walking controller and having it walk blind through a field or something like that right so this forces us to be able to not only have perceptions able to extract meaningful information quickly using sensors on the robot but you know do something with that information to change the dynamic behavior so that the robot actually succeeds and what wants to do and the third thing is that it's sort of characterized by variety so this isn't a utilitarian get from point A to point B method of locomotion it's you know explicitly superfluous and that it you know it's doing spin jumps and flips and all these kind of crazy things along the way and so that really puts an emphasis on us being able to rapidly create a variety of behaviors that are in this a very athletic and dynamic space okay so that puts our kind of constraint on the tools that we have to create in order to fill this goal so I'm going to start in terms of videos with a one that we've already released I'm guessing most of you have already seen this video before so this is Atlas doing a sort of floor routine that combines a bunch of different behaviors that you know include rotations around different body axes you know touching the ground with different parts of the robot's body fluidly moving from one thing to the next but it doesn't quite include yet is one of the pieces to the puzzle which I pointed out as perception so the robots kind of you know just executing these behaviors in a big open space so it really just has to care about where the floor is but this was a great starting point for us and exercising the tools that we've been developing over the past couple years and showing a you know P get this sort of variety and dynamicism that were trying to create okay so I want to start with a couple sort of guiding principles that we've been using in order to make sort of technical choices about how we're approaching developing parkour controllers for Alice the first is as I mentioned this idea of doing rapid behavior creation and this is something like being able to go from a concept of a behavior like a a 360 jump to your first set of robot tests in the same day right so that's kind of where we wanted to get you what this is not saying is something like being able to have you know any person off the street come in and author you know a fancy parkour move for Atlas it's okay if the person doing this still needs an engineering degree but what it shouldn't be is something like having to write a completely bespoke controller every time you want to do something new for the robot the second piece of the puzzle is now that we're sort of talking about behaviors as discrete things ideally we'd like each one of these behaviors to be as extensible as possible meaning that it's reasonably robust in the the types of perturbations we'd expect the robot to experience and it's also adaptable to whatever is going on the robots local environment so for example if I was able to easily create a behavior that it amounts to the robot jumping on top of a box but I had to create a new quote-unquote behavior for every possible relative transformation between the robot and the box at some point this becomes less and less useful so really we'd like to be able to create a large number of different behaviors but sort of within each kind of behavior class we want to have as few elements as possible and so that that puts an emphasis on having really good at online control so this is basically the architecture that we've been developing in order to do this and it's basically broken up into two parts first is an offline phase and this is really where you might use the term pre gave your creation and we're leveraging offline non linear trajectory optimization in order to create sort of template behaviors for the robot and these you know can range and you know in variety with the kind of things that you're seeing here so you know jumping on off ramps and doing spin jumps and flips and and so on but then these form a sort of database of behaviors that are available to the robot online and so it's really the job of the online planning control system to look at the environment around the robot use information that's available through the perception system to select from among that behaviors in its library and then it has to use online model predictive control in order to adapt those behaviors to successfully execute them given the current situation of the robots hand and so the sort of decomposition between offline and online we found is a really nice balance because on the one hand the offline computation makes what we're doing online through able to be fast and feasible but it also gives us a sort of nice way to iterate over behavior designs without having to run robot experiments so I want to talk a little bit about how we're going about formulating and solving these optimization problems so there's a bunch of different ways where you could reasonably think about formulating a trajectory optimization problem for a robot like Atlas and the particular choice that we've made is to decompose the problem into two parts one where we're solving for the momentum dynamics or the robot sometimes also called the Centauri all dynamic s-- and then we subsequently solve for a full body kinematic trajectory that is consistent with the centroidal solution and also it takes into account various other may be task specific kinematic constraints so this is an idea that lots of people you know particularly the last five or so years have gained a lot of traction with and you know we're no exception that the group that forms the Atlas team you know many of us are from the same places in academia previously and we have experience using exactly these types of models so and there's some papers now at the bottom here that you know are good examples of people having success with models like this but this is just a particular choice and we make this choice for you know basically you know computational expediency this tends to be a nice middle ground between model complexity and and computational efficiency so it can sort of represent a lot of the behaviors we care about while not being so numerically troublesome that it becomes hard to use so one thing that we're doing in this decomposition is we're sort of mirroring the structure both offline and online so we're solving similar problems offline and online but the details of how those problems are set up and solve differs so what happens offline is that were typically solving longer verison more densely sample problems whose costs and constraint structures end up being quite task-specific in general and these are computed things basically from scratch so if I wanted to optimize a front-flip behavior I'm you know adding in specialized constraints or you know starting at point-a on a rant on a box and landing at point B on the floor and maybe accumulating too high rotation and pitch over a flight phase and things like that and it's the job of the trajectory opposition to produce a sort of high-quality template motion in the form of a full state trajectory that is going to help online optimizations subsequently when the robot executes sit so on off line we have long horizon dense big nonlinear problems that aren't fast to solve online we have you know a very task generic problem structure where much we're thinking a much shorter horizons much more sparsely sampled problems for obvious computational reasons and and they're going to be largely driven by the the behaviors that the robot has in its library so the cost structure is going to be very tied to sort of trying to stay close to these motions while also adapting as necessary to what's actually happening with the robot so I'll talk just very quickly about what does it mean to solve these two optimization problems and I'll do it without math because I actually think the math doesn't add a ton of value in explaining these ideas so first of all with momentum optimization what we're really talking about is solving some kind of an optimization problem that's gonna tell us how hard we're and what direction we need to push on the ground or other surfaces in the environment and we want to figure out how those forces you know integrate over time to affect the center of mass and the orientation of the robot where you know now we're talking about a centroidal models we're talking about center of mass locations and thinking about the robot as you know so called single potato where you know the inertia of the robot can be represented as a single rigid body like an ellipsoid as you're singing pictures one of the sort of details that was interesting about the types of problems that we're solving that involve things like backflips is that the the ability to change the inertial distribution of the robot during the behavior can actually be pretty important so for example if I want to optimal is a front-flip you know if I knew you know in the optimization that I could shrink by inertia in flight and that means I you know ultimately need a lower angular momentum at liftoff in order to achieve the same rotation through the flight phase and that could be pretty useful if something like a front-flip is riding the limits of your actual robot so we've played a lot with different formulations that you know have the flexibility to represent these kind of changes and I wouldn't say that there's any particular set of choices that seems to be the magic the magic combination to make this work really well but what does really help us having a flexible implementation that allows you to sort of turn on and off these features in the form of constraints and and and you know variables in the optimization problem so that you can sort of only add the complexity that's relevant for any given behavior so the output of this is again going to be a sort of centroidal trajectory that you know is basically set our mass motion and maybe a lip sword orientation over time and we have to sort of turn that into some reference motion for the entire rest of the robot and as I mentioned that ends up being another optimization problem and so here's an example where we have a centroidal solution up at the top where you can see this sort of ellipsoid starts standing on a box which is invisible unfortunately in this animation and it's rotating you know 360 degrees around its pitch axis and shrinking its inertia in flight and then on the bottom you can see another input to the optimization which is a pretty lame initial guess at what a back flip should be right it's basically slurping the pelvis and 2pi on the pitch axis and and we take these as inputs and put it into an optimization that is trying to again solve for generalized configurations and velocities that are consistent with the momentum trajectory that we computed in the previous step plus any other sort of geometric constraints and so on that might be relevant for the problem that you're trying to solve so these could be things like if I wanted to jump up onto a box I might it's pretty important that my feet don't travel through the box along the way right so you might want to have constraints that say make the feet go around the edge of the box with some clearance so that when you run it on the robot isn't sub its toes so this is a really just an amazing tool for us you know being able to just realize you know really graceful motions using pretty sparse inputs in a super useful tool alright so I wanted to show you some examples of what you can do with sort of what I've described already along with the NPC controller which I haven't explicitly talked about yet but these are on the top trajectories that were optimized offline and then these are executions of these trajectories on the robot through our MVC controller and the key here is that the robots executing them in sort of conditions that roughly match what was used to compute the offline solution right so every optimize a box jump on our 50 centimeter box we run it on the robot or the 50 centimeter platform works just fine that's great and likewise you can do other things or here's some examples of jumping down off of a box and in this case using hand contacts in order to brace as part of the landing as a sort of an early exploration for us into using hand contacts as part of the behavior library and actually be able to track those forces on the robot here's an example of just using a single foot kind of touchdown jump off a ramp here's another example of just a bigger box jump so we were working on box jumps one day and thought oh how high can the robot go so we went to the gym and found this 76 centimeter platform and threw it in the lab and got the robot to jump on it interesting this is maybe one of the few times where the robot accidentally excites a resonance with an object in this environment and then manages to camp it out so that was that nice little happy accident and it's also important to acknowledge that not everything works beautifully first tries so here's an example of an attempted trying to do a back flip so you know spent the whole time optimize the back from trajectory rant to a simulator looked beautiful running on the robot reality happens right so here's the toes slipping on the top of the platform you know running into some actuator limits and that means the robot doesn't take off with the requisite angular momentum so it doesn't get all the way around to land on its feet and and that's what happens there's a similar example this was you know actually looks kind of dark in the lab so this was late on a Friday Rob indeed tonight had a great week we're crackin through a bunch of behaviors and we thought we'd try something really daring which was a standing front flip on flat ground and we managed to go and gather a couple people who were still in the office and really go check this house is totally gonna work first try and and we made it about half way and subsequent attempts didn't go much better than this so sort of two points here you know if nothing else the robotics is kind of an exercise in resetting expectations as soon as things start going really well you you get overly confident and then reality comes crashing back down and the second thing is I think this is just a great example of the kinds of things we're able to do with this robot that are super enabling right so the robot after this stood right back up and we were able to run something right after and that happens way more than you would expect so this is just an incredibly reliable robot and even when we do break it we have the sort of luxury of having a top flight support team who can swoop in and disassemble robots and reassemble them in working order and unbelievable lots of time this I can't overstate how important this really is for making progress as quickly as we feel like we do ok so I've kind of talked about just a trajectory optimization piece and how you can use the outputs of that to do you know behaviors in isolation on the robot but that's not really what we're set out to do right what we really want to do is as I said look at the environment be able to see what's going on and then draw upon the library of behaviors adapt them as necessary so that we can do something that looks like our core so we need some extra pieces in order to do that so I'll talk about those next so the first thing is we need some way to be able to see the world and there's lots and lots of potential complexity in this problem and what we've done is is leverage the fact that you know parkour can be reasonably defined with lots of structure in the environment and so we're leveraging the structure that we have in order to make this problem as simple as possible and so we can make progress so here are basically using depth cameras and algorithms for plane fitting in order to identify local planar regions in the robots environment and then using those to plan contact events in the future which become inputs to our MPC controller speaking of MPC the you know as I said before we're mirroring the same problem structure as the offline optimization but making sort of various technical decisions for computational expediency so we still have the non linear momentum dynamics of a robot there's no escaping that and so our approach as a sort of online controller is to iteratively linearize these dynamics and solve a convex QP at every time step and in order to do that efficiently we're exploiting you know problem structure so when you write down a problem like this you basically have two approaches to be efficient one is make the problem as small as possible and then you know have a dense problem that's tiny or you can make the problem bigger and sparse and then use smart linear algebra operations to take advantage of that sparsity we chose to do the latter and what this controller is really doing is computing contact wrenches over time it's adjusting contact locations in the world based on what's happening with the robot and it's also able to adjust the timing of the behavior itself and that's really really important so you just think about something as simple as jumping up and down on the ground it's entirely plausible that when you run this on the robot that it won't take off from the ground with exactly the right momentum that that matches what you had in your offline plan and if you're just naively tracking that offline plan things are going to start the Troublesome right around the touchdown event F for that jump because you're not going to adjust for when you're actually going to hit the ground and where you're actually going to put the ground and so being able to sort of always be recomputing that solution on the fly and then optimising touchdown configurations for the whole robot that are consistent with that solution is a really important key here and things just wouldn't work if you are more just naively tracking trajectories so this is a just an illustration it's a little bit similar to what was on the previous slide but I think it gives you a little bit of insight of you know how this is all working so this is from an actual robot log being played back at one-quarter speed and so you see a few different things going on here first the robots actual estimated state is this great version of the robot here this red robot is sort of our predicted touchdown configuration that there were less actively trying to get to you can see these blue arrows coming out of the robot or its planned center of mass trajectory into the future for the magnitude of the arrow corresponds to the planned momentum of the robot you can see the contact forces that it's planning to use both now and into the future in order to achieve that center of mass motion all right I want to show a couple videos that give you an idea about what robustness MPC affords you over just naive sort of trajectory tracking and so this is independent of the perception part of the system this is a sort of perturbing the robot blind while it's trying to execute a jump trajectory so in this case the robot sort of trying to jump into the air and do this sort of sidekick pose and some mean engineers off to the side yanking on the robot really hard with around a hundred pounds of pulling force and so what you see is the robot sort of quickly throwing its body back and catching itself there we have another version of a video like this that I'll show that gives you a maybe a little bit better idea of what's happening under the hood so here you can see the robots again jumping straight up and down this time not doing a fancy pose but on the bottom right you can see what is happening you know sort of in the robots head if you will so initially there's a plan to sort of jump straight up and down and you'll see that conveyed through these little red and blue coordinate frames hopefully they're visible what you guys are seeing and so you can see the robots are planning to jump straight up and down and then as soon as it's estimate realizes it's being yanked it's now deciding to deviate from this plan and go all the way back here throw its feet down and apply forces once it gets to the ground coordinate linear angular momentum in order to eventually come to rest above its feet again so this is the kind of thing that you know I think would be hard to get if you were more naively sort of doing time-based tracking of trajectories like this ok so we've talked about perception a little bit we've talked about you know doing this online pretty good control we have to think a little bit about the sort of connection between those two in order for the story to kind of make sense and so you know if we imagine we have a relatively sparse library of trajectories that do things like jump on top of boxes you know it's generally going to be the case that the boxes and the location of the robot relative to boxes in the robots actual experience are going to differ from the specifics of whatever the template motions contain unless you're very careful while curating your robot environment and an introductory library which we're trying not to be so really what we want from perception is to give us new sort of target locations relative to the geometries that are actually in the robot environment and then that becomes an input to the model predictive controller in addition to whatever template motion we select and so the job of the NPC is really to do the heavy lifting to figure out ok I need to sort of stay close to this nominal motion but also I really need to sort of hit the ground here and it has to do all that computation on the fly in order to you know translate and all the robot jump higher or jump less far etc in order to do with the perception systems asking it to do so here's an example where you can see Robin going in and moving boxes underneath that list so this is an example where we have something on the order of four trajectories that the robot can select from and it's really using the fact that it can retarget those trajectories and using online optimization in order to fill in the details to make the robot actually go where we want it to go so there's sort of another piece of this puzzle what you saw in that video was a robot sort of sequentially executing a bunch of jump behaviors one after the other stopping in between which is cool it shows an element of adaptability because we can take trajectories and make them do things that are slightly different than what they would represent nominally but again it doesn't quite tick the box of parkour for us right really what we're after are things that are sort of fluid motion from one we gave it to the other without stopping so we've thought a little bit about ways that we can do that and and this is the sort of solution that we're taking so far we're gonna assume that the robot always has available to it a queue of behaviors that are sort of drawn from the trajectory library how this queue is populated we have different answers for so in the case of the floor routine video I showed at the front of the talk a person just decided this is the sequence of moves you're gonna do and so go but more generally this could be a job for a planner that is using information for perception and maybe other sources of goals in order to intelligently select which bait behaviors to try and then again we're gonna allow NPC to do something heavy lifting here by allowing it to sort of splice up these trajectories in different ways as well as having the horizon of the MPC controller itself go across the boundary from one behavior to another in order to make smooth motion happen and so here's an example of a video of a robot that's jumping doing to one meter jumps one after another where the there's nice blending that's happening in the middle where normally the robot would have jumped stopped and then started a subsequent jump behavior now is sort of able to smoothly execute these two things in a row so see what that gets us on the real robot so here's a on the left as the video I showed you in the previous slides where the robots sort of executing one jump after another and slowly making it straight through the course and here we're seeing you know very similar jump library where it just has a bunch of box jumps but now the NPC control is able to splice these things together and blend it in a way that it creates really fluid and fast motion so this is starting to get a little bit closer to what we think of when we do parkour unfortunately if I were to put you in this space and say you know please traverse these boxes I don't pretty much that you would kind of bunny hop around like we're showing in this video right so this is clearly a you know not a very natural or beautiful or efficient way to get over obstacles like this so it leads to a question of you know how could we improve this situation by simply you know adding through the robots behavior repertoire and naturally leads to the question of whether we can do sort of single footed motions which is probably what you would do over that terrain can we do that using the same sort of framework that we've been building to do these more sort of whole body episodic motions and we're finding that the answer is yes so you can imagine solving these offline problems where we compute a centroidal solution that has certain symmetry and periodicity properties so here's an example on the left where you can see the center of mass motion of the robot as well as the planned contact forces that goes from left foot to right foot over a single stride and then we can take these and we can solve correspond to kinematic problems that you know match this centroidal solution while also having other nice you know postural properties that we you know would find appealing for running and then we can sort of just mirror these things and concatenate them together to create extended sequences of jogging and so you can do this and then it turns out you can put this on the robot and you could again see sort of what that gets you and so now on the left we have the example from the previous slides where we're doing blending with two footed jumping and then going over more or less the same boxes but using now our augmented library that has mirrored single leg trajectories and so you can see the it's it's really an interesting a system that we have here that we're able to kind of add to a library and drastically improves the capabilities of the robot in some cases but I think we're really just starting to scratch the surface on on what we think we can do with with tools like this all right so this is just one more video that I'll show that's kind of very recent work that we've been doing in the lab where you know the robots you know continuing to do this sort of single leg motions and you know starting to look at different types of obstacles what I like about this video is that it gives you a perspective from the robots point of view so what you're seeing is what the robot is on quote seeing and additionally we have sort of overlays from the output of the online footstep planning system as well as the model corrective controller so you'll see sort of the decision-making of the robot happening in real time as it traverses a course like this you can see these blue vectors which again correspond to the center-of-mass trajectory into the future you can see the planned footsteps at leading out in front of the robot and these are going to be you know planned again with respect to the geometries identified by the perception system and then you can see on those footsteps the forces of the robot intends to impart in the environment in order to get this body from point A to point B and I think like one of the things for me when looking at this it really just reflects how fast all this stuff is happening so they're really I think that constraints on wall clock time for you know doing sophisticated computation like you need to do in order to you know connect to sort of discrete level planning of footsteps to continuous behavior there's really just strong requirements on making that as fast as possible all right so I want to leave off with with sort of you know maybe a look back and a little bit of an assessment of where we think we are in terms of our broad goals on the project so again I you know I think we're really just starting to build a foundation for what we eventually want this robot or the next generation that's robots become and that is sort of meant that we focus a lot on kind of development of corage ility and focusing a lot on control problems and estimation problems and so on in order to show that we can you know in principle create performance limiting very exciting athletic behaviors but eventually as this the technology matures were basically creating a the next generation of problems for ourselves when we have to think about perception and autonomy systems that allow us to really leverage the capabilities of the robot as well as making the robot a sort of fashion of interaction meaning that it can both sort of affect its environment and useful and interesting and exciting ways as well as people who are able to use this robot as a for example a very advanced tool to do work in environments so for us I think we're thinking you know several years into the future that the growth areas for Atlas and and probably for the company more generally or more focused on these notions of perception autonomy planning and machine learning and interaction that's all I've got four slides so I'm happy to chat with all of you and try to answer your questions now thank you very much Scott for it is very exciting and entertaining talk top of all of the controller details I loved hearing about how you integrate perception as well we can't wait to get started with a panel discussion and we have many questions ourselves are also submitted by the audience so just just a few logistics below the livestream on the webpage you can submit questions or you can upload existing questions and we will pick up these questions during the panel discussion and part of the panel today our dong-hyun Kim Krishna Tony Watson Rachel Halliday Neil dashi Nima facili and the usual suspects dr. Rodriguez Marco Pavano myself jeanette book and we also have a great guest panelists today Stan Bay Kim professor sang became as the director of the biomimetic robotics laboratory and a professor of mechanical engineering at MIT his research focuses on biomes by add row button by extracting principles from animals and the work I know best from Sunday is the MIT MIT cheetah this robot is capable stable auto running up to 13 miles per hour and Tana was jumping over an obstacle at an efficiency of animals so with that I would like to invite sang Bay to ask his first question to Scott thank you very much and thank you very much for Scott for great parts and as usual is just impressive impressive work I about like endless list of question I can probably spend like an hour by myself but trying to prioritize for for the audience so I'll start with the more like like a cliche question maybe but as I think it's a high priority in the list you show a lot of different model based controller and but I will start seeing especially mostly from graphics community we start seeing mixture of machine learning and data driven parse that like doesn't really work with the model-based approach I'd like to ask question would you willing to replace some of your work or teams work with the machine learning if if yes which part if it is not why I asked that question first yeah great so you know I would say broadly as a team we're super open-minded about the algorithmic choices that we make in order to solve the problems we're interested in solving and so you know we're particularly interested in a lot of the results that's happening in the reinforcement learning literature over the past several years particularly for you know robots that are doing things that are in the sort of behavior space of the sort of things that I've showed you today and we've spent some time working on that ourselves as well you know building up some ability to do reinforcement learning in our group and developing simulation results that were you know pretty compelling so I think it's it's less a question about replacing things that we're doing is what and more one of adding value to what we're doing so you know what we're doing is is making certain simplifications to make the system work right so the model granularity that I discussed doesn't capture everything that's important for a robot like Alice right so there's really important details and actuation dynamics that really matter that come up when you run these things on the robot details about weird corner cases for state estimation and delays and all these things that you know the community knows are very important details and these are places where data driven approaches can potentially add some value and so what we've been doing is kind of picking you know picking our problems and trying to find out ways that we can make the most progress as quickly as possible but we expect that at some point we're gonna run into some walls right there's going to be some limitations of approach that are more or less fundamental and we're excited about exploring opportunities where you know data driven techniques could be pieces of pieces of puzzle to making you know just them go from you know ninety percent to one percent okay thank you very much um another question it might be a little bit complex I guess is about complexity of the structure of the controller and we learned from school like ABC small component tool in mathematics and in theories but eventually in order to make this kind of work robot work and not even like reality that even just research to make behavior achieved you need to stroke your yours to do like architecture control algorithm really well you show ready like the two levels of abstraction of model but you know how would you divide this abstraction of level and what you know band is computation time is required to represent their abstraction and their what kind of tool we have to use there there's a lot of complexity eventually it's like one design problem and we have a hard time even teaching like a mechanical design as a teaching tool how as a person who's been professor how would you teach this to student or for the community this is a bunch of but most people here are trying to learn because it's not something we can learn from school easily and if I add one more question is how do you deal with the generality versus versatility I think the most anonymous might be the probably the art frontier on this like doing multiple different thing with the common tool I guess is these two questions that kind of related so if you can get on how to teach yeah so I think I'm the teaching front there's no replacement for actually doing it right so I think insofar as we can create courses or educational experiences that expose students to the realities of getting a complex system to do what you want and you know the better because it's hard to sort of do a blackboard discussion of you know how to debug a you know joint controller or you know do things like that so I think you know there's no there's no replacement for getting your hands dirty and working on these problems for real and so for students that means I think trying to take as many project based classes as you can or trying to get involved in research labs if you have the opportunity that you your University I think many of us who've you know had careers in robotics started out that way where we you know just we're helping out trying to you know make progress on other people's projects and robotics labs and then eventually grew into defining our own let's see so in terms of you know the trade-off between complexity and sort of like versatility you know it's it sort of depends on the the scope of what you're trying to do so if we're trying to just build a bipedal robot that just walks around is really robust and I think that lends itself to a certain set of simplifications whereas for us parkour it was interesting in that it basically led to having to have more complexity in the model than you might need for something like humanoid walking and so it's really I think defined by what the problem spacing Orion is and for us you know I think we all know that the simplifications that we're making are more than we should technically if we had the ability to capture all development dynamical effects of the robot in our models and use those and we certainly would but for practical reasons you just can't do that so we've been trying to ride the line between representation of the important effects that matter for the robot and and trying to balance that against you know computational feasibility basically thank you Scott so dumb Ian you had some questions from the audience Duncan you have to unmute thank you and I have a question about that control hierarchy the first two you spaced by the foot placement and Jaime I believed it after that point NPC does not change the step sequence or tiny and then you project it to the full body dynamics how much information is used in the food sequence footstep planning and timing and is that MPC doesn't change any location or timing yeah great question so I think that there's a spectrum of answers there and there's a huge I think one of the important open problems for us to work on the future is making a more sophisticated connection between the sort of discrete decisions decision-making layer of the system which you know in this case could be footstep location planning and the underlying continuous dynamical motions that were actually executing so for now we've been mostly using pretty simple geometric heuristics there's no timing information embedded in these sort of high-level goal locations of where the where we want the robot to contact the environment the timing information comes from the this were the dynamic trajectories that are present in the library and again because we're using predictive control online the rope it's able to both adjust the location of where it actually hits the ground it'll be biased towards trying to get to where we want to go but it doesn't have Q and it's it's free to adjust the timing and as well that the forces that it applies whenever it's touching something I have liked for a little more questions so NPCs solve the solution with a centroidal dynamics and you've predicted to the whole body dynamics san-tomás trajectory is kind of easy because there's a radius on each link but centroid that angular position is something hard to define are you project that angular position to the body yeah so we have some some answers to that that I probably don't want to share totally in public but you know there's there's some answers that you can look at in the literature so like a key word to look up there with the angular excursion and so if you have a trajectory for inertia and angular momentum over time then you can end some starting and or a configuration then you can numerically integrate that around along a path to get some angular configuration into the future thank you thank you very much Krishnan you had some questions from the audience yeah so I think there are a few questions about what kinds of applications are enabled by humanoids like specifically like maybe bipedal robots or I guess more generally like what are the benefits of adopting sort of anthropomorphic robot designs for certain applications like could you talk a little bit yeah so I mean for us Atlas is you know we're not we're not trying to have like a go to market strategy for Atlas so you know for us as an R&D project that's just meant to really just push forward the science of robotics as much as we can humanoids are a great platform because you know it's a very capable form factor you know human beings are able to do lots of interesting things and it has this sort of interesting challenges associated with scale and complexity and all these things if you were to think about why you might build a biped product yeah there are some probably some form factor considerations you know we can be dynamically balanced we can go over really challenging terrain and it would be limiting for wheeled systems or kind of tall and thin which is maybe more useful than having a Quadra pet or some other bigger multi legged robot so I think there there some arguments you can make for why a humanoid or bipedal form factor would make sense in a practical way I think if you're you know picking any one specific set of problems that you want to solve and maybe build a product around the chances that the optimal design that you might come up with this humanoid is maybe slim but in terms of sort of versatility excitement and its ability to present challenging problems for us to do research on I think it's a pretty cool thank you I could add a follow-up oh sorry go ahead Christmas yeah are there any like I don't know things that you've learned from working with that list that have made it into some of the other robot products as well that Boston Dynamics is working on yeah good question so I think all the stuff that I've described here you shouldn't interpret as being representative of the choices that are being made and other projects but that being said our goal is to be a sort of Rd unit within a larger company and so we're trying to develop you know in addition to pursuing our own specific project goals and leading to specific experimental results on Atlas and the process of doing that we're trying to build general tools which can be in the form of software libraries and things like that that have started to make their way across the company so I would basically I would say what are the details of what I'm saying in these slides shouldn't be interpreted as being related to necessarily what any of the other robots were doing but a lot of the technology that enables and underlies these sort of ideas I have talked about are sort of general-purpose things that were taking advantage of thank you Scott so Nima you have a question from the audience yeah thank you for the great talk so we had a great question from ruff how he asked what do you think of mechanical compliance and is it good or bad for control and following up on that do you wish you had more or less of it yeah so I'm happy with no mechanical compliance analysis but at least no intentional mechanical compliance I think it's I think it can be really good if you are able to at design time decide what the right compliance is for your robot so I think insofar as you can do that then it can be really beneficial because then you're just basically building you know part of what would have to be at the control solution into the mechanism itself but for a robot that's meant to do many different things it's sort of hard to figure out exactly what those you know compliances should be and so insofar as we're able to kind of just implement that at the joint control level I think thanks got Neil you had a one question from the audience yeah there's been a bunch of questions about design and so I was just asked in the beginning you said when your objectives were pushed for the frontiers of what Alice can do in a design side and so is there a principals ways of different differentiate between control limitations of your behaviors or hardware limitation and put like how do you go back and close the loop with the people that designed the robot yeah it's a good question now I think there's not always a crystal clear answer to that right so you know you can imagine using our kind of tools to develop a behavior that would be through the aspirational for what the current robot can do and you know there may be ways to do that behavior that would be slightly different that would still qualify as say I don't know a double backflip or something like that there may be details on how you can control a double backflip that could be within or outside of the envelope of any given hardware system so I think it's mostly just us looking carefully at things and in trying to decide yeah this is the sort of a real limitation that seems to come up across multiple behaviors so that if we can only make this part of the robot better this would really enable us to push further so it's more of a less of a precise thing that we can point out and more of an iterative just you know discussion between different types of engineers and and then we come to a consensus consensus about what the path forward should be thank you thank me you had more questions from your repertoire one our discussion yeah I'll read a lot of the audience question Stephen your dancing is said in your opinion how important is exacto cuttings precision of the torque or force control in low level oh yeah I mean having very good joint level control is is really important if you're doing a bad job at you know whatever style of control you're doing force control or position control if you're doing a bad job at the joint level it just makes everything harder so yeah don't you know there's there's still wins to be had at tuning low level control spend a week doing it yeah but would you say like 95 percent is it good enough or you have achieve 100% or no thank you I don't know how to put numbers to it okay I think you know I don't think you need to go crazy with all kinds of really isolated tests and doing chirps and sign you know step functions and all this stuff and making sure that your joint level tracking is perfect I think if you're running experiments on behaviors you care about and you can see that there are certain joints that are just doing a bad job you should fix that I have a one more following question with it how about the intuition between the torque versus position which one is more important I don't have a general answer to that Michael you has a question Scott great talk and fascinating talk just a high-level question so if you were to update your humanoid robot will be your first desire don't gosh well I want you to make it 10% lighter and 50% stronger that's cool not a question more related to the MPC controller that you have do you ever run into situations where the optimization takes too long or maybe you have taco convergence yeah so it's certainly there's no guarantee that you won't be accidentally creating an infeasible problem or something like that usually that happens when things have gone really wrong on the robot and you know you're probably gonna have to like bail or switch to different roller or something like that I think you know in general when the robots kind of doing what we wanted to do in feasibilities and variants install time are not really an issue and part of that is because of the way we've architected things we're leveraging lots of high quality offline solutions in order to make NPCs job a lot easier so I think if we were instead doing something where you know we were having really complicated generative NPC controllers that were figuring everything out online I think you you know and they might be more robust but I think in many cases they'd be numerically thank you thanks god um I actually wanted to ask a perception and vision question so I thought that for now you've been working a lot with like flat surfaces mainly and explanted in different ways but can you maybe talk about what are kind of the biggest challenges there of closing these control Louvre and MPC loops around perception and maybe how can you get into more unstructured environments yeah I think you know we're definitely interested as a project of you know broadening the scope of where and what Atlas is doing and that involves you know going outside and being able to do responsive dynamic behavior and unstructured environments as well you know I think we've just been you know we're a small team we've basically just been biting off pieces to the puzzle in order to sort of march towards this bigger goal of you know you know being able to do this kind of behavior for real and we've kind of been picking our problems and one of the things that we've been okay with is you know leveraging more structured environments in order to make progress on the control side but I think you know going forward there are lots of ideas about how to make things better so you know doing more as for slam like things so that you can better keep track of environments when things are going in and out of the field of view and you're doing crazy spin jumps things like as well as yeah getting away from assumptions about simple geometries in the robots environment so that it can you know go outside and and do cool tricks off of natural features okay thank you battery you had a question yeah I I wanted to ask Scott so you so you sort of showed these approach where you have an episodic nature of trajectories where you combine sort of behaviors from a library and then an online an MPC controller that deals with smoothing and the robustness and I wanted sort of like a long-term vision not Boston Dynamics perspective but your perspective if you think that that is the end or that is a step towards something that we haven't figured out yet like for example just to finish a question if I look at the visionary animation that you started with at last sort of jumping sideways and using the hand I'm not sure that I can see that as an episodic behavior or if that had to also be inside the library yeah so all right so the last point about could that behavior be represented in what we're doing now I think yes you know and it's one of the things we're looking at is behaviors that are sort of akin to that is something I certainly on our radar and so we have we have ways that we think that that makes sense in the framework that I talked about in terms of like the grand vision for like what is the right solution here you know we could start with what would be nice if it were possible and that would be you know being able to do all this optimization online you know I'd love for that to be the answer or you just get rid of all this library stuff all this offline stuff and just do everything online that's just hard and it's not hard just because you know we need better computer chips it's hard because this is a really hard non connects problems and so getting to the point where you can not only solve them fast enough but have them be solved with enough reliability is just a really tall order so I think you have to leverage some kind of pre computation or to make that optimizations feasible in our case we've chosen to use trajectories because it's a nice way to sort of summarize prior information about what is a good you know spin jump but there's lots of other options you could consider too right you can imagine learning cost structures like value functions or something like that in order to make MPC smarter so I think that you know there's many different possible technical answers but the gist of doing online optimization and then leveraging something the pre-computer or to make that more effective is a solution that I would believe in as a sort of long term thing the other thing that we're not really doing here is sort of improving from experience right so the robot could do you know 10 backflips in the lab and it's never going to get better at doing backflips it's never going to learn anything from the data that it's generating in those backflips unless we look at it and make some changes so that seems like an opportunity that we could pursue in the future where you know robots could sort of slowly get better over time instead of you know what usually happens is they slowly get worse over time because hardware types and things like that thank you so we have more audience question Neil your head what yeah fortunately coupon I think observers mention that ROM but what kind of context models are you using for the robots and I guess I'll add on to that how does that influence the design of your optimization and the controller ah yeah so you know I think for the there's a bunch of different notions of contact that are throughout the system I won't get into the details up but you know broadly speaking a lot of what we're doing is thinking about points of contact on the robots geometries like its feet and things like that so I think you know in terms of you know lots of special insights around like contact modeling and simulation and stuff like that we just there's nothing there's no magic sauce there I think thanks nema you had one question from the audience yes this is a question from anonymous they ask when is Atlas getting at hands with fingers so you begin getting a hand with fingers yes well there are hands I've seen them on the shelves and in the lab but I mean it's really a function of what we're doing at the robot right so one of the things that Atlas was used for in the past was sort of moving boxes around and in that case it made sense to have hands on the robot for us right now we would just smash the hands to bits you know on a weekly basis it doesn't really make sense to put hands on the robot so I think as we're you know continuing to broaden the scope of what we want Atlas to do and what we try to do with the robot then things like you know dexterous hands maybe come on the table but for us since we're mostly focused on sort of full-body athletic things having weight thank you krishnan had a few questions from the audience yeah so there are kind of a couple questions about planning with the unexpected or real-world surprises that may happen when you're executing a trajectory that you calculated offline so one of them is do you have any insights on how you can formalize the reality hits outcome between planning a trajectory that works offline but will likely fail on hardware and then a follow-up would be when the robot fails to recover do you think that if your actuators had more power embassy NPC would be able to stabilize them they're both from anonymous okay I'll have you repeat the second question after I finished answering the first one so the first is I guess a question about how to prevent as much as possible false positives in our offline / simulation development step you know I think it's it's a hard thing to do in general right so by and large we use simulation very effectively in our workflow not only for sort of unit testing things and making sure that our codes not breaking over time but as part of our evaluation of new behaviors and it's important to do things like heads is the same you know control and estimation code running on the robot is we have in simulation we also have some really approximate notions of the actuator model that exists in simulator that wouldn't exist in the models that we're using for optimization so you know a simulation for us is a pretty good net that catches bad things before we put them on the robot but occasionally something that looks good in simulation doesn't quite work on the robot and I think the best answer we have so far is that we've learned a lot of the ways in which the hardware would fail where the simulation might not and then we try to look at the simulation results and try to analyze those signals in order to get a feeling for whether we think this is really going to work on the robot or not but you know having you know more automation there for example that we could use to kind of classify the results of simulations I think be really cool but it's a hard problem in general and sure I can repeat the second question if you like they were asking when a robot sort of fails to recover I guess and falls over do you think that in those kind of instances if the actuators have more power behind them the NPC would be able to stabilize in those cases it depends on the details of what happened so if you know the robots you know careening sideways through the air is not much that actuators train its going to do for you but maybe in some cases where you know you just ran out of meat work and if you could push a little bit harder you would have actually cleared the box that you crashed into then sure it could help in that case in terms of like failures you know we don't do it a ton right now to prevent them all the way try to prevent them but we don't do a ton to try to actively recover we have a couple things where you know we're using some existing control strategies for doing footstep placement to recover balance as well as just a sort of you know safe fall behavior which was a common strategy as well in the DRC's a lot of meet that famous DRC fail real has the robot some kind of soft squishy position control thing where it falls over and looks kind of like a soft statue thank you Thanks so you know you have one last audience question okay I don't like another compound audience question its what type of trajectory optimization can you use on Atlas direct shooting or some type of differential dynamic programming and maybe some people ask more generally what your person searched checked out my mission is and I'm getting on this problem specific but if you could just kind of stand on that yeah I mean I don't have I don't have a strong bias I think the you know the sparsity pattern stuff I showed on the slide should lend itself to indicating that we're using a direct method but you know I think I don't I don't suspect that there it would be impossible to produce similar results with either class of algorithms like I said there's ways to efficiently solve problems that take advantage of you know small problem sizes and the sequential nature of these problems and also ways that you can write it as a big sparse just nonlinear program we tend to do the latter but I think you could easily have success doing it the other way - great thank you can I ask one last question sure so Scott I know a lot of students are watching this and a lot of students are dreaming to set up controller do you have any advice like you know as I said children could be biased because there are structures in schools and you know schools and you know what's going on and come first you know the company and you just given one other wise advice you you know how to do this for a job is that it's not a question well you know they want to be a great robot control designer in the future and if you if you or your one-word guidance can change their you know oh yeah so so all right so a couple things one I'm gonna reiterate what I said before there's no experience we're just doing it so as much as possible get your hands on robots join research labs take project courses and get the experience and so you need to really understand the ins and outs of getting things to really work on robots the other thing I would say is be very open-minded with your approaches to control I think you know it's easy on your student to you know get really excited about a very narrow set of ideas and then look for all the places that you could apply that set of ideas and order solve problems I would encourage us think about it the other way where you're just trying to find the most exciting problems and evenly evaluate all the possible algorithmic approaches that would be a good sort of engineering solution to those problems and then learn as much as you can about the ones that look the most promising and dry them up thank you so much it's wonderful oh it's a time to wrap up let us thank again Scott for the insightful talk and the tips for the students I'm sure that they will be very useful and I absolutely sure them so I have good news for the people that are listening today robotics today is going to continue in July we're going to have two additional talks one on July 10th and the other one on July 24th the 110th will be given by Nara of ikemen at the UIUC and the one on July 24th would be given by CDC Nevada so stay tuned for the details and we look forward to seeing you next time 