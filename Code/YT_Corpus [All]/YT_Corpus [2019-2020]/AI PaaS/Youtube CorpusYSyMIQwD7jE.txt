 hello everyone welcome to our session where we will be talking about democratizing data by creating common data models and configuration driven pipelines to enable AI platform my name is sean ol guy and I am a senior manager of software development at Blackbaud presenting with me is Cindy hi I'm Cindy Madison AI architect at Blackbaud today I'm going to be giving you an overview of the problems that we were facing and a brief description of our journey to build our data platform cindy will be going over our architectural decision the common data models that we created what the configuration driven pipelines are and the transformation building blocks and the AR feedback loop before we get started just a brief word about Blackbaud and who we are Blackbaud is the world's leading cots offered company powering social good we serve the nonprofit and philanthropic space globally let's get started by understanding the problems that we were facing first we have single tenant and multi tenant databases that spend multiple products built on different technology stacks and hosted with different cloud providers of course all of our value is stored in those silos second this has led to similar domain entities being stored very differently by products for example a constituent may look like X in product one Y in product two and so on sometimes that translates to new fields sometimes more radical departures even though describing the same core entity third through acquisitions and new development efforts over the years we have actually compounded the problem by adding even more variety then even within each product data entry inconsistencies accessor beit's discrepancies in data sets finally and has become painful to get access to all data and it takes a long time to get access to that data so how do you scale getting new data sets in any sort of reasonable way almost always we are tied to some legacy product that has all the value the question becomes how do you get data in and out of those products and make it accessible to micro services deployed in the cloud so you can modernize and provide insight to customers looking at our journey we had a few data Lake projects that were scattered throughout the company so we began to build consensus towards building a common data model in the data platform and being a Microsoft Partner we started off by leveraging existing Azure tooling such as data factory you sequel and Azure data Lake with a target of using as many paths tools as possible we wanted to keep it relatively simple and only do batch processing of data finally we picked a small project the required getting data from a legacy product that would allow us to test things from end to end this is what our initial architecture look like the data would flow in from our various data sources into Azure data like storage and move from an initial transient zone by data standardization services to the raw zone from there the data would be moved by data quality services to a cleanse own and finally data enrichment and modeling services would further enhance data and enrich them the enriched data would be fronted by a data services API or be delivered to additional databases to finally be consumed by our products it didn't take much time for us to find some flaws in our approach which led to some pivots in our architecture we found it painful to add new readers for different sources that were not supported such as Avro and parquet there were some gaps in the azure data tooling that we were using for our specific use cases we realized that support for batch only processing was insufficient and we wanted to have a single path for both batch processing and streaming of data we need to have a compacted version of our records to recreate datasets from our legacy products in the data platform to make the data more consumable and useful for the rest of our processes and finally we needed to use more standard standardized tools so we can hire data engineers more easily with the right expertise that can get up and running quickly so let's look at our solution at a high level we bring data together into a common data Lake that perd data sovereignty region we catalog data sets and transformations making data easy to find access and manipulate we are bringing out we're building out common data models so we so new services can operate on data and not worry about differences in source systems we set up in deploy configuration driven pipelines to support new transformations more easily we developed tooling and infrastructure to allow teams to spin up their own SPARC pipelines quickly and in a consistent way we incorporated technology to match records across products and entries creating a unique clinking key and we have infrastructure to support faster time to value by development teams let's a little bit more about the different components of that architecture we leverage Delta Lake as a storage layer on top of Azure data Lake since it offers acid transactions on spark the data catalog service captures metadata about our data sets but it also captures metadata about the transformations that happen within the data platform to more easily find access and manipulate that data the lake authorization service was put in place to automate and better control access to data sets within the data platform the ingestion service streamlines the flow of data into our data platform and of course the output server standardizes the flow of data out of the data platform and back into our ecosystem and finally the async contract broker service that seems a bit out of place but it is our in-house message schema repository that proactively prevent services from releasing breaking schema changes so taking a closer look at the async contract broker traditionally this type of functionality would be an application platform service but we saw an opportunity to get developers to do the unimaginable we got them to actually catalog and annotate their own data by using the async contract broker developers get to enjoy cogeneration and peace of mind that they won't introduce breaking changes to their async messages schemas and at the same time the data platform gets to use the metadata they generate to populate entries in our data catalog another problem which we solved with this service was that it was hard to convince development teams to start sending us data especially when we didn't have an immediate benefit we could offer them right away but we already had a lot of services sending a lot of good data over async communication and service bus made it very very easy to add an additional subscriber so we a process that lets us automatically subscribe to topics on a voluntary basis and push all produced messages into the Data Platform data that flows via this route is saved as a Delta table almost right away and catalogued as a separate entry in our data catalog but with the same schema captured in the async contract worker therefore everyone wins everybody's happy and we profit now that we've talked about architecture in our journey I like to turn it over to Cindy so she can walk through how we leverage all this great data that we have ingested into our data platform next one now that Chiron has walked us through our journey with the data platform architecture I want to dive down a little into how we make this data easily available in a conformed and consistent fashion across all products in clients we accomplish this by creating a multi terabyte common data model this common data model has a common defined structure consistent naming of tables structures and fields consistent representation of common values and consistent across all applications and application types and it is integrated with our value-added services there a common data model or CDM is a collection of objects each object represents an entity in this case the entire entity is a person the object contains all the information about that person including demographics phones emails addresses transactions etc these objects are stored as a row in a Delta table here we see that the name field is a structure of all aspects of the name this the address phones and emails are arrays as there are multiple of each one for one person let's look at the phone's array these are arrays of nested structures with collections of various information about the phone including auditing information and their score phone information if if we dive down into the core information we finally see the phone number along with all the other possible information around a phone number there is an array entry for each phone type home mobile work etc the the common data model is a stream of change events coming from thousands of relational tables CSV table files JSON parquet across all of our products and cross all of our clients so we have the the data is in all forums normalized data do you normalize data structures nested objects simple flat files and comes from many different databases sequel server Maria DB Oracle black files etc since we are processing streams of change events we need to be able to update each change event without clearing other tables considering each source table can be billions of records we need to be able to update a single field in a structure in a column in the common data model to do that we created a configuration driven pipeline how do we get the relational data taint base changes events into this format at scale we use the CDP the configuration driven pipeline we designed a configuration driven pipeline that has a common ID a common ID is a requirement in any common data platform that it is required to allow our your tables to merge into the common data model using the common ID destination data sets are defined by creating a map from the source table specified in the data catalog to the destination table the map is created by a module that presents a list of available source tables and columns definitions provided by the data catalog and a list of valid transformations the CDP supports in a list of the available destination table fields the metadata map is constructed to relate each source field to the destination table with the transformation it required to transform that into the correct schema SPARC structured streaming processes change events interrogate the data catalog to determine lineage from this source table to all destinations it receives this metadata map of source to do destination and uses topological sort to resolve dependencies in the map generation for each lineage it applies transformations on each field specified in the map and then merges into the destination table transformations are basic building blocks that allow the pipeline to take any source schema and transform it into the destination schema our transformations we've determined we've [Music] we've decided that we've come up with a list of transformations that are building blocks that if we can apply this these six transformations to any input any relational input and be able to create the desired common data model schema output these these six filters that we've defined our filters transformations or filters views one to one with sequel transformation and lookup where we take one field and we put it directly into the destination field with possibly with a sequel transformation one row too many rows where we take one row of data and input change events and end up creating multiple rows to represent that data and an example of this is when we get denormalized data we want to break that up into multiple rows and then on pivot operation we have many rows to a two array in one column that's this is a case where if we have phones somebody's phones and multiple rows at their home phone and one row work phone and another row mobile phone in another row this transformation will allow us to collect all of these phones for that individual up and put them into one column in the destination table and the final Agra transformation that we need in order to be able to transfer and transform any of these as aggregations be able to do max min and other types of a great vacations on this data so with these six filters were able to transform any relational data exchange event even which is you we just got a change event which is a partial update to a record and we can figure out through the map and to the and use one of these transformations to successfully push it into the proper place in their common common data model the filters are straightforward just but we have pre filters and transformation filters where the free filters will be applied to the stream as its streaming in the change events and they close their transformation filter will be applied to the transformation once it's in process and a view is instead of writing to a destination table we just create a view from the map so we can map any combination of source and transformation to destination and create a view of that so this allows us to for example an easy way for us to provide non PII data from what a huge table by just making a view that's that hashes out the PII data and provides the rest of the data so with these transformations we're able to do config the configuration different pipeline can create any common data model or any destination model in this demo I will show how the configuration driven pipeline processes incoming change events to update a multi terabyte common data model the change events in this example here there are no denormalized data for phones associated with a person are processed by a spark structured stream when the change event arrives the configuration different pipeline calls the data catalog service to return maps to any destination data set that has a lineage from this source this map specify the source field any transformation needed and the final destination table field when when the some common configuration driven pipeline processes all of these Maps it pushes the results up in updates of the terabyte common data model person in this example so that's how the CDP receives these change events over a stream applies the transformations and pushes them into a Delta table of other common data model now we will look at its more specifically at an example of what spark API calls are used to do this here we will start with a change events that are just name events a very simple just named events we have for product a we have person ID 1 and their first name and last name and ID 2 with their first name and last name these change events come in we interrogate the data data catalog service and ask for any lineages based on these sources and we return this map of source field transformation destination field where you can see that the source field here is the ID and we're going to turn that into a common ID in order to go into a common data model by simply applying a transformation that's a concatenate product de with the ID and then we'll push that into the destination field so let's apply that map right now and to apply it it's very simple we take our deltas that are submerged change events that came in and we add a new column that's the destination column column my common ID and we applied these transformation the single transformation that was specified and we use a source import of ID and so here we've simply created a new column of ID applying the transformation and we have our new column common ID with a new common ID but which is product a calling up one in the product a equal and two at this point will process the remaining two or three in the map which are very simple as well if you remember the map is 4 4 2 & 3 it's just first underscore name goes to the destination of named up first and the last underscore name both lame name dot less so we apply that transformation and we can see that we have our change events that came in we have our common ID that we've added now we have this column named up first and name dot less at this point we have finished processing all of our transformations and the CDP is ready to just hand it off to the CDP finisher that takes any all any combination of transformations and then cleans them up and pushes them and the way we what it does the CDP finisher is responsible for crepes structures it will determine what destination structure columns were extracted and nests and extract the nested fields necessary to fill in those destination structures it also conforms the rest of the stick schema to the destination schema setting up fields as necessary in schema formats as necessary and then it merges the this Delta table they into the destination table as you can see here named up first and name dot last obviously look like structures but they're two columns here so that the CDM finisher will take look at that and say we want a nested field name name and we and the subfields are going to be named up first and name dot last so to create that to gather this structure we simply do a new column and that's the field of name so we're going to say new column of names name and we're going to create a structure out of all of the columns there are subfields of that structure in this case name that first name dot last so if we run that we will see that we now we have a name column with structures underneath it named at first there's a good then that the part of the CDP finisher that looks to make sure that the schemas are the same as the destination schemas or look at this and say the destination schema has a middle name as well so in order to set that up we'll just simply add that column to the nested columns but we'll just make it a literal a blank so this time when we gather up our structures we will gather them up with the fields that we're providing and the fields that are necessary to support the schema and the destination at this point that's this is ready to be pushed up into the destination so we'll just select the destinations part of the CDP finish it we'll just select the columns that are necessary to be pushing to the destination in this case all we need is the common ID which are going to join on and the name column because that's all we want to push into the destination so to up cert this into the common data model we will we'll do a merge we'll merge into the destination table and we will use the common ID is the join on so join on common ID and when it's matched then we'll start we have the CDP finisher has gone through and created an update list based on all the schemas and a destination and source and so in this case it'll say a destination named up first equals dust source named at first but you can see we did not do the middle name because we're not updating the middle name because that did not come from the source so we will just update the source fields within a structure that we have new data for when it's not matched when this is a new row to be added to the common data model we will just simply insert common ID and name and in that case since we have set up this EEMA to be the proper schema for the destination table with middle name and as well the new we will be able to insert this into the table with the proper schema so with that we have the Dustin the destination table it says we would want it to be where we have the structures nested nicely and a common ID so that's it for the simple transformation processing now that we have a common data model that has all of the data across all the products and clients in a conformed way and an easily accessible way where they can where data scientists or other product people can use our leak authorization service to simply request access to this table or they can access a view that has a no PII in it now we are in a position where we can start creating an ml or AI feedback loops we can have the full cycle of model development where the malla can have a model that pulls data from our common data model and trains a model and then we deploy that model and it starts scoring some new input data and we can turn that the result the labels that are created from that model back into the into our system and that data will become available for them we can retrain our model on the on the labels that were created by the model itself and do a complete AI feedback loop so when we look at it all together data is not flowing from various products and services it is automatically ingested into our data platform we can transform the data through configuration driven pipelines and into common data models data sets can then be leveraged much more easily and generically to produce value to our customers and finally we have baked in feedback loops so we can further iterate and improve so how did we democratize data we made it easy for our data scientists and engineers to access datasets from common data models in a secure manner that limits our liability and maximizes the benefits that we can draw from them they no longer have to worry about the how when and where of getting access to data sets they can more easily manipulate data create insights develop models and output results to our ecosystems thank you very much for your time we are ready to take your questions you 