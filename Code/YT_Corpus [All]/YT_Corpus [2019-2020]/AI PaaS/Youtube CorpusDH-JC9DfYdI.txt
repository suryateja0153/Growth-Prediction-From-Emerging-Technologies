 [Music] my name is Kurt I'm a product manager at Google I work on the healthcare and life science team and I focus on cloud healthcare API we're very lucky today to be joined by David from Stratis medicine CEO he'll be speaking in a little bit and let me also say actually before we jump in I must freely admit we were just talking about this when David and I saw that we were at the 5:00 p.m. slot and not in the main venue we were more than a little concerned that no one would show up so I just want to thank everyone from the bottom my heart for now forcing us to stand up here and talk to an empty room so thank you in today's session we're gonna be talking about three things in the first bit I'm gonna focus on the role that cloud healthcare API has in healthcare infrastructure to approach again server list service machine learning I'll then talk about a general solution architecture that we use and we take with customers who are looking to implement a similar solution and then I'm gonna hand over to David who's going to talk about with a heavy dose of pragmatism and insight from the field what this looks like in a real deployment so let's get started I think it's useful in healthcare to help contextualize conversations about caring for patients working with providers in the context of in this case a nurse so this is Christy she's formerly an ICU nurse currently a nurse practitioner and those of you in the front row here may notice that Christy bears a striking resemblance to me it's a keen observation christy is in fact my little sister Oh now the reason I mentioned that is Christy and I had this little game we play of one-upsmanship at family gatherings and such where she tells me all the things that she's seen in clinical settings and I try and counter with all the things I've seen in healthcare IT and needless to say you know Christy again I see you rolling inside and out of the Edie Christy has seen things and Christy usually wins but I asked her you know hey I'm gonna be giving this talk I'm gonna be talking about data-driven problems can you give me some examples where you think there ought to be more data in the EHR your medical record system and you just can't quite find it at the moment you need to make a decision and she looked at me and sort of said well you know side we're to even want to start and I mean when all of us in this room are familiar with healthcare IT and clinical decision support cognitive assistants you quickly realize there is an abundance of opportunity here there are thousands of problems that need to be solved and let me give you a couple examples in this case assume there's a small inpatient clinic relatively rural doesn't have a lot of specialists or to the extent it does they only visit once a week this is problematic because if a patient is admitted gets their usual labs and it shows an elevated troponin troponin is an early indicator for cardiac arrest the question is that doesn't always happen so should you refer this patient from this rural care facility to a larger care facility that has cardiac specialists I don't know I'll give you another example assume a patient patient just suffered from a cerebral vascular accident it is stroke so laying in the bed just recovering one physician says oh we need to keep the blood pressure up we need to basically soak the brain and blood oxygen rich blood help it recover now the physician says no no no look at the CT scan look at that see that bulge by the heart if we keep the blood pressure up we're gonna shear the aorta what do you do what should the blood pressure be right these are just two examples of the myriad of problems that exist in clinical decision making and the question is is there data or is there a process to sift through the data and arrive at a support and a solution that can augment and extend the clinician and I want to touch on a paper this was a paper actually presented when I introduced healthcare epi last year at next I mentioned this paper because of the time that had just come out this is a paper for my blue lay eye team and it shows among other things that first of all you can map a bunch of data to fire welcome back to that in a second but secondarily it shows that having mapped your data to fire you could make highly sensitive and specific predictions for adverse clinical outcomes so again I mentioned that last year since last year we've been working with customers and partners to start to bring these tools into clinical workflow one of those examples I've highlighted here this one's public so again not disclosing anything it's already public but this is with our partnership we worked with Emory institution in the United States where they had a an issue with blood borne infection in their ICU so sepsis now there are a lot of algorithms out there for addressing sepsis but they had a particular set of conditions that meant they needed to build a model this team saw the work and saw tensorflow and said hey could I build a pipeline like that could I ingest my labs and vitals could I deploy a solution and in fact they did and not only did they do it they then want a grant from the HHS Health and Human Services to expand the program hisses grapes it shows that this pipeline is actually feasible in real clinical settings so with that in mind right so I've set up the problem I'm not described why are we even approaching this problem why are we building what we're building and I think this is where I want to shift into talking about cloud healthcare API and I think I think a lot of people who approach data science and especially data science in medicine you go into it this mindset that there are these three phases right these are common to any data science project discovery model training model deployment the question is for people who are newer are these equal in time most people who are again new to the field say oh yeah sure you know I'll just do some discovery I'll train a model to play it these are not equal in time in fact most of us in this room who have lived this experience know that we spend an enormous amount of time and energy in that first spot in discovery training turns out to be the easy part training to be turns out to be the part that goes fast and then there's deployment yeah there's a lot of issues around patient safety and security of the data but the end of the day it still pales in comparison to the amount of time we're spending in discovery put this problem with slightly different way on the one hand you've got all these clinical systems that institutions have been investing in for the last 10 20 30 years you've got them over here they're largely on-prem over here you've got this thing called cloud and cloud is fabulous College where we got 10 to flow at scale cause we've got chips designed to run tensorflow clouds where we have bigquery running but there's this enormous gulf between these two worlds because you can't just rip these systems out and move in the cloud so the question is how do we fill that gap and that's the role of the healthcare API the whole purpose of the healthcare API is to help bridge these existing systems with the capabilities of Google cloud and in particular and I like to emphasize this point because it's often lost in these discussions at the end of the day we see an important step in deploying healthcare API is that we need to necessarily speak and support the formats and protocols that are already native to the industry data that these institutions already have they're already generating because if we can speak these formats and protocols we can bring this data into Google and into cloud specifically and again this touches on this our particular topic today in our specific use case we want to apply a variety of analytics machine learning services to this data which again the role of healthcare p.i is to facilitate that integration now briefly I just want to touch on this to answer the question I get a lot of question that well where does this fit in the Google product roadmap in terms of platform the service is a gmail right fully managed or fully managed by Google SAS is that raw infrastructure of rock cloud VMs on so forth on GC on compute engine and it really sits in the middle it shouldn't really matter from your perspective if you're throwing one resource in 100 million resources in one DICOM image 100 million cycle images it should scale transparently for you that's our goal now from an API perspective we call it cloud healthcare API because it's very much an API surface you have this hierarchy of your project right you control the project within your project you specify location this is critical in healthcare because that's informs the API for where to store data for the rest of that path you have you create a data set a data set is a grouping of multiple modalities of data so you can have imaging data over here and fire data here and fee to date over here for clinical messaging and finally you have your stores right stores implement the modality-specific API and I emphasize the point that it's an API because if you're building an application you're just addressing an API and a very familiar way this is no different than any other API you interact with the key difference is that under the hood it's scaling and implementing these specific modality datatypes for healthcare and of course I would be remiss I don't want to spend too much belabor the point we have entire sessions on this topic but I do want to emphasize this whole architecture is designed to support the storage of protected health information so yes it is covered by our business associates agreement ok now I love this slide and every time I show this slide oh so boring Kurt why do you put this in your decks and I like it a lot because what it shows in four commands let me explain what's going on in this example here but in these four examples it shows oh we created the dataset using G cloud or CLI we've created a V to store for clinical messages we've created a DICOM store for clinical imagery and we've created a fire store for fire data and we did this in four commands and it doesn't matter again you can send it one image one hundred one hundred thousand images we have I take the fact that it's boring as a compliment we have made the ability to create this infrastructure so boring that this is the trivial part of the process and that's great the more and more we can make this type of work mundane the more time developer teams that we work with can span in the application space helping patients helping providers and that's where the value is going to come from and briefly I just want to cover some of the configuration options these end up to be useful and I'll cut you'll see why as they start to talk about the solution architecture but in this particular case when we're talking about an hl7 B to store there is one configuration option for segment term is I don't spend too much time on that but for those who have deployed v2 in the field know that you can configure that in a V to store the piece I want to mention explicitly here is that every single store in called health KPI can be associated with a pub subtopic this becomes important I'll get to this in a minute DICOM same idea how it can be a search to the pub sub topic every time a new image is sent to a DICOM store it generates a notification saying hey I got a new image same thing with fire you look at a fire store fire we have a number of parameters that we support for various fires very complex spec so I don't want to dwell in the spec itself but the things when you're looking at the fire specification know that the different parameters there in the spec can be tuned via configuration on the store again the important point that I want emphasized for art's discussion discussion today is the fact that you can associated pub sub topic with the store so what do I mean by that again you have your healthcare store it stores healthcare specific data when data is written when data changes generates a notification - cloud pub/sub that triggers a series of notifications at any number of applications that you build can be subscribed to and from there this is an example of what that message looks like so it's just like any other pub/sub message namely there's a data element that's basics for encoded like all pub/sub messages on google cloud that contains the half of the resource that changed so any subscribed application can grab the data that changed this will become an essential point in a second now I emphasized earlier that it's just an API so to get data in and out you simply call the API in this particular case we have a sample resource we're gonna pull it out for some application that's great now this is where things get interesting I said at the beginning one of the part one of the key roles of called healthcare API is the ability to export data and project it from its native data type to bigquery so you can run sequel based analysis against it that again is just an API method you you invoke on the store say hey take this fire data projected to bigquery take this daikon data project the metadata to bigquery and that under the hood will trigger a job that we project that to your own bigquery store for the case of fire there's a Community Supported specification for an analytical schema we implement this specification so to the extent possible this is an important point we are implementing industry standard specifications on Google cloud technology so again the projection is based on this schema and to trigger it you just said an additional parameter highlighted in blue on this slide to say I want my projection to follow the analytical schema based on the community spec last point I want to make about interaction with the firestore is that all of these fire stores rather can export and import directly to bigquery I should actually should mention this point earlier they can export and port bigquery and be connected to your business intelligence tools so you've got R you've got tableau you've got all these services that you're used to running they can interact directly with that projected data so you don't have to teach your data science team to use a different tool to connect to the projected data there's the point is trying to make when you're ready to export your data for the purpose of training a model we have a convenient API call to let you say hey store take all my data and drop it in a bigquery and you can see again through a single API call that that method will take your data whether it's the DICOM data or v2 data or fire data export it to cloud storage in this particular case so you can train the model or in case you wanted to move it to back on Prem or take it from on Prem and shove it in the cloud healthcare EPI all of this is possible as an API method now I last point I wanna make before we dive into a solution architecture is to talk about the fact that it's in beta finally yay over the last year one point of emphasis has been performance one of the point of emphasis has been conformance so those are the two things that our team is largely focused on and again I mention this before it's in scope for the BAA and importantly for term service okay so I've set the stage for why cloud healthcare API I want to spend a little bit of time now talking through the architecture for how we help these teams deploy a solution which sets the pattern that you'll see repeated when David speaks about his particular implementation so before I dive into that there is one essential point I like to make because this is a recurring theme as we talk about these implementations and that is namely the focus on fire so fire serves two important roles in our approach and that is namely it's a very extensible data model which is incredibly convenient the second is that it's a graph so any graph based querying and technology use can work against fire data and then third is actually the fact that it's also an API specification so you can use it as a transactional target in addition to being the target for labeling and training data also there's a reason this actually came out last week CMS announced that they're transitioning some of their services over the next few years not immediately but the initial wave the demo was released last week so you can actually extract claims data in fire bulk format for those of you who are paying attention to the regulatory environment during hims you probably also notice that both CMS and ONC released proposed rules with a very heavy emphasis on exposing api's the ONC rule 700 pages mentioned fire three hot little over three hundred times right so there's a clear both data science reason to be of taking this approach and a clear push from the regulatory environment to say maybe this is something worth investigating further for the purpose of our conversation today I'm going to talk about a very specific implementation and that is something called c.d.s hooks in the clinical space CVS hook is one implementation of clinical decision support there of course many best practice alerts flowsheet integrations Clin kb integrations the point is there are a lot of disparate integrations that are possible I'm going to focus on this pattern because I want set up the discussion for what you'll see when David presents his work but a CD s hook is fundamentally no more than a web hook so it triggers an alert a hospital an event occurs inside a care facility that triggers an alert just like any other web hook you're used to now in the case of a CD s hook there are a number of events that trigger that a patient view is the most commonly implemented to the extent CD s hooks are implemented but there's others when an order was reviewed a medication prescribed the order select and down which I've put in grey those are technically in the spec we haven't seen them widely implemented but I mentioned them in the interest of thoroughness now the CVS hooks service is in turn expected to respond with a payload that's designed to trigger a native rendering in the source EHR so in the case of Cerner there's a specific UI element that gets rendered when this Web hook is answered this gives you at least for the purpose of our discussion a target when we're talking about an architecture for delivering clinical decision support cognitive assistance into workflow the other convenient property about it which I like for discussion and developer purposes is that there's an open sandbox so you could leave the session go test it on the stand box that's developed by Cerner's the CERN is one the larger EHR vendors in the world they have a to process to just test these locally again convenient when you're doing development activities so let's return to the three phases I setup at the beginning you have discovery you have training and you have deployment in the case of discovery we actually spent a lot of time on this already right I talked about the ability to project data from the healthcare EPI to bigquery so you're ingesting these v2 messages you're ingesting as fire data the DICOM data and you want to project it for the purpose of discovering what are some abnormalities in my data connector our process to do some sifting around or do I have some biases expressed because of my collection procedures right all that work is enabled through your projection in the bigquery for training this is another area which I'll point you to some open source work that our friends at Google AI have performed there's a whole step by step process we've identified which is again we've tested a number of scenarios but the idea is how do we take fire data how do we label it annotate it and then how do we train a model this has a step-by-step guide on how you would do that after exporting the data and the basic flow which I'll cover for the purpose of when you're deploying it on cloud did you take your fire data you export it to GCS you then label it however you see fit using the again the tools that I just showed in last slide and you create a model which you can upload to ml Engine now the nice thing about the the work that the AI team showed on the last slide was that that also works locally so if you're looking to develop this kind of tooling you can test it directly on your machine now in terms of deployment we're largely talking about ok how do we implement this c.d.s look how we're going to build a solution that pieces together various parts of cloud in a way that answers that Web book request so in this particular case I've used the kubernetes engine logo for this EHR relay and that's because you're going to be ingesting clinical data somehow and I say somehow because for the purpose of our example what we want is a stream of fire observations we all know there are very few EHR systems streaming fire observations today which are realistically getting is an hl7 v2 message streaming labs and vitals in this case I used to know are you for an example so the question is ok great now how do I turn that into a fire observation there is some work on our side for ingesting hl7 B to message a Mis and open source project that speaks mllp so minimum lower layer protocol is the protocol necessary to ingest v2 messages we have an open source container that you can run on kubernetes engine that's why use the Careers engine logo earlier to ingest these clinical messages now we also work with partners so you may have interface engines in your institution today that are capable of directly integrating with called health KPI one of them this is a team we work with built a integration for mirth or next-gen to ingest v2 messages directly from an existing mirth installation so that a system that already invested in mirth next slash next-gen could easily connect their data to cloud so somehow someway using either of those services or when your existing interface engines your streaming v2 messages to cloud health KPI and I mentioned earlier that is connected to pub sub this is the essential feature because now you're getting it you have a mechanism to know when new data was written to your v2 store and because you haven't mechanism to know that you can ultimately trigger data flow again either directly or this is an area of investment for our team or how can we make this more of a managed service but that Maps that B to data grabs the message and turns it into a fire observation right so this is the target somehow you have this little engine that's mapping v2 messages to fire on a real-time basis so we go back to the example where let's assume for the sake of argument and for the sake of my diagram that were streaming observations from my source which may be this data flow process under the hood now we'll go back to the notification idea so now you've got these out fire observations being rent to the store again you can lean on the fact that pub/sub is sending out notifications about this new data so your cloud function can directly attach and why it's called function necessary here well because somehow you need to invoke the model with the right fire data so because the pub/sub notification contained information about what fire resource was mutated it can grab the patient everything bundle from health care epi which effectively is a representation everything about that data and send it off to ml engine for prediction once it receives that prediction it can then generate a risk assessment and write the risk assessment back to the fire store so now you've got this pipeline of you're ingesting data the model that you trained in phase 2 is running on ml engine looking at the patient everything bundle which you again trained using a tensor flow sequence example and it's generating a prediction which you're converting into a risk assessment so that whole process is running asynchronously right that's as new data streaming in that whole that whole engine is churning but now you've got the CD s-hook component because this the hospital says oh I have a new patient or of clinician looks at a patient that triggers a notification to a cloud function because for those promote called functions one of the triggers you can use is an HTTP invocation so now it's oh I got a web book request from this hospital I know how to handle this let me reach out to the call of healthcare API grab the risk assessment and format my payload nice skipped over the fact that CTS hook has an authentication authorization step but that's in there that would be in your business logic for the called function but what you're generating here is now you've got that asynchronous loop running generating risk assessments and a synchronous loop to inspect the data and generate a response for the care facility in the immediate workflow again I want to point of emphasis we have an open source project that shows how that pipeline is built this project actually also has a UI though I want to caution people what this demo does is it says I'm going to go to the country X do I need an immunization that is not a great problem for machine learning freely admit that and the reason is because if you're gonna go to country X you would simply consult your relevant Authority and get your appropriate immunizations you don't really want to predict that but the underlying pipeline is the same and so we wanted to pick a topic where it was clear that the pipeline is feasible to show you how the pieces fit together even if the problem space should you should not get immunization is relatively trivial that pipeline is consistent across all the implementations of the solution that we've seen also for imaging data I have focused on fire and clinical data for the purpose of this discussion but there's a collab also available on github for running this exact same flow except using imaging data so again whether you're using streaming clinical data or data from an EHR you have a consistent system and architecture to apply that's able to generate these sensitive and specific predictions and with that I went ahead over David who's going to talk about this in practice hi my name is David verdict and I'm the CEO of Stratis medicine at Stratis medicine we partner with health systems to help them accelerate their innovation programs we leverage technology and data science to really make impact in healthcare and I'm going to talk to you today about a case study where we've actually applied these healthcare API tools so like I mentioned we applied technology and data science to healthcare and we've worked with both internal innovation teams and external vendors and we're leveraging medical records and genomics images and machine learning methods to really impact clinical care clinical decision support health system administration and what we've seen frequently is that innovators in this space don't have access to the tools that would really accelerate their workflows you know it's oftentimes relative to data security data use and privacy which are all very important things so how can we help healthcare IT move to the cloud right right now they're in this world of virtual machines they've built a nice security workflow it works for them the boundaries are well diffract defined but they can't incrementally move towards infrastructure as a service or platform as a service where there's a lot of value because ad-hoc use is risky you're not going to turn your developers loose with an API key and in fact when you do you get things like the data breach it u-dub medicine this spring which I was involved in well my name was involved in where somebody just put a bunch of about a millions patients million patients worth of data into an unsecured cloud bucket and Google indexed it so you know it's expensive for IT to productize what the public clouds have created it's hard for them to support new trends like serverless or kubernetes and so our viewpoint is that you really want health care to jump from the VM you know gen to computing world to a software-as-a-service product that wraps up the public cloud offerings into kind of an opinionated workflow that's specifically tailored to health club health care and healthcare application deployment so now what we can help health systems do is create a secure workflow for application development and deployment enforce security best practices do active breach prevention and because a lot of health systems don't want to hire data scientists and software engineers in-house we can provide professional services for that so if we stand on the shoulders of the public clouds with their infrastructure and platform as a service and we narrow that down to a specific SAS offering then we can focus on applications that provide real clinical value and we want the team's building those to be able to iterate quickly and not do it with specialized skills so let's talk more about how we can do innovation with these modern tools and specifically clinical decision support so for the last five years we have been a strategic partner of the Sun yet sand Cancer Center in Taiwan they're one of the premier cancer centers in Asia they have a 325 bed hospital that provides both inpatient and outpatient oncology care and what's really interesting is they have been operating profitable bundled payment for over 18 years these are five-year long episodes of care and in doing that you know their outcomes are as good or better than top US institutions in doing that they've seen what the keys to success are in the value-based care world they need to leverage data appropriately have really good care management and effective use of their resources both in terms of what treatments they use and the use of their staff so the case study that we're going to talk about is distant metastasis in breast cancer so distant metastasis is when a breast cancer spreads to other organs and the typical progression goes like this you have the initial adjuvant treatment so that is now your first-line treatment and then in some patients unfortunately the cancer spreads to other organs there's a regimented follow-up so every three months initially then six months but unfortunately there is this period there's this kind of problem here where the cancer spreads but the patient might come into the clinic before a regularly scheduled follow-up with some symptoms and are those symptoms related to a recurrence or they just pneumonia or some other comorbidity so it's up to the physicians discretion to say well should I use advanced imaging right that's expensive and invasive imaging maybe a chest x-ray and determine is this related to their breast cancer that they had four years ago or is this just something else so how do we help physicians make that decision and how do we standardize that across the different medical oncologists in the group so the goal here is to create kind of the earliest identification of recurrence and follow-up and we'll create a model that stratifies the risk of recurrence for each individual patient with the result being that we have effective and efficient use of advanced imaging and we can reduce by three to six months the time from the actual cancer recurrence to start meant of recurrence treatment so generically the process looks like this we have a die comstor for images we leverage fire for modeling the medical record and we use registries for curated parts of the medical record we take these three data sources feed them into a machine learning framework and bring actionable insight to the clinician let's focus a little bit on fire and registries so like I said we're gonna model the entire medical record in fire here and then we'll use the registries that are extracted from that medical record to be a you know a specific clinical area so in breast cancer this would be what are the patient demographics what is the pathologic staging for that tumor what was the initial adjuvant treatment and then what is their stage of recurrence right have they had a local regional recurrence have they had a distant metastasis these registries are super valuable to the Sun yat-sen they use them for retrospective analysis and things like SAS and tableau they use them for training models and they use them for billing because when you're in a value-based care reimbursement program you need to use kind of these deep pieces in the medical record in your billing so what is fire probably most people are familiar with it but if you're not it is an hl7 specification that both models medical records data and also provides an API it's a directed graph of resources and these resources are things like the patient practitioner diagnostic report medication prescription and you know fire means a lot of things to different people it says young inspect less than a decade and it was originally built for health information exchange so some people view it as health system to health system a lot of people with apples entrance to the market view it as a direct-to-consumer model but I think that has real value for data science and so because it can really help us standardize our data engineering workflows it's a well-documented specification where canonical data is stored in a really consistent location throughout the spec so for example here's a procedure resource and the procedure resource has subject key which is who the procedure was performed on it also has a key for the code which is the identification of the procedure and we would like it to be a snow med CT ontology that describes that and there's just one of them and has codable concept the nice thing about that is that we don't need a data analyst to work hand in hand with our programmers to understand the data to build applications on top of the medical record one organization that we've worked with had a very large their medical record had a very large oracle schema all the tables were eight characters long and all caps of course and you know the knowledge built into that schema to get the data out we needed to create clinical applications was was very very deep right you don't want that to impede every one of your developers experience in accessing data to build applications another great use of fire is to normalize variability from data sources so whether you're combining legacy data sources or you're combining the data warehouse with a real-time store you can use fire and its model to just normalize the data input and have a consistent data engineering workflow like I mentioned creating structured registries from fire is really valuable of course in the healthcare API is you can dump directly to bigquery but as most people working in the field know a lot of value is in unstructured data in medical records and so working with health systems we found that there's a lot of value in writing very small targeted programs that leverage natural language processing or parsing multiple ways where you can just write these ad hoc programs to create highly structured and really valuable datasets so now that I've told you why fire is so great let me tell you why scalable fire implementation is difficult the specification is really a deeply nested data structure and when you try and model this relationally you end up with hundreds of tables and thousands of foreign keys and if you try and lay an ORM on top of this you can quickly overrun the databases capacity to do select queries because of a lot of that deep backlinking the spec also you know these references between this graph of medical record models is essentially a generic foreign key and so you have a ton of generic foreign keys which again is really hard to deal with at a scalable implementation level so we were actually talking about building a fire on top of spanner when I came across the Google healthcare KPIs and what's great about this is that they already did it for us it's a standards compliant hl7 implementation and so essentially we can just drop this in into any of our previous fire implementations we don't have to take something off the shelf manage the index size manage the database size it's fully managed and high scalability and this is just an example of how I think that you know platform-as-a-service for healthcare it can really accelerate it right we're not doing the nuts and bolts of building our infrastructure we step back and look at imaging in healthcare I think in imaging is interesting because it's one you know it genomics are one of the few high dimensional data sources in healthcare so that's where we can really extract insight with machine learning so of course we can label our images and build a classifier in this case it would be in mammography and we can take tensorflow off the shelf we can train a model and we can spin up a tensor flow server or use ml engine to serve our model and you know off we go but the problem is that data sets in healthcare are small there's not a lot of health information exchange there are no national aggregations that are widely accessible and so transfer learning is really key in healthcare if you're not familiar what transfer learning is especially in the context of images so where you kind of stand on the shoulders of a previous convolutional neural network and then you tack your model onto the end and so what's encoded over there say like Inception v3 from image net is you know what are the features of an image generically can we take millions or billions of images and and do you know line detection feature detection and then we'll just tack our model onto the end of it again we'll stick our huge team of data scientists inside our health system onto that model and then deploy it but as you guys probably guess there's not a huge team of data scientists at every health institution so how do we do this quickly how do we iterate and leverage machine learning on this high dimensional data for us we found a lot of value in auto ml vision so autumn ml vision automates that whole process for you you don't need special you know specialized tensor flow of knowledge to be able to train a model on your DICOM images the DICOM api also does a nice thing which is transcode your DICOM images into JPEGs which you need to do for training and then once it trains and you find the right level of sensitivity and specificity it automatically deploys your model for you so now that we have our tools together we're gonna use the DICOM api for image use the fire API for a fire store we use cloud sequel for registries you could also use bigquery and automail vision let's talk about how we're actually building this in an architectural level so this is work that's in progress at the Sun yat-sen now and if you've ever built an ETL on healthcare data you know that it's a real challenge you want it to be very robust you don't want to have missing data and so how you do that can be a little bit tricky our opinion is that you want to write once and succeed you don't want to have a chatty API you really want the rate limiter to be your internal system and in this diagram I'm going to use blue arrows to denote a pub/sub asynchronous message queue just for clarity so let's say we write a fire resource into cloud storage it kicks off a message into a pub sub and the nice thing about asynchronous message queues is that you'll get durable delivery right it's at least once delivery but at least once means that it's maybe twice or maybe three times so we'll leverage cloud dataflow which will give us exactly once delivery and that's where we'll write our deduplication will leverage the fire specs use of identifiers to query the fire API for example for this patient say hey do you have this patient with this MRN if so create it or if not create it if so update it and similar for the DICOM store from there when we create that resource in the fire store that kicks off an asynchronous message that cloud function consumes and updates the registry and I can't emphasize enough how important having a living registry is that's constantly being updated that is a huge value for a health system both in retrospective and analytics and also for machine learning so now that we have all of our medical record and our images in we can use the firestore and the registries to create a training test and validation data set hand that bulk over to auto ml vision and have it train a model so now we're all ready to go a new let's say a new fire resource comes in through the API or in this case a new image lands in the DICOM store again we get an asynchronous message that is consumed by a cloud function that will actually run our model and deliver it so we use the cloud function to look into the registry and select appropriate models for this patient and for the image type we then run that model in Auto ml vision and we use an asynchronous queue to deliver it back to the health systems EHR that surfaced clinically in the workflow and you know what we have built here is an entirely serverless system that is easy for us to iterate and create new clinical decision support products on we just have to essentially do that upfront investment in ETL so what we found that the key to successful innovation is that you need to have a SAS cloud work row right you can't do everything yourself and if you if you can if you can make a secure workflow that allows you to access these cloud tools all of a sudden now number two you can move fast with these platform as-a-service things that I've been talking about you're not reinventing the wheel you're not running in a database you're not taking a fire store off the shelf and so now that you can quickly create healthcare specific applications you're able to leverage your expertise in-house right the hard work is to empower individuals in your organization to solve clinical problems and without specialized skills to really improve the patient experience and health system operations [Music] 