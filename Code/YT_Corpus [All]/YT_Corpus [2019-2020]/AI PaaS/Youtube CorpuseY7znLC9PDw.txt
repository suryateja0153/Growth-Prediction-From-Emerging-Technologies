 [Music] so much for coming to the AI huddle if you don't know what AI huddle is it's Google clouds open collaborative and developer first forum with Google AI expertise and our whole goal is to connect Google AI expertise with developers and customers who are running their AI workloads preferably on on GCP and the goal is to have conversations with you around AI what problems you're trying to solve and basically learn more from your experiences and and have a dialogue have an exchange of ideas and thoughts so with that I will actually let me do a quick intro of speaker for tonight okay I got I got to pitch okay cool um so speaker for tonight is Johan who's an engineer in Google Flowery I Johan let me take out Johan is a developer program engineer in cloudy eyes since 2017 and previously he worked as a machine learning engineer and full-stack engineered multiple startups so with that I will hand over to you han who's gonna do a deep dive and tensorflow topic thank you and welcome I would like to talk to you a little bit today about how to use tensor flow in Tucker it's not a new topic and I suspect many of you might already be doing this for your for your work flow if that's the case there might still be one or two things that's interesting for you to hear about today and I mean it's really clear that I'm really talking about how to how to run machine learning workflows more [Music] efficiently in a sense slightly different from from the user when people talking about your machine any workflow reduced efficiency and we talking about your own development cycles I'll share a few personal stories in my in my previous learning work and and how that impacted me and how I wish I had to use darker with a better habit and I'll probably save me a whole lot of time just in case maybe some of you have not used Tucker before me you heard about it but we haven't even gotten into it just yet Tucker's read about dependencies and in one word just dependencies and that's that's a way for you to declare and pin dependencies in a really really nice way how does that have to do with tensor flow or or machine learning workload in general I would like to to sell you this this vision when I think about a project that's been down there are just three components I usually think about one is the developer that's you who is usually writing a code that's the code in in one of the corner of triangle when I couldn't just run in a vacuum but rather he runs on some kind of infrastructure there's some environment that supports the code that's that can be run in that environment Internet is being set up by you again the developer and so that's triangle here now if you are a single developer on one person team as yourself working on a single project a single code base in an always consistent environment say your laptop then probably you don't need docker to be completely honest with you but any moment if any of these three corners I guess of the triangle changes docker might help you in some ways that's that's sort of the pitch I have tonight so you can imagine what how can the developer cannot change well you might need to share your project with someone else on the team if you're working on multiple person he maybe need hand it off to your engineer if you are in the role of a data scientist all the code you might be working on multiple project at the same time you might be maintaining and in fact improving more than one machine running based services in your production environment at the same time environment you might be doing experimentation on your laptop in in notebooks or some other kind of configuration and you do want to move it to the cloud somewhere for serving or for for larger scale training and so that is when you have changed the environment angle of a triangle and so whenever any of these changes you might be in a situation of dependency inconsistency and so that's when doctor might help you I do want to point out that Parker is an only solution for any of these problems right there are other ways to deal with this for example normally for for users who do need to work on for developers we need to work on multiple projects in the same environment be local and your local machine you see people would go with virtual environment same height down and most programming languages nowadays have some form of dependency management stools for you so so that is sort of handled and environment as well there you know many different ways to to migrate from one environment to another without all kind of headache that I hope to take a little bit about tonight but the awkward happens to be a sufficiently flexible and powerful tool that handles all these in a consistent way and so the the kind of good habit I'm hoping to promote in this setting is that with a fairly small amount of overhead when you set up a fracturing and when you work on a project you'll be able to remove some of this these obstacles later on if you first see that they'll be a point in time when I need heads of Penny's project off to someone else or even just start collaborating with someone else or if you see that at some point you want to you know smoothly switch between different projects or move to different environments you might consider start out with sort of good habit and this is really general about software different software development projects and not specific about machine learning workload but I want to say that a lot of times machine learning workloads have have this hidden dependencies that people don't talk enough about in my opinion so in my own example I had a previous team I had worked on machine learning project back to back back then we used psyche learn and that he provided my model so great and then when I move on to the next project I thought I will upgrading my cycle and version and to use a new feature which which I need it for for next project No so saves me a lot of time only when I deployed that I realized I was breaking the previous project because the previous project could not be loaded into a newer version of psyche learned and I don't know if it happened to you for tensorflow I had the exact same issue as well I had old checkpoint trained with older version tensorflow it just does not load for me anymore with a newer version and so you can imagine if you have scientists working in this situation you might have deployed the whole model in some way already and the requirement has changed or the needs has arisin so that you need to improve that model you would like to load the old checkpoints whenever we do all the experiments it doesn't work anymore because you've uploaded so you have updated your tensorflow version so then that can be I can be really subtle because these things are really hard to pin down there are times people go with keep requirement files and say this is list of packages that that's needed and that's that's what we did before a previous team only when we realize that we're using a single requirement file for the a single monophonic monolithic code base and that's causing prom for ares everyone who's using the same code base because i'll be saying hey guys I need update this particular cycle inversion to her a fancier version because I needed and oppress everyone else to work because they can't use it and that particular case happened because cyclin was making very significant change they move a bunch of code from pipe down to into into Java back-end and so that caused a whole of issues because some interface suddenly disappeared we do hope that similar things would not happen to you but if he does you'll be allowed a long headache and the end result was something like I had to spend the whole month trying to to figure out what's the update path for the whole code base so that no project will break I can still use a new feature I want all models who can still be deployed to production a lot of problem and so these are the benefits of docker exactly corresponding to those three color changes I was talking about for consistency if you want to run code in the exact same environment talker helps to do that for isolation if you want to work between different projects with very quick switch yes virtual environment is super fast too in terms of switching projects Parker can do that as well collaboration is really the bigger piece here impact collaboration I mean a little bit more than code level collaboration a lot of times you do need a hand off a whole deployable project a bundle of some sort to to your engineer and then you deploy that to somewhere on your infrastructure and doctor makes this much much more seamless because the environment is first of all declared in understandable way that's very important and making making the results much more reproducible so I want to just for for the benefit of those who might not have been using docker for a while I have a little bit of mini cheat sheet here she showed me some comments I'll be using today for I think you know like in a few mins out I'll switch into more like a live demo mode and show you one particular workflow so illustrating all these points I was making here so the piece of darker there's these two things that are often confused there's a darker image and docker container that means you should think of that as a software bundle that comes with it the dependencies already installed in there and the container is a running instance of that so the same image could be run multiple times so you could have one image being run five times and get five different containers Oh run in the same image so think of them as five processes running the same code and so these are some basic commands you can pull image that people have published for ISM here I'm showing a standard weight was identify attack image so you see our sense for Google Cloud the registry it holds darker images for you so you can pull it if you have built and published docker images and you can talk her space image it's basically so this all the images you have pulled from from repositories are mi spent stands for remove image that's really handy there'll be a time when you have way too many diaper images on your laptop removal and to run them you basic use the same syntax also by the way here whenever I mentioned repository color and tag that's the same thing as up there so that whole things you see are that I owe everything up to : is a entire language called a repository and after colon is a tag and so you might be pulling different things like different versions and if you have a campaign and that's running you can kill it again talker a campaign that is thinking you think about that running process that runs a specific piece of binary perhaps that's it could ask you if you think of it that way question so far good okay a little bit more and dr. fire is really the declaration of how to build image you are going to go through a really baby example today and then this next two things are already interesting I'll make some more points in in a minute like a mount value so a darker campaign there is a process that's running on a host machine maybe on my laptop so so what happens is that on the base here that my laptop has its own operating system and on top time could be running multiple darker containers at the same time so those are called docker containers processes running on top of the host and so this will communicate the hosts in the container themselves and these are two basic waste waste by mounting a volume even the share part of your hosts disk with a process which is extremely important I'll get one in a minute and another way is to forward the port so maybe your campaign is running a service and by exposing some kind of port on a container you do want the host to be able to access that from outside of the container and that's how you do it you will just forward a host port number : a container port number so it's the same syntax host container ordering with all the other nice things I could say about about docker they are a few of you got jars and so one of the most important thing I cannot emphasize this enough I try it I have all capital bold interleague phone and in highlight everything into in a container will be gone again will be gone I've seen this happening to new Utica users and experienced users as well they've guys who got back her they feel so at home and they run okay so I've download stuff install things and you know drunk old they saved a model locally and then the exit process all that is gone so the interesting thing is that a type of container feels just like a shell right you can connect to it it's a process that's how environment feels like a new computer almost you can do all kind of things in it even has a file system in there except that file system is not persisted the moment you exit the container everything is gone well unless unless you were doing things carefully and connected the running container with a host in the first place that way everything you are writing into the disk in a container does get written to your host disk that way you not be gone and so that's really one of the biggest catches I have wasted hours to recover work just just because of this after I learned how to mount disks but I think about this at this downside of mounting disks because means your package has some additional extra dependencies there like so you were mounting a disk and a lot of things write code happily but all our code is no longer in the docker container so that means this docker container or image thing cannot be handed off is any more because now it has to carry this additional package of whatever code or data or model that I have created on the side so that's some some trade-off we need to think about normally I recommend doing things with some of the cloud storage until one if you can if you foresee that eventually it's going to be you'll need anyway start writing things to tooth cloud somewhere that way you and your team can easily share that another thing that is also gone is that you know you connect into your docker instance and then a process and you install things it will also be gone and then you come out run the code again somehow you don't work anymore is that complaining about some package is missing you said wait it's like an IDE install that before only you have exited the process and so so here the actors provides a really handy way for you to sort of remember to keep the know for that is through the mechanism adapter file so every time we are in a container install something make sure you record that in your doctor file that way you'll never be lost and everyone anyone can come and reproduce that some other guys will not really get into today is networking it's a little tricky it has to be the port forwarding business we mentioned before because docker containers are designed to be well it's a one of the goals is to be really secure so we don't really one containers to be talking to the outside world very easily for for good reasons and so sometimes people will have struggle with networking a little bit if you are in a situation when you do have to manage a large number if it just indicates the multiple containers that need to talk to each other or outside world and the hosts that becomes really complicated complicated graph you might consider using some frameworks that help you do that such as kubernetes that's some other articles out there too and credentials another issue as well on your laptop usually just happily connect to any kind of service services that you are authorized to use in the container it's usually running in a different identity right remember that's like a whole new computer I happen to be running on your host machine you might know nothing whatsoever about your host machines credentials or environment veil variables and things like that so they are issues there as well but what permit not get into that today okay in questions up to this point yes dr. compost is I don't know what do you think it is I am not used up before I heard of good and bad things about it that's why I say I don't know does any anyone know what Parker Campos do for you maybe you could look it up together there was a nun question or a comment there's something like that - yes thank you so you did pull from typically public repositories very much like github and in fact this is something called docker hub that has published docker images I need to download so you can download individual images from there so for instance that was just in the suite for tensorflow session earlier and then they mention darkus where and then you can they should go to Apple's TF hub repository and download the swift swift Tucker that way you can run things on your on your local machine that's right to do something like that yes yes yes so the question was that how do you do like incremental update of optical images yes when you peel the tapping image that sure has this constable layers usually usually you know that the attacker the software attacker itself handles that for you in saying that it caches right you're building an image which really means you start somewhere a starting point and then start adding dependencies to it was you your project needs and eventually adding your code that has to be executed on top of all that dependency you're not really downloading everything every single time what cannot see that in a bit so parts that have been downloaded once will be cached on your system somewhere and then next time when you build on top of that you'll not be redownload it again okay but that's so handled for you automatically yes for a for docker or oh inside of docker I think for a lot of times so I think yes if you are in your saying using software in the docker image does has license mmhmm yeah I think at least if you publish things done like a docker hub you can you have an opportunity to put in your license there so to specify to place by that okay so the kind of workflow I want to really quickly show today is that how would I go from experimenting locally and develop a simple machine learning model or workflow rather and you see I was trying to do things carefully on the local level or the even locally I don't just run on my host machine but rather I already do inside the docker container environment so the later I can literally lift up this whole docker in the move somewhere else exact same reason will be repeated just remember that to say my work in a volume and then I will try to try to show you how to build a really simple minimal docker image and if we have time I suspect we might run out when I show how to deploy this this container before this images container somewhere to do training okay let me exit here on the slide okay so my starting point is that I need start with something they start with a docker image that has some stuff that I already need so I'm just going to use the example I mentioned before so Google cloud already published some of this so I'm going to run a local container workflow I'm just going to take this because that already has notebook installed for me so I can run notebook experiments which I feel more comfortable and a bunch of stuff skip over and here's a key point here how do I know what kind of Parker images are out there that I can't download here's one place where I can actually get a list of one particular repository and see what kind of images this particular repository tree has included so I'm going to just run that and it lists a bunch of them right so I see some titled docker images I see some are images as the tense of images and compiled in different flavors and so those are packaged environments I can is it visible maybe they make me look bigger let's help okay so I can just doctor put one of those I'm going to pull the 2.0 CPU image today so I'm gonna copy this this identifier here and sometimes you can add : with the tag there if you know which text you are I'm just gonna pull the latest so in this case I'll be running dr. pol and exactly that identifier there and it's going to to use a default tag ladies thing says it's done well and let me be honest with you this normally takes a while because that image is such a six gigabyte in size so it takes some time for the first time you run it but subsequent runs will not require to redownload that again so you know this saves a copy of that and as I can also run one who commander was Tucker images and you'll see a bunch because I have been playing with this before but at least we do see that it shows up somewhere right here this one here so that's one that I already pulled three weeks ago and it's in size is a scary six gigabytes and when I look at this list of images I was kind of worried right well I have a whole bunch of images each one of them reads just 60 gigabytes so I guess something like a 6 x 20 of a bunch but that's not the case although so in the visual images at 60 gigabytes but they do share the cache layer and so if they all come from the same base image that base image is no longer repeated in different images all right so our handles are for you really nicely ok so I have this what do I do I'm gonna follow instruction here and say here's the way to run it and actually run one of image here here on documentation shows 113 am I gonna run I'm gonna run 120 2.0 and I'm going to mount the volume as well so let me copy this and then modify it so I don't want to run this this command but instead of 113 I'm going to run to oh and I want to I do one to forward the port if you see the adat port that's the default port for Jupiter lap and I want the amount of all you i'm not going to mount this path local i'm going to mount it as i'm to the mount I'm on my current directory and the man way around me I am okay I'm gonna run it and by the way you also see Oh image not found did I maybe not have the right gonna change the name on me so I'm gonna run now again copying this to make sure I don't maybe type it on there okay now I just be some hash which is of no use and somehow I'm returned back to my share I can do things so now here again what happens that because we had a dash D in there is in a detached mode so or taemin so the image will try to run and if ax is accessed by this case it that's not exit because the way image this image is built it just turns on the server that sound 8080 because the port forwarding I now can access that's that service that the two-bit elapsed or server that's a to run inside the container through my host which means my browser can just access that and this mountain here means it connects whatever directory I am with the home directory inside attacker so let's do some experiments and I have a few a couple files here I'm gonna make a fake file I'm making a new file here called delete me in here and I'm going to connect to my Jupiter lab localhost 8080 and there it is and on the left hand side I see the exact wrist exact same three file story showing up right here including the one I just made delete me and so to just prove to you that these are really connected so just to make sure that's also good happy to have to make sure that don't we lose code here and we'll come outside here remember this shell window here in terminal window here is a host environment I'm not in Parker there I'm not in that container a right hand side the Jupiter lab I'm instead of shocker so it's another machine that's running in here I'm gonna I'm gonna remove delete me because it says that so I'm gonna come back here and then refresh and it's gone again okay and so this really validates that these two volumes are connected one unhoused which I know it's persisted and the other one on the talker container that would have been gone if I were to AXI the process but because we've mounted volume that's okay and right now I'm gonna simulate develop maybe developing an experimentation for a model I'm going to just clone I'm going to go in home first so I see the same file so this is the home in the inside of docker container now there's another machine that shared the disks and I'm going to just clone this repo I this stencil for examples that contains a lot of community goodies in here so I'm gonna get from that okay should've done a shadow clone but it's okay okay we got it so now I cannot think again if you still don't believe me I'll come outside here oh it's not on and I'm going to twist files now I see the directory examples now has been cloned also onto my my my host that's my my laptop here okay and this one particular notebook I would like to experiment with so let me say let me just copy it out sorry copy example now let me try to make the speaker see if that's more visible there I'm going to copy one of the files out so I can work with that I always go so deep into inside of the repo so community I don't have English I'll take this auto-encoder notebook and I'll copy it outside of course he shows up by the way almost right away I opened Python three notebook with it and so so I get I get this code here now again it's running not on mine well technically yes is running on my machine vices Ronnie run run the inside of environment that's completely contained in a container something to run all to make sure that everything works question three is point by the way may other ways to access access as question is that how so the talker brought the talker contained there what are the ways I can access I can do it in the browser because I have exposed I have run I'm running a Jupiter service on the docker container and that has been forwarded to my hosts there other ways to do that as well from my terminal window I can actually SSH into it as well so SSH not really except rather just I could just connect to it directly you can that's what I can do so for example you can see a little bit here here there is the shale that's running on the tack container you can do it from terminal directly as well so you can you know if you know what is already installed on the back of container you can do whatever you need on it yes yes I think so I think well what do you mean mmm-hmm it has approached GPU right yeah yes but but there is some work so tell me about shaving other kind of resources right mounting a volume is sharing the disk that's a file system with without container and other resources can be a little romantic I think GPU yes is something that I'm sure is a solved problem because I know that kubernetes can do that thank you also the other cal resources you might want to share that's also very pragmatic for example you might try to share the microphone of your host without a container that can be very hard to do so but I mean there's way there's a path forward to be sure other questions or comments yes you mean the one on the right the one on the left has not changed the one that left has that change because I was running in a detached mode so that I could run come here let me go back here for a bit so he was saying run it and then let me go back to my own shell right so it has not changed I'm still in my own TF in Tucker she'll still my host machine right the files are connected through this this bound mount sorry the dash V commend this dash V here says right now this directory I'm in where I run back around command I want this directory to be shared with the home directory is that it and so it's like to do process um I have a shared memory of sorts that's right exactly exactly so that now everything I do inside the container will be persisted on my host disk if it's a question that if you if I used a data container and sharing the same volume that still works because that sum goes through the hosts probably not recommended now so unless you you're you know fully in control of all different containers are doing with that with amount but yes it does work right and you can you can mount this exact same directory on my my host with something else with another container that's right and so that would be physically still the same disk but then the containers remember they are religious processes they can access those at the same time still except when you're in some container they look like some local directory that's right that's right so that's why it's so important that's why so things will not be gone the exact yeah that depends on what you're trying to do with the things right I mean because then you start in a situation where you might have race conditions or you know new thing about when you lock this file so that you know these containers accessing you what if the other container is trying to do something about it things like that but yes in priests well you can do that but then you think about coordinating this as a man you feel full control of what the containers are really doing no problem if you're not sure where right you might have something let's say running training and writing output checkpoints two to one to two a disk and I have another process to in evaluation for example something like that nice totally makes sense because you're not you and I have like raised right conditions yes collapse when you say collab do you mean like this collab this collab depends on your use case I think so in this case I think the collab instances are gone after certain amount of time so you need to need to somehow put things in different places can be a little harder to install dependencies for example like here it comes with specific version tensorflow that you may or may not want to use for your project things like that so different limitations Oh in that case you can write that's all you're saying right okay as opposed to the round handle provided by this thank you other questions or comment ok so where we were was that I also Ronnie's also encoded notebook which I did click run all so just scroll down and see okay there's a rain seem okay and they scroll all the way all the way down to the bottom I do see some outputs okay you did run through training for 20 bucks and show the nice graph here and it's also in whole decoder so you try to produce reconstructed images of these endless digits so great okay okay now right I'm gonna pretend that I work on this coding and now it's time to hand it off to someone what do I do I'm issued to redistrict here and try to make this sanctuary a Python script that's roundabout I don't want to keep the notebook as a source because ease experiment with but I don't want to manually copy code out so why would use this tool called arm and be comfort now many of you might have used in this case um I think I don't want to want to be a little careful here so when I'm experimenting I'm going to run not 20 parts because could take a while so I'm gonna run this one epoch let's say just when I experiment and then in a graph I'm gonna show maybe okay but then what I'm doing this I'm kind of getting concerned because this is like a temporary change I'm not going to to cubillas Tweety Parks as hard qualities I'm going to come back up here and try to introduce a parameter here I'm going to go all the way up here right below the from future importantly that must be the first thing I'm going to try to make this into into a into argument that can be passed into not a notebook but rather the Python script that I'm going to create with notebook comfort so I'm gonna do something like this in two hours and now have some like this and that and I want to I want to just just introduced a epochs parameter that I can change so I hope they'll have some like I have something like optional EEP hogs and with type of end with a default of c2 okay and so once I have this and after of course I want to really parse arcs I don't remember if it's no marks or maybe I run it and then rely on normal underscore arcs okay in the I'm gonna parse that so now these arcs variable holds epochs the list right so they've run this just double check arcs will be this namespace that has this thing called epochs that's tool and they take a pass in different numbers in the parameter so I'm gonna go down here to change those things now I'm not really interesting training to epochs until I mean once I'm out of the enemy once I'm out of the you know local experimentation mode I'm going to remove our so I'm gonna take our X epochs okay and replace the other two as well arcs okay okay so that was okay and I'm going to try and run this after I do know become notebook convert and come out here clear and see I have copied a copy of the notebook out there and I really don't want don't like to keep two separate sources and so I'm going to call this ipython in B convert and I need to specify what kind of output it is there it does a lot of interesting things you can turn into a markdown getting into HTML ji-sung is I think the basic format so I'm gonna turn it into Python of which file autoencoder at that notebook okay write something Raisa is writing a number of bytes to all going towards the dot pi and I see the file showing up here right now it's showing up here okay and so now I can run it I'm going to just run Python 3 autoencoder dot pi and I can pass in an epoch parameter name saying it's French and remember my default values was to I'm gonna train like 40 parks right now and now it basically runs in some ways once they're at a time but but there's a little bit gotcha here if you have not used a notebook convert before if you use the magical command magic commands inside of the cells it does not get converted and so does not run okay oh my bad because arcs actually start let me go back and fix that because I used parse known arguments here and then notice that this arc ski is really a tuple of things one thing is the parameter I really want to pass in the other thing is this thing that notebook just carries along with it that's also part of why I had to use parts known arcs so I'm just gonna ignore the second part of it I run this again rx not just to okay I'm gonna run the same come in again convert it so actually me meat cleaver yeah I'm going to run the convert which again takes the current state of the notebook making into a Python script and I'm gonna run it and Python 3 photo encoded up high it will be after the conversion and then passing he passed for oh and sorry if my screen turns yellow and the night shifts coming out okay so nice running and this has a little bit noisier print out here and is running first epoch second epoch and so on okay okay so that's great so now I have a Python script that I can suppose you run wherever I want okay not quite right I do know that this Python script runs really well in this starker container that's what the mental model here I have the host somewhere I have a bunch of other machines I might be able to use but I really don't know if this script runs anywhere except for this particular container I know that it runs well here it trains through for me it has a little bit easier with showing images because I mean in in shell okay there's not a problem here it also does not save the model which I neglected to do so it's got threes again I had an encoder decoder I want to save the model so where was a training loop here's a training loop I don't want that I did end of training right after the for loop I'm going to save the model so I have the model save save the model for maybe say from that it's not here if I'm going to say that it has a flaw safety model format okay as opposed to the h5 format okay so I have this and then I would say there's a little cut right here if I if I just run this it will you complain I cannot save me a major show you I'm running this notebook here and I try to say if I just try to run the training loop is gonna say I'm missing some stuff because they turns out to save that tensorflow save save the model format which is much better sorry which is really good in a way that it's much more portable you can movie to other it's a most self contained so you can you can deploy that for prediction more easily but it's complaining now it says it doesn't know it doesn't know it's saying I should call some like this call steady inputs because to say that enough for me it needs to know what input shape is luckily it's not to difficulty the error message here is really helpful it tells me as you call set input inputs I'm doing the pass in the whole the whole input I think that's called X train and I run this again again only to epochs the default value here then you're safe it was safe I'm a saved save the model from that the safe model for me and it's gonna show up on the left side here refresh I see saves for save the model directory and again because we've mounted a volume and so that exists outside now so I come back out again that's in my host machine on my Mac now I look at this I see the saved model directory and as it was inside of it it has SS he has saved model maybe just do this so he has the variables in SS already saved for me again even if even if the training process was running not on my on my laptop directly but rather inside this container and they could share that okay but let's see what happens if out if I were to try to run the same coming outside right so I also have I also have the converted notebook commerce' Python script right outside here what if I were to run it I can do Python 3 auto-encoder that PI he's gonna fail right away because I don't have any of the appendices that was installed in a container right so this is literally the same script because I'm the shared volume but running on different machines that's the environment the different environment or changing the environment be that was talking about and it has different output right so in the container that's a much more secure and inconsistent environment that now I know this script here that mean runs if I hand hand the scripts or someone else you will not work by either hand them the script along with the darker container it will work for them ok question so far yes I would think so so um the question is like maybe which is a shared directory yes so in some case if you do have the your team has infrastructure set up so that you can have a six year to drive or something like that and that worked pretty well I had one of my previous teams we did have we have everything hosted in the cloud and so that way everything would be already shared by default but that has other kind of issues because race condition again so our versioning right other questions or comments yes Oh in this case I are you safe and now safe weights because I was saving as is that question like you save weight suppose in this case I just used safe but I specified that the format to be to be tested to be tensorflow so that like already has variables in it like this file here predefined wait no for this notebook I think he just randomizes the initial weights and it's only meant right but but then here is safe that the waste after training right we could another way so this safe the model directory is really nice this this this whole tree here if you and this whole thing off you can really deploy that and and other people be able to be able to surf the model very easily but you are not just easy to pick it up in the continual experiments that's sort of the gotcha here as well so you know it's very self-contained it has the structure of your graph and that's the save the model to PB and he has the weights the Train weights but if someone wants to actually load that in a continue with the experiment so much paragraph in some way that's a little bit trickier thank you good question so how about a dataset so in this case in this particular case there's one step here where it downloads data set where do I have it I think this amnesty in this case right so little can see it as well so this is part way downloads they have said it just so happens that the downloaded data set leaves on the container but not in a home folder but somewhere in like a temporary directory so it will be gone once they turn off the container okay and so so this is a small thing that in the keep in mind if you mount value only the value must be mounted all to share otherwise not yes thank you so if I want to load the model and train for 30 box beyond two or beyond for I would not be able to do it with this format is that right you can't do it but that's that's why we actually as you change allow the epochs parameter to be passing from outside so you can either change that but right now it looks like I can run it on my hosts rice is what you're meant no so I want to trade further epochs on it to continue from where I have trained it already oh we save the model I think you will have some trouble with that it's doable just a little bit more work and so in that case you might want to just save that as a check point okay there is much easier to reload it thank you if you want to continue the experiment he also saves more than just the weights but somehow interface is set up so it's easier to easier to reload back into tensorflow it does safe then we're not training data for sure it does save the weights and the model our architecture so the graph itself as well by using slightly different format so it should usually think of this safe the model format is sort of an icy realized format so that's you know transportable to other places but for a checkpoint you you still need to rule we get the stuff it even purposes so checkpoints usually it's like the purpose is to resume training and so you do need a copy of your your code where yes if you say things up so I'm a little concerned about about about saying that is the wrong time status of that it's not everything now so in some cases only settings up some of the layers might lose their parameters and things like a batch norms it carries some of the hidden parameters normally you will keep those as well but yes it's sort of like in the middle training was a state right now so that we can continue later on and save the mod is more of or I'm done with training I want to save a copy that I can give it someone else to serve and that's good for that purpose because much faster then reloading from checkpoint and the serve is very slow so they have different purposes okay and the question comments I do want to have a despite more means of your time or tweat we go on your 8 o'clock or 8:30 8:30 yeah good thank you so we go anymore I don't want to show you the dockerfile at least right so we've done all this work what for exactly right unless I it's good a point when we have a package that I can tell you someone okay so so this great know what I have something that I can run but only in this specific campaign and there's also separate right I have this container I happen to be running on my machine that's great but then this script here that leaves outside cannot be run right can already stand alone because I have cut a connection between the cold and in the environment I have the tacker fire already feel so I'm gonna show you it's called tiger fire no surprise okay looks just like this it's not very much it says I'm starting from something right that's the base container which happens to be the container I was running sorry that's the image just running this container that serves a notebook right so that's where all the experiments was happening in the notebook the Jupiter server I went starting from the at that point let me copy this code here copy this that's no the real outcome of your hard work that they indicted runs then these two together completes the bottom edge of that's right angle I was talking about there's a code that works in that container that environment so now this is a pretty complete bundle I can handle anyone now they can just run and I'll say okay so this is the run comment that says every time when i run this container which i'm building on top of this other base contain and I had before I literally took the container that someone else published on the Internet I downloaded it to some experiments and created a piece of code that can run on it and I say okay and I'm gonna be done on top of that that includes my work now okay so I'm just gonna building so I would say Tucker build could have it here so when I build doctor I would always use a tag I'm doing whole tag it like TF world why not and I put a pad here which means the current directory because I need to specify the what's the word a context when you build a talker so so that means everything in here is fair game if I and so that that basis specifies you know these are the files I might be interested in when I build my own Tucker image for myself or others to use so this start here really is the current directory and you will look for in a directory a docker file which I have in a building and so I'm gonna kind of build it you will know I hope it doesn't download a thing no it does not okay so you see it does not try to download this whole six gigabyte monster here because I had done that done that once before and all this that's why it took such a small amount of time oh it does this literally copy this file into it and specify an entry point so that I get a run comment okay so now now if I clean it I run docker images remember these lists all the images I have here I just see the image that's called TF world it's right here now I just built okay so it has very little really the Delta between that and this other Tucker base image that other people publishes the only difference is just at one file that we created and entry point specification now I can run it now this is an image that's packaged with the correct dependencies in place let's run it so saying docker run TF world and what happened is that it will load up this this image my new image which again has all dependencies because that starts from something and you'll try to run a file when I run it this way you will just run it with the default parameters so usually you train to epochs because that's what we said and now you see that is downloading the data again so every time I run I was able to redownload that because that was not part of my my my attacker file so if you want you will do something additional here in the back of fire and say somehow download training data perhaps in some way so that you don't have to always read download it and then I starting out again the environment is actually this other thing this can pack a container that happens to be supported by my laptop as the base hardware if you like like 20 bucks and done right so that exactly what we expected and and I'm running into into a same no this show in the picture showing the graphs issue and what if I want a more parks here maybe try three so then you should run the same thing again but now you'll notice that you have to redownload it because the previous background command has exited already is everything else is gone the downloaded data the saved model that was in there is also gone because I did not mount about him right notice that here when I when s attacker run I did not mount volume anymore so whatever is created in the process running that the container is totally gone now the nice I'm around a g-strain 3d box any questions by the way or comments okay so you turn three parts and it's done and I'm not sure why it's not printing at the error message anymore oh it does it is up here okay yes performance I would say just by the way it works it should be slower because this additional layer of communication that's going on but I don't have hard numbers to say that how much slower it is I don't I have experienced too much of slow down myself unless there's a lot of communication between your container with your host or your container with other things otherwise this whole process is running in itself right so if you if you can somehow limit a networking or this guy out I don't I don't think it should be top to the gauge yes right what you already have maybe on your host machine let's say you could set things up this way so so a couple different different options right if you only have that on your host machine you can choose to depend on use case perhaps you can you can you can choose to copy other stuff in here so and in the text editor here I could say something like and copy checkpoint and into also home directory in that case whatever you have currently at the time of build in the image and your host machine will be moved into it so that's one way to do it that way it contains a copy for your starting point another way to do a if you if this does not serve you need you might might again mount volume maybe some time you run it on but then again mounting value means you'll remember to carry carry you know that multi-volume with you if you want to move it somewhere else so this right over here and on a point about changing back row file to be to do more things if you have other dependencies you may also run here it's very flexible it's literally like like you're inside of a shell except you need to tell it with the special keyword name every run people install like a tensor flow data set for example something like this is now really the image but you somehow you want to use it you can just run this only one time the first time when you view this image you have to literally run this for you in that Parker image and then just freeze that and you'll be there forever questions okay um let's see I have a couple different choices I can go from here let me show you how to run the same training somewhere else now going through all this work there's a purpose for it the purpose really that I really don't want to train for 200 bucks on my laptop also not in the tactical opinion I have it wrong in my laptop right that's just doesn't make sense I prefer when training on some kind of cloud services and so this is whole package that you can literally put it and give it to some cloud service that will train for you but there's a gutshot there and everyone should think about before you really do that in that where is the output gonna go right like the locally if I mount things carefully running running on my host machine or inside of a container doesn't matter that much I assume as I remember to mount the volume right up will be on my laptop still by restraining in some machine the whole machine would all be gone there's somebody in the cloud and so did remember to to make the changes properly so let me actually just quickly adding a couple lines of code to make it happen I'm going back to this notebook here remember before adding the arguments right so to make it more flexible we we allowed ourselves number of epochs which well assuming I'm gonna use some cloud computing source else train 208 park instead they made another one I'll say model directory default tools some just default will just local right so something like this okay what's that gonna do is that you will live it more in any tool now that if I run this you have the arcs will carry that as well for me my love the rectory or what if I pass in around time I mean past different things right by passing things I mean some like this remember we were allowed to pass whatever on a left-hand side screen now I can I can pass in additional parameters there see if I'm just running this Python script I also remember that what do I wipe whether when that I want to write a mine I'm trying to save the mallet there we go I'm trying to save the model to a good place okay so I'm going to do some path join here and I have arcs model directory and I'm going to in the model directory have a saved model directory that contains the safety model okay as you see see how this agent make a difference for us I save I go out here in comfort again because until I as you convert that change I made is this only the notebook that's a single source right so the whole thing with this note will convert is that okay it's real quick so it's done already and so now I know my Python script outside of the container or my host also has been changed I remember that Python script doesn't run so I have to build again doesn't run now my hostess is right so even if it has changed right so it has just changed a few seconds ago at 7:53 but so it contains the change which is made of adding in this order to save it but it doesn't run until I build it right again reminding you what happened here was I my host module simply doesn't have dependency to run the script but when I bill the image I can so talk of it again here world dot-ne Oh oops I think I kept the install tensorflow TSS you could hear couple seconds so you also install taser Proteus a so he install it so if I had to use that in my training script now it can be used in this environment as well and then you successfully build forget about the red don't worry about it it does copy the most recent version of that now so we can do this now so check I have one stuff I'm going to run back around with my talk around for me and talk around and we we can now additionally pass in the different model directory I'm going to call it output and I run it nice wrong it is updated script that has this additional ability or flexibility of saving it's saved model to somewhere that I specified called output and I see where it goes it could become a tuning know what I think about is any questions or comments while we're waiting for this yes for you and you know the scientist yourself shouldn't be worried well and that's my recommendation but if you have appetite for that you might try to figure things out okay that's a save here because I did not put in the correct price rice is running in still not not not in the home folder yes it's possible to do what if every time when I do a so just take away all these men your steps now this time I do things slightly differently you somewhere in a command I selected home slash output as the model directory now remember home home directories mounted and so it's coming to my my local directories I expect to see Oh I expected to see oh yes I do I do see I don't see the safety model directory now no it's not right it's no that's from before isn't it right I ran it before I'm not sure why I didn't show up oh I didn't mind you in a trainee thank you that's so it's thank you just make sure everybody goes I saw that this this mistake I made here is the gotcha number one gotcha opportunity I still don't remember myself all the time I did say that yes after your work is done save the model to somewhere but that somewhere is gone now because I didn't connect that with the host machine so let's do it again but it's maybe with a few worried Park so we don't wait for it one but in the wrong command as you mounted a mount again the syntax is that host directory : the container directory so like this oh sorry I also do why not to go into home just to be sure there okay any other questions yes they can look at a coat here just to clarify that point maybe I took quickly dinner so so here the output is passed into us this argument here which is used as the prefix of the path to write output - okay so you strain one epoch as we specified and now I see this directly called output that has the thing that's just created right so it has inside of it there's only one directory called save the model because that's what what I told you to do I had whatever you were given the model directory create another level call saved model and that's the save the model bundle there okay so it looks like this now again okay now why we're going through this once again backtracking the reason why we're adding this additional power to the Python script is that so I can save it wherever I want now now imagine if I had access to some cloud computing resources I can just say round this container for me with whatever resources you have but write output to a you know like some cloud storage solution but if we can do that so I don't want to show that real quick maybe it's five more minutes and we can go into just more general Q&A it's that I don't need to push this image here right so once again darker images all these images are things that exist on my local machine only in particular this thing here this this package here that now now a can be run wherever that can run docker B accepts epochs parameters I can specify a large number a path if I wanted to and see it also accepts additional model directory parameter so I can I can save the output to somewhere else so let's say you try to push that I'm going to use Google and I platforms training but only because I have access to that and I have a cheat sheet here for myself I remember the command of how to push an image okay here's what I have to do - push okay a little trick here I need to rebuild it with a different hack I think different services - things differently you use docker hub I think you need to use their tooling to push and then you specify attack different ways I'm going to I'm going to review the same image and call it a I think so I don't want even darker because why not I'm gonna build it I'm in the wrong directories you see what what's happening here is that it's trying to look for a file called Packer file in where I am right now but remember I just changed the writer into the output directory who it doesn't have it it has only save money again built with this and iced it doesn't be a new tag luckily it's very quick again only because literally I'm just renaming it okay now I have this this new image you here just double-check that images I see it somewhere up here this is a tag now I'm looking for something that's that's created just seconds ago so let me change it I don't want to see that silliness I just created okay so sick successor attacked dr. images do I see it I see this one now Oh see I was a bit worried when you said a minutes ago why do you say Amy disco because nothing has changed in the past eight minutes right so doctor handles that for you as well if any of the files I'll copy into the image has changed it will rebuild that for you if nothing has changed it would just use the cache so you don't have to think about Oh have I just always run it you would get the most the current state of things if Dan Tien changes you're not wasting time wasting your time doing work either and then I have secret homage sheet here also the push command this bunch of other stuff just so that I can access that but feel free to ignore that I need to authenticate I don't have keyfile let's see if that works still you may work just is the same and I'm gonna push I'm going to run docker push this image here to somewhere see that done is tagged here the tag I use which looks really long right before I should call it here world super is remember why do I need this long and messy juicy other i/o and so on because it gets parsed and so g-cloud command knows that knows where to put that thing into just to be sure let me push the one that says one known let me push the same one just so you don't take too long okay so it's gonna push oh and I'm afraid you could take some time because I also did install sorry did install tensorflow datasets just now which is additional into upload okay pushed you see how this bunch of stuff here layer exists one of those layers those are the base layer the Cystic abides of stuff which I previously had pushed before so it's it's gonna get reproached because there's no need for that attackers internal it's using these hashes to identify individual the year so he knows oh I see this hatch here let me look for in all the past push the layers before if something already existed there's no need to Dec a just booting from there okay so it's pushed by where is it is pushed to this directory here it's on Google I'm in a council now for TCP and I have container registry and for this project I have a TF in Tucker repository and it has a few images and then the one that we just pushed it over here is just now so it's just pushed ok and so this is open requires also you think of this sort of like a docker hub so now if you have a teammate who manages running of experiment right well meanwhile you are exploring and designing experiments someone else handles running for you you can just say here it is now try to run it for me with whatever infrastructure you have maybe run hybrid parameter tuning if you want things like that which you can just you know create a large number of them and I'm going to copy this image name here which is pretty messy right has all the hash at the end but there's value in that so the specifies exactly this copy of the image so that it the result can really can be reproduced and I'm going to run it in a I'm gonna paste in here again this is my cheat sheet I already have the command already already written down but we can talk about this a bit so what I'm trying to do is I'm gonna use g-cloud command again try to run a training german air platform because i don't want to run out i'm a laptop i'm going to give you a name called something TF world i must admit the training job in the certain compute region i want to turn out computer resources there and then here's an interesting flag here master image uri that kind of ties back to the question we had before what do you have masters in and workers you can even have different images for a whole cluster of machines if you want but here i'm just saying i only want one machine that's wrong son that runs this particular thing and i'm going to specify my model directory to be a google cloud storage location code even darker maybe even talk of one why not just take world and then run 3d box i'm gonna run this command and see what happens i'm going to run it it's only three parcels not very long by an imprint in in the real situation you probably only for 300 or something okay so here's command once again oh it's not formatted correctly and properly so I'm running I'm submitting a training job and the only description of the job is just I don't even upload code anymore kita I don't return that I'm just saying Ronny's container for me with these parameters can I expect reproducible results and it says cute like acute where it's not running my laptop it's running wherever I stopped me the job to which is yeah platform here now you see this job is running when go in there and look at log so there's not a whole lot to see people have seen that many times is this even already it's gonna be exactly the same thing because literally all this does this that is going to try to secure some time to research this for me and then run exec that container and because the whole environment is carried over right every dependency that we have used in two different ways my the local to be the Jupiter lab when I was doing experimentation as in in certain environment and then when I was testing the the scripts outside of here that's the exact same environment and furthermore here this online in the cloud training job that's running yet again the very same environment there so I should expect that to happen very soon any questions I would disagree works no comments and if you use kubernetes if you already maybe you and your team already have the kubernetes cluster already said that we have multiple nodes complete resources for you to use you can deploy this as well that's again one package that can can be moved where we want - training data unless you you do things carefully as well by the way issue one yes yeah you can totally do something like that so the question is somebody we have really large dataset and you want to solve part process different parts so that you parallel perhaps right don't you don't care if one part processed or trained first before another you can also again you don't want to create the same code for different different workers jobs there you might this is something I was try and do I was adding at me and again another another argument here I'll say something like they had the right theory or something now I can additionally pass in and in that way when I when I spawn up different containers I can just pass them different directory target maybe they are different directories containing allowed tazer flow files that is it for record file sorry things like that then I can just you know past seen that way when I create a new container process again in the exact same environment only thing I want to change is the process even portion of the data out even shard of data I can pass this in okay probably because of tensorflow world is not running first but I don't want to show you that what happens when this does work is that I expect to see a right what would we pass in before I passed in this as a target location to write the save the model out - and I think / change the from inductor TF that was a this is a I think pocket and here I expect to see a new directly being created that contains the safety model when a job completes and exits if you would just run normally wouldn't take that long you seen my previous experiments usually take oh it's your up to ten minutes before it completes but I think the actual running takes some time so what's happening here is that I specify this this new Tucker identifier remember this is long and ugly string here that has this whole hash number everything so the infrastructure does have to pull that the thing I just uploaded somehow so he's pulling that and I try to run it so but we can just let it run and then later we'll see what happens but I think that's about concludes everything I want to show you for this evening why don't we switch to a more of a chatting Q&A session if you if you like and you know it's pretty late if you I thought this is the tend to feel free to to go tend to them and I'll be here on to 8:30 if you want to chat with me thank you I all the time is finished thank you so look let's finish let's say should go here and really quickly see make sure we save the file and now it does say if it does just created this thing here and that has the saved model for me and so again the checkpoint checkpoint kind of scenario you write the check points also down to some storage so that your team can come in and maybe evaluate maybe figure out how to deploy maybe continue your experiments for you [Music] 