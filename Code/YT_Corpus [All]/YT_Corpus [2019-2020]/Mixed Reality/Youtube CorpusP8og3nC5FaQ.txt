 my name yeah all right good morning everybody thanks for coming out my name is Julius Schwartz and I'm an engineer and I work on input and interaction for Hollins hi everybody my name is Andrew Borden and I'm an engineer and mixer I really toolkit so today we're gonna talk about mixed reality it's it's a new form of computing that Microsoft has been developing for a past several years but before I continue who here has seen Hollins to launch event the videos yeah you remember the amazing Hollins video is just beautiful I love it you guys like it cool do you remember Julia from that event great you still talk more about that all right so then Zeno Collins and mixed rally computing is basically when we blend the digital world with the physical world so I mean consider this right now we're looking at a 2d screen for content for digital content it may not be the best to for example interact with a solar system to inspect the solar system all interaction is often indirect you use a keyboard and use a mouse but with wholeness one we put whole games we put digital content into the world around you yes exactly and that means that soul system can be right here and then what Holland's - we want even further we enabled instinctual interaction we allowed you to touch Holograms so consider this let's say I have that soul system right here we have the Sun we have earth we have Mars we have a car if I wanted to interact with the earth the instinctual interaction would be to pick it up now that that interaction of whole again sign interaction of digital content not only it's direct its natural to us it's computing that's natural to us as it should be and using these two capability mixed reality developers today are building unique experiences to solve real world and they're bringing into fruition what only recently was in the realm of science fiction interacting with Holograms and what we're here to talk about is you the mixed reality developers who here by the way has developed four horns okay for the rest of you welcome to a whole new reality of development and that's what we're going to talk about we're going to talk about the people that haven't developed ones and the people that have and the tools were building to empower you to build the next generation of experiences one of those tools is mixed reality toolkit it's an open source project for unity3d that's built by Microsoft berrak team and it gives you the ability to build new holland's applications very quickly even if you're a new developer or enhance existing applications and out of the box just opening up a sample scene you can start playing with Holograms interacting with them resizing them we give you a lot of functionality that's actually my favorite bounding box the way in reacts so unity to I was I'm sure many of you are familiar is a game engine it's for developing games but it's also for developing mixed reality applications for Hollande's and Windows mixed reality and there are other devices mixer Ally toolkit is built to support horns - horns one and windows mixed reality it also supports any device running on top of open VR and we're making strides into supporting a our core and a arquette will show you some of that today when I say out-of-the-box you get a lot with mcsalley tool kit that's available out there right now you get buttons you get a system keyboard many many different other controls in the bounding box it's built on top of UX guidelines that Giulia will talk about later and then you get a bunch of functionality you got eye tracking you get spatial and standing teleportation and many many more so with that let's switch gears for a bit Julia very passionate about digital interact with human interaction with digital content she participated in a lot of studies that a lot of prototypes a lot of research and today she's going to share with you some of the X guidelines and learnings we've compiled over the years after that she'll talk about the Mobile World Congress the video and how simple it is to recreate that using mixer le toolkit so actually out of curiosity so whole Institute personally for you what do you like the most about working with that ok what I like the most about working on holland's - that's easy it's definitely getting to work on all those input and interaction techniques and basically discover this new way of interacting with objects for those of you that didn't see it a couple months ago we announced holland's - in Barcelona Mobile World Congress and this is actually a video of me getting to show all of those interaction techniques that we spent years developing and the thing I love about it is actually had to pinch myself before I went on stage and asked was like was I dreaming because when you when you think about it this kind of stuff is the stuff that you normally see in movies with like actors you know waving their hands in the air on a green screen and then they sort of all the artists put that all up in post-production but what you see here on stage this is real and what do I mean by that I am literally seeing the Holograms that you're you're seeing this video I'm actually interacting with those sliders with the buttons that I'm pushing in that's me it's actually what I'm seeing and that is really incredible and in fact to prove to you that this is real it's not just smoke and mirrors we're actually gonna recreate the buttons that you see here and those sliders that you saw earlier today in mixed reality toolkit and so yeah the the thing I love about about this too is that it's actually kind of incredible that we can do all this stuff and it might look really easy sort of when you when you see it here done like this but let me tell you it's actually was quite a lot of work to figure out what the right way to do these interactions was sort of how how you should actually interact with these with these interfaces and we're sort of in the Wild West of 3d UI and augment today and we're all discovering this together maybe and you know in some time in the future all be commonplace and everyone's gonna take it for granted but right now it's definitely not obvious and when you get your device's you're gonna want to try many different things but be so excited like I was when I started discovering all this and you absolutely by all means should try everything because for sure you know there were a small team of us working on this discovering things but there's you know hundreds of you thousands of you so you're gonna discover new stuff and I absolutely recommend that you try everything that you can and explore and then of course they'll publish and share your learnings like we're doing here but what I want to do today before I actually get started in showing you how we coded this up is give you a little bit of some guiding lights about where to get started sort of have all these ideas but what are some grounding things that we've learned based on our own prototyping of the things that worked and didn't for how is what's a good way to build interfaces for this so I'm going to just provide some basic guidance I am NOT gonna have time to go over everything and I'll tell you where is a great place for you to go online to to go over the the docs so you can see later on and by the way unfortunately today I'm not gonna have time to actually show you all the prototypes that we built to get where we are today and to discover this model but I gave a talk last week at Mr dev days where we basically shared most of the prototypes that we built while we were discovering the ways to interact with with Holograms and we sort of what we did is we shared a prototype showed what worked what didn't and then the principle that we learned from it so we're working to publish that all online and make it available so I'm just really encouraging you to look up Mr dev days the hand interaction crash course talk to learn about that more all right so let's get start with the basic idea of this new input model this new way of interacting it's actually very simple the the core idea of holland's to is instinctual interactions so you shouldn't need to learn how to use a devices you just know how to do it and it's actually quite quite simple because you see these sort of free objects and we know how to interact with 3d objects in the real world you just you know reach out and directly interact with them and that's the same idea that we have here with hololens now as I mentioned before I'm not gonna be able to cover all of the details of all of our interaction principles that we learned so just leaving this link here aka dot ms /m our Doc's this is the place where all of our design team is actually putting up all this guidance they're making gorgeous pictures and thinking a lot about the right way to interact so I'm just gonna leave this up here for folks to take a picture of and get that get that in their mind but for now let's talk about a couple of basic ideas that we're actually gonna touch on a little bit in the demos today so the first idea is very simple actually it's great when you can have a very simple interaction model because if it's simple if it's easy to explain then it should be easy to use so the first idea is that you should you know be able to press buttons with your finger sounds obvious right but actually when you think back to it when I was starting out on 3d interaction actually started with Kinect like like many of you probably did so I was building all these 3d interactions of like pressing buttons in the air but mapping to a 2d UI and that sort of disconnect of where you're doing these 3d in inputs to a 2d output in the end sort of made the UI difficult to use so the amazing thing about hololens is that now we have 3d input that matches to the 3d output and that makes these interactions work much better so when we demo to press and all sorts of other folks at mobile congress it was really rewarding so you would you would show someone a button and you would say just do what you think you should do do what comes naturally and people just reach out and press the button and it works and they don't even like get excited that it works I get excited that it works but they're just like oh of course that's exactly what should happen so this is sort of a very simple idea but an important one now the details of how to build these buttons to make them work well that's actually sort of the doubles and the details for this one for sure some basic ideas are that it's very important to provide audio and visual cues what do I mean by that so visual cues so it's very important to show the like a proximity light where your finger is in relation to the button sort of have nice shader effect as if your finger had a light and it's sort of illuminating the button and the reason is that sort of gives the user a sense of where their finger is in relation so that they can target better we actually have a thing called the finger cursor that you'll see in Holland sort of a ring attached to your finger that actually changes in radius as you get closer to the buttons and press them and that helps communicate not only the location of the finger but also how close it is to the interactive surface to help people target and click on I mentioned clicks sounds is very important for buttons interactions actually so it's very important to play audio sort of to communicate all the states of your buttons so when I'm pressed on contact on press on release and the reason for this is because when you think about what you're doing maybe some of you saw me earlier earlier it's sort of testing our demo I'm just poking the air so I'm gonna actually you know touching an actual surface and because of that we're we're missing a lot of that feedback that we get from our fingers when we're touching things so we compensate for that by using audio and visual cues so my usual piece of advice is sort of crank up all of the other input sources that your brain can get so the audio and visual to compensate for the fact that you're not actually touching something and when we were starting out prototyping we actually weren't sure whether that would work and it turns out it does actually work quite well that you can compensate for those things and have people successfully press buttons and the data that I have to do that as we're in a lot of studies but also I just know if people are able to press buttons all right spent a lot of time on that slide I will go faster on the rest but I really you know as you can see I could talk about buttons all day but we have to keep going so when you get your hair holland's to or any sort of device that has hand tracking you're probably gonna want to stick a Collider on every single one of your fingers and do something like type with all ten fingers and in fact that's what we tried when we got our devices first and what we learned very quickly is it's actually very hard to do very accurate interactions and typing and by accurate I mean when a user intends to press one of three buttons like button a on the left that they actually press the left button or not the one in the middle on the right and the reason is because of false activations and this is because we're on a surface again your fingers stop when they hit that surface but in the air they're all gonna go through so it's quite actually difficult to make the UI sort of recognize which finger you're intending to use for the press in a general way and certainly this is a really active area of research but a really good low-hanging fruit is just to have a Collider just on the index finger of each hand and the nice thing is that this actually works in at many cases so even if people use a different hand pose like their full hand to press a button it's still gonna work because the index Collider is gonna go through it and in fact I had a story where we at Mobile World Congress we had somebody try to press the button with their pinkie and I was a little worried but actually it worked and the reason is because the index finger still went through the button so again this people thought oh I can personally thank you but actually it's just the index finger so this is a really nice trick that that works quite well so as I mentioned there's a lot of details in buttons to actually make them work really well and so luckily in mixed reality toolkit we're actually providing with a button and also an input system that correctly assigns colliders to the fingers so you don't need to worry about this and we'll get into that a little bit later okay so push buttons with their hands now how about moving objects another very basic idea grab objects directly with your hands to manipulate them now something that's kind of interesting about grabbing objects that we learned when we sort of showed this to people in our studies was that the shape of the object actually really dictates the way that people grab so for example for a smaller object people are gonna do like a finger pinch or maybe a leave we have we gave the just different gestures names so this is our baby shark kind of a open hand and those are actually really easy to detect so they're highly reliable so grabbing small things like small objects generally tends to work quite well when you go for the large ones people try to use the fist which also actually works quite well with our hand detector and now you notice in this in this diagram I sort of stopped at the hand size objects but what if I wanted to pick up this podium here so we asked people to do that we would so people a large object we would say pick it up and they would be like okay let me pick it up with both my hands sort of like you'd pick up a real object and there's a couple problems with that first is that your hands are outside of the field of view of the the hololens so what I'm actually going to see hands if you do the bear hug style pickup and the other problem is with with these sort of framing gestures is that they they can work and I did implement something like this but it's actually quite hard to do it well if you're also enabling two-handed manipulation because when you're approaching something from the sides it's ambiguous whether you intend to do a two-hand manipulation or a framing gesture so then you end up building this UI that has a lot of ambiguous state and while it's a very interesting research project it can actually lead to a lot of complexity in the code and make it quite difficult to debug so my advice about grabbing objects absolutely grab objects to manipulate them for larger objects provide handles or obvious cues for people to grab instead of forcing them to grab the whole thing because if you don't provide an obvious uiq they're gonna treat it like a real object and try to pick it up like a real object and in fact what we learned when we ran studies where we basically showed people objects with and without these handles is that if you just need to show Google handles once if you show them once and then you hide them like we have in our shell we sort of have these handles that appear and disappear they remember and people are gonna pick them up from that handle I mean if you never show handles people are just gonna try to pick up objects like they were real objects and that's actually gonna be a lot harder to do reliably although it is very interesting research alright next going back to visual audio cues I'm just gonna repeat myself because it's very important never harm never some harm in that very important to provide visual and audio cues to communicate the state of your object so for example when you're hovering near a ground will object change the color of the grabbable so that people know that when they grab that's the object that's gonna grab it's like basic stuff it sounds like but makes a world of a difference also press playing sounds on grabs start and end very important because again you want to communicate the state of your system to your user it's critical to do this in sort of holographic applications for manipulation you also want to support one and two handed manipulations and sort of this is a lot of detail a lot of stuff to do right supporting one-hand manipulation two-handed all those state transitions luckily again we're giving you the manipulate handler that is basically your Swiss Army knife of manipulation it supports one-handed manipulation to hand and manipulation switching between the two for those of you that used hollow toolkit previously this is a combination of the drag-and-drop Handler and the two-hand manipulation component and it works both directly and in an indirect way at a distance and it works on Holland's one as well speaking of indirect distant interaction another thing that we learned when we showed these sort of holograms to people when you basically show them a bunch of holograms and say how would you move that coffee cup that's on this side of your table to the other and not surprisingly we learned that people are kind of lazy and they don't necessarily want to walk over and grab something directly they just want to do this indirect interaction so how do we interact with the distance on Collins now that we have these articulated hands well we're using these hand raised so you can think of that as if ending but view are familiar with VR and motion controllers in VR you've got rays that are sticking out of those motion controllers we do the same thing for the hands so you've got a motion control hand with a ray shooting out of it and then when you get close to the object the ray disappears and you directly manipulate it when you move your hand away from the object that ray shows up again and now you're doing interaction at a distance so this really nice sort of works both near and far it's one single input model and we kind of like that and in my MRI dev days talk I go to great detail explaining you all the different other ways that we tried interacting at a distance that that you know and why why we ended up with this and why we ended up with the algorithm that we did it's actually really I'm really happy that I'm able to share all that with you because it was a you know the thing that you you're good thing that you're gonna want to do that you're gonna be like oh why are the hand raised not doing this I wanted it to do this there's a reason and it's I talked about it in my talk for why so again in mixed reality toolkit we are providing you with the hand raised and again I'm showing you the motion controller because we do in mixed reality toolkits virtual reality as well showing you the motion controller with the virtual reality and how it parallels to the handwritten nice set is Holland's too is not shipping with motion controllers so that this picture is is just showing you that nice parallel and there's a link up here that I highly recommend folks take a look at aka dot ms / mr TK docks this is actually where the materiality toolkit team is putting all of our documentation for on github it's actually parsed from github and to get a pidge's and it's got a lot of detail about how to get started with mixed reality toolkit and also all of the different UI components that I've talked about here so again here's the same link and wanted to point out one specific scene in mixed reality toolkit called the hand interaction examples scene and the really cool thing about this scene is it's actually showing you pretty much all of the guidance that I talked about here with directly grab objects press buttons to to activate them at a distance use hand raise it's showing you all those UI interaction principles with the set of UI common UI controls that you can then copy yourself to use in your own applications and we've got sort of all sorts of things here and of course in keeping with our principle of supporting both near and far interaction everything here works near and far and in fact if you had a VR controller it would work with VR - it just kind of great so yes please go to aka dot emeriti Kay Docs to learn more about mixed reality toolkit okay let's get coding already so let's actually start rebuilding the demo that I talked to you about and sort of to remind ourselves of what we're going to be doing so so I can remind myself - okay so here we're gonna make buttons so let's look at this button a little more to understand it so what I'm doing you're not hearing any sound here but we're playing a sound when the button presses and releases we're also visually changing the state of the button on touch it's a little hard to see and we're doing a little bit of a pulse effect okay keep that in our brains the other control we're gonna make is the slider control so this is the control that was sort of the most successful because it really gives you that tactile sensation when you pinch with your hands and move and it's also going to be playing sounds as we move across and as we grab and release and now you can also see that importantly that it turns bright blue when I grab it and we're actually undo one extra we're gonna have it turn blue when I hover near it okay so let's get right to it and go up into my unity here so here we are in the Unity game engine editor for those that aren't familiar and we've got a lot of stuff here already because it takes a while to sort of import everything but when we've done is we've created a new project and then we've gone ahead and imported mixed reality toolkit so if you go to M R TK getting started you'll learn how to do that you just need to download two packages Foundation and examples and then go to assets import package to actually import those two packages so we've got that and then we've got this demo folder with the finish scene just so that you all can see what the finished product is going to be and also a couple of prefabs that our artists have given us that have all these gorgeous meshes and buttons okay so let's go ahead and just get started so we're gonna make a new scene alright so because I'm using mixed reality toolkit it's actually gonna prompt me and say hey you don't have mixed rally tool we configured for this scene would you like to configure it with the profile now the profiles for mixed reality talk to you is basically your set of settings mixed relic toolkit has a lot of stuff in it you don't need everything always and so this profile lets you basically enable and disable all the different systems that you want for now I'm going to use the default one which is pretty much everything and now you can see that in my empty scene I've now got this mixed reality toolkit object here that's how you know you have a mixed reality toolkit in your app and then you've got a bunch of settings here and we're gonna go through that later now I like to do sort of rapid iteration and deployment so I'm gonna go ahead and press play right now to see sort of what changed on that so we're gonna hit play and right away we see this frame rate counter that comes from Mr TK that's gonna tell us if we're performant because we always want to keep our apps running at 60 as possible and then this is cool check it out I've got a hand got a fully articulated hand with that hand right I can move it in and out I can press it and this is basically emulating the articulated hands that you're gonna get in the hollow holland's too so even if you don't have a hololens - you can already pretty much develop your whole application here with this these simulate hands and we actually have ways to support multiple poses you can record and support arbitrary poses and switch between between them with different key presses alright so this frame rate counter is nice but I kind of think it's gonna get in a way for our demos so let's go ahead and disable that so to do that it's actually part of the configuration so I'm gonna make a customized version of my configuration and then the frame rate account is in the diagnostic system so I'm just gonna uncheck that box and now when I hit play it's gonna give me the scene in my beautiful hand and no diagnostic system awesome now I like to do one other thing which is I like to make my background black it makes everything easier to see so I'm gonna go ahead and go until this is like a little bit of a trick for for folks a little bit of a I don't know if it's an easter egg but I like to set the skybox to none and now when I hit play it's actually gonna be black background boom so now it's sort of this is most of our scenes like the hand interaction all seen does the same sort of trick alright so buttons let's get started on that so for that I'm just gonna drag in my button prefab the prefab is basically like a premade set of controls that you can sort of copy and paste around and let's have a quick look at this panel so this is the overall panel I've got my large and my medium buttons and then in here I have this thing called the button component and that sort of has all of the logic for doing it now I'm just gonna cover sort of the parts that make up the button component and then we're gonna actually hit play and see what we get so for the button component we want to have our Collider here so this Collider is gonna give us the region that the finger is gonna be tracked in so when your finger is anywhere in this region you're gonna be getting touch events so touch started updated and ended events or pointer events pointer events are sort of our more general way to handle both near and far interactions and motion controllers etc we also have the NIR interaction touchable now if you want to make a Collider touchable you have to add the NIR interaction touchable component to it so our input system knows where to route the inputs otherwise it would be sending it to all collide bowls which would be expensive so we can see that we've configured it too send touch events instead of pointer events and then we're also specifying the forward vector of the of the touchable there and that's sort of saying that the button is touchable from this particular angle right now our surfaces are going to be touchable just from one location we're working on adding sort of touchable volumes and etc alright so for the button itself we can see a lot of a lot of stuff here these these planes are actually describing the different sort of events and stuff that happens so the front plane here the cyan one is where the the touch tracking starts and then the the blue one here is actually the maximum possible distance of the button so it will not go past that and then the yellow plane is going to be where you're gonna get that press event and the red is where you're gonna get that release event so when the button goes through the yellow it'll send a press event and then back through the red it'll get a release and now you can listen to all these events to give your user interface various kinds of feedback and do actions like you know clicked about handlers and etc so what we've done here is we've hooked up the events to an animation controller and that actually our artist hooked it up and but the animation controller is actually going to change the visuals of the button to make it for example light up when you touch and then when you press to do that pulsing effect also where we're hooking it up to play sounds when the button gets pressed and then released okay lots of talking let's go ahead and see what's actually going on here so I press play I can use the WASD keys to move around I've got my hand ray and then when I actually move up I can actually see that when my finger touches it it lights up and then when I press I get a pulse I'm gonna release I get a second pulse and with audio you can actually hear like the press sound and the release sound you can see the pulse a little bit better here so press event presses the pulse and then release does the other pulse let me show you something cool so how about we make that button right now you sort of have to press in pretty deep for the press to happen let's actually make it sort of press right away so to do that I'm gonna go ahead and make these planes editable I'm actually still the game editing the scene directly and I'm gonna move up the release the press planes and the max push distance planes so that they're much closer to the front so if we take a look here now basically I shouldn't have to move the button at all basically and it should activate so let's go ahead and do that all right so moved them up still in the game and now I get that press event and I don't move it very far and even when I keep moving my hand I'm basically scrolling it's still not going in so that sort of shows you the max push distance and also how nice it is to edit an editor because you can change parameters live in your game and iterate really quickly this way and this is why I love the fact that we have in editor simulation because having to build and deploy this every time would be very time-consuming so it's really useful to have these sort of things and the nice thing is when you stop the game it sort of resets everything back to where it was all right so you have our buttons now let's go ahead and add the sliders so I'm gonna drag on my slider prefab here oh it's a little close to the button so let me just move it over and now let's see actually what we get out of the box for the slider so I'm gonna just go ahead and press play and now I can move over to my slider and bring up with the by the way to bring up the hand I'm pressing and holding the spacebar for the folks that don't know and now I wanna you can see that when I get near I'm sort of hovering to indicate now if you grab at the slider when you move that it's would move the slider and you can see the text value here updating so let me actually show you what's going on oh yeah let me show you one cool thing so I talked about near and far and again because we're using mixed reality toolkit we get all of that near and far manipulation out of the box so everything here the channeling pointer events are gonna work both near and far okay so let's take a look at what's actually going on with the slider so the text is basically has a script that's essentially just changing the value of text based on when a method is getting called and then what we're doing is whenever the value of the slider gets updated we're telling the show slider value script to basically update the text it's pretty simple and we've also got events for when the interaction starts and ends so when you grab and release and when we start hovering that thumb and uncovering it we're also again changing animation states here now the thumb is actually configured here so we're telling the slider component that this is the thumb that you're actually gonna be grabbing and it's moving along a track that's configured here the start and end now let's look a little bit at that thumb to see what's going on here so in the thumb kind of deep in the the hierarchy and it's sort of set up this way because you know this is how I got it so from the artist but it's actually kind of nice to separate all these things out we've got the Box Collider and this is actually the object that's receiving all those pointer events and it's grabbable nearby so you can reach out and directly grab it and also the collider is gives you that region that can be then interacted of distance so when the hand or the Ray grabs this component it's actually sending a pointer start an event that bubbles up all the way to the pinch slider and then this script actually handles it does the math to figure out as you're dragging to project the slider root down to the axis of interaction and move it and send the updated events and we've also got the slider sound script that's actually again listening for these events and then playing sounds on grabbed release etc we are absolutely publishing this online it'll be much easier for folks to look at the the contents when we're there so so that'll be much easier now another cool thing is we can actually just change the start and end of the slider live so let's say I only wanted to be short and again we could also change the visuals by changing the track visual scale here but now sort of if I update it we're just gonna be able to get in close so I can move my hand but it's only sort of moving this short distance and if we wanted to we could also sort of change the axis of the slider by having it move along the y axis here so now it's gonna move vertically and probably look very funny but it there we go alright so lots of stuff that we can do with the slider and we can interact with it at distance which is great now I want to do one more thing which is the hand raised look really cool but for our demo that we're gonna show you when we're running live um it's gonna get in the way so let me just turn off those hand raise so to do that this is again a mixed rally toolkit configuration I'm gonna go ahead and the input system in the Pointer setting so I'm gonna make my own input system and then customize my own pointer profile and then in the pointer profile the Ray are basically called the default controller pointer so all of these inputs so oculus devise Windows mixed reality controllers are all going to create Ray's and right now articulated hands also create hand-raised so we're just gonna uncheck it so that now articulated hand is not making the race and now we're not gonna have any for our interaction with the hand race so you can see I move away and I don't see it so now we can sort of do that now I also want to be able to turn off this hand visualization when I'm running live so for that I'm gonna use the toggle feature panel so that's a component that comes in for free and mixed reality toolkit here and then let me just bring it a little bit closer so here we go so now that's gonna let me turn off and on the visuals alright so there's the panel and then I can like let's say I want to turn off the hand joints visualization so I go here and then now boom now all we see is that nice little ring cursor that gives you that proximity okay oh that's what I think now sort of I've tested everything it seems to be working this is my normal flow now I'm ready to go on to the hololens so to do that I'm gonna build my uwp solution here hey Julia yeah before you do actual question so we tried this out and I feel confident using the device projector oh you want to sell me using the phone so instead of that big expensive rig that we had on hololens - we're gonna just film me doing the hololens interactions and you're gonna see it on your phone yeah okay good yeah so what do I need to do okay so find go to American mix really toolkit change just need your dragon The Spectator to be pretty fun yeah and then I'll come over to my Mac and I'll explain the rest all right sounds good let me go ahead and start compiling it all right so with missionary token we're building out a couple pieces around sharing and I'll get to that a bit more later and one of the pieces is spectating so I have the same project here and I need the same project from Julia because I'm gonna be relying on the same meshes however I don't need the same scene so what I'm gonna do is basically I'm gonna build a scene for a Ark or device and drop in the spectator view prefab way of building out in March okay and let me just go ahead and do that and then and we're gonna switch to spectating all right so what this gonna do this prefab works off of local network their homelands which has a host version of it will act as a host and then the phone will connect to it and then we have I've configured as your spatial anchors here and we're gonna use that to localize if you're unfamiliar with the who here has actually attended the spatial anchors session on Monday great alright so for those for those who haven't it's basically a service running in the cloud that allows you to share location between multiple devices between ha lenses AR coronae arkad the way it works is basically your Holland's will create a spatial anchor in its understanding of the world serialize that data and communicate with the service my phone afterwards will also communicate for the service asking for the spatial anchor and the service will figure out where the phone is in proximity and send that information back once we have this I once we have agreed on the world position relative to each of the coordinate frames we can start synchronizing data and again so this scene is empty because code we have built up will regenerate the scene based on what julia is running I'm actually gonna skip the compiling step because we have all the wiring hooked up but it's you can try it out for yourself later this month this exact code alright so just gonna enter my P here so the IP of the Holland's which was six so we're gonna connect alright and now it should be localized yep there we go alright so this is gotta say really a dream come true for us because we really wanted to to create this using tools that are freely available to our developers and just the phones that they have in their pockets instead of meeting a custom setup or anything so it's really special to get to share this all with you and let you guys take videos yeah and it's great I mean we see what Julie is doing we can participate right now inspect ation only with this and this is real code that's running that's gonna be in McSherry toolkit shortly yeah what do you guys feel like we're gonna take a screenshot post on social media yeah let's do it all right I'll post it later hashtag my2k bill 2019 alright so is that awesome cool let me get back to the slides here all right so I talked about spatial anchors for more information than that go to a k-8 that Emes slash spatial anchors it's an amazing service it will allow you to do a lot of shared experiences speaking of which remember at the start I said that mix rally competing is a blending of digital content for the physical world so we have Holograms with Poland's one we have instinctual interaction with horns too and what and that was waterhole julia was doing up there interacting with the whole games except that was the left hand so this sharing where Holland's can interact and in a shared session with any other device be it an air core a arcade is what we strongly believe in and there's many applications out that are currently in embed this into their application what we're doing next rally took it is we're building the proper support to allow developers to enable full sharing capabilities in any application but Assad today is real code that's running expectations' one way synchronization and it'll be available very shortly and that's only the tip of the iceberg if you want to learn more go to aka domestic a it is our github repo you can consume our code from there we can interact with our developers you can reach out to me Julia we can also you can also submit feature requests or implement the features yourselves and we will go review the work with you to get in it we we simply love engaging with our passionate community a passionate community of developers that are excited to build with mr and we're excited to build with the developers like yourselves the future thank you very much also if you guys follow us on Twitter we'll be announcing one we're gonna post the all the the demo code that we saw today we're working on cleaning it up and getting it up already for you all yeah questions for questions please coming up to one of the mics there's several in the middle who's who here is enduring building that's the most important thing yes yeah I have a question about web X our web VR there plans to have support in the toolkit for WebEx our I think we're investigating that right now I could you actually you open up an issue on the github page and we'll follow up there sure thing yeah thanks cool awesome yes his voice commands kind of built in or voice commands yes its speech configuration profile you can add any voice commands that you want and map it to these things called actions and then you can there's a speech command Handler and you can basically listen to specify which voice command you want and it'll respond go ahead could you share any of your experiences that you had when you're creating like two-dimensional holograms so for example like how is it different to handle the interaction so it's like the 3d holograms she showed versus say like three to two met three dimensional rendering of like a 2d web browser yeah I spent a long time on that so actually in the mr dev days talk hand interaction crash course I go over a lot of the prototypes that we built because it's actually so a couple points on it and I shared learning there one super and so first of all the basic principle of its sort of following touch interactions so if it if you treat it like a touch attract a touch screen that basic principle works well but there's a lot of details in the implementation so one thing that can be challenging when you have a very flat bright object is it can be sort of hard to see where your hand is relative to that object now you can render the hand on top of it which of course we tried and the problem then is that you're including the content like the web browser content that you want to see and so but because you can't see your it can be kind of hard to know when your hand is pressing the content so what we do is that finger cursor that we that you sort of briefly saw in mixed reality toolkit which is a ring that kind of shrinks in size to communicate both the location of the finger and proximity to the slate can it's basically like an aiming aid so that actually helps you click on really small links and we actually have this problem in the shell like in we have to have people use edge so it's like the hardest possible problem that we have so the other thing that works that's critical is to have a shadow projected onto the slate so it's basically a shadow of that cursor and so then what the eye does is it sort of uses the that connection between the decreasing radius ring and the shadow to know when you're touching then to further communicate that you're contacting we play a pulse and we do a sound and then on top of that we have to adjust the dead zone sizes that we have for the scrolling touch versus clicking disambiguation of the 2d UI to make them larger and there's actually really interesting observations that we had for hand movements because when people click air there have their and their hands are actually moving in arcs so they do these sort of arc motions which means that they move a lot if you project it down into the plane they move a lot in the XY plane so it's it's challenging but doable so that's the long answer I just like talking about that kind of stuff so sorry oh go ahead please yeah do you have any future plans to crates or like haptic feedback gloves to kind of solve some of the problems of people moving through the buttons um I we've definitely researched it I'm not aware of any plans that I definitely not that I could speak of but I'm not aware of any plans but we were I've looked into it a lot I know a lot of act there's a lot of really active very interesting research in that area my main task when I was thinking about how to do this for the show was knowing that that's like an extra piece of hardware is can you get away with not having it so the answer was yes but the the haptics is definitely active area research does it work with unities default UI system one of your slides showed that yeah so do you have to do anything to make it work to like set it up yes it does actually we had a contributor from a partner team actually make it work with the unity UI system and so actually if you add unity UI canvas as long as you're using mixed reality toolkit I believe it'll just work it's actually pretty cool I mean I could I could run it on the hand interaction scene example scene but you can basically it will work at a distance with the Rays and with if you directly touch it that's really cool yeah thanks so I have two questions the first one is why did you guys replace the bloom with the wrist temp for the Start menu what was the thought there and the second one was is there any support for radial menus around a closed fist okay yeah I can take the bloom versus risk top because I spent a lot of time on that so very quickly we realized actually we realized it almost instantly with a very first prototype where we stuck a leap motion onto a hololens and the first thing people do is they go like this and that blooms you instantly and then even when people get past that sort of amazement when you grab and release something half the time you do accidentally do a bloom gesture and furthermore we got a lot of feedback from a lot of our customers that when they're just doing tasks in their apps so my favorite example is like in medicine somebody was like training how to give somebody a shot or something and they would just like let go of whatever they were holding and it would bloom so there was a large number of false positives so we needed something that was more constrained than the bloom and we also wanted to do something that was a little bit more following instead of a symbolic gesture more of direct manipulation so originally we actually thought we couldn't do the the the wrist tap because we thought we couldn't do two hands close to each other but the hand tracking team figured it out and this is why we sort of went with a button here and the idea behind the UX of it is we found that people really like menus around their hands and we're we actually I've seen designers prototyping the radial menus around the already so I believe there's no like radial hand menu component but you can do it with like a handsaw a solver component but that's great feedback and it would be great to have an issue around here we want a hand menu and so the UI system is that sort of anything below the wrist is system and then anything above the wrist is sort of the app has for their menus thank you so to that point Julia was referring to actually solvers built in timer to key so you can download it today and actually try out the radio menu as long as you follow the guideline of putting it up out of the Rizk so quick quick questions what do I need to get started that seems pretty cool I have nothing but a laptop and Visual Studio at home what do I need what do you need to get started with a mixed reality toolkit yeah good question so you need unity right now we recommend 2018 3 you need to download actually go to our AKMs slash mr 2kay on the first page and the readme the details you exactly so you don't have to remember to answer the getting started page but it's it's not and then there's this windows I stick a 1 8 3 6 2 which is now if you just get the latest update I believe it's included so you don't need to install like a preview windows but actually you don't need Windows SDK to do the in editor simulation so all you need is just you like none you can even do like a personal license of unity okay thanks welcome back to the to the wrist option are there any other modalities that people can opt in if they only have one hand for example of they need to work with on yeah absolutely so this is um I'm very impacted also about the accessibility of that and so we are very actively sort of planning to support a one handed option and it's actually funny because that one-handed option is sort of the one that that we started with and then we went with this two-handed thing and now we're gonna support sort of a hand up when you bring your hand up there'll be a little icon here that you pinch to open so we're actively working on that wordplay to release it really soon and like the very first update so it's like a timing thing for us either julia has there been any work done on a taxonomy of interactions in AR maybe it's a dissertation that we should suggest to someone but hasn't started anywhere i am i sort of personally try to collect these interactions myself so there's a website that sort of is collecting AR and vr interactions that sort of a student created i wouldn't say that there's like one like definitive guide that I'm aware of there's also research papers that talk about you know given the they do these like elicitation studies where they say how would you do this gesture to close close an app and then they record the five different ways that people want to do it in the air and then they sort of try to come up with a gesture set that way I've seen a lot of those but I haven't seen really detailed taxonomy except for the ones that I've you know personally worked on haven't yet published on my wish list yeah okay yeah that's pretty cool and my question is is there any motion learning in the system that kind of learned your gesture in the way it it gets to use your way of doing certain things um we do have some machine learning in the hand track I mean our hand tracker is based entirely off of a deep neural net to find to sort of segment out the hands from the background and also to align the depth mesh to the hand pose we also have it's also learning the size of your hand but as far as gestures go it's a very like a very it's a great idea and it's like an active area of research and something we're really want to include which is sort of personalizing the gesture recognition for now we're just we're sort of just scratching the surface I can't emphasize that enough that we're just getting started with all this so active research for us yeah what can you tell us about eye tracking and perhaps not just the technology but sort of the ethics and privacy concerns around that I can take that one okay we actually have we were first of all we support eye tracking Holland's - and second of all we're actively supporting in a mixed reality toolkit and we have some really awesome examples that I'm Sophie's tell Mac has been working on to show you how like effortless eye tracking can be and how it can make it feel like it's really reading your mind she's made a lot of examples in mixed reality toolkit she also has a really great talk that gives a lot of detail about the implementation of it and as far as the the ethics question goes I know that I can't speak to it directly because I just I don't know but I know we're actively like it's a it's an active like very important for us to make sure that we're not sharing that sort of data anywhere so I know that we're actively caring about that Microsoft has a very clear windows privacy policy and I believe if you reach out to us online we can direct you to exact details for eye tracker all right thanks thank you is it possible to simulate eye tracking with an external camera we don't have it with the external camera oh that's a great idea like you could stick up the Tobii eye tracker but you can simulate the eye tracking in mr TK you actually just simulated it's the camera movements you could just have it be instead of the head be the eyes so there are ways to simulate it in mixed reality toolkit and then having external inputs is a really interesting idea okay it's on my wish list okay yeah actually submit a feature request to get home that's actually a critical idea to use the camera tracking simulation he showed up hololens today but where are you in regards to using your phone a art kit a art core and using right so officially on the page right now we're supporting Holland's one two windows mix reality and devices based on open BR what you saw today is we're looking at experimentation of how to go to other devices because for example for spectating this developer workflow where someone is working on something and i want to very quickly participate and look at it and show it to the rest of team it's highly valuable so this there's gonna be more details coming into how will support it but this specifically at least we'll have spectating capabilities this month just follow us on twitter we'll be posting the updates yeah for the hall ones one you had research mode to access the sensor data is there something similar in hall ones too it's a good question but I think that's outside the like we can direct you again so if you reach out are the right person to officially answer the question yeah that's a great question by the way mixed reality toolkit does work with hololens once all this stuff will work if you deploy it to Holland's one as well cool oh thank you very much yeah the rest of your building 