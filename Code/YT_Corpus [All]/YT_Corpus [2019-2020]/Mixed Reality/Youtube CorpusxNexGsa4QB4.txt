 hi everybody we're back now don't forget to send us your questions on Twitter for to answer during our session today now I'm Nicola and I'm here with Ryan and Jackson from Epic Games and Microsoft now why don't you tell us a bit what you do so Ryan sure I'm the lead on the XR team over at Epic Games I'm kind of working on all the various AR M R VR headsets we gave up just one what Dex are so right and I'm on the mixed reality developer platform at Microsoft and one of my responsibilities is getting the hololens platform integrated into the Unreal Engine right so let's talk about Unreal Engine and hole whence - now there was this amazing demo that we show it with landing on the moon can tell us a bit about what Day announcement is around all Unreal Engine and whole lens - sure we're currently finishing up the native integration it's been a multi-year effort at this point cutting everything up and running it's going to be coming out relatively soon we're good basically full support for the native platform both on the device and then also the remote streaming from desktops right so we're talking about having a whole scene rendered on a remote desktop and the stream - a whole and stew right so I can invent the full fidelity on one desktop and then I can see it all in the whole lens right so it's pretty cool capability being able to leverage your desktop and like the full rendering of package that unreal provides and having stream that right over to the Holland's and then have the holo and screen back all the odometry and tracking your information back to the desktop so you can kind of work as if you were just tethered directly to the machine wirelessly and then the other half that effort is actually getting out native running on a device so we can run on the desktop we can also run directly on the device right there so when would I choose one of the other when would I run under if you yeah it kind of depends what you want if you have access to a giant machine a lot of our users do and it makes sense to kind of leverage that and yeah go for it it's been it's awesome you can just like basically press a button to do that if you want kind of the mobility and being able to take it out on sites and wear something like that then the other one on the device is kind of the best option right now as far as the demo that you show can you talk a bit about the demo without demo was and how it was built yeah sure it was we want what we wanted to show is kind of a multi-user interaction with lots of people kind of looking at the same holograms in the same space and so the general setup was we had a dedicated server which was kind of connecting to all of the various clients we had to users and then another kind of Steadicam that was using a WR headset for tracking so all three were basically networked on a dedicated server and they were all localized in the same space using the spatial anchors so that means the city cam operator and both Hollande's users could all look at the same hologram in the same place and interact with it and talk about it which is a really cool kind of collaboration and opportunity that I'm looking forward to seeing what users do in the future and at the same time since we are running on desktops were able to leverage kind of the full power of a ridiculous workstation and so like the rendering fidelity everything was really high right yeah we were pushing about 15 million Poly's with full procedurally based or physically based shaders right so it was a and that's not possible to directly on the device it's not like the device is powerful but it can't handle 15 million poly yeah we were we were running some pretty pretty intense hardware backstage it's actually really cool to be able to see that kind of rendering fidelity on a headset cuz you're used to you know the simpler what you would do from a mobile chipset but being able to see you know what a giant BP GPU can push and seeing that in AR is really cool all right now the whole lens has some really cool features about it specifically around hand tracking and eye tracking how does that work with remote rendering how do you actually handle that it's basically seamless dependent if you remote or native it doesn't really matter the underlying kind of layer that you would build content to you doesn't care if it happens to be remote there's a layer that handles that PI system user so you just work with it as if it was you know local right yeah so all of our articulated hand tracking and I guess gestures they're all just plumbed through into via the unreal layer and if you target any other error or XR device on unreal you'll be able to seamlessly bring that over into Howell once so right so all the sensor and telemetry that this device is actually gathering is passing it on to the remote device and ruin the device decides what seemed to render and sends it back to to the actual Hollens is there a new consideration with latency or how does that or anything else yeah there's obviously more latency when you're running off a wireless link and it's not you know run directly on the machine but the reprojection and kind of compositor technology and hololens does a fantastic job of hiding that and I think in general also air is more forgiving in that sense because you can see the real world behind it so if there is any sort of float you know it's not nearly as effective to the user as it would be in Br but it really is I need very little latency guys it really well okay no this was great but can you actually do this through a cloud let's say I have a device in advertising our vendor ring is that possible yeah moving forward post post launch will be adding all of our remote services and after services into into the device so we have an existing mixed reality toolkit and we'll be working on bringing that over into unreal and something that'll include things like adverse patch linkers and as your remote rendering okay so let's talk about the mixed reality toolkit so that's something I already exists what are you doing more on the next rally toolkit yeah so that already exists and it's a way for for developers to specifically target our platform and the the big wins and our platform over maybe other platforms but it does not currently exist for unreal so we need to take what we have and write abstractions into unreal we have the Unreal kind of provides a lot of building blocks to allow you to build content and they aren't necessarily kind of full feature solutions for a specific thing they're more like low-level building blocks you can assemble to build full feature solutions to things and so as we look at what the animart toolkit does it's basically taking the building blocks that reality provides and then building up solutions for common interactions that you would want so it'll basically ship is just contents as you know you would get in the other content but build a nice drop-in solutions for things that users want to do but I haven't built it themselves and if I already have a game or something I've built with unreal can I how easy to support that through hololens - right now sure so we're building this on top of our XR platform and it's the same platform that we build all of the other devices that we support on top of so any place we have a common functionality layer will kind of target that and any place that device you know kind of diverges with you know a feature set that is specific to the device if there's something cool we will have a way to hooking into that as well so if you have content that exists for other AR devices other VR devices you'll be able to target directly to the hololens and anything that the holland's gonna do that the other device to do will basically transfer right over which means if you have content already you can you know already get up and running on the device and then you can start looking at what the Hollande's can do and getting those specific feature sets like you know thing you're tracking in case tracking integrated on top of that it's also a really nice way to start if you don't have a Hollande's now but you have a WR device for example you can our building content today and deploy the device once you get it and then circling in those additional features that you can get once you have the device in here yeah last year we added a native support for the windows mix reality headsets as part of the xir platform and unreal so anyone who has been working on native wmr in unreal already has code that should just work in in hololens but that is amazing now you also mentioned things about spatial anchors and after special anchors can you tell us more about what that is yeah we have a whole address stack of AI creating a spatial anchor which is a similar to a local spatial anchor it's a point in space relative to feature points that the device identifies after spatial acres goes one step beyond that and allows you to upload those anchors to the cloud for any device to to find and localize for them so you can have massively multi-user scenarios in the same space looking at the same content that is incredible so if I'm in a space and I've put a hologram somewhere somebody else later on can come in and because I've already mapped that space they can happen so someone else on any device on a hololens on a phone I can come in and identify future points pulled down that spatial anchor data from the cloud and see exactly what I've what I pinched there and I assume that's all full support it will be fully supported with Unreal Engine and it'll just be a spatial anchoring content for us and the fact that is being you know sent up to the cloud and they distribute out to all the various devices and the abstractions between the devices the user and the developer does need to worry about they just need to know that when we put an anchor here and the content sticks an anchor right and with things like remote venturing you have a full power of the cloud to be able to render everything in full fidelity and be able walk you around the world and just playing with Holograms you know have my holographic dog and I can walk around in the park and then interact with other holographic dogs so though it's all full fidelity that's great make sure you put it what's coming up in the future like when is all of this available like when can people start using this this is the Cheryl we're actively working on it right now we're gonna push basically a early access kind of experimental version on github at the end of us and so you be able to pull that down and then we'll continue to update that as we bring in new features and just polish things and then once that goes out we're gonna start working on our next Unreal Engine release which would be 4:23 and that would be a bit fully official support it relates for Hollande's and and minutes which is reality and then you later transfer over that and that comes out later this summer and in parallel to that we'll be working on the the mixture LD toolkit that we talked about earlier right is there anything else that you can talk about about Unreal Engine that people can that you super excited about that people can start doing right now I'm just I'm excited to see what people build with it honestly it was a lot of lot of gears have wanted for a long time they've had to use other devices and so being able to take the content they built and bring to the hololens which is a super awesome device to use I'm looking seeing what they build with it yeah and this project has been a couple years in the making and obviously microsoft loves unreal we've got a CEO theives rep right here so obviously a game that was made with a ue4 so i'm really excited to see what people make with this but that's great now you also talked about the XR platform and how you can take a lot of the existing content that you already have can you talk more about how that ties in to other devices as well and how people can pick that sure basically we have an abstraction in the engine both on the code side and in the content side which takes common paradigms for four devices you know for tracking for rendering for stereo rendering various optimizations for rendering input methods is the controllers is the touch buttons is it hand gestures is it voice input that kind of thing we tried to abstract it away so users basically just look at what is a Billy that I want I want to render in stereo I wanted to be head tracked they don't have to worry of handles that which is a lot of work under the hood to make that happen so you go content once and then you can deploy each of those devices as long as those capabilities exist in the device then that abstraction works but each device generally brings something special to the table because everybody kind of wants to pick and choose something and it's still a lot of experimentation also in the space like what what works what doesn't so hand tracking I think is a really good example in the Hollande's it's it's fantastic and we don't have that on the other devices so you may have built something for a VR headset you deploy with hololens and now you want to build some sort of input methodology that uses hand tracking will have a specific way of doing that specifically for the hololens as time goes on and people see how awesome hand tracking is and they're able to actually reproduce that at some point in the future we'll start building a layer on top of that so instead of having to target a specific API we can target multiple devices so we've kind of been doing that over time so so I brought in head tracking and rendering for example those were pretty perversion now they're basically the same thing and I expect more features as the industry decides they're a good idea to come online we'll start doing similar abstractions so you can kind of go content once and deploy multiple times so how do developers get access to the hand tracking for example how do they actually get that is that abstract that or is it specific for the whole lens only it's the the input method coming for the hololens is specific and then whenever we have a kid building like that we look at what already exists in the engine is there something that we can connect this capability to that users are already you know are using and so for hand drag specifically we have a technology called live length which is abilities streaming and animation data and use it for various various things we should have awful lot of juicy demos and things so we have a live link connections that we knew how to use live link you can use that immediately and then we also just have kind of a blueprint library that allows you to pull in those points directly and do whatever you want with them and we're gonna be showing later today in another talk but basically like I had to get started with on a real thing and part of that is showing directly that like how to like hook up hand tracking and using my application it's relatively simple right so you have a talk today that will be streamed and recorded so people should definitely go and check that out as much as they have is there any set of assets I think that people can share and use as part of this unreal engine that work really great with the remote rendering and people can take advantage off well we'll talk built a lot of content for the talk so we'll give that away at the end kind of it's a bootstrapping for us it's people to get started and if you have on your content that exists today and it makes sense for it to run on a device it will run on the device what about the demo that we saw with the the moon landing is that something that people can use right now or take a look at that the capabilities to build it are all there I don't know what we'll do with the assets but I hope at some point they're available in some form because they're really cool assets but they build specific to that projects and I hope they have life on beyond it because they were yeah the amount of time that went into making them historically accurate and technically accurate was immense cuz they wanted to kind of show it as a museum piece so yeah I hope you chance to check it out cuz it really cool and hopefully have a way of see him in the future I imagine there was a whole team of designers and developers working on this demo I mean the quality of this demo was amazing and it's like a movie actually you're watching in front of you yeah we basically have like a very talented Special Projects team who worked extremely hard on that for a long time and it wasn't wasn't just us you know bringing up the big the hololens basically as it was a prototype to try to build something on top of it of that fidelity was a challenge that microsoft did a lot of work on that side too awesome well I appreciate so much for taking the time and talking about this is there any last call to action do you want to tell developers right now how to get started no take a look at the talk and take a look at the github release when it comes out and then look for the full release in 4:23 awesome thank you so Cristina has been exploring the floor all morning so let's go ahead and see what she's up to you right now 