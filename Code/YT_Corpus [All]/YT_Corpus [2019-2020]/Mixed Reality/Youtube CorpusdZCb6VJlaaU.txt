 hello everyone and thank you for attending today's session on azure object anchors for those of you who may have missed my introduction this morning my name is jessie mcculloch i'm a program manager on the mixed rally developer ecosystem team that's a long title to say that my job is to help grow and manage our community developers designers and creators around the world the picture shown here is from our first mixed rally dev days event at the microsoft headquarters and many of you may even be in this picture and since spending time with developers is my favorite thing to do i'm very excited to be here presenting to you all virtually for mixed reality dev js japan azure object anchors is the latest service to join our growing family of mixed reality services we started off with our first service azure spatial anchors which allows you to place digital content to a real world location and recall that content in the same location across many different devices azure spatial anchors is in general availability and available for use by you all today the next service we introduced was azure remote rendering which allows you to offload the rendering of content into the cloud and be able to deliver content to a hall lens device at a fidelity that cannot be accomplished using the onboard computing and gpu power this service is currently in public preview and we encourage you to try it out for yourself last but certainly not least that brings us to the topic of our conversation today azure object anchors azure object anchors allows you to align your 3d digital content to physical objects instead of locations this service is currently in private preview because of that this presentation is going to be very text heavy as we don't have a lot of images yet to share i apologize in advance for that in media today's mixed reality scenarios there is often a need to align digital content with physical objects many customers complete this scenario via physical markers such as qr codes and other computer vision methods combined with manual alignment azure object anchors enables mixed reality developers to automatically align and anchor 3d content to objects in the physical world object anchors aligns 3d content to real world objects saving significant touch labor reducing alignment errors and improving user experience with object anchors developers can build applications to automatically detect a specific physical object in the user's environment and align 3d content to it to it without using any markers object anchors leverages the depth and geometry data about your object and your environment from hololens to to enable object-specific anchoring and automatic 3d content alignment we can see in this example from toyota one of our early partners where they're using azure object anchors to understand what vehicle model they have in front of them and then align and overlay the electrical system for that particular vehicle for the maintenance technicians throughout our talking with customers and partners over the last few years we found that there are many use cases that benefit greatly from being able to align digital 3d content to real world physical objects the first of these use cases is task guidance this is the ability to overlay step-by-step task guidance on a physical object without the need to use any sort of markers on the object this works really well on manufacturing and other industries where anchoring task guidance just by location is either hard if not impossible the second use case is very similar in that a lot of customers want to use mixture la to help simplify training development azure object anchors allows you to do this without the pain of having to manually align digital content with the real world objects as it's currently how many of these applications force their users to work finally we heard lots of feedback on wanting to use mixed rally to do visual inspection using existing 3d models of objects in your physical space and tracking the instances of those objects in your environment next i want to go over the high level overview of how the service works there are actually two parts of using the service the first part is what we call the training experience the second part we refer to as the runtime experience the training experience of the service consists of uploading a 3d model or models to the objects you of the objects that you want to align with we will run the models through our ingestion pipeline and train a machine learning model that the hololens will be able to use to recognize the object the other half of the service is the runtime portion where your application uses the model created to actually do the match on the physical object this this slide provides a little more detail on the training portion of the service as i mentioned before the first step is to upload the 3d model or models of the physical object that you would like the hololens to to be able to recognize and track these models can be in a few different formats including gltf glb obj fbx and ply once the model is uploaded it kicks off a process in azure which we take the model into our pipeline create a machining learning model train it and then output a binary file that the hololens 2 can then use to recognize the objects this portion of the service requires that you have an internet connection in order to upload the 3d models and download the trained model binary here are some more details on the runtime portion of the service which takes place on the hall lens 2 itself we have an sdk that you will add to your hololens 2 application that will work with the binary file to leverage the depth and tracking information from the hololens 2 to help recognize the physical objects to be tracked when looking at the code portion of your application there are a few steps that you will take to make this happen first you will start a session and then load the object model binary next you will set the search area where the hololens will try and detect any matching objects after you set the search area the hololens will work asynchronously to detect the object or objects in the search area and then align the 3d model of that object that is recognized once the alignment is made that will be locked and then a call will be made to render your 3d content in relation to the tracked object our team has made a sample project that uses unity mrtk and object anchor service so you can have an example of how to use the service and to do so with some of our other tools this has just been a really quick overview of how the object anchor service works and what it does next we'll move on to some frequently asked questions first what devices does azure object anchor support currently azure object anchors works on the hololens 2 device this is because of the need for the depth information as well as the tracking information that comes from the hololens 2 device to be able to map out the physical object and then match it to the 3d model what 3d formats are supported as mentioned earlier azure object anchor supports 3d models in gltf glb obj fbx and ply formats we find that this is is a fairly standard set of model types that covers the majority of use cases that we've seen in partners so far can i use azure object anchors without internet connectivity as mentioned earlier you must have an internet connection to upload your 3d model to azure and then download the trained model however when running your application on the hololens device the processing is done locally and there is no need for an internet connection this allows for offline scenarios to happen with the actual recognition of the physical object when will azure object anchors be available for public preview at this time we do not have any publicly announced timeline for public preview often what we do during these private previews is work very closely with the partners that have been onboarded to get feedback and improve the service so that once we're ready for public preview we have a much more robust and complete solution for the wider public to try out and finally how do i learn more you can learn sign up for the azure object anchors private preview at this address shown on the slide there will be a form for you to fill out and that will go over to our azure object anchor team and the more information that you can provide on the form the better they'll be able to assess whether you're a good candidate for the private preview sign up if it's decided that you're not a great candidate for the preview sign up we will notify you as well as notify you when the public preview opens up so that you can go in and try the service at that time 