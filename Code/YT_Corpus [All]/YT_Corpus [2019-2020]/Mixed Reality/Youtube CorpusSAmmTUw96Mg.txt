 thank you Robert so this work has been done that AIT live at ETH well together with on the fight and off Mahela guys so this video only plays every second time I start the presentation so let's try it again see so mixed reality allows to display personalized contents anywhere and anytime and put and essentially with any time with any arbitrary amount of information and it complements current technologies such as desktop or smartphone computing what we are arguing is that mixed reality is inherently context-sensitive so the interface changes every time the user changes the tasks or moves to different environment so manually designing for all those different situations or that combinations is infeasible so context sensitivity should avoid two main problems the first one is constant manual adaptation so here the user is switching tasks from reading a paper to taking a break and if the interface is distributed in 3d they have to adjust all the applications to fit their needs for example to that they can see when the next string goes or to watch a YouTube video or when their food comes or news so this they have to do every single time they switch tasks which is tedious and cumbersome the second problem that happens is information overload if you would go for a trivial solution where all the information is displayed constantly all the time that would be in the end pretty annoying I believe so the question for our research is really how can we automatically adapt virtual contents to avoid constant manual adjustment and to not overload users and here's an example of our approach so here the user writes down notes in their office and experiences medium cognitive load as an indicator as indicated in the lower left the visibility of the virtual elements their placement and level of detail are automatically decided by our system they can be adjusted by the user but they don't necessarily have to so prior work has mostly focused on layouting of virtual elements to avoid visual clutter for example I work by calling colleagues on Flair or they work aggressive diversion chilly air both adjusted the amount of displayed information to avoid unnecessary visual complexity or the filtered information also tomorrow in the morning I recommend going into the session where my colleague Christopher Hart is going to present the reinforcement learning approach for information filtering in mixed reality so most of these works really focused on visual complexity what we wanted to do is that we wanted to jointly optimize an interface for users current context and with context here I mean the cognitive load their tasks and their environment so let me explain how what we did and how we did it so based on users current context we want to automatically decide on the visibility placement and level of detail of applications so visibility is pretty straightforward we want to decide should an element be displayed or shouldn't it be displayed for placement our approach decides if an element should be anchored in the view or in the world so if you anchored I mean displayed as heads-up item so that's another one of these videos beautiful so we decided on this more simplistic model of placement since it allows users to exploit their spatial memory but it could be potentially extended with an adaptive placement method so here the users adjusting the position of the items in the world and we decide on if it's in the world or as heads-up display thirdly we want to decide on the level of detail of an applications so here you see an example with the email application with five levels of detail and all with increasing bill our information from an icon to your last three emails and we do this with a mix of rule based decision making and combinatorial optimization so in an essence what we want to do is if we have an increase in cognitive load we want to show less virtual elements based on the task and environment to do so we have two main inputs ones that are provided by content creators or designers and the other ones that are measured live by the system so the content creator provides the applications so for example email application and for each visually designed levels of detail and then they also provide a cognitive cost of an element so how cognitively expensive is it in order to display something and then they provide a set of tasks and for each task that provides the utility of an elements or how useful is it and the frequency of uses the hardness is used currently this is input with a set of simple sliders but it's easily feasible to actually have a data-driven approach to to replace this manual approach here the second part is measured by the system so for one we just geometrically measure our elements actually visible to users and the second part is our cognitive load estimation so here we build on the work by two calfskin colleagues on the index of pupillary activity the index of pupillary activity or IPA is essentially the frequency of meaningful pupil dilation and this is correlated or it's yet it should be coordinated at least with cognitive load so with higher frequency of meaningful pupil dilation and we have a higher cognitive load and the same holds for a low cognitive load in our approach this could also be replaced by other methods if they are found to be more reliable we have a three-step process in order to solve that problem where we first decide if an element should be world anchored or view anchored secondly we decide on its visibility and level of detail and third on if it's decided that it should be visible and view anchored we decide where it should be displayed and the heads of display so to decide if world of view anchored we use our knowledge of the environment so as you see later we have a depth camera mounted in front or if you would apply this approach in VR you have all the information you need anyways and an element is displayed world and cut so anchored in space if it's visible to users so it's in the field of view of users and it's not occluded by geometry the second part in what I think is also the core of the method method is where we decide on the visibility and level of detail so in a nutshell what we want to do is we want to display as many elements with the highest level of detail while not exceeding the cognitive load of users we use integer linear programming to solve that so what we take is the input from the content creators where we take the utility of an element for a specific task and it's frequency of use we then use for a combinatorial optimization that allows us to find out which elements to display and with which level of detail so note that the trivial solution here would be to always display everything with the highest level of detail which does not solve our second problem with information overload so therefore we introduce two main constraints the first one is a cognitive load constraint so we take the current cognitive load of users and the cognitive load of the displayed virtual elements and we say that they should not exceed a certain threshold - the small alpha - not max out cognitive capacity of users constantly the second constraint is a level of detail constraint which essentially says only deep display each each application with a single level of detail so we don't have duplicates in in our virtual world the third part is then that we place view anchored elements so if an element should be visible and view anchored we greedily place it in the best slots so what do I mean by best slots so here's an example for example how the sky pikin gets placed so the best slots are essentially a function over the users field of view where we see the foveal area is of low quality so that you don't have the virtual elements constantly in front of you the mid sorry the foveal area the mid peripheral area is of high quality where we say this is a good balance of visibility but also it doesn't disturb you and the far peripheral area is of low quality mostly because of constraints of current hazards and once we've decided where to place it we place them or if an element has been previously assigned we put it into the same slot again to exploit spatial memory so this is our three-step process to automatically decide when where and how to display virtual elements in terms of implementation we implemented our prototype in unity with steamvr and we use the groovy solver for the linear program for the integer program this runs in real time at roughly 7 milliseconds for 12 elements with 5 levels of detail we also are in the paper you can find the details that we checked on the on the scalability of the approach and we found that it scales linearly with the number of elements and their levels of detail so for example if we were to have 200 applications with 4 levels of detail each this takes roughly 40 milliseconds for the solver to solve note that while this is not real time I do not think that this is a process that should happen at 30 frames a second but really only when there is a meaningful change in contexts or tasks or cognitive load for example in terms of hardware we use a VR headset and HTC vive pro with Rosetta mania camera mounted in front and the integrated pupil labs eye tracker for the cognitive load estimation we decided on that approach since it gave us a larger field of view compared to available AR headsets like the hololens one or the magically I'm going to show you three scenarios which we implemented so for the first scenario the user is just doodling which I would argue is a low cognitive load task and which we also measured here and there's more information available a lot of information is at hand so the user has all the information and because there they can handle it in terms of cognitive load second is brainstorming which was measured as medium cognitive load activity where we show less information that is more task relevant for example inspirational images or the current tasks the last one is where we read a difficult scientific paper where we only show very minimal information to not overload users we also evaluated our approach to find out about the usability and applicability so we had 12 users that follow the dual tile paradigm and they completed three different primary tasks with different cognitive loads so counting backwards in steps of 17 which is for many people are for most people at high cognitive load tasks counting backwards in steps of 2 and I can search as the secondary tasks that were asked to answer questions verbally so the system with text-to-speech ask them questions and they were and they needed a couple of applications that were in the virtual room in order to answer those questions as dependent variables we measured primary and secondary task performance as well as the number of interactions needed in order to answer the questions we performed to study in VR because the setup I introduced earlier introduces a lag of roughly 60 milliseconds which is okay for kind of testing but it introduces a bit of motion sickness which we avoiding and also it provides an even larger field of view we tested two main conditions where the first is the manual condition so here the user is counting backwards in steps of 17 and then gets asked a question for example who is the second Skype contact the answer the question we're really verbally which gets locked by the experimenter and then they move back to the task in the optimized condition our system has a rough knowledge of the tasks that the user tries to achieve and then suggests the placement so that when where and how of the virtual elements and users can still adapt it in order to to fulfill the secondary task in terms of results we found no difference between in task performance between manual and optimized this is not unexpected the performance is really dominated by the primary task here but our system did not disturb user and fulfilling the task which is for an adaptive system already kind of the first goal definitely what we also found is that our method actually decreased the number of necessary interactions by 36% compared to manual which makes us believe that this is definitely beneficial in the long term and also makes it a plip applicable if you really have this context adaptation so in conclusion we're arguing that manual design of mixed reality interfaces is not feasible because of the constantly changing context what we are proposing is to jointly optimize mixed reality face based on this context and we thought we provide a first implementation based on rule days rule based decision making and combinatorial optimization there's still a set of open challenges so for one cognitive load estimation the IPA introduces a lag of roughly 30 milliseconds and I think that a sense of fusion approach for example with GSR would be a way to go that might be more reliable and also faster also predictability is super important for any user interfaces probably all of you know and also we have mostly tested applications so that were inspired by desktop computers and not more deeper augmentations like for example what happens if I actually change the color of a physical object through HMV how would the user react to that and also we did not yet investigate larger changes in context for example changing the context from the home to the office to outside I think this will yield a set of new and interesting challenges that will require computational approaches to make mixed reality feasible so to sum it up I think that mixed context where mixed reality really balances virtual contents and the physical world to avoid overloading users and to avoid the constant manual adjustments of virtual element so the last thing I want to say before Robert cuts me off is that on this we published our source code even with a small 2d playground for you to get started with integer programming if you're interested in that and everything is published free and gets up so thank you very much for your attention and I'm happy to answer your questions [Applause] thank you very much let's see if there's any questions in the audience right so I have a question actually this is about the cognitive load estimation it seems the so you said there was a 30-second delay yes about estimation the the values were shifting during the actual yeah what's that from so I mean we do constantly do the eye tracking and the calculation of the IPA but we need roughly 30 million of 30 seconds of data in order to compute kind of the current value so this is what is delays so we have a sliding window of 30 milliseconds and an event that has happened like 15 seconds ago will only kind of show up in the cognitive load estimation with with this delay we are currently investigating kind of if you can do it actually with 15 seconds or 10 seconds but I think especially with that method it tends to be more noisy the shorter your window is we are talking about a frequency of like to change is a second or something like that so if you only have have 5 seconds here you're kind of you have a very weak signal to noise ratio here the values that you show here those are all based entirely on the pupil dilation numbers yes yes so I think really kind of some something where you have you combine it with GSR with stress level for example would really be an interesting approach way which is more immediate um and then you couldn't find out if that actually works better or not thank you 