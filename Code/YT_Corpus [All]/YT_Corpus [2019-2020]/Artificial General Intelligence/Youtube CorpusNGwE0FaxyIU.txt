 - Fifty years ago in March of 1970, a monumental document came into force with ratification from 43 states. The Nuclear and Nonproliferation Treaty. Fast forward to 2019, we are on the cusp of the fourth industrial revolution fueled by artificial intelligence. In Stanford Professor Andrew Ng's words, "AI is the new electricity. "However, just like electricity, "AI is a double edged sword." Advanced AI systems can be used to enhance education and prove the efficiency of transportation, fight epidemics, support the elderly and support better governance. However, AI based technologies can also deepen existing inequalities, create new inequalities, and exacerbate geo political tensions. How might we policy makers, academics and private individuals come together and establish appropriate governance structures and support mechanisms for artificial intelligence? That is the motivation of today's breakout session. My name is Stone Yang. I'm a second year undergraduate student studying computer science and philosophy here at Stanford. My student organization, computer science plus social good has been leading the conversation on campus around maximizing the social good led by technology and mitigating its harms. We are extremely honored to host the AI Global Governance Breakout Session today, and we sincerely thank all of you, speakers and audience for coming to today's session. In our session we will hear from four distinguished speakers coming from a range of different backgrounds and interests. They are Professor Eileen Donahoe, Executive Director of the Global Digital Policy Incubator at Stanford University. Mrs. Jessica Newman, AI Policy Specialist with the Future of Life Institute, Research Vowel at the UC Berkeley Center for long term cyber security and Research Advisor to the future of society. Mr. Peter Cihon, Research Affiliate at the Center for the Governance of AI, and the Future of Life Institute at the University of Oxford. And Mr. Phil Baldwin, Senior Vice President of the Research Group and confounder of Element AI. After their individual presentations, we will have a panel discussion followed by an audience Q and A session. Now, without further ado, let's give our warmest applause to Professor Eileen Donahoe who is going to present on human rights as the foundation for global governance of AI. Welcome. (audience applauding) - Thank you Stone. So, the basic purpose of my talk today is to show how the existing international human rights framework can serve as the foundation for global governance of AI. And in my time I wanna try to do three things. First, I wanna step back and ask the question what do we mean by AI global governance? Second, I'll try to answer the question, why turn to human rights for that purpose? And third, I'm gonna talk about a few emerging mechanisms for global human rights based governance of AI. So, what do we mean by AI global governance? Just as AI can mean many things to many people, governance also can have many meanings. Governance can refer to formal government regulation, institutions and structures. It can refer to internal accountability mechanisms for the private sector or NGO's. And governance can also just point to the underlying normative principles and commitments that guide a governing entity. This is a very quick strip down snapshot of a variety of AI governance activities that are happening to date. On the government front what we see is that a number of governments, I heard something like 25 governments have put out national AI strategies, primarily focused on R&D investment in AI. At the local level we've seen a smattering of regulations focused on government use of AI such as bans on facial recognition in law enforcement. We've seen that in San Francisco, Oakland and a couple other cities. At the regional level I'd say the best example comes from the EU where GDPR Article 22 speaks to the right of data subjects not to be subject to decisions based solely on automated processing if those decisions might have significant effect on legal rights. And then at the inner governmental international level, at the UN there has been over the last three years a global summit on AI for good which is focused primarily on the application of AI to achieve the UN's sustainable development goals. But there is not really any expectation anytime soon that there will be an international agreement on AI global governance. In the middle category, the multi stakeholder realm, various groups from the tech community, civil society governments have crafted a wide range of normative principles for ethical AI. Examples include the Asilomar Principles, the IEEE principles for ethically aligned design for autonomous and intelligent systems and the EU guidelines for trustworthy AI. And then in addition we've seen some leadership from a variety of private sector companies, tech companies. Google, Microsoft sales force just to name a few that are choosing to bind themselves to internally drafter AI principles. And all of this arguably counts as governance of AI broadly speaking. So, bottom line so far, most of the energy is going into development of new ethical principles for AI. I think a recent number I have heard is something like 260 newly crafted sets of AI principles and we don't see much action on formal government regulation yet and not much activity in the international realm on a global framework. So, to my core point. Why turn to human rights? I believe the existing international human rights framework can provide both the normative and the legal foundation for global governance of AI. Very briefly for anyone unfamiliar with the international human rights framework, at its core it includes the original Universal Declaration of Human Rights from 1948 which is a rich normative expression of the rights that adhere in the human person, and two foundational treaties. The International Covenant on Civil Political Rights and the International Covenant on Economic Social Cultural Rights which are legally binding on states and spell out the substantive and procedural rights of individuals and the obligations of states to protect those rights under international law. And then in addition I'll mention a third piece of the human rights framework of relevance here, which is the UN guiding principles on business in human rights, adopted by consensus in 2011 at the UN human rights council and those principles define the responsibilities of the private sector to respect human rights. My core point is that this human rights framework can do a lot of work for us, unlike the new drawn ethical principles which actors can choose to embrace, the human rights framework is already universally applicable and legally binding and I believe we ignore this foundation at our own peril. So, why is that? What's the risk? Here is what I think is at stake both normatively and geopolitically. We all know we are in the midst of a global battle for dominance in AI technology, but we also need to recognize that we're in the midst of a geopolitical battle over the values and norms that will inform regulation of AI, and guide governance of digitized AI driven societies more generally. The big picture normative and geopolitical debate from my vantage point, it's about whether AI will be used to enhance human dignity and reinforce the liberty, equality and security of people as envisioned in the human rights framework, or whether AI will be used to undermine human dignity and autonomy to restrict human freedom and to move the entire digital world consciously or unconsciously toward digital authoritarianism. I think a global drift toward digital authoritarianism would represent a complete retreat from the human rights vision and also betray the original optimistic vision about the potential of tech and AI to make the world a better place. Further I'm just gonna say at a pragmatic level, if we are actually looking for a global governance framework, I don't think it's realistic to think that at this particular geopolitical moment we have a prayer of getting global agreement on a new comprehensive governance framework that provides anywhere near as rich a basis for assessing the human impact of AI as does the existing framework. And I will note the human rights framework was forged in the aftermath of World War II which was a rare moment of global consensus about the need for a new normative global compact and that context is very important. So, rather than start from scratch and try to develop a whole new set of normative principles, a whole new legal framework and then try to get global agreement on it, I believe we should work with the existing framework, acknowledge where it needs to be, adapted and do the hard work of figuring out how to apply it in our radically transformed digital societies. And here I will say I do think the very important insight already developed through AI specific ethical reflection need to be embedded into human rights doctrine for the 21st Century and I will say, my view is that I believe people working on AI ethics who are unfamiliar with the human rights framework, may be very surprised to learn how much work this framework can do for us. So, here in that regard I wanna identify four particular features of the existing human rights framework that make it particularly well suited for a, as a global governance structure for AI. And I'm gonna quickly take each of these in turn. The first of which is the human rights framework starts with an assertion of human dignity and the centrality of the human person as the focal point of governance. And here in the human person by virtue of humanity they are universal. These are normative commitments that cannot be taken for granted in an AI future. As everyone in this audience knows well, there are many existential questions swirling around about the future of humanity and the relevance of the human person. Will the human person matter as robots and machines take over so many realms of work? How should we think about human agency and autonomy when so many decisions are made for us by machines? How can we ensure safety and security for humans if autonomous lethal weapons are developed? And how should we think about democratic accountability or human accountability when governance actors do not understand the basis of algorithmic decisions they rely upon? The human rights framework can help us address these questions and this starting commitment to human dignity and the human person I believe is the essential first step in moving toward human centered governance of AI. The second feature I will highlight of the existing framework is that it established a broad spectrum of substantive and procedural rights that speak directly to the most pressing societal concerns about AI ranging from concern about the dramatic erosion and privacy associated with digitization of everything. That can be addressed through the right to privacy. Concerns about how algorithmic curation of information impacts human autonomy and agency can be addressed through the lens of freedom of expression, freedom of assembly and association. Concerns about the risk of embedding bias in data that in turn facilitates discrimination under the guise of objectivity can be addressed through reference to the rights to equal protection and non discrimination. Concern about labor displacement and exacerbation of income and equality is covered by the right to work and by the right to an adequate standard of living. Concerns about the threat of lethal autonomous weapons can be addressed through the lens of the right to life and security in persons. And then concern about loss of human and democratic accountability due to reliance on opaque machine decisions can be referred to through the lens of the right to non arbitrary processes and effective remedy. The basic point here is simply that the existing humans rights doctrine provides a substantial basis for policy development to address the wide range of societal concerns about AI. Third feature of the framework that I think is useful to global governance is that in addition to normative principles, it defines the roles and responsibilities of both government and the private sector. The original declaration and treaty framework places the primary legal obligation on governments to protect against violations by other state and non state actors and to also not violate rights themselves. The UN guiding principles on business and human rights lay out the responsibility of private sector businesses to respect human rights, to engage and do diligence processes to assess the impact of their products and services on human rights and to remedy human rights harms. And given the increase and impact of private sector businesses on the enjoyment of human rights, especially in digitized societies, I think this was a really important normative development. Last but not least the human rights framework enjoys a level of global recognition and legitimacy that is extremely valuable. The framework was crafted through global, multilateral and multi stakeholder negotiation. It enjoys status under international law and has been embedded on national constitutions and legislation around the world. It's universal applicability has been established. The bottom line is that human rights provide a shared language that's understood globally and enjoys unmatched legitimacy and I think this combination of features will be almost impossible to replicate. So, I wanna close by drawing you attention to a few notable resources and initiatives and leaders in this space who are getting traction and I do this especially for students in the room who maybe interested in becoming part of the AI and human rights movement. Among governments I will sight the leadership of the Australian Human Rights Commissioner who is doing cutting edge human rights based analysis on government procurement and reliance on AI as well as on the obligation of governance to protect citizens from harmful impact of AI. In the multi stakeholder realm I will site the partnership between the World Economic Forum and Business and Social Responsibility, BSR. They have a fantastic white paper on responsible tech that brings the specific insights from AI ethics analysis and design principles into the human rights framework so that they can work together. At the international level, the office of the High Commissioner for Human Rights at the UN has started a BTECH initiative which is intended to articulate more fully how to apply the existing UN guiding principles on business and human rights specifically to private sector AI applications products and services, and then last but not least I will highlight a couple of private sector initiatives. Element AI, the company is a private sector company represented here today. I see them as leading advocates for multi stakeholder development of human rights based approaches to AI governance across all sectors. And I will highlight also Microsoft combined with Article One advisors is leading the way in developing human rights impact assessment mechanisms specifically for AI. And these initiatives make me feel like we're on the right track. So I'm somewhat optimistic that the global community will increasingly turn to international human rights as a foundation for global governance. Final word takeaway message I wanna leave you with, if we hope to establish the human rights foundation as the basis for global governance of AI, we need to see human rights and AI ethics as synergistic. Not as competing approaches. The human rights community and the AI ethics community need to work with and learn from each other and I will say the many AI specific ethic insights that have already been developed need to be embedded in human doctrine. Last point is we also need much more cross disciplinary, cross sector collaboration and education. I think CS people should be getting much more exposure to the existing human rights framework. The human rights community obviously needs much greater exposure to how AI works. Product designers need to work with the human rights community to develop this emerging concept of human rights by design and I wills say that I think this Stanford Pi event is a great example of what needs to happen and I am really glad and proud to know that Stanford is leading in this AI governance realm. (audience applauding) - Thank you Professor Donahoe. Now let's welcome Mrs. Jessica Newman. (audience applauding) - Okay, hi everyone and thanks so much to Stanford and Pi. This has already been an incredible day and it's really a pleasure to be here with you all. So I am Jessica Cousins Newman and I'm a research fellow at the UC Berkeley Center for Long Term Cyber Security where I'm excited to lead the new AI security initiative. I'm also an AI policy specialist with the Future of Life Institute and an advisor with the Future Society. So, in this panel we're discussing the governance of AI. So, to build off of the framework that Eileen mentioned, I wanted to share some thoughts about the word governance. We use this word to mean a lot more than merely regulation which we already heard a lot about this morning. We include other political and social phenomena such as standards and advocacy. We're not only concerned with laws but also with norms, power and language. And importantly this means that we're not only concerned with what happens after technology is built with regulating something that already exists, but also in everything that contributes to its development in the first place. Questions about what is built, why and for whom. The picture you see here highlights a facial recognition technology identifying visitors in a display at the Digital China Exhibition that happened in Fuzhou in May of this year. We know that AI does not emerge out of a vacuum. Technological developments are shaped by business models, by public discourse, national values and structural power dynamics. Every aspect of AI development and use from who was in the room to what tools are built to what training data is used to how to systems are tested and how they're monetized, all of these inputs in context are shaped by human decisions and historical realities about power and technology. This is always true for technological developments, but it's particularly noteworthy for AI, and that's because we're creating computer systems that automate human decisions and practices. Which means that the values and the biases that we encode into those AI systems are going to be amplified around the world. And if we decide to just wait and see where market incentives lead us, we might find ourselves in a world where few of us wanna live. So, all of that makes governing AI hard, but it also means that there are numerous opportunities and avenues through which to create change. It's not only the technologists and the lawmakers who have decision making power. It's also the journalists, the tech workers, the advocates, academics. All of these people can also change the discourse and practices around AI. So, I think this is really important. We're facing a general purpose technology that is automating processes of communication and decision making that scale to millions of people. It's influence over our lives is already substantial and it will be significantly more so. So, everyone should have a say over what happens next. Here are just a handful of recent headlines that give a sense of what we're up against. AI systems have inherent flaws including bias and a lack of robustness. They are being used for purposes of cyber crime, and AI competition may be aggravating geopolitical tensions which could seriously threaten global security. So, for the rest of this talk I'm gonna try to shine a light on what we're doing so far to govern this technological revolution. My question is how are we shaping the future trajectories of AI and are we doing enough? I'll share a brief analysis of how the AI governance landscape is unfolding using a comparative lens to highlight gaps and opportunities for action. Of course numerous companies, organizations and governments have made strides over the past few years. We now have countless strategies and principles which you've heard about today to guide AI. But how do we make sense of them and relate them to the important issues that we face? So, this is a framework that we developed at Berkeley to help assess and compare what these strategies are really doing and where they're focusing their attention. The framework is called the AI Security Map and it includes 20 policy priorities for the development of AI across four different security domains. These include digital and physical, political, economic and social. Each topic describes a priority intended to help mitigate an issue that could cause significant harm to people or cause systems level destabilization. So, these are some of the most pressing issues in the global security landscape of AI, which are top contenders for our governance efforts. So for example within the digital and physical domain, the map includes the priority of having AI systems that are robust against attack. And the responsible use of AI in the military. The political domain includes issues such as mitigating technological labor displacement and reducing AI induced inequalities. And the social domain includes issues such as privacy and data rights. I won't delve into these issues now which many of you are familiar with, but I'll highlight how the AI Security Map can be used to provide a visual representation of high level synergies and divergences among key actors in the AI ecosystem. There are of course limitations to this framework. Please keep in mind that the maps only show a snapshot of priorities as indicated in single document or initiative and should not be interpreted as fully representing the priorities of an entire institution or government. But to take a first example, let's consider Google's AI principles. These were published in June of last year and they got a lot of attention. When you review the topics that are covered, this is the map that you get. The principles prioritize the creation of safe, reliable and robust AI systems that are appropriately transparent, fair, non-harmful and in line with international law and human rights. They seem quite comprehensive. But then again if you care about the holistic role in AI and society, you'll notice that the map does have some gaps. Google's AI principles do not talk about issues related to the political security domain such as the spread of disinformation or the opportunity to support government services or checks against surveillance. Moreover the principles don't talk about issues related to the economic security domain including job displacement an proving educational resources, preventing growing inequality or supporting market competition. As you can see Google's priorities for AI as represented in these principles fall solely within the digital and physical and the social domains. And these gaps aren't that surprising for a tech company. But they do highlight a reality that industry leaders are probably unlikely to address the full set of global security challenges posed by AI systems. Of course that's not new to many including those within Google. So, a white paper that the company published earlier this year pointed out some contentious uses of AI could have such a transformational effect on society that relying on companies alone to set standards would be inappropriate. Not because companies can't be trusted to be impartial and responsible, but because to delicate such decisions to companies would be undemocratic. The gaps that we saw on the map from Google suggest that governments and civil society also have a key role to play in the responsible governance of AI. So let's see how they do. On the government's side there has been a surge in attention to AI over the past few years. This graph which is from the AI Index Annual Report shows the number of times that AI or ML were mentioned in the U.S. Congress. As you can see attention was relatively stagnate until 2017 and then spiked dramatically in 2018. And a similar trend can be seen in the number of mentions of AI and ML within the UK Parliament. In fact, as you've heard more than two dozen countries have no released national AI strategies. So, China was one of the first countries to do so. In 2017 the state council released the new generation AI development plan. And this map gives an indication of the areas of focus included in that report. As you can see it was fairly comprehensive and it covered everything from reliability and international collaboration to educational needs, transparency, ethics and sustainability. However the strategy also exhibited some gaps. It didn't address issues of AI robustness, inequality, disinformation or surveillance. But it turns out that these and other gaps are common among the world's governments. Here is the map for the United Kingdom. Based on the government's AI sector deal which was published in April in 2018. As you can see significantly less is covered. The document focuses on AI R&D, training and education, to some extent on transparency and privacy, but it doesn't delve into AI implications. It has no focus on issues relating to safety or reliability. There's actually quite a similar picture for Canada. Looking at their pan Canadian artificial intelligence strategy which launched in March 2017. If you look at the map for Singapore which is based upon AI Singapore launched in May 2017, you'll notice an even more targeted focus with just a handful of these issues mentioned. Interestingly if you compare that to the artificial intelligence technology strategy also from March in 2017 from Japan, you'll see quite different priorities. In this case the majority of attention is in the digital and physical domain and much less attention is on the societal implications. In fact, no true countries exhibited the same set of priorities across the board. So, this slide shows a heat map based upon the priorities indicated in national AI strategies and policies from 10 countries around the world. These included Canada, China, France, India, Japan, Singapore, South Korea, United Arab Emirates, the United Kingdom and the United States. The lightest boxes show topics that were only mentioned by one or two out of those 10 countries. So, as you can see issues around disinformation, surveillance, inequalities and sadly human rights are relatively neglected across the board. The need to protect against the malicious uses of AI was also under represented in these documents. However there was widespread agreement about the need to promote AI R&D, update training and educational resources and improve government expertise in AI perhaps not unsurprisingly. So, out of all of the AI principles and policies that we reviewed using this framework, there's actually one initiative that managed to produce and entirely full map, and that is the European Commissions high level expert group on artificial intelligence, which is comprised of 52 representatives from academia, civil society and industry and if you look at both of the groups published documents, so their ethics guidelines on AI as well as their policy and investment recommendations, you'll see that they do address every single one of the topics as shown here. And that points to one of our takeaways from this analysis, which is that enabling multi stakeholder initiatives however painful that can be at times really does help to reduce blind spots. So that's the first takeaway here. If your goal is to have a more comprehensive policy, increasing multi disciplinary and diversity is critical. Taking advantage of multi disciplinary as well as community expertise goes a long way to support responsible AI development that address more comprehensive implications of AI. The second takeaway is that industry self governance is unlikely to fully protect the public interest when it comes to powerful general purpose technologies. So it is encouraging to see that there is significant effort being made from those in government as well as from civil society. The third takeaway is that despite the differences you saw between nations approaches, there are also numerous synergies. So there are many opportunities for governments and organizations to coordinate internationally. This is likely to be increasingly important as many of the challenges and opportunities from AI do extend well beyond national borders. And admittedly this recommendation can be hard for national governments to do on their own. There of course also have to consider issues of national competitiveness, but left unchecked that could incentivize a dangerous race to the bottom which would not serve anyone's interests. So the role of inter governmental initiatives is likely to be really valuable here to support in this recommendation. For example the OACD AI recommendation and the international AI policy observatory that they are now developing are really encouraging examples of this. So 42 countries including the U.S. as well as some non OACD members have signed on to the OACD AI principles already. So this is the first intergovernmental AI standard that we have. This was later supported by the G20 which does also include China. So, I see international coordination on AI as not only critical but also possible. And finally the last takeaway is a reminder. AI will impact everyone so everyone should have a say. It's really important at these relatively early stages of AI governance that we make the effort to hear from people who may otherwise struggle to have a voice. So, thank you. I look forward to your questions. (audience applauding) - Thank you Mrs. Newman. Now, let's welcome Mr. Cihon for his presentation. (audience applauding) - Okay. So, with the first two panelists we can see that the topic of our panel today of global AI governance is quite broad. It's important to think about the actors involved but it's also important to think of the institutions and in particular I would like to draw your attention to questions of institutional design in thinking about AI global governance. So the past few years have seen a flurry of policy making on AI at the global level. This flurry is fragmented that with governance spread across multiple institutions that have dispersate participation among nations, states, and dispersate procedures. Adjust our attention to the OACD principles which are very important to highlight here in their passage in May and subsequent adoption by the G20 which is important to highlight because that represented an example or an instance of China, Russia and India among the other members of the G20 explicitly endorsing principles that have quote, "AI actors should respect the rule of law, "human rights and democratic values," and this is an important starting point. In 2018, France and Canada proposed the International Panel on artificial intelligence which was inspired the intergovernmental panel on climate change and would have produced expert analysis to enable international collaboration and coordination and the development of policy as well as identifying policy gaps in need of prioritization. This was vetoed by the United States in August at the G7 meeting however subsequent negotiations are ongoing for the launch of GPI which would preform similar functions within the OACD. The UN has numerous initiatives related to AI and I can only highlight a few for you. The high level panel on digital corporation earlier this year offered advice to the Secretary General that call for an open form on institutional reform at the UN to better handle challenges including AI. The chief executives board for coordination has approved a plan for AI capacity development within the United Nations and among countries in the global south. And as Eileen noted since 2017, the International Telecommunications Union within the UN has hosted a recurring event on AI for global good. It's also important to highlight that since 2014 even earlier the parties to the convention on certain conventional weapons have been discussing lethal autonomous weapon systems. So, these initiatives that you see here are not exhaustive but they give us a sense of the active state of play today and it's important that as I said that this is fragmented across institutions. At this point it's unclear if this institutional proliferation is a good thing or a bad thing, but one thing is clear. We're actively in a period of institutional development today. And this development responds to numerous opportunities and challenges, many of which have been raised today. So I can go through this fairly briefly. But there is an important push on thinking through AI development and diffusion through the global economy. McKenzie for instance estimates that by 2030 global output will be 14% higher due exclusively to AI and to put that into perspective, that's the size of the entire U.S. economy earlier this decade. Beyond contributing to growth, the benefits to tackle the UN's sustainable developmental goals on healthcare and poverty, education, sustainable development, regards to climate change are tremendous opportunities. But there are a number of challenges worth highlighting. Some of those fall within the umbrella of political economy. Seven of them attend largest publicly traded companies today are leading tech firms and they're all leading the way in AI research and development, but they're only headquarter in the United States and China. Benefits are not evenly distributed and absent intervention this will continue. In the medium turn, labor automation will pose significant challenges. Furthermore this concern that AI could enable robust digital totalitarianism. We're already seeing visions of this possible future in the people's Republic of China the use of facial recognition and tracking applications to systematically repress the weaker minority. Military uses of AI including but not exclusively lethal autonomous weapons also pose significant risks. They can threaten to destabilize geopolitics and could result in inadvertent or an unmanageable war. And it's important to also highlight that the implications for artificial general intelligence both in the pro and the con are so large that it warrants proactive thought. Even if we sign and arbitrarily low probability of achieving such technology in our lifetime say of 2%, it makes sense from a risk mitigation standpoint to be proactive to think about global governance. An expert opinion puts this probability probably much higher at a 50% chance of AI systems achieving human level performance on all relevant employment tasks within 45 years. All of these risks and opportunities and many that I haven't discussed are reflected in the ongoing flurry of international policy making today. But this is important to note that this development is underway but the research in the space on global governance has not kept pace. It's small. There's a small literature and it's growing but it's too early to speak of a consensus on governance institutions or even question its institutional design. There is several pieces that I'd like to raise for the audience here today. Allan DaFoe, the Director for the Center of the Governance for Artificial Intelligence has produced a wide ranging research agenda on AI governance that is certainly worth reading. Steven Kay and Sean O'Haggerty published earlier this year a nature in Mission Intelligence an important piece that raised the significance of path dependencies in the development of the technology and bridging short term and longer term concerns in AI. So this is important to emphasize that nearly all decisions being taken today, whether they are questions of corporate development, civil society engagement or government regulation they all have the possibility to set precedence that will shape the development of the technology going forward and it's important that we have our eyes open to these possible path dependencies. Martina Coons and Sean O'Haggerty also offer an existing overview of international law that applies to AI which is an important step to consider before offering further work and although the field if fairly new this hasn't stopped numerous design proposals in addition to the ongoing work that they hired earlier from being suggested. Now it's good that this literature is growing because one can say that informed design matters but it's important to note that policy makers are rushing ahead of this literature and we want to see them consult the ongoing academic research in this space. Now a key parameter that is implicit on both the ongoing efforts and previously literature is that of centralization. Now when I say centralization I mean the governance of a particular issue would lie within the authority of a single umbrella organization, and we can think of this as a spectrum between ideal types. So, closer to a centralized regime for trade, you'd see example of the World Trade Organization and closer to a fully decentralized regime, you would see the proliferation of multilateral environmental treaties to tackle particular issues. Now, on the environment side there have been questions of the effectiveness of this approach and a number of academics have pushed for a world environment organization that consolidate these agreements. More broadly we see a variety of centralized governance regimes ranging from issues that may be quite contentious like nuclear to the mundane dealing with coffee. And so this begs the question of what of AI? To catalyst thinking on this question I've written a paper together with Mattheus Moss and Luke Kemp, two researchers in political economy and international law. Now we consider the history of other global governance regimes ranging from environment to trade to he internet and from this history and in established literature we draw out a series of considerations on a trade off between a more centralized or decentralized regime. And I won't have time to go into all the details of these trade offs however I'd like to flag that we'll have a paper draft forthcoming to dive deeper into this. So, I would like to highlight a couple of these. For one, centralization can support political power of international governance. And this power can be of a relative sort in the sense that it can be relatively powerful over regimes in the way that the World Trade Organization achieved dominance on trade initiatives within previous environmental treaties. It could also be a functional power in the way the OACD has steered the course to retain its authority over international taxation governance despite efforts from other organizations. In the context of AI such power can be quite useful in order to ensure that new technical developments and foresight analysis can ensure the timeliness of international negations on new policy problems as they arise. However, on the other side centralization can increase institutional brittleness. And this is perhaps not surprising. In so far as a institution is unable to adapt to changing circumstances whether that be fundamentally due to technology or due to politics. We may see coordination or governance fail and this probability is higher when you're speaking of fewer institutions as you'd expect. For example the International Labor Organization has struggled to adapt to changing norms, global governance with its structure and participation that does not include civil society. Centrality while similar in context of brittleness, raised the concern of regulatory capture whether that be AI corporations or another stakeholder group taking undue influence within governance if there's only one institution. I'll highlight just one more again for time, but I'm happy to jump into these later. Centralization is also on the procot side useful for improving efficiency and inclusivity. So with a proliferation historically of multilateral environmental treaties, it's been quite difficult for countries in the global south to staff negotiations around the world. They've actually in cases had to send ambassadors without technical expertise to technical negations. Similar difficulties can be be seen with civil society being unable to fund participation in multiple for a same time for instance. Now, today given the expense of machine learning expertise, imagine having a central repository of this expertise within the UN may constitute an economy of scale. And we also today see the difficulties of a diffuse governance network in the sense that we have a two day Stanford conference on AI policy while simultaneously in Paris the French government is hosting a forum and people are forced to choose between the two. It's a good example of this difficulty. Now in conclusion for the sort of central takeaway of our sort of analysis, the centralization offers real benefits but also real dangers to global governance. And although previously global governance has tended to respond retroactively after harm has been witnessed, it's very encouraging to see that there's a lot of proliferation institutionally today, we hope that our work can catalyze discussion on the questions of design and particularly centrality. So, today we have this sort of fragmented status quo and we want to hold off judgment to say whether or not this is truly a good or a bad thing. However there's an important element to actually follow the emergence and development of this regime and that's an element that we go into in the paper as well. Proposing a few ideas here that we flagged for monitoring the emerging regime, in particular to look for conflict between institutions, coordination between them and the ability to which they're able to catalyze and fill governance gaps internationally. This could present interesting research for academics or also for the OACD policy observatory. And there are a number of different methods one could think through. One interesting approach maybe to use, natural language processing, fact checking techniques to look for contradictions between policy outputs from particular institutions looking at meeting minutes or looking at the principles themselves. We hope that future academics research will consider the importance of centrality but we also understand that many other stakeholders can benefit from this consideration. In particular it could help and answer questions like does the U.S. indeed wish to pursue centralized governance within the OACD as an explicit policy? Does the engagement of element AI and DeepMind among others at many forum represent a policy of pro decentralization? And should advocacy groups like the campaign to stop killer robots way the pros and cons of pursuing engaging strategies to ban lethal autonomous weapons at multiple forum? We hope that our considerations can inform stakeholders answers to question like these. And so I wanted to end on a meme and I thought I'd spare us from the "Terminator" imagery and offer us a brief overview which is important probably considering AI global governance we need to think through the particular institutions at play and a key question that has bee under explored to this point is how we go about designing them most effectively. Thank you. (audience applauding) - Thank you Mr. Cihon. Now last but not least let's welcome Mr. Phil Baldwin for his presentation. (audience applauding) - Good. So, we've heard mentioned in the previous talks a number of frameworks. One of them has came up twice already in the first two presentations here, and it's the framework for trustworthy AI that was proposed by the European high level expert group earlier this year. So what I propose to do here today is a slightly deeper dive into that framework that covers a lot of things and that actually is built on human rights. So building upon human rights and specializing them for the purpose of AI. But before I get started I wanted to give you just a bit of context about Element AI and why we care about questions like that so much. So, about Element AI, we're a product company. We build AI products and since our inception three years ago, we've been closely connected to academia. We've been investing in research at all maturity levels which means that internally at Element AI we do fundamental research and publish papers, we do applied research and we go all the way to what we call machine learning engineering and turning this research into product or components that are useful in companies around the world. And we do that for a wide variety of industries. We not only do it for the classical big tech companies. We do it also in the financial sector and manufacturing and retail and logistics and others. So, this is a big challenge. This is a broad challenge wide and deep and we don't believe we can do that if we're not doing it within a thriving and collaborative system. And that's why since the beginning we've been actively nurturing this ecosystem in our home in Canada where we're based, and every year since we've started, we put out a report which we call the AI Ecosystem Report in Canada that highlights the different stakeholders and we're looking at academy colabs at startup companies, large corporations. We're looking at private investors as well as the governments and actors from civil society that are playing in the Canadian AI ecosystem. So this is a very complex map. You can see here. But if you're interested about learning more of the Canadian ecosystem that we're building at Element AI on I invite you to check out the white paper we published very recently on this ecosystem. So, in addition to this thriving ecosystem that we believe we need in order to be successful, we also think that an important competitive advantage we can benefit from as a transverse AI company is a strong legal and ethical framework that will encourage the development of human centric AI. We think that without that there's a big risk that we will lose the trust of the people we're building these products for, right? So, we need trustworthy AI and we are investing as a company in helping it come forward through a lot of these multi stakeholder engagement. And one of the initiative we've engaged in is this European Commission high level expert group on AI. In fact, our CEO, Jean Gagne, the only non European person to sit on that expert group. And as was mentioned before the expert group was very diverse. It includes 52 experts from academia, civil society, research and corporations like ours. The goal of this group, the very clearly acknowledged goal of this group is to make trustworthy AI their foundational ambition. They care about building trustworthy systems. And the way they want to do that is by relying on a holistic and systemic approach. And the development of deployment of AI system is recognized as concerning not only the technological aspect but the very complex socio technological system that needs to be in place in order for this trustworthy AI to exist. If you look at most products we use today, not only the AI products we use but all the products that we use in our every day, they typically come with guarantees and responsibilities of use from the product makers. It means that they're, but these things are not necessarily easy to incorporate into AI based products for a number of reason. Actually AI products differ in many ways. Let me give you a couple here. So first of all, AI based products performance is highly dynamic. Whereas traditional products tend to be based on various tricked rules that can be followed in a principled way, AI based products are based on training data and they change their effectiveness changes based on the situation. So very highly dynamic performance. Another difference is the way in which accountability occurs in AI based product. It's much more complex because a system behavior is not always again clearly attributable. You've got these systems that have been trained on data. It's unclear who's responsible for which part of it. So, when you want to clearly define the expectations between the stakeholders, this becomes difficult. You can think of what are your responsibilities or the accountability within an AI product of the user, of the vendor, the designer, the engineer, the researcher that played a role in that product. So it's much harder to define than for a traditional product that has been based on very strict rules. If you look at all of these issues, these challenges-- - Hey, hey, will you-- - Your slides aren't showing. - Your slides aren't showing. - Yeah, I only have three slides. So, I'm gonna talk a lot over each one of them. So, yeah if you look at the danger of these systems is that because they're different, because they're new, because accountability is hard and because their performance is highly dynamic, there's a real danger that this will cause a lack of trust from the public from society in these new technologies. And that's why the high level expert group has been working on ground rules that could help commonly acceptable standard that could reduce the high transaction cost that risk existing right now if we have to go into that trust transaction for every new AI product that comes forward. And so in that regard the guidelines are taking an important step to move us from trustworthy AI in the conceptual realm to trustworthy AI at the practical level. So the guidelines are incurred in human rights, and the first thing that the framework proposes are four principles that are derived from these rights and that are particularly relevant in the context of AI technology. The first one is a respect for human autonomy. This principles says that we should not coerce the seize, manipulate, condition or hurt humans in any way using these AI products. As you can understand this is pretty relevant today given some of the issues we've seen with existing products that are based on AI and that we use everyday. So, in a sense what this resect for human autonomy is about, it's about leaving meaningful opportunity for human choice every step of the way in these products. The second one is prevention of harm which should exist with any products we develop naturally, but with AI product it's not only about physical harm but it's also about human dignity and human mental integrity. These products are being so important that I'm playing our behaviors or being aware of our behaviors to such an extent that there is a real risk they might impact human dignity and mental integrity. Fairness is another important one, where the high level expert group recognizes that fairness can have many interpretation and they go through a couple of them. But interestingly they distinguish between substantive fairness and procedural fairness. Substantive fairness is defined as the equal or just distribution of the cost and benefits of these new technologies. Whereas procedural fairness highlights the fact that we should have the ability to context and seek redress when these products do not behave according to their expectations. Fort is explicability. Explicability is seen and it's a complex and challenging question when it comes to modern AI system for technological reason in part, but at least these products should be transparent in the sense that their capabilities and purposes should be made very clear and communicated. We should know what the purpose of the product is as user and sometimes it's not obvious with a lot of AI products we use today. The decision these products make as they go should also be explained to those that are directly or indirectly affected by these products. And they should be explained in a way that understandable by the stakeholder no matter who that stakeholder is and their level of education or understanding of the technology. Finally, we should take careful consideration to challenge of the black box AI systems that you may have heard of right? These new deep learning models tend to be black box-ish in the sense that the systems itself is so complex that if we try to explain it using simple rules or simple explanations, we will not succeed. But it doesn't mean we shouldn't have explainability. It should mean we should seek other explicability measures like traceability or detectability of these systems. So, the framework goes one level deeper and proposes seven stakeholder oriented requirements and these requirements can actually be implemented. They can be implemented both through technological advancements or technological requirements or through non-technological means given processes and organizational structures around them. These requirements according to the expert group are the ones that should be government regulated. They should be government regulated in the same ways we are used to seeing standards be government regulated. So, I'm gonna name the seven that are here and I'm gonna deep dive into three of them that I think are particularly interesting in the context of AI. So the seven are human agency and oversight, technical robustness and safety, privacy and data governments, transparency, diversity and non-discrimination and fairness, societal and environmental wellbeing, and finally accountability. These should all play a role together and impact one another and there should be tensions at time between them that should be resolved. But let's dig a bit deeper into human agency and oversight. So, according to that framework, AI systems should act as an enabler to a democratic flourishing and equitable society. Which means that they should allow users to make an informed decision regarding the outfit of these systems by providing tools to comprehend and interact with these systems. The kind of tools that should be provided are those that would allow a user to self assess or to challenge even the system on the decisions that it makes. It should also allow users to make more informed choice in accordance to their goals. And this is one I want to stress in particular, 'cause it's unclear that our technological environment today is aware of our goals. Very much aware of our behaviors, the behavior we adopt when we use them, but our goals as humans are typically very different from the behaviors we will adopt. And therefore what the group recommend is that AI systems we make, we should be very aware that they can't harness subconscious processes in their users including an unfair manipulation of the users, deception, hurting users or conditioning them. Let's dive into a second one. Privacy and data governance. So, privacy is a fundamental right that is particularly affected by AI systems as you all know. Being aware of building privacy respectful systems would mean including information that is initially provided by the user and making sure we provide privacy on that information but not only information you come forward with, all the information that the user will create as they interact with the system. These too should be taken into account and the privacy of both of these should be respected. Another aspect of data governance that is particularly important is making sure the quality and the integrity of the data is really high. So this include being aware of social biases, inaccuracies or mistakes that could happen in the data, that could exist in the data, as well as potential malicious attack that could exist in the data, the initial data or that can be introduced as the system is built. Which means that in order to respect privacy in the data governments these systems should put in place the planning, the training, the testing and the deployment processes that would allow for this privacy to be respected, sorry for privacy and data governments to occur correctly. Third and very important topic is transparency. Transparency in the context of AI means according to this expert group that the datasets and the data transformation processes should be traceable. This would include the data labeling, the avenue used et cetera. Tractability is required because this is how we can allow indelibility of these systems. So essentially, transparency might not mean that we understand every step of the way, these system being black boxes and complex, but they should be identifiable. Explainability, another aspect that might be difficult to implement but that's important would allow a human to understand specifically or to trace specifically the decisions that are made about him or them by the system. So, this would include the recognition that there might be a trade off between a system's explainability and its accuracy. This is an important one. It might be possible to build a more explainable system at the cost of accuracy and we should take that trade off into account as we design these systems. Finally and interesting one is that users should have the right to know they're interacting with an AI system. This means that transparency would force such a system to not be, not represent itself as a human. We've seen that in the past in some systems and this framework actually clearly stipulates this shouldn't happen. The third chapter of the framework proposes a very practical pilot version of a trustworthy AI assessment list, and they invite anybody who's building AI products to use that assessment list as the pilot. This include very practical questions around the seven requirements that were highlighted here. And these should be assessed by different stakeholders within the corporation including the board and the management team, legal departments, products and services departments, et cetera. And the idea here of this pilot is the expert group is looking for a feedback on how well this assessment list work in order to build something that might eventually be used as a practical framework to build trustworthy AI. So, to conclude what I would like to highlight here is the need, the importance of these comprehensive multi stakeholder consultations. In particular the next step in that process would be such a multi stakeholder consultation and the impact of AI and human rights and everything that's necessary for regulators to understand how these guidelines can be complemented by legally bending and forcible protections. Thank you. (audience applauding) - Thank you to all of the four speakers for their amazing presentation. And starting from right now we're going to have a panel discussion on some key issues that we identified from their speeches. And we will also be taking audience Q and A as we move into several questions of the panel discussion. Thank you all again. The first question that we would like to ask is since a lot of you has mentioned right now compared with several years ago the problem we see in the field of AI governance is not a lack of rules. Rather it's fragmented as Peter mentioned, fragmented regulatory ideas. So, how do you see the multitude of fragmented AI governance structures? Is it an opportunity or is it more of an obstacle and should we even try to aim for a more centralized government structure? - I-- - Should I go ahead? - Yeah. - Okay, great. Great question. I think what we've heard is it's a mixed bag. There are aspects of the proliferation of all these new sets of principles that are very positive in the sense that around the world we see the best and the brightest gravitating to this subject and a lot of creative energy that's very exciting and that's excellent but as we've also said, there is a concern about undermining the existing global normative framework that we have and I am very concerned about that and I think the proliferation of all the different principles can unintentionally figure in to the erosion of confidence in the existing framework and that makes me nervous. Two other things that I will mention is that you just said that we've had a proliferation of principles. I don't think we've had a correspondingly adequate development of what do those principles look like in practice? So we're still at this conceptual level and we're not yet breaking it down into what does it look like to actually live up to those principles? I will also say that in the existing global human rights framework, we have this set of principles that are well understood. There's international doctrine about what it looks like in practice but they don't necessarily require homogeneity in implementation. You do get to factor in context, and so I think we need to keep that in mind as well, so that there isn't a sense that the principles are being developed in one place and you're imposing your vision on us. That there is some flexibility in how you make those principles manifest but there is also a bottom line and this is where I think the human rights framework is so valuable I telling you you have to be concerned about free expression, privacy, the right to life, the right to security, equal protection, non discrimination. All of those core principles have to be respected even while there is a cross cultural diversity in what it looks like in context. - I can just respond to that point. So I think we're at an early enough stage in this where diversity of ideas is really valuable and I wanna see more of that which is not to say that coordination isn't traditionally useful and beneficial and needed. We're seeing good examples of that like such as the high level expert group and the work at OACD. In terms of the issue of implementation and in terms of how do we actually get from principles to practice which is where a lot of people are focusing now, I think California is a really interesting example with this since we are here today. California did endorse the Asilimar AI principles. So that I think is a beneficial statement of caring about, that we see the trustworthy and beneficial development of AI. Not just the development of all costs. But the additionally we have seen a lot of regulation come recently from California. We have the CCPA, our data privacy law that's going into effect in January. There are two laws surrounding deep fix. So one about, that prohibiting it in the upcoming months towards an election and also giving people who are represented in a deep fake in a sexually explicit way the right to actually sue on their behalf. We have a law about facial recognition for law enforcement. And other examples. So I think we are actually a good case study of some early efforts to try to implement some of those principles. - Stone, I'm happy that you asked this question 'cause it sorta gets to the heart of the paper that I'm tryin' to raise. I think it's too early to say as Eileen's saying that we have proliferation principles then this question of as these principles become sort of realized and practices as a very important element to follow. Hence, this emphasis on monitoring the regime over time. I can give you an example I guess of a point where we might expect quite a bit of conflict in the market standards context. IEEE and ISO/IEC are two distinct organizations both pursuing market standards for AI and so this is a very good example perhaps of where we would expect a conflict perhaps. But in practice the two have established liaison relationship to follow each others work quite closely, and so this will be an interesting element to follow if that type of relationship will be emulated elsewhere. Hence the need to sort of wait and see, though I think it's important to emphasize that we don't want to wait too long on the principles. OACD sort of contextualize their principles by describing their previous work on privacy but those sort of came to fruition in 1980 and I don't think any of us would say that they were realized and practiced soon enough and so it's important to note that the principles are there but monitor and perhaps catalyze the development. - And I don't have much to add but I want to stress Eileen's point because I think there is widespread recognition that AI technology is special or different. There's something there that wasn't there before. But it is different to the point where we would need entirely new set of principles? And I think the answer that the High level expert group gives is pretty interesting saying no. This can be rooted in human rights. We just have to understand how these human rights need to be specialized or understood in the context of the new challenges AI technology has. So, what I think is missing now is some form of more global leadership to try to align some of these initiatives. A lot of really good ideas out there, but I think we've reached a point where we actually need to see some government or non-governmental organizations step forward and try to have some leadership in this collision or trying to make all of these work together, and in my opinion this High level expert group is actually showing the way in an interesting way and should probably look at the work that being done there. - Thank you. I'd like to jump off the point about from conceptual to practical and also about global leadership that Phil mentioned. So, if I think we are having the consensus that multilateral cooperation is something that we wanna pursue. If that's the case how much multi lateral effort take into account the wide variation across countries in their risk appetite, in what areas they try to focus on as Jessica mentioned, how might we go about doing that? - Well, I mean, yeah, I think recognizing the importance of diversity is key and this framework should focus on the human rights as interesting framework that should be recognized widely but then on the specifics of the implementation should have enough freedom for diversity to be expressed. - Yeah. I think we shouldn't assume that there will be the same principles kind of universally around the world. Particularly when it comes to regulation there are real trade offs that have to be made and not every place will wanna make the same prioritization and trade off in how they wanna manage these technologies. I mean just for an example if you live somewhere that is more dangerous you trade off with public safety versus privacy might look a little bit different. We shouldn't sorta assume that one prioritization is gonna work universally in different contexts. But to the point of supporting more global governance, we really do need to see more centers of gravity. So not just in the western world, but these kinds of conversations happening in the global south and more integration between what is already happening there and what we're doing here. So that's kinda the way to I think push the needle. - So, I would articulate it slightly differently which is a little bit more what Phil was saying. I do think as it relates to human beings we have the principles we need and we need to be very careful not to erode commitment to those principles. Under the existing human rights framework in a rule of law context, there is always and there always has been a balancing of the rights and the tension between the rights. Free expression and privacy, democratic participation and free expression, all public safety, privacy. There has always been the necessity to sorry of optimize for the values you choose and so I always am careful to say we don't need new principles as it relates to human beings. We do need to allow new modes of optimizing for the various principles in this digitized AI driven context, and also cross cultural variability depending on context. Both of those things but it's not so much new principles. Now, we'll just clarify even further, some of the principles that are being developed aren't so much focused on the human person, they're focused on the AI itself, and so I think the safety, robustness, those kinds of things are a different kind of principle, that you're right maybe emphasized in different ways cross culturally, but I hold on to that idea. We have universal principles as they relate to human beings and we need to hold onto those. - And to that point it's promising that the G20 has endorsed their own AI principles that draw heavily from OACD's with that particular emphasis on human rights. And again the question is to what extent this will be implemented in practice and of course in that implementation you need to take into account the local context. I think there's an interesting sort of piece in what your raise Eileen of emphasizing that there are sort of the human centric principles and its development and deployment but then there are also important emphasis on sort of the technical particularities of technology and safety as an emphasis is quite shared globally. But I think in this sort of vein it's important that we come back to this context of diversity and multi stakeholder involvement knowing that there is quite a bit of history thinking through human rights principles and governance in particular context but when it touches sort of technical questions it's important that people at the cutting edge of the research themselves are also at that table and having that kind of contextualize conversation to realize both the principles and practice and then further their development and what we're seeing here today and other stakeholder groups actively is promising on that. - If I may add just a little thing about the challenges of diversity when you think about the global south is for sure there might be no differences in principles but maybe in the set point and the tension between these different principles, but something that's very real is the state of technology right now is not uniformly distributed and one of the challenge that exist and that is one of the, because of this diversity, is not every country especially if you look at the global south is at the same AI enablement level today, right? Access to technology, access to the internet, the sign, quality of the data sets that they have to solve their problems might not be the same. So that's actually one challenge we will have to take if we care about fairness of these new technologies. - And relatedly I, to some degree the proliferation of principles I don't see as kinda reinventing the wheel every time. I think there's a lot of organizations looking inward and saying okay, we're developing these technologies, we know they're having these impacts in the world, what is responsibility in our context for our consumers in our market look like? So, I think them going through that process and putting out a public statement of this is what we care about, that matters and every single company could do that and it wouldn't necessarily be reinventing the wheel or moving away from the human rights framework. It might just be a sort of additional signaling to their constituency that they care about these issues and are taking them seriously. - Totally agree and I would just say that they could do that but they could also-- - Yes (laughs). - Explicitly do human rights impact assessments. - Yes. - In conjunction with there, or committing to their own self developed principles and applying them to the products and services that they're providing. - We will ask a final question and then we'll open the questions to our audience. So the final question, I like to tie the two themes together. One is about the industry or private sectors self assessment. The other one is the global self or companies that are located in other countries, other than Europe and North America where we have a lot of regulations already. How much government or public sector work with and collaborate with private sector and civil society to get their buy in and corporation within global governance if we're talking about it in a global sense? - I think that's easy in the sense that I think everyone has come to recognize governments can't do it alone, private sector can't do it alone and that the public has to be brought into this conversation to develop trust. And so I think everybody is clamoring for a multi stakeholder approach to policy development and I don't think anybody has to go kicking and screaming anymore. Governments don't adequately understand the technology to do it without the private sector or technologists. Technologists don't adequately understand the governance realm, the human rights responsibilities, et cetera and generally speaking I think if we are going to capitalize on this technology it is incumbent upon us to make sure the public really has a basic understanding of what does it mean to live in an algorithmically driven society? And how are we being affected? And I think we really risk much greater social political disruption if the public doesn't understand the basics because they're gonna be dramatically effected in their autonomy and agency and it's not gonna be a pretty picture if the public isn't onboard and understanding what's going on. - Yeah. - I, yeah. Couldn't agree more Eileen. I think one litmus test that I propose in order to see how efficient these multi stake holder initiatives can be is to see which organizations or which groups speak against them or refuse to engage in them or don't engage wholeheartedly in them. I mean we hear a lot of excuses for these to not work ahead of time, right? These will be too slow, these will stifle innovation, these will cause the wrong kind of outcomes. Well, I do believe that what we're seeing right now in terms of multi stakeholder initiatives around AI frameworks, AI policy frameworks, AI global governance are actually working pretty well and they do involve a wide diversity of factors. So I would actually say again, the litmus test is are their actors that actually refuse to join these or refuse to actively make them work? And let's just encourage them to take part in this because I believe the public will be informed, the governments will be informed, the large corporations will actually be able to innovate if we all sit together at the table and we bring our various expertise with the real desire to make this thing work. - I agree. Oh, sorry, go ahead. - Okay. I do think there's more that governments can do though to support industry to be following with the global governance initiative. So, in federal funding and support that's going towards AI development, encouraging that that funding is being directed towards work that is actually following international standards for example. You could incentivize industry to focus on those issues in that way. Also when governments are procuring AI technologies actually care and do these technologies abide by international standards. That's a really obvious way that a lot of governments aren't yet doing. And then finally just a lot of governments aren't actively engaging global governance themselves. So, leading by example would be a good first step as well. - Yeah, so I'm very happy these panels sorta started to emphasize multi stakeholder governance models and I very much would agree that the future of governance is multi stakeholder. And I'm happy you feel that you sort of gestured towards this idea of monitoring as an important element. In particular I guess I would caution some of the history of thinking around multi stakeholder governance initiatives. The UN internet governance forum comes to mind first and foremost as a place that tends to sort of compartmentalize conversations and not necessarily lead to policy outputs and if we were to monitor participation that themself also would not necessarily give us a full picture in that we can see attendees come from any of the leading corporations in the world as well but they don't participate in the sessions with civil society and academia actively. And so we need to be quite careful when we look at ongoing initiatives that's in the context of internet governance. They're talking about AI as well but more particular to our topic of AI governance that we need to be very careful about thinking through what does active and collaborative and constructive participation look like and I think coming back to an earlier point that was raised thinking of sort of capacity development or the need for a global south kind of representation, I think it's quite important to understand the dynamic that Joy raised earlier of this possible sort of data or digital colonialism and we want to sort of help with development and think through the power of AI technologies for this, but in so far as we're saying oh, we want the technology to spread. We should encourage companies to participate in those for, we need to make sure that there's a very eyes open, very, very conscious of the power dynamic that is there. And so I guess to wrap that sort of in a neat bow it's promising and encouraging to see the high level sort of work coming out of the UN for the chief executives board for coordination trying to support the capacity development within the UN itself but also in countries in the global south and the ITU is taking the lead on implementing this road map. It'll be encouraging and exciting to follow this work as it happens in the coming years. - Thank you. Do we have the mic? - Yep. - Now we open the panel for our audience question and feel free to raise your hand. We will have our volunteer to deliver the mic to you and please do include a self intro before you ask your question. Yes please. - Thanks so much for your talk. So I wanted to raise a potential kind of difficulty when we want to think about the human right framework to this ethical principles. So it seems that sometimes when we talk about human rights we talk about some, we use some qualitative concepts like dignity, that somehow we don't measure or it's questionable whether we can measure them or not. On the other hand when we have ethical principles and we want to implement them, like by their own definition they should be measurable or we have to somehow make them quantifiable, like when we have fairness or explanation or whatever else. So, it seems to me that it should be a kind of a difficulty when we want to bridge or make basically all of this ethical principles on this human right framework and it depends on how we think about like dignity if dignity is something untouchable and it's impossible to make it quantitative, then how this ethical principles want to fully capture human rights? So I was wondering how do you think about the human limits of this like framing this debate on the human right framework? - So, I'll jump in there. I think that's a really interesting point and question and I think it's fair. And I would start with an understanding of the human rights framework almost as a metaphysical assertion about the reality of human dignity and the inherent value of the human person. And I suspect a lot of technologists think what the heck is that? But I think that that is the, that's what happens when you end up coming out of a crisis like World War II where there is, the world actually came together to say we're going to create a global normative compact that says the human person is valuable, and then provided a framework for analyzing the different dimensions of the human personality that needed to be protected. I agree with you. It's hard to develop metrics in some technical sense, but it is not hard to do analysis about whether you are or are not protecting privacy, free expression, equal protection, non discrimination and with that last one, equal protection, non discrimination, some of these new ethical principles are really almost directly coming from that concept in the human rights framework. So, I think there are different levels of abstraction to the different rights and I think we need to get comfortable. The way I understand the Stanford human standard AI program is really about making that move to say human beings matter and one of the three strands, there's a lot of the technological work about mimicking human intelligence is one important strand. The other one about human machine interaction and augmenting human capacities. But the third bucket is about human impact. And so Stanford is starting with that commitment to say that's what we care about, and I do think the human rights framework is a great global framework that can help bring the conversation here about human centered to the global arena. - If like, if I may, I mean, I'm a nerd myself. I'm a technologist and when I think about these questions, I find it fascinating because I always wonder how can I bring my system at the technological level to understand them? But what I like about the way these frameworks, some of these frameworks are approaching the question is they are approaching them from a socio technological angle. It's no longer a technological system. It's a socio technological system with the luck of potential feedback loops that are around the technology itself. And for questions like human dignity I think this is, this makes me a lot more comfortable knowing that there might be I don't know, legal systems that are about composing competing analysis in order to identify whether or not human dignity was respected in a given situation. I find this is a much more promising approach to reaching an interesting framework or an interesting set of mechanisms than if we stick to the technological angle. So, yeah. I would say having these frameworks graduate outside of university labs is promising. And this is one strength that I find, the recent proposals. - I agree with everything that's been said so far. I'd also just add a question of, like pushing back I guess on sort of prioritizing quantification too much 'cause there's another area that people tend to think can be dealt with quantifiably in the sense of fairness looking at the distribution of the data protected classes et cetera, but that loses the largest context of the way of even if you have say a system that can recognize the intersectionality of say, black women's faces with 100% accuracy then how will you use that system in practice? And so in so far as you seek to quantify this particular element you may forget the largest contexts of the way you're deploying the system after the fact. And so it's important to not throw off the qualitative approaches necessarily. And in the context of other metrics, you can think it's encouraging that the European Commissions advice emphasizes key performance indicators as a potential area for companies to actually implement these things. And so if you can think about trying to again move up to a higher level abstraction to think through ways that you can optimize metrics in a corporate governance context to address the key elements whether it be human dignity, accountability or fairness. - Yes. - Hi, thank you for a wonderful panel. My name's Jamie Susskind. I'm the author of a book called "Future Politics" and I practice discrimination and human rights law in England. I wanted to just add if I may to the discussion about human rights as a framework for this kind of regulation because from my practice in my experience I have some concerns about it as an effective way to regulate the kinds of technologies that we're talking about. Just to take one example, the problem posed by misinformation on Facebook for instance is one that erodes the quality of democratic debate in a country as a whole but it's not immediately clear to me how it resounds in terms of human rights, how any individual who perhaps sees a piece of fake news has suffered a sufficient legal wrong that they can go to a court and ask for them to have that right vindicated. It seems to me that the problem we're facing, there is a communal problem, one that faces everyone through a thousand small cuts rather than one egregious breech of someone's rights. Another example might be face recognition technology. The English course recently said it doesn't breech an individuals human rights for the states to take pictures of their face without permission. But if you step back and look at society as a whole you might say that the overall effect is one which is unacceptable to us. So what I'm concerned about is a system that's based in individuals vindicating their individual rights pulling aside the cost and expense of taking someone to court be at the state or a big tech company, might not actually adequately protect people in the future and I was wondering if you had anything to say about that? - So, the first thing is I do think disinformation implicates several human rights for individuals which is the right to freedom of expression and access to information, the right to privacy that's being compromised through data collection, micro targeting in disinformation campaigns. As well as the right to democratic participation. And so I do think it provides a basis for analysis. It is very complicated subject to go into what is the basis for regulating disinformation in a democracy. We could spend the whole day talking about that topic. Generally it's understood that it shouldn't be on a content basis. It's much more on a manipulative behavior basis but it is because it's eroding those rights. And then I would also say the human rights framework is not solely about individuals. It is about the community and the values of a society being collectively eroded whether it is the erosion of privacy, the erosion of civic participation, civic discourse and democratic election processes. So I think there is a valid human rights entry point to the disinformation threat. - To add a little I think the framework itself can be the right one but I do believe it needs to be specialized. Even if it's only to help legislator make sense of these new challenges that are brought forward by AI technology and the examples you give the decisions might be rooted in human rights with an imperfect understanding of the impact of these technologies on all of society. That's why the work that's being done in order to say like let's dig deeper into human rights principles and figure out in the particular case of machine learning tram systems and data sets, what are the implications of these systems on the human rights? In order to better inform all the systems in society. All the actors in society as to the deeper impact on human rights. I don't know what you think about that Eileen. I'm not an expert at all in human rights but I do believe this displays an important role, this like exercise of going a bit deeper. - I agree. - Thank you. My name is Delphine Halgand-Mishra, and I work for press freedom organization Reporters Without Borders and Signals Network, and I have two questions that are related to the topics already discussed. First is about Eileen's point. I agree that the existing human rights framework should be used. I also use it for many years to obtain the release of journalists. So I also know as you know also first hand that is has some flaws and some of them are slow implementation and a lack of accountability when violation occurs. But I still want to believe in that way and so I would love to hear your views on how you see it efficient implementation of this existing human rights framework related to AI? And my second question because I work for a press freedom organization, so I'm obsessed with press freedom and the fight against disinformation. So of course I was shocked when Jessica you showed the AI maps showing that actually the fight against disinformation is at the top bottom of priorities of the AI principles. So, I want to be optimistic here. So I hope all of you have heard of promising initiative that use AI to counteract and fight disinformation. Thank you. - Sure, yeah. I mean, I think on one hand it speaks to how quickly the field is moving and why potentially having more of a precautionary principle would benefit us because a lot of those national strategies came out in 2017 and I think people were just grappling with the massive effects of disinformation, and so I think they're sort of playing catch up. I think if you looked at some of the principles or policies that have happened maybe within the last six months, maybe you would see a little bit more focus on that area. At least I would hope so. There's certainly no reason for us to be ignorant of its negative impacts anymore. - I will just add that reinforcing your point, I think with the disinformation threat, everybody, every sector was caught off guard. The national security community, the cyber security community, the human rights community. Everybody. And basically all agencies of government were caught flat footed certainly in the 2016 U.S. election and globally we've seen this problem. So I think the early attention has just been put on diagnostics and understanding how does it work? And there's several, we have a new caper policy center here at FSI and at least one of our pillars, Alex Stamos and Rene Duresta are really in the business of doing that diagnostic work so that technologists and governance people can get their heads around like how does this even work? And I suspect, I think disinformation as a threat to democracy globally has really risen much higher in the policy agenda and I bet you're right. If those frameworks were being crafted today it would be, disinformation would get a lot more attention. - Thanks. Scott Forshdall. I led the creation of iOS in the iPhone. So I'm fascinated by this idea of using the human rights framework as a basis for this but I guess I wanna dig into the next level of that which would be would you believe that it can apply to all 20 of the principles that Jessica put up there? I guess this is a question maybe for Jessica and you, and is there a paper that describes a practical way of turning that into the human rights framework into a set of principles that cover all these items? - Great question. So, the two things I would point to in what would it look like in practice and where is it written or what can you turn to? Go look at information particularly I would say about Microsoft's human rights impact assessment for AI. They are developing the processes along with an entity called Article One Advisors. And this concept of human rights impact assessments has been out there for awhile. It flows from the UN guiding principles on business and human rights where there was an assertion of the need to do responsibility of the private sector to do due diligence processes to make sure they're not harming human rights and that idea is now being applied to AI specific product and services. And I would say Microsoft is kind of on the leading edge of that as a private sector entity because not only have they embrace the human rights framework but they've chosen to use that framework for their due diligence process. And so they are leading in that space. The other thing I would point to is what I mentioned in one of the slides which is, in Australia there happens to be a humans right commissioner. So, not every government has a human rights commissioner but Australia does. They have a human rights institution and the person in that role's name's Ed Santow. He is I think doing the cutting edge analysis of what does it look like for a government to do a human rights impact assessment as it relates to procurement of AI products and services from the private sector, reliance on AI products and services in governance decision that impact citizens rights, and then also in terms of the proactive obligation of governments to protect citizens rights. And so he's done a lot. There's substantial writing and reports by the human rights commissioner in Australia that I would suggest you turn to for analysis on government. - I would just add that this is a really hard problem and if you look at the issue of lethal autonomous weapons as an example where you would think so obviously that this would be a human rights violation on multiple counts, the UN has not been able to do much on this front. The CCW has held meetings over numerous years and because it requires a consensus no action has been able to happen. There's a few states, the U.S. included that don't think something like an international ban is the way forward. But here's an issue where we're talking about technologies that we've heard today are inherently flawed. They're not reliable, robust or explainable. They have kind of biases baked in and we're talking about actually implementing them onto the battlefields making life or death decisions with impacts for an entire nation. So I think this is a really misguided way forward that would not be in the interests of any of the global powers let alone anyone else. And so as a kind of, what should have been an easy case study for how we implement a human rights ideal into an international global governance framework, we haven't done so well yet. - I just have to jump in there because I think this connects with a question earlier about ethicacy of the framework. We have to admit the UN itself is a global institution made up of what? 194 governments and in many regards like a lot of governments we see in and of themselves at the national level, the institution of the UN is not always high functioning and there is a lot of dysfunction in the institution. I think you can separate that. I mean there's geopolitics involved, lots of global dynamics. The framework itself isn't the problem. It is human beings and governments and companies living up to what the framework requires. So, everybody can get involved and make it more efficacious if able. - And just to add the third practical source. The framework I presented, chapter three of that is actually a trustworthy AI assessment checklist. So you can go through that. It's a very practical checklist. It's still at pretty high level given that the technology is not there yet to be super specific about the technological requirements for example. But it's still a very practical checklist to go through if you're interested in assessing whether or not you're on the path towards trustworthy AI. - Due to time constraint we will be able to take one final question from the audience. Yeah, there in the back. Yeah, there in the back. - Thank you. My name's Emma Day, I'm a human rights lawyer, and I've been working for UNICEF for the past few years based in Southeast Asia, and one of the reasons I came to the U.S. was because children in most parts of the world are using American apps and those countries don't have jurisdiction over the data. There's not a lot they can do to protect children using those apps. And I'm a human rights lawyer. I'm trying to use this framework but what troubles me learning about the U.S. legal system that it's not right spaced, and I'm wondering given the domination of these American companies how can we hold them to this human rights framework? And what's the process in America? Because not just from a legal point of view but even from a commercial business point of view, companies in other countries look to the U.S. big companies for leadership. So we have to find a way to set an example both in law and in practice. - Do you guys? - So, that's a, this is... That's a complicated question. Let's see. I will start by saying in terms of holding the private sector companies accountable this is why they're own self embrace of a human rights frame is valuable and if they self commit to these, commitment to these principles and the human rights impacts assessments, you have an avenue by appealing directly to the companies. On the U.S. legal system, I would not agree that it's not right spaced. I will agree, it's constitutional rights. Many of which overlap, there's tremendously with the human rights framework and there's a lot of coherence between those two frameworks in terms of the actual values. I think what you're pointing to though is not a lack of values. It's sort of the jurisdiction problem in the legal system and here's the strange irony is that the post World War II liberal democratic order and the post westphalian world is really built on the concept of nation states defined by territorial boundaries. That jurisdictional concept doesn't really work in a globalized, digitized information ecosystem, and I think that's the real problem you're pointing to, that we still have national legal systems, geography based and the ideas that governments are there to protect their own citizens or people within their own territory and jurisdiction and we need a new jurisdictional principles that help us protect rights across borders. And there is some interesting work happening in that realm but I think it's one of the most complex areas. - I guess I would just add that there's the question of what we ideally wanna see and also like what we can do with what we have right now and to a large extent these companies are engaging in exploring in self governance mechanisms and there are examples of that working more well and less well. Famously Google's external AI ethics community did not last very long (laughs). But I think Microsoft is a good example here. They have the example that you mentioned. They have also instituted ethics into their regular product checklist from what I understand. They also have what they call the ether community with top level executives to assess the implications and safety and reliability of all of their AI products and who they're selling those products to as well. And part of that is a hotline that any employee can call if they're concerned about any kind of instance of abuse or kind of example of something that might be unsafe. So, that's small potatoes in the large scheme of things, but as maybe one example of what a company can do at this point with the limited kind of frameworks that we have in terms of what's instituted. - And that marks the end of today's breakout session. I would like to invite everyone to join me in a warm round of applause to all the speakers and their amazing viewpoints. (audience applauding) And I would like to invite each speaker to stay around for a little bit longer so our audience if you have other questions can come up to the stage and feel free to interact with them. Thank you everyone. (audience chattering) 