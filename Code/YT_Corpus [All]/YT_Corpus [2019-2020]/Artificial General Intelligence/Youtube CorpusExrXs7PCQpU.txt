 I think I was about seven years old when I wrote my first line of code it was probably something simple printing my name to the screen or a two-dimensional shape that could be twisted and stretched by my sequence of carefully typed letters and digits it was an extraordinary feeling a first sense of where the logic defining power of the computer could take us but it wasn't until much later that I started to come across the term artificial intelligence or AI and well what a world that would open up AI holds enormous promise for the future and I think these are incredibly exciting times to sort of be alive and working in these fields we want to kind of understand and master increasingly complex systems AI must be build responsibly and safely and used for the benefit of everyone in society and we have to ensure the benefits accrue to everyone you know I think AI can be one of the most exciting and transformative technologies we'll ever invent that is the voice of demis hassabis the CEO of deepmind the london-based artificial intelligence company for demos ai will allow us to create computer systems that can learn to solve complex problems by themselves in his words society could use intelligence to solve everything else cancer climate change language energy in short to advance scientific discovery but just how far-fetched are these goals can researchers really crack intelligence and just how much of an impact would that really have I'm Hanna Frey and this is deep mind the podcast for the past year I've been at deep mind HQ in London for an inside look at the fascinating world of AI research and where it's going we will be telling you the fast-moving story of the biggest challenges in artificial intelligence or AI so whether you just want to know more about where the technology is headed all want to be inspired on your own AI journey then you've come to the right place we will focus on the projects that scientists researchers and engineers are actually working on how they're approaching the science of AI and some of the tricky decisions the whole field is wrestling with at the moment and whilst we're here we've explored the rooms full of computer screens where scientists run their endless experiments the meeting rooms where people write intricate equations on whiteboards packed to the rafters the robots and the laboratories where banks of repetitive robot arms grapple with piles of plastic bricks and we've talked to a huge number of people to try to understand what is driving this new frontier the voices that you'll hear in this podcast are from the people that are at the cutting edge of AI and machine learning and quite a few of them are talking about their work publicly for the very first time but if we want to solve intelligence let's start with a fundamental question of AI what exactly do we mean by intelligence if we're trying to make machines intelligent what are we actually aiming for this is sort of something that's debated a lot in AI world is like well do we want to you know have our AI agents act exactly the same way that people do like should they be exactly human-like intelligent or should they just be intelligent in general this is Jess Hamrick a research scientist at deep mind her specialism is imagination and mental simulation there's sort of I guess like you know one group of people who like to say that you know we want to build something that's just generally intelligent that's really able to solve a lot of different problems in the world that humans aren't necessarily able to solve that has an intelligence that's higher than humans so this might be able to solve problems like how do we cure all diseases like maybe maybe and artificial intelligence might be able to help us solve this problem and that's you know something human society and human civilization hasn't yet been able to accomplish but then there's also another group of people who say that it's really important for us to build AI that is similar to human intelligence at least in some ways I would consider myself to be sort of in the latter group why does it need to be similar to human the reason is because as we build AI we as humans need to be able to interact with AI and collaborate with it be able to understand the predictions that it's making or the recommendations that it's making and if we build AI in a way that is maybe we are able to build AI and it's generally intelligent but it acts in a way that's so alien to humans that we just can't really understand what it's doing and I think that actually would be a really bad scenario to be in because either it means that people don't trust it and then people are very unwilling to you know use the recommendations of this AI maybe it says oh do this one thing and this will like cure this disease and but people don't understand why it's making that recommendation maybe we miss out on a lot of opportunities to really do a lot of good in the world we need Rai to understand the world in the same way that we do it needs to be able to explain itself to us so we can be sure that we can trust it take for instance the story of an AI that was trained to diagnose skin cancer by looking at photographs of skin lesions taken by dermatologists the algorithm did a good job of correctly labeling the images but the researchers soon discovered that the AI wasn't looking at the cancer at all to make its decision it had simply learned that lesions photographed next to a ruler are more likely to be malignant not exactly trustworthy it's crucially important that artificial intelligence is able to grasp the subtleties of human thoughts we want it to do what we mean it to do not just what we say we mean but that doesn't necessarily imply it nice to think in exactly the same way as people do there can be drawbacks to trying to imitate human or animal brains too closely we get into discussions about where the strategy can limit you this is Matt Banach Matt is the director of neuroscience research at deep mind where he draws on his experience in cognitive neuroscience and experimental psychology Matt believes the human mind is the inspiration but AI research has to take things further in its own way you know the Wright brothers when they solved the problem of flight you know that people like to say oh they solved the problem when they stopped trying to copy bird's wings which you know in some technical way might be true but they wouldn't have gotten to where they were right if they hadn't spend an awful lot of time and if other people hadn't spent an awful lot of time looking at bird's wings and noticing the airfoil pattern and thinking about the the dynamics of the of the air that flows around an object with this shape so yeah we we do believe that we can look to the human brain in the human mind for inspiration but we also talk about them when the moment comes where we need to kind of step away from that and just build something that does what we want it to do so what is the neuroscience equivalent what are the birds wings of our brains the aspects of our own intelligence that we can use for inspiration as we build AI well one area that seems to hold a lot of promise is memory and in particular something we all do known as we play replay as a phenomenon that was discovered in a part of the mammalian brain the medial temporal lobe including the hippocampus where you see neural activity that suggests that past experiences are being replayed especially in in navigation for example a rat will go through some environment and a particular pattern of activity will arise as it goes through the environment and then later if you have electrodes in the hippocampus you can see that the same pattern of activity the same sequence is occurring suggesting that a memory is being replayed of that experience and that idea now has a firm place in AI if you lose your car keys you can run your mind through where you've been to workout where you might have left them well I first went into the kitchen and took my coat off in the hallway put my bag down on the side and oh yeah they're in my back pocket that ability to replay your experiences and learn from that memory after the fact is a key part of what researchers want AI to be able to do here's more from that the way that that's implemented in deep minds agents is it's not exactly what you find in the brain it wasn't as if people were trying to slavishly recreate the the biological mechanisms but the idea of replay which was inspired by neuroscience came in handy in 2015 replay played a pivotal role in a famous deepmind breakthrough the team managed to build an AI system that could play arcade classics to a superhuman level the old Atari games like space invaders pong and breakout the AI use something called deep reinforcement learning but behind the scenes it kept a memory of moves it made as it played and how those moves had impacted on the final score by replaying those memories the AI could learn from his experiences it could work out what sequences of moves worked well which were mistakes and find strategies that otherwise wouldn't have been obvious but there's more to our human memories than just a giant database of facts of course you can remember the name of the capital of France but you might also be able to remember jumping on the bouncy castle at your 6th birthday party or the pranks you played on your last day at school this is a phenomenon called episodic memory and it's something that holds a great deal of promise for AI we talk a lot about something called episodic memory which is simply the cognitive ability to retrieve a memory of something that happened to you before we started recording we were joking about like what did you have for breakfast your ability to cast your mind back to that moment when you were eating breakfast and retrieve that information that's a function that psychologists and neuroscientists refer to as episodic memory and we have this category both because psychologists work hard over decades to fractionate memory into particular domains or kinds but this is a pretty high-level idea it's not like replay it's just hey there's such a thing as episodic memory which is very important for human intelligence maybe our agents should have episodic memory what would that mean what would it mean for an artificial agent to have episodic memory this is an intriguing possibility an AI that can transport itself back in time and recall entire events and experiences rather than just facts when you stop and think about it this ability to link one memory with another is an amazing human skill and if researchers can get a better understanding of how our brains actually do this it could be replicated in AI systems giving them a much greater capacity for solving novel problems let's think about how that works for a moment imagine that every morning you see the same man in his thirties walking a boisterous collie then one day a white-haired lady who looks like the man comes down the street with the same dog with those events stored as episodes in your mind you might immediately make a series of deductions the man and the woman might come from the same household the lady maybe the man's mother or another close relative perhaps she's taken over his role because he's ill or busy we weave an intricate story of these strangers pulling material from our memories together prioritizing some pieces of information over others to make it coherent it's something that's been the focus of recent research by the neuroscientists here a study in September 2018 demonstrated the critical role of the hippocampus that shrimp shaped seat of memory in the middle of the brain in weaving together individual memories to produce new insight Jess Hamrick is also looking at another way that a eyes can be made to respond more flexibly to new situations she takes our inspiration from a different human ability mental simulation what you and I might call imagination imagine that you're on a beach you'll have like this mental picture kind of spring to mind of you know mine at least is maybe a sandy beach the bright blue ocean maybe some palm trees on the slope and so this is an example of what we would call mental simulation it's like we're mentally stimulating this picture of the beach and then you can do things with that simulation so you can imagine adding other people to your imagination you can imagine what would happen if you like threw a ball if you're playing volleyball or something like that so these these sort of mental simulations are really interactive and really rich and I think that they underlie a lot of our human ability to understand the world and make predictions about the world I should pause for a moment here to explain what Jess and maps mean by an age in here it's a word that's used a lot at deep mind remember when people are talking about artificial intelligence they're really just talking about computer code with the freedom to make its own decisions and an agent is just the noun that they use to describe the part of that code that has agency Jeff's is hoping to build agents that are flexible enough to adapt to all manner of environments it's a very grand ambition but one with real potential to see why let's go back to the RKO and like game of space invaders mastered using deep reinforcement learning to create an agent called deep Q network or dqn dqn was really sort of an amazing technological feat because it was able to be trained to play many many different Atari games directly from perception from pixels this is something that hadn't been done before but the way that dqn also works is that it really just goes directly from inputs to output so it takes in the image of the video game and outputs immediately what actions should be taken to maximize the score in that game so maybe it's a move left maybe it's you know push the trigger to shoot all of these actions are being taken just to maximize that score and the agent doesn't know why that action is good it only knows this action will give me a higher score and so the agent isn't able to really do anything else besides that you can't ask the agent to say hide behind one of the pillars until that pillar is destroyed or destroy all of the incoming space invaders in one line and none of the other space invaders so these are all kinds of like different tasks that you could give a human and they may a little bit weird but humans would understand what what it means to do this and that's because humans have this ability for mental simulation to imagine what will happen if they take different actions and so by giving our agents the ability to imagine things and also plan according to different tasks that it might be given they're able to act more flexibly and deal with these sort of novel situations but humans aren't the only form of intelligence we can draw inspiration from we can also learn from our cousins in the animal kingdom let's bring in researcher Greg Wayne within neuroscience Greg's thing is memory and cognitive architecture one of the things that is quite clear is that animals have a remarkable ability to deal with for example very long time scales so experiences that can be linked across periods of time that is way beyond our our current sets of agents the great example I think is the scrub Jay the Western scrub Jay they bury things they prepare for the winter by scrounging up a lot of food and putting it into depositing different places hiding it from each other and they love to steal each other's food - they're scavengers and they can remember thousands of sites where they've they buried their food so once all at once and they they can even no detailed facts about it they know how long ago they buried things that they know if they were being washed while they're burying things they know what thing they buried there they have an incredible memory for these events that they have produced themselves how can you tell that they know what they buried because they have a preference you'll see that they'll they like maggots more than peanuts they'll go back to those maggots first having a kind of large database of things that you've done and seen that you can access and that you can use to then guide your your goal-directed behavior later you know I'm hungry mmm I would love to have some maggots right now where should I go find those that's that's the kind of thing we would like to replicate and there's another big lesson we can learn from animals if you want to teach dog to sit you don't write a list of instructions move this muscle bend your leg 45 degrees anything like that instead you repeat the same task over and over again offering punishments and rewards as you go and if it's good you give it a little bit of food that's how we train dogs now I have a friend who trains dogs to do things on iPads using reinforcement learning so we've already started on the path in AI of merging reinforcement learning very closely with how our AI is make decisions and so on and that's how we train them so you're essentially training an artificial intelligence an AI in the same way that you might train a dog rewarding them for good behavior ignoring that behavior very nice okay how do you treat an AI what does it mean to reward something that isn't interested in doggy biscuits here's dem assess Arbus well with artificial systems or they really care about is ones and zeros so you can construct artificial reward mechanisms for almost anything we've now moved away from programming the system solution so it now learns for itself so now going up a meta level so now what we're really programming or designing is reward systems so it's kind of interesting that that now is becoming the difficult part is like how do you design curricula how do you design breadcrumb trails or rewards so that eventually they learn the right things these systems but there's also the idea of unsupervised learning which is how do you learn things if in the absence of any reward and actually that's the issue with reward learning in the real world as humans or even as children there aren't very many rewards it's quite sparse the rewards even as a dog right the dog gets a doggie biscuit every now and again but has to decide every moment like what to do and actually I think one of the answer to that is what we call intrinsic motivation which is internal drives that have come through in animals have come from evolution but we could also evolve or build in those drives are very strong and they guide the animal or the system even in the absence of external rewards so of course that might be things like joy or fear or even things like hunger these are all primal kind of internal motivations that drive your behavior even in the absence of any external reward you're listening to deep mind the podcast a window on AI research while rewards might be a key part of how to encourage AI to learn one of the main aims of machine learning is for AI to be able to teach itself to notice patterns and shortcuts between tasks and make themselves more efficient learners in an ideal world engineers would like to reach a point where AI can learn in a similar way to humans picking up the essentials of a new task in a matter of minutes back to mat botnik an example would be I went on holiday recently to South America and I wanted to brush up my Spanish and I knew exactly how to do that I knew what resources were out there for to begin with but more importantly when I sat down to brush up my Spanish I had a whole repertoire of concepts that really guided me like I know what it means to conjugate a verb right I know that in certain languages there are masculine to feminine forms so this background knowledge helped me learn much more rapidly than if I just sort of was dumped into the middle of you know a new language without understanding what it means to learn a language and we want systems we want artificial systems that come armed with these concepts it's not just about language it could be video games we could sit down in front of a new video game that you've never played but if you've played video games in the past you kind of know how video games work and that helps you to learn rapidly the AI is what's known as narrow in its focus now that might be diagnosing cancer or playing video games but the ultimate goal is to create something much more powerful something called artificial general intelligence with precisely this ability of being able to adapt to different situations to be able to use the high-level concepts it's learned in one environment and apply them in another we don't want just a system that's it's really good at one thing we want a system that's really good at lots of things but really what we mean is we want a system that can pick up new tasks that it's never performed before we want an intelligence where you say ok you've never solved this kind of problem before but let me let me tell you what I want you to think about now and you could introduce them to organic chemistry or something and they would be able to work with that humans can do this but getting machines to do this is really really tricky and it's not the only thing that we humans can do that AI finds hard Gregg spends a lot of his time trying to understand the detailed mental processes behind apparently simple human tasks you have breakfast and you drink your orange juice and you run out and then you think to yourself god when I leave work I'm gonna have to pick up some art shoes you go through your workday you don't even think about orange juice once and then it springs to mind you know immediately as you're leaving the office that you need to go pick up some large juice when you are going to buy the orange juice it is actually of no value to your present self the only self that will benefit from buying the orange juice is yourself at breakfast the next day so you're actually you have to do something that is incredibly prospective or thinking forward thinking about the context of your future self this is something I mean people here really sit around and sort of talk about and try and work out what is it about your brain that reminds you to buy orange juice at the right moment yes because you can easily construct virtual environments with tasks for agents that we normally have that have properties like this like thinking minutes or hours ahead or remember something from hours ago that our normal agents completely stumble on they cannot do why is that they seem easy seems easy to buy orange juice there's a theme emerging here back in the 1980s hands Moravec and his colleagues pointed out that when it comes to artificial intelligence everything is a little bit upside down while the things that humans find tough like maths and chess and data crunching require very little computation the things that we humans manage without even thinking turn out to be monumentally difficult for machines it's a phenomenon that has become known as more of X paradox like other neuroscientists and psychologists here I I find myself thinking about stuff that seems really simple stuff that I do and other people do really without thinking about it and it just doesn't seem that big a deal but it turns out to be those some of those things turn out to be very difficult to engineer into artificial systems so picking things up putting things down planning a route through a building things that we can just do without really much mental effort sometimes proved to be quite difficult to engineer an example of this just came up as we walked into this room we all realize that it was quite stuffy in here and that we wanted to try to cool it down so we all huddled around the thermostat and tried to figure out how to get it to do what we wanted and it seemed to be resistant and at some moment I thought wait a minute maybe maybe the air conditionings just broken and again that seems like a super simple thing like you know what's such a big deal about that but actually in AI research we have a name for this which is latent state inference we're trying to infer some aspect of what's going on which is latent or hidden and it turns out in order to do that seemingly simple thing you need a very rich model of the world you need to understand air conditioners and thermostats and what it means to be broken and what's the probability that it's broken and so forth more of a paradox is often talked about as some kind of profound mystery it's used as evidence that while the jobs of animal some lawyers might be at risk in an age of AI gardeners receptionist's and cooks are secure in their careers for decades to come but deep mines founder demis hassabis has quite a different take I think it's quite obvious as a simple explanation for it when Marivic was doing AI the dominant paradigm was expert systems so hand crafting solutions directly to AI problems think of it as building big databases of rules of course if you're going to do that that in itself is a very explicit task programming that out you know you have to know exactly what you want to write and what rules you want to incorporate and what that means is the only tasks you can do that with are the ones that you explicitly know how to do as humans yourself and that's things like the logical based like maths and chess so weirdly the things that we do intuitively ourselves and effortlessly like walking and seeing and you know all of these sort of sensory motor skills seems effortless to us and the reason is is because there's actually huge amounts of brain processing is going into that it's just that it's subconscious its areas of the brain we don't have conscious access to we probably knew less about neuroscience at the time so we didn't realize quite how much processing goes on in the visual cortex for example and so now we know both of those things we know how the brain works better and we've built learning systems like alpha zero and alphago so it turns out actually vision is not any more difficult really than playing go it's similar if you approach it in the same way it's almost impossible to reverse-engineer our unconscious skills using the old methods of handcrafted programming you have to have a total and complete conscious understanding of how something worked before you could ask a computer to replicate it but now the machines are just beginning to mimic our subconscious processes like vision and pattern recognition there's no reason why more of X paradox needs to necessarily be a barrier in the future [Music] I have to be honest with you this single idea more than any I've learned in making this series is the one hit home and underlines the power and potential of AI for me all that we've managed so far in everything that we've created with machines are only the things that we consciously know how to order them to perform we're only just at the very beginning of artificially mimicking our subconscious processes - and that means that there is an extremely exciting journey ahead but this partnership of studying neuroscience and artificial intelligence alongside one another doesn't just help make our AI better here's Matt Botvinnik and Jess Hamrick again to explain we often talk here about the virtuous cycle the opposite of a vicious cycle right there's a virtuous cycle between AI and neuroscience where neuroscience helps AI along and then AI like returns the favor one of the reasons why we can get this virtuous cycle between neuroscience and cognitive science and AI is because fundamentally we're all trying to study the same thing which is intelligence and so if we ask sort of these more abstract questions about what should an intelligence system do in this situation we can ask that about humans what would a person do in this situation and try to come up with an answer we could ask what should our a AI agent do in this situation and try to come up with an answer or if we have an answer already in one of those fields we can take the solution and apply it to one of the other fields and I think that's sort of really what enables this this ability to transfer between the different fields [Music] this isn't just a theoretical flow of ideas there are real examples of ideas from artificial intelligence finding their way back into neuroscience so there's a neurotransmitter a chemical that conveys messages in the brain called dopamine in the 1990s people were finding ways of tracking the release of dopamine in the brain and very clear patterns were being identified but nobody really understood what they meant why does the brain release dopamine in this situation and not that situation and as I understand the history some papers hit the desk of some people who were studying computational reinforcement learning people like Peter Dayan and Reed Montagu and they just saw immediately that the the patterns of activity that were being reported in these neuroscience papers the dopamine data could be explained by the math that's involved in reinforcement learning that has led to a real revolution in the neuroscience of learning if you give a monkey a treat they get a little heat of dopamine in their brains it's the same in our brains too a little burst of pleasure whenever something good happens but in the 1990s researchers realized that dopamine wasn't actually the response to the reward it was reporting back about the difference between what the monkey expected the reward to be and what it actually received if you're walking down the road and you unexpectedly find a 20 pound note it's much more exciting than if you're collecting a 20 pound nose that's owed to you by a friend and if a monkey is expecting you to give it a grape and you hand it a piece of cucumber it's gonna be a lot less happy than if you just surprised it with a bit of cucumber from nowhere thing is AI researchers were already using something that acted in a very similar way in their algorithms they'd get their agents to make a prediction about what was going to happen next and compare it to what actually occurred but remember in all of this the idea is to just take inspiration from the way that our human brains work not to make a straightforward artificial copy because our brains on exactly perfect so we've heard how we can take inspiration from the human brain from the animal and even the bird brain to create AI systems but this isn't just a working theory anymore researchers aren't just talking about what they want to do they're also talking about what they've actually managed to do let me tease you with car I cover cholo director of research at deep mind it's a simple problem of course you can write a program to solve that but the idea was try to do deep reinforcement learning try to come up with a system that we think can generalize to different problems more problems and once we saw that it was a matter of weeks we had ten or fifteen Atari games being solved if you would like to find out more about the link between AI and the brain or explore the world of AI research beyond deep mind you'll find plenty of useful links in the show notes for each episode and if there are stories or sources that you think other listeners would find helpful then let us know you can message us on Twitter or email the team at podcast that deepmind comm you can also use that address to send us your questions or feedback on the series let's take a little breather see you shortly 