 This video is brought to you in thanks to Brilliant, a problem-solving website that teaches you skills essential to have in this age of automation. In the last video in this series, we discussed the ancient origins of artificial intelligence progressing forward to the beginnings of the development of modern computing-based artificial intelligence, encompassing the philosophies, theories, and inventions of many talented individuals and groups. The focus of this video will continue right where the last one left off, so sit back, relax, and join me on an exploration on the official birth of modern artificial intelligence leading to present day. While artificial intelligence in concept has existed for thousands of years, the term was officially coined in a new field born at the 1956 Dartmouth summer research project on artificial intelligence. This was by John McCarthy, one of the founding fathers of modern artificial intelligence. Other prominent members of this conference include Marvin Minsky, who created one of the first machines incorporating a neural net, SNARC, and Claude Shannon, the father of the information age and an avid believer of a computer-based artificial intelligence. The purpose of this conference is best said in McCarthy's own words, in the proposal drafted a year earlier in 1955, quote "the study is to proceed "on the basis of the conjecture that every aspect "of learning or any other feature of intelligence "can in principle be so precisely described "that a machine can be made to simulate it. "An attempt will be made to find out how to make "machines use language, form abstractions, and concepts "to solve kinds of problems now reserved for humans, "and improve themselves." End quote. Expanding further on this quote, the proposal outlines the seven original aspects of artificial intelligence the conference would revolve around. One, automatic computers, simulate higher functions of the human brain. Two, how can a computer be programmed to use a language. Natural language processing and understanding. Three, neuron nets. Arranging artificial neurons in a way in which they can form concepts. Four, theory of the size of a calculation. To be able to measure the complexity of a problem. Five, abstraction. The ability to take complex topics and abstract them into simpler chunks using sensory and other data. Six, self-improvement. Seven, randomness and creativity. The golden years of artificial intelligence refer to the time period after the Dartmouth Conference which had centralized the philosophies and theories which came before in the late 19th and early 20th century under a common banner. Compounded with the official development of the field of artificial intelligence was the fact that computing as a field was beginning to come into its own, with the switch to the transistor and hardware and hardware architectures progressing every year, the result of this was the creation of the fields of software and computer science and a request to program these new machines, which also allowed the implementation of many AI theories and the algorithms. Some such algorithms and solutions were A, reasoning as search, now referred to as means and analysis as an algorithm which achieves a solution by proceeding step by step, making a deduction at each step, as if searching through a maze, and backtracking if reaching a dead end. Many early AI programs, such as logic theorist, or chess players, for example, use this algorithm. B, semantic nets. These are the precursor to natural language processing. These nets represented words that were correlated to each other as nodes, and built more complex relations as additional nodes were added. One of the first chatbots, Eliza, in 1964, used this method for communication. C, micro-worlds. These are the precursor to machine vision. In the late 60s, Marvin Minsky at MIT proposed that AI research should focus on abstracted simple models of the world known as micro-worlds. These worlds were composed of simple geometric shapes, which the computer could recognize and even stack. Since computing was still a young field, and artificial intelligence even younger, these algorithms and the results they produced were extremely groundbreaking at the time, infusing the AI field with a lot of optimism, and leading individuals such as Marvin Minsky to hypothesize that, "Within a generation, "the problem of creating an artificial intelligence "will substantially be solved." Also during this time period, American psychologist Frank Rosenblatt, contributed immensely to the field of AI due to his interest in neurobiology and cognitive systems. He was the pioneer of the Perceptron, the simplest abstraction of a neuron, creating the Perceptron algorithm in 1957 at the Cornell Aeronautical lab. Perceptions are referred to as the simplest abstraction of a neuron because their activation function is a step function, that being, they will either output a zero or a one based off the weighted sum of all their inputs. At this point, I just want to take a brief moment to state that any discussion on neural nets or their components we have in this video will be very low-level as the next few videos in this series will go very in-depth into them. Back on topic, from software in 1960 this Perception algorithm was translated to a physical hardware implementation in a custom machine, dubbed the Mark 1 Perceptron. This machine consisted of an array of singular Perceptrons that were connected to a camera and used for image recognition purposes. - [Announcer] In the 1950s and 60s, scientists built a few working Perceptrons, as these artificial brains were called. - [Man] He's using it to explore the mysterious problem of how the brain learns. - [Announcer] This Perceptron is being trained to recognize the difference between males and females. It is something that all of us can do easily, but few of us can explain how. To get a computer to do this would involve working out many complex rules about faces and writing a computer program. But this Perceptron was simply given lots and lots of examples, including some with unusual hairstyles. - [Man] But when it comes to a Beatle, the computer looks at facial features and hair outlines, and takes longer to learn what it's told by Dr. Taylor. Andrew Cruickshank's wig also causes a little path searching. - [Announcer] After training on lots of examples, it's given new faces it has never seen and is able to successfully distinguish male from female. It has learned. - [Narrator] The results produced by this machine were nothing short of remarkable, especially given the fact that they were produced nearly 60 years ago. This prompted interest in the technology by the US Navy, and generated a lot of hype. With the New York Times writing about this electronic computer, that it expects it'll be able to walk, talk, see, write, reproduce itself, and be conscious of its existence. This level of hype paired with the excitement of new computer science algorithms, like those we discussed previously, were generating led to massive amounts of funding, primarily from the government for AI research and implementation. One of the biggest funding sources during this time was from the Defense Advanced Research Projects Agency, DARPA. From 1963 to the early 70s, they gave the AI group at MIT, founded by Minsky and McCarthy, approximately $3 million a year. This equates to approximately $20 million a year today adjusted for inflation. Similar grants were made to other research institutions, such as a Stanford AI project, again founded by McCarthy, as well as Allen Newell and Herbert Simon's AI program at Carnegie Mellon. It's no surprise that these universities are at the forefront of AI research today. With such large amounts of funding and new breakthroughs in computing and computer science year after year, there was a lot of optimism during this period, with many researchers, such as Marvin Minsky in 1970 stating, "In three to eight "years, we will have a machine with the general intelligence "of an average human being." This was truly the golden age of artificial intelligence. Little did many know, winter was coming. In the mid-70s, the first AI winter began. AI winter is a term that refers to a time period where there is reduced funding and interest in AI. This first AI winter was caused by the fact that optimists had raised expectations so incredibly high that when promised results failed to materialize, funding disappeared. The reason these promised results were not achieved was because AI researchers had failed to appreciate the difficulty of the problems they faced. Some of these problems and fundamental limits were solved in the coming years, as we'll explore in this and future videos, and others are still yet to be solved to this day. To list the five biggest problems, one, the field of computer science was still being defined during this period, classifying the difficulty of problems and what problems could be solved. This was essentially defining the boundaries of the P versus NP problem, in other words, what computers could reasonably solve versus what they could not. This meant that previous AI algorithms like reasoning a search, for example, wouldn't work for a vast array of problems as they essentially made educated guesses based off a list of pre-programmed rules and backtracked if they were wrong. For problems in the NP section, this would result in exponential combinatorial and complexity explosions. Two, whereas initially breakthroughs were made with limited computational performance, after a while, it became clear that computing and memory, power and size, were very limited, essentially bottle necking software advances. For example, work on natural language processing could only handle 20 words at a time because that is all that would fit in memory. Coupled with limited computation performance was also the fact that there was simply not enough data, and no one in the 70s could even build a database large enough for the amount of information, for example natural language processing, or vision AI applications would need. Three, Moravec's Paradox. This was a theory postulated by AI and robotics researcher Hans Moravec at Carnegie Mellon. It essentially states high level reasoning, in other words, most of the tasks we as humans do for work, require very little computation as compared to low level, close to subconscious tasks like abstract thought. Marvin Minsky also supported this line of thought stating, "In general, we're least aware "of what our minds do best. "We're more aware of simple processes that don't work well "than of complex ones that work flawlessly." John McCarthy also acknowledged that standards logistic logic alone can't solve most problems, and that other methods of abstracting thought are required if the field of artificial intelligence is to progress. It is to be noted, McCarthy's line of reasoning delves into the domain of artificial human intelligence, AHI, which will be covered in a video of its own soon. Four, the dark age of connectionism. Rosenblatt only used singular Perceptron nets, and while extremely promising, were unable to recognize many classes of patterns, such as XOR. As such, in his 1969 book, "Perceptrons," Minsky stated that Perceptrons had been overexaggerated and were severely limited. Ironically, Minsky's own words on the Perceptron greatly slowed down the field of AI, and it took nearly a decade for connectionism to revive as we'll see shortly. Five, the last major reason that brought upon this AI winter and something all too prevalent today is what McCarthy stated. "As soon as it works, no one calls it AI anymore." In other words, our definition of AI kept changing and evolving, pushing the boundary further and further away. While many of the reasons stated previously for artificial intelligence still held true during this time period, the rise of expert systems had given many a newfound optimism on AI again. An expert system is a program that answers questions and solves problems on a specific domain of knowledge based on logic programmed into it from experts in that said domain. Expert systems denote an important turning point in the field of artificial intelligence, from trying to develop a general intelligence, also referred to as strong AI, to narrow domain-specific based AI referred to as weak AI. The benefit of these weak AI systems was that they had tangible real-world impacts as opposed to the theories of other AI domains. For example, XCON, a logistics expert system written for the digital equipment incorporation was saving the company almost 40 million a year. As such, businesses around the world took note, and by 1986, sales of expert systems, that being the hardware and software involved, was approaching a billion dollars. Government agencies such as DARPA at this point once again came back to fund expert system based AI endeavors. Expert systems were also used in many other domains, as opposed to just business efficiency targeted use. Much of these domains being those for which a lot of data existed, such as stock analysis and medical diagnosis assistance, to list a few. A more fun implementation of an expert system was with IBM deep thought, the first computer to beat a chess grand master in a regular tournament game, Ben Larson in 1988. Another key point of this era of AI was a revival of connectionism. After little to no research done in over a decade, in 1982, physicist John Hopfield was able to prove a type of neural net, now referred to as a Hopfield net, that could learn and process information in an entirely new way. These neural nets were more biologically representative with all the nodes being both inputs and outputs to one another, they also draw parallels in how they solved problems as compared to neural nets today. That being these networks incorporated a global energy term with the goal to get to some minimum energy which would be our solution. Similar to the function of gradient descent and neural nets and optimization problems in present day. Around the same time that the Hopfield net was introduced, backpropoagation, a method for training AI, was popularized by David Rumelhart short for the backward propagation of errors, essentially it takes an error calculated at an output node and distributes it back through the net's layers, to resolve the error by adjusting node waves. As a cool side note, in John Hopfield's paper, neural networks and physical systems with emerging collective computational abilities he states as the name suggests were the large number of nodes in a neural net or equivalent components, biologically representative properties can emerge in computational systems. Moving on, also during this time period in 1987, Geoffrey Hinton, the great-great-grandson of the infamous George Boole, and who many now call the father of deep learning, demonstrated backpropoagation in multi-linear nets. Using this culmination of knowledge, he went on to co-invent the restricted Boltzmann machine, RBMs for short, expanding on the ideas presented by Hopfield nets, but with less errors. Instead of interconnection between every node they are separated into a visible input layer and then hidden layers, every visible layer is connected to the preceding hidden layer and there are no connections between nodes on the same layer as opposed to Hopfield nets. The structure of the RBM is a precursor to how many deep learning neural nets are structured today. Enthusiasm for expert systems was spiraling out of control in the sense that more was promised than they could do. Just as with the causes of the previous AI winter, cracks in the AI hype narrative began appearing. Expert systems while domain specific are nothing like the deep-learning algorithms of today. They query information from large databases and come up with conclusions from pre-programmed conditional instructions, in other words, if than L statements. This means they are brittle in the sense that they require data inputs to be in very particular formats to get desired outputs, which doesn't scale well when increasing to more complex problems. To add to this, as the field of software development was advancing, other non-intelligent programs began eating at the edges of what was thought only able to be done with expert systems at reduced cost, and as so, the value proposition of having expert systems began declining. Also as with the previous winter, while computing had advanced quite some ways and more so every year following the trend on Moor's Law, which had in fact led to the initial rise of expert systems, there was still not enough computing power to process data or memory to store data at affordable price points for more complex applications. If only there was a way to purchase computing power on an as-needed basis, giving the ability to store and process infinite amounts of data without purchasing your own data centers. The final nail in the coffin which led to this winter was the fact that black Monday had occurred in 1987 in which world markets crashed nearly 25% leading many to reevaluate their funding to these AI systems, including DARPA, once again reducing and retracting funding towards AI and putting it towards projects that would quote, "Yield more immediate results." At this point in the mid-90s, we have now seen the rise and fall of two AI hype cycles, with the golden years to the first AI winter, and the AI boom to the AI bust. While new innovations in AI have come out of both, the hype surrounding them has often muddled the narrative. Reflecting on these past videos on AI, from the thought of an artificial intelligence and beings dating back thousands of years, to the advent of computing technology and all the philosophies and theories in between, it's no surprise that the mass change that computing technology brought would trigger confusion. Over the past century, the field of artificial intelligence, more specifically, computing-based AI, has been becoming more and more defined, constantly changing and evolving what it means to be an AI. From machine and deep-learning to artificial human intelligence to super-intelligence and more, this information like in past hype cycles is once again becoming even more muddled and convoluted due to us reaching the peak of our third AI hype cycle and what the next few videos in this AI series hope to decipher, separating the types of AI and the future hype from the truly factual, practical present applications, such as a domain specific expertise of deep learning systems. Speaking of domain specific knowledge, Brilliant is a fantastic place to acquire it. If you want to learn more about the current state of artificial intelligence and neural networks, and we mean really learn how they work, from their foundational building blocks, Perceptrons, to more advanced architectures, then Brilliant.org is the place for you to go. Now what we love about how the topics in this courses are presented is at first an intuitive explanation is given, and then you are taken through related problems. If you get a problem wrong, you see an explanation for where you went wrong, and how to rectify that flaw. In a world where automation through algorithms will increasingly replace more jobs, it is up to us as individuals to keep our brain sharp and think of creative solutions to multi-disciplinary problems, and Brilliant truly is a platform that allows you to do so. For instance, beyond the courses Brilliant offers, every day there's a daily challenge that can cover a wide variety of topics in the STEM domain. These challenges are crafted in such a way in which they draw you in, and then allow you to learn a new concept due to their intuitive explanations. To support Futurology and learn more about Brilliant, go to Brilliant.org/futurology and sign up for free. Additionally, the first 200 people that go to that link will get 20% off their annual premium subscription. At this point the video has concluded. We'd like to thank you for taking the time to watch it. If you enjoyed it, consider supporting us on Patreon or YouTube membership to keep this brand growing, and if you have any topic suggestions, please leave them in the comments below. Consider subscribing for more content and check out our website and our parent company EarthOne for more information. This has been Encore. You've been watching Futurology, and we'll see you again soon. (upbeat electronic music) 