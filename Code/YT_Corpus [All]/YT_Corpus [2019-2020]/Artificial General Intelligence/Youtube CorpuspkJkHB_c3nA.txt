 so let me introduce max max tegmark or max to the weekly CBMM seminar it's great to have you max and it's a special day because you are here and because it's your birthday happy birthday to you everybody feel like there is a let's see like reactions like this oh happy birthday and for people don't know you have imagine not many but max is a cosmologists the physicist is the director of the future of life Institute is a very good writer this book that he wrote our mathematical universe is great highly recommended and he also likes AI so being a physicist you will speak today about physics for AI and AI for physics or vice-versa max thank you so much so it's a great birthday present for me to talk with you about stuff that I'm super excited about and to see you all here yeah physics and physics for ai ai ai for physics I mean how this is just news AI to do physics better and by physics for yeah I mean kind of the opposite how physics can hopefully give something back the help machine learning in the eye we all know of course how amazing the progress has been in AI in recent years just think about it not long ago robots couldn't walk now they can do backflips not long ago we didn't have self-driving cars now we have self flying rockets but can land themselves with AI not long ago hey I couldn't do face recognition well and now it can not only do that but it can simulate Tommy Poggio space saying things he never said not long ago yeah I couldn't save lives and now we have actually quite useful AI machine learning Diagnostics for prostate cancer lung cancer eye diseases we have AI the tune to win the annual protein folding competition and do such a good job on matching the three-dimensional measured x-ray crystallography shapes that it's probably gonna help accelerate drug discovery soon not long ago hey I couldn't beat us at go and now it's crushed not only human gamers let go and a chess but more interestingly its its crushed the AI developers who've spent decades handcrafting software to play these games which is all obsolete now by having google's alpha zero display against itself 400 a right thanks to Tommy boy Jewess postdoc demis hassabis and his gang of mine so if all this progress is happening how can it be used to help physics in lots of ways obviously for example just right here at MIT in our physics department I put together this long list of how a significant fraction of my colleagues there are using machine learning to do their physics better I'll just give you a few examples here to start off we have machine learning use right now to detect gravitational waves better as massive black holes a billion light years away or where their eyes crotchy into each other distorting space-time to such a small amount you have to measure the twenty few decimal places to see the thing machine learning is great for picking up these signals other colleagues are using machine learning to detect extrasolar planets and other solar systems that's part of the reason we've had we have over 4,000 discovered now machine learning is being used analyzed data at the Large Hadron Collider at CERN coming in at these crazy data rates where you can't have grad students do it and actually most of the hardware costs right now for CERN isn't the building the magnets and stuff it's the hardware for the running the machine learning physics how can physics pay back if that and help machine learning a little bit well both to hardware and through software on the hardware side for example my colleague Madden so Josh it's in the physics department boom number of you know has developed this optical chip for faster machine learning so it looks like your regular chip a little black thing but instead of the computation being done by electrons moving around in two dimensions the computation is done by photons moving around at the speed of light and it turns out the basis incredibly well-suited for matrix multiplication which is of course one of the key things we do in our neural networks which you can perhaps due on that scale about a million times more energy efficient than today's chips I think we'll be seeing a lot of improved hardware that will help a lot what about on the software side algorithm side what what can physics do there for AI well of course AI still has plenty of challenges right not only that there are tasks we don't know how to do but also problems for example you've all heard about how this machine learning system was deployed across courtrooms in America that was the people this hadn't understood well enough how it worked and hadn't realized that it was actually racially biased and there are so many other examples of where we just didn't understand our machine systems well enough and that caused problems Boeing certainly wishes that they had better understood the very simple automated system that control the 737 max before they deployed it and the traitor that Knights Capital certainly wish they had understood that understood there oughta made a trading system before they deployed it it lost ten million dollars per minute and kept going for forty four minutes until someone finally caught on and and turned it off now can you raise your hand if you've ever had a yahoo account see if you see any hand yeah so if you did you were hacked because all three billion yahoo accounts were hacked right and raise your hand again if you have a credit card okay it was probably also hacked because Equifax all other credit card information was breached these sort of problems the the first reaction people tend to have is say oh that has nothing to do with computer science it's because of the evil hackers the biddest but then we know better the truth of course isn't here too the fundamental problem was that these systems the security systems were not well enough understood by those who deployed it they hadn't understood that there were actually loopholes in them that the hackers could exploit right again showing how there's a lot of value if you can just get things you understand better and all right I'm particularly excited that you Tommy are here today because they love how you you Tommy draw the distinction between the engineering of intelligence on one hand and the science of intelligence on the other hand and you like to point out that the engineering of intelligence is very much about just trying to make things work at least work well enough that you can make money off of them whereas the science of intelligence aims not just to make it work but ask why does it work how does it work at a deeper level and I think this is really the key to addressing the kind of challenges I mentioned get a deeper understanding of your systems and my research group I'm so grateful I get to be a part that I get to be a part of the CBMM here's what some of my students look like before the lockdown and below you can see what you look like after the lockdown where they all have rectangles around their heads and that we focus on what I like to call intelligible intelligence which is exactly this idea that the more you can understand how your machine learning system actually works the more reason you might have to trust it I say intelligible intelligence not explained ability because this is a more ambitious goal I'm not talking about a system that can say some blah blah blah to you in human terms and explain why it diagnosed you with cancer I'm talking about able to understand things at a deeper level so you actually have reason to trust it because you understand it it's an ambitious goal I'm going to tell you today about four projects at the interface between physics and AI but all have bearing on this so let's start off with this one which is a paper together with a former grad student Eileen Wu who is at Stanford now and if you're here kyleannie hello so to motivate this let's start by taking a maie machine learning task which is very easy these days we just retrain the queuing networks from google deepmind to play this Atari game which may have you have played as kids in the beginning it sucks it keeps missing the ball all the time but pretty quickly it gets quite good catches the ball every time plays much and discovers this trick but you should always it put the should make a hole and then always keep putting the ball up into that hole in the corner and just rack up the points this feels intelligence right how does it actually work is this intelligible can we for example trust that it's always gonna work this well well in this case I can show you exactly how it works because we just train this network in on one of them computers at MIT and this is how it works it takes the pixel values that give the colors of all the pixels on the screen multiplies them by a big matrix apply some nonlinear estimation some more matrices that etc etc we know all of these 860 7408 parameters is it triple clear now how it works no this is completely useless as an explanation right I have if this were instead it's a very mission critical software that was controlling the vision system my self-driving car or whatever I would have athlete's no guarantees because I just have no clue how this is actually work so so can you do better I think yes and up I want to start by just dispelling a myth that some people seem to have internalized that the fact though that the power of machine learning somehow comes from its mysterious instability I think this is complete nonsense but some people seem sort of resigned to the idea that we can only get this great power because it's the secret sauce is somehow related to their the inscrutability I think rather that the power of deep learning comes from its differentiability by which I mean every single choice of parameters in a neural network still does something you can still take the gradient and you can therefore get information about how you should change the parameters so you can quickly get to the right place in an exponentially large search space instead of just practicing at random and the that opens up the possibilities that you could maybe have something that does just as well but it's much simpler so I'm gonna do a little test of your own neural networks by just showing you something more complicated than that ball in the break out game let's make this a little bit interactive try to predict where the ball is gonna go next and whenever you see any sort of pattern with your brain shout it out so we can all hear it don't forget to unmute yourself start by the most obvious things to come just don't overthink this what into any regularity at all what you see here well sometimes it bounces off a wall but then there are other times when it just turns around right before it hits the wall so there's the confusing part very good it seems to bounce sometimes against walls so there seems to be walls where are the walls are they just in the triangle shape or like Cirque is it a circular wall or what kind of what where are the walls would you say I think you're tracing the inside of a name this digit yeah can anyone say anything about which what any kind of wall what where it might be or what its shape is seems to be a square reckon it seems to be a rectangular wall yeah that's so it hits it seems to bounce and then when it's not bouncing it's is it how is it moving is it always moving in a straight line or is it like there's some sort of force acting on earth or what language particle does it seem like the law that whatever the force is that is the same force everywhere or could it be like different kinds of forces on different parts of the screen it could be held by an elastic band yeah maybe it's held by an elastic band so that it's doing like a harmonic oscillator or sinusoidally also at least on some part of the screen it looks like a gravity well one of you said that in one part of the screen it looks like it's doing harmonic motion and then another part of the screen it looks like it's gravity doing you yeah like a slingshot maneuver it spins out sure so so your neural networks are really great right you're not just telling me you're a vast number of parameters you've given me some real insight as to what's going on here so what happens if we just throw a write a simple feed-forward neural network at this and train it to just predict the next position from the past two positions for example but minimizing the loss it can do a pretty good job of predicting it but if you try to preach far into the future it starts sucking more and more what intuition does that give us what understanding does it give us nothing basically here are the parameters we got when we train in Orland to predict this so how can we do better so what ty Lina and I did was we borrowed for very old ideas that have been successful in physics and deploy the machine learning version of them so let me talk about these four ideas one one at a time may I there's a squirrel attached to the feeder here taste it the way yeah some it's neural network is very smart and it's been looking at this bird feeder for days and Wow so the first one is Occam's razor in physics if we have a simpler explanation that's just as accurate as a more complicated one we tend to prefer that one in an ray solomonoff put this principle on a firm mathematical footing with Kong with complexity theory together with Komal Gaurav and Jaden and other grades the only problem is a bear definition of of simple or complexity is is empty hard to evaluate generally so what polina i did was we figured physics got a pretty long way made a lot of progress using Occam's razor even though it was a little bit vague and fuzzy and how we had to find it so maybe we could - so we imp we defined the much simpler complexity criterion that's very fast to evaluate we just we said that them if you have an integer the amount of bits of information you need to store it is just how many digits long it is in binary so basically you take the log of the integer if it's a rational number it's just the two integers the complexity of the numerator plus the complexity of the denominator if it's a real number well you can convert it to an integer by dividing by the precision floor of your CPU and then take the logarithm of it so if you now want to just know I make a plot of how complicated how complex is a number I have if you look at the diagram here you get this very very interesting thing on the y-axis if you if you take generic real numbers the complexity just grows as a logarithm of it that's the red thick red line there which could scale up and down you change the precision floor but of course if it happens to be exactly 5/3 then it's certainly much simpler and fractions with small numerators and denominators are simpler and you'll see it later on that the code we have will automatically trying to minimize the complexity and discover simple fractions when they're there if we have a lot of data if you have a model with many parameters then we define the total complexity is just the sum of the complexity for all the parameters in it but we also look at the complexity of the data so if you just have a really lousy model that always predicts zero or something then you just have to store the whole data set if you have a model that predicts the data set pretty good then you only have to store the errors that you make in your predictions after you applied the model so we sum up the total complexity of everything and if you look at them how this plays out I wanna give a little shout-out to this this very simple measure of complexity of numbers because it it actually automatically gives you a very much more robust method of fitting data than chi-squared or minimizing the MS mean squared errors you can see here because if you have one bad data point it is in the left side the mean squared error will give a lot of weights to those points that are quite far from the model and it'll always compromise and pull away from the good data points towards the bad data points a little bit where's this other information theory based method a little disks it has the opposite incentive it has an incentive to just keep him doing even more accurately on the things that can already do accurately and ignore the rest so it'll just fit perfectly on on this stuff which is good a second idea from physics that we throw into the mix here is one that goes back to Julius Caesar so divide and conquer there is a story that when Galileo was sitting in church four hundred years ago maybe being a little bit bored by the sermon he noticed that the shampoo chandelier was swinging and try to model this he didn't try to make him all to predict everything about our universe at the same time he ignored what the priest was saying he ignored the color of the chandelier everything else just focused on the angle of the chandelier as a function of time and try to predict that using his pulse and when he did this you've revolutionized understanding of mechanics in physics so in the same spirit what we do here is instead of trying to recreate one big model that's gonna predict everything we put an ensemble of models that each is in sentiment especially specialized and and do well on some aspect of the data for example in the case of the ball you saw maybe on some dip some part of the screen and we basically could take the harmonic mean I have well the different models do and we can we can prove that this in distant courage is specialization you're gonna see how it works we also use lifelong learning a human physicist doesn't have to invent everything from scratch every time I see a new problem similarly we put this AI physicist in the series of environments and they can use what it had learned previously and unify and apply to the new ones so let me just show what happened so we take this kind of data and we feed it into the computer like this we just give the x and y coordinates of the points and off it goes a bunch of training it discovered entirely by itself that there seemed to be four different domains where the rules were different we didn't tell it but they were supposed to be four domains we didn't tell it where the boundaries were either it just learned that and within each domain it was able to do quite well and we can already see now as human physicist that you guys were all right whoever it was who said it looks like the harmonic oscillator you were right that's what it was doing in the upper left I didn't catch who said gravity but you were right then the up and then lower the left corner here it was an electromagnet field on the lower right and there was no force in the upper right so if you look more specifically now for example in in the lower left what it has learned is when minute when we use all comes razor to simplify down the neural network is that the predict that is the x and y coordinate of of the next step of where it's gonna be you take the previous X&Y coordinate and you multiply by this this matrix in the upper left corner and then you add this vector constant also now when you're a human though when you look at this if you see it's a one point nine nine nine nine nine nine zero what do you think what is this what got reaction what is it trying to tell you you it's probably point error it's probably trying to tell you that's really supposed to be too right but fortunately we have this quantitative information theory framework to test whether it actually fits better if it is too well and it discovers sure enough that that should be a - you can see on the next line but it also similarly discovers the small numbers on the right they're not supposed to be replaced by zero and if you just read get rid of the matrices and just write out what this is doing it's discovered this difference equation which you can we know how to transform into a differential equation which we recognize is Newton's laws which has been auto discovered here we ran this on a thousand or a hundred different worlds like this with different domains and different laws and it in the summary here you performed about a billion times better than just the simple neural network in terms of accuracy and it was also able to learn with a lot less data and a lot faster in addition to so the so intelligibility isn't nice just for helping gain trust and what you've learned but can also really aid the performance let me give you another example now of some of these physics tools applied here the physics formulas that have discovered were kind of simple right so together with Silvia my inner disco solute sealview was also on the call here he decided to see if we could be tougher harder ones this is a paper that was just published in science advances couple weeks ago and the symbolic regression what is that well that's Josh Tenenbaum and others here who work on it and tell you it's simply the challenge of taking a bunch of data and discovering a formula that fits it well for example Johannes Kepler spent four years looking at data like this from measurements of Mars and Cantley discovered that it was an ellipse wouldn't it be nice if we can what can be we could do that automatically if the function is linear then of course linear regression is so easy we do it all the time even if it's a function of many variables but if it's an arbitrary function this is known to be np-hard for the simple reason that there are exponentially many possible formulas that you could do if you if you just make your list of all formulas from the simplest of the gradually more complicated by the time you get to even relatively simple ones you might have waited a million years or longer than the age of the universe to get to the flying black body formula so that's obviously not good enough so in response to that there's been a lot of nice work Hardin Lipson has had had the best symbolic regression software to date when we started our project they used the genetic algorithm but we decided to see if physics could help okay so we we have this vision the lot of the problems we actually look at even if the random formula is and be hard to solve have special properties so Tommy Poggio for example has emphasized that most formulas we care actually are compositional even if it's a formula of nine variables you can usually rewrite it as a bunch of combinations of functions of fewer variables often two variables or less we often have symmetries as well or separable ax T where maybe the function of eight variables is just one function of three times one function of of the other five we also tend to be often be smooth with functions that neural networks can do well on how can we combine these ingredients from better to build a a test set we took you the 100 most famous or complicated equations out of the Fineman lectures on physics stuff like you see here and for each one of them we made a big table of numbers which is the starting point for the software to deal with so you put one column for each input variable given random very values and there the last column is what the formula evaluates to it your task is look at the table find the formula linear regression isn't good enough because these are not linear functions just so you see Julius Caesar here that's because we try to the divide and conquer strategy again using some of these physics ideas to see if we could break problems the simpler ones for example we would train a neural hour to fit the function really well still having no clue as to what the function actually was but then and then we do experiments on the neural network to test if it had any of these simplifying properties for example if the neural network discovered that actually the only way it depends on column 2 & 3 is by the ratio of the two then we could get would replace column two and column 3 by one column which was the ratio of them and and restart the software on a data file with one column less similarly if it discovered that there was it was separable and then you could replace this by two problems both which had less variables and this turned out to be very helpful because the basic region symbolic regression is so hard is because of the curse of dimensionality where problems get exponentially worse with the number of variables I'll skip over the details you can ask me later but if you look for example at the function here at the bottom the optics formula right you can see this somewhat messy expression is separable it's a function of theta sorry a function of Phi times a function of Delta and n it can train and earlier then it can discover that and that one breaks it apart under two problems and we have this recursive loop it keeps going until the poor individual parts get so easy that a brute-force search or a polynomial fitting or something like that and zap it so as an example look at this problem here of 9 variables Newton's law of gravity it's it's faced with the table with all these columns the dimension analysis first so it can reduce the number of variables a little bit and then it discovers Oh translational symmetry depends on C and B and upper right corner here by their difference so it can eliminate one column now it has one variable s then it discovers that it only depends on e and F by their distance one more simplification done and then it discovers this is separable so we can factor this into two mysteries etc until it can solve the whole thing and the results of this we were actually quite happy about this because this really nice eureka software that i mentioned from hardin Lipson in addition to costing money it can only do 471 out of the hundred mysteries the week threw at it our code that cereal did a heroic job on which you can find on github for free solves all 100 of them then we decided to see if we can break it by digging going back to his physics books like graduate textbooks and pulling out even more complicated equations like these ones and it still solved so this time the eureka software failed on 17 out of 20 could only do three whereas our code still solve 18 out of 20 and we have a new version of it now it can do even better and this is an example the way I think about actually of data compression lossy data compression more broadly I actually think of all of physics it's in a sense being lost data compression because we keep me walk around in the world beautiful sunny day and we almost immediately throw away almost all the information that comes into our senses and keep only the part that's really useful for us for predicting the future if you have a table of numbers like we gave to AI find them and you run gzip -9 on it for example to take up less space on your hard drive right if you were to have discovered that the ninth column is some function of the other eight can compress it even better and the more you can compress things the sense the more useful your formula is and if you take this information theory point of view of what we've done here is what came out of the a fireman's inner workings in the process of tackling the particular mystery of figuring out the kinetic energy formula and special relativity you can see it in the lower right and it's full glory and it there's a trade offs and you do data compression right between how much information you retain about what's useful and with the opposite of being lossy this is this trade-off between inaccuracy on the y-axis on one hand and complexity on there on the x-axis so you can get very low in accuracy by having the full formula the opposite extreme you could predict something super simple like always predict that the kinetic energy is zero now you get a huge loss huge in accuracy but the complexity is very low that's the upper left corner here all right now what do we do means tenth the value we tend to value things which do pretty well on both of these criteria right Occam's razor says simple is good but we also want accurate and you notice there's one other point on this frontier fritto frontier which is in a corner where it does pretty well on both complexity and accuracy which one is that please shout out there if you have a suggestion and be squared over 2 yes we were very excited about this because we had never taught this AI anything about you know high school physics and that particular approximation to kinetic energy that we humans found useful it's all on its own decided that MV squared over 2 is a really useful approximation fork in it for kinetic energy just from looking at the data because it's it can get a lot of accuracy with much less complexity than the full formula so I'm quite interested in the usually this is a tool not just for discovering the exact laws but also for discovering really useful approximate formulas for things because science and if thinking about science is there compression again more broadly this segues into a third project I want to tell you about just very very briefly suppose you this is the paper that I also wrote with with pileinn whoo very related to what Naftali fishby spoke spoke about in his recent CBMM talk suppose I have a bunch of cats and dogs here and you want to class your task is to classify these pictures whether they're cats or dogs and suppose I tell you that you have to do some sort of lossy data compression instead of sending the whole picture into a classifier you have to just do it clustering and Devine all the pictures in the group save three groups for example and you can only tell me whether it's the images in Group one two or three and based on that integer you now have to predict whether it's a cat or a dog and now what's the best grouping to do right should you just it's not so obvious and and there's a lot of this interesting literature about what's the optimal number of groups to have and what they should be and so on so we were actually very excited that we were able to solve exactly this problem for the special case where there when it's binary classification like cats versus dogs we Moran examples also for M list four to two different digits sevens and ones are easy to confuse and for fashion M list and and without getting into detail what we found was that the front the trade-off between how simple things are which always simpler if you have few groups or just lower entropy in your compressed data set so farther to the right in this case is simple there's a trade-off between that and how much information you retain about whether it's a cat or a dog in this case the C 4/10 curve is the one we're talking about if if you look at the original images there's 0.7 bits of mutual information it's not 1 because it's kind of hard but if you only do two clusters one we're the ones that you guessed it's a cat and the other cluster where yes it's a dog the mutual information between your best guess there and whether it is a cat a dog is only 0.6 bits do a lot better with 0.7 and the fun physics link here is that if you look at these corners we saw in the plot I just showed you before with the kinetic energy that the cool the things that we humans find most interesting are in these corners where you do sort of unusually well on both simplicity and inaccuracy the corners here which also pop out they actually correspond to face transitions in the machine learning where you have these bifurcations be going from two classes the three classes the four classes and so on and a significant fraction of all papers and physics are about phase transitions and I would love to chat with many of you afterwards late if you have ideas so I think I just have a sense that there's a lot more fruitful work to be done by linking up phase transitions in machine learning with with with stuff that's been studying the physics the very last thing I want to leave you with is coming back to discovery of physics equations from looking at moving things and start by confessing there I felt that Kylene and I were kind of excited about this by physicists parties we felt we kind of cheated because the truth is we had these pictures of the moving dots right but we didn't send in this the image we sent in the X&Y coordinates and there was a lot of human intelligence that went into figuring out that you should marry the manager the x and y component coordinate of the of the moving dot wouldn't it be not much nicer if you could skip that step and to start with the raw video like suppose suppose you just have a video of something moving like this on some weird background any any compliments about the artistry of this coat let's see your money on the rescue who made it wouldn't it be cool if you could just send in the raw video and it would discover that it should map this in data compress this mapping into some latent space which actually involves measuring the X&Y coordinate all by itself this might sound so no and and then you could study that late in space and try to figure out what the actual equations are but this is actually a much harder problem than then you might think because if you can solve it then with any kind of image then you will also be able to automatically solve it but the way we phrase the problem it doesn't even matter that an image it's an image we just send them you know vector of ten thousand numbers or whatever it has to figure out a way of mapping out ten thousand dimensional space into a two dimensional space it should also be able to solve it if you're looking for example at the image through a strange distorting lens like this and which case what you want is not at all to measure the action y-coordinate of where it is right you would like the mapping that the AI the machine learned discovers the undistorted image so you actually get back the actual useful coordinates of where things are this is what we humans do every day when we walk through the world because the stereoscopic projection means that where we see around us is actually kind of distorted we don't just get X Y Z right so see leo Miami Driscoll worked very hard on this you had a very simple architecture we send in these video frames we have an order encoder that maps it into a lake in space and and make sure it can map it back and then we have a time evolution neural network that tries to go from the latent space to the next step or the last tube to the next and the the big challenge is here we want to bring in or come again with his razor and ask what kind of latent space will give us the simplest laws of physics for how the latent space evolves so we thought a lot about how can we define simple in in a differentiable way that we can train and we started thinking about this guy about Einstein so if you have a mapping but a neural network does right from its input to its output you might use Einsteins work on curved spaces to say what may be a space I'm at your neural network is simpler if it doesn't curve this thing so much if this mapping has very small Riemann tensor components for instance and you can code this up but it's numerically very painful because you can't take many derivative many gradients multiple different third derivatives or the higher and so on and then we start realize there's much simpler thing you can actually do which is if you just put a penalty on on the actual derivative of the gradient do you encouraging the gradient another network to be constant if the gradient of the neural network is constant then as you can see from the second Einstein equation here all the curvature is always zero and it's nice and simple so we decided to try that first and remarkably it actually worked great in the beginning we had a lot of problems where it would discover weird latent spaces that looked like a cat even though the lines were supposed to be parallel and so on I could tell you more about amusing reasons this happened but eventually CW persevered and it was able to discover my simple latent spaces for everything even though even though we were distorting it in some cases with this weird lens so that the original just really taking the X&Y coordinate the image it would have gotten some really weird late in space where the laws of motion were super complicated it discovered instead undistorted motion and I can either stop right now because it I've been going for about 41 minutes right or I can take three more minutes and tell you a little bit about how I feel this is exciting also for physics again which do you prefer oh for another three minutes okay I will I will do that so if I put on my physicist app again this stuff of latent spaces you know we tend to we used to think in physics that there really was Euclidian space out there that we just discover it right but now you those of you who would do awesome neuroscience I've discovered that we organism will from the deter that invent internal representations and latent spaces and so on so you might wonder could it be that even what we the physical space is actually also a latent space and I actually think that's true because we learned from Einstein but this whole thing with inertial frames for example the way you're supposed to have the x coordinate and y coordinate and the z axis all perpendicular to each other and it's not supposed to be accelerating it's not but you don't have to do that in general relativity Einstein said oh forget about that you don't have to be in an inertial frame you can have any coordinates if you use general relativity why do we still use these this for simple latent space where our axes are perpendicular and and and things aren't easy and an object address that remains at rest I think it's because our brains are choosing for the regulating space where the laws of motion are as simple as possible again we interpret it that way and I want to just show you Siddalee and I were able to automate that process in in this case so we gave it five different examples with magnetism and I'm on a cost layer and a nonlinear quartic oscillator etc etc in all of these five cases even though we sent in video sequences it mapped things into a two dimensional latent space five different ones right because these are five different neural networks and if you look more closely you can see that they some of them are very squished relative to the others because the laws of physics in some cases would still work out just fine if you scale the axes by different amounts or in fact did any affine transformation where you just take this two-dimensional space they take each vector and multiply it by a two by two matrix and add another vector so there are these six degrees of freedom left and we were wondering could it be that if you just insist on using these degrees of freedom that you have to make the equations look as simple as possible if it would actually discover what we you wench consider simple so sealview max first fine he took all the rocket images from all the videos and map them into all of the five latent spaces figure out just what I find mapping connected the different spaces so you could put them all together into a single unified latent space and now we still have these six parameters we can play with we can shifting sideways for example put the origin wherever you want on the right side or lower left on the lower right corner rather you see the equations that that of motion that sealview had put in to start with and you can see that some of them don't care if you change the origin like the first one but the harmonic oscillator it cares those equations will be the simplest if you put the origin in such a place that where the same to the harmonic oscillator was then there it's gonna be less terms in the equation is insured us if you if you try and see if it tries all the origins and all the different shifts and plots how complicated all the equations are together he finds that there's a certain shift that's the best and then you can start rotating the space what's best rotation in this case there are some of these formulas which which get much simpler when you rotate in a certain by a certain angle and others we pick that and then similarly you can do a little bit of shearing and at the end it discovers exactly what we would consider the simplest space where in fact the X&Y coordinates there's no relative stretching and the equations are beautifully simple and I suspect that this is very much what's going on in physics actually we keep coming up with a representation of the world such that the description that comes as simple as possible for us if because if we have that internal representation in our brains then that minimizes the amount of computation we have to do when we try to predict the future and figure out what are the best actions to take so in summary I've told you that it's not just the case that machine learning and AI is helping physics enormously in so many areas but I also feel that physics that physics and the science of intelligence were broadly really helped machine learning in various ways I've given you a series of examples for example when we came by combining Occam's razor the writing conquer another idea is how you can go broadest or the video to find a latent space where the equations of motions are so simple that you can use AI Fineman to actually discover the equations for them and I'm also showing you how you can also find which are the most useful approximate equations for fields where maybe an approximate equation is the best that you can hope for and I would like to end by saying that we would love to collaborate with with those of you on the call here if you have any data set lying around or on your own your own hard drives where you think there might be some patterns yet to be discovered we really really fund it to see if any of these tools could help discover them thank you thank you max great let's gain from maxsa great applause that was great 