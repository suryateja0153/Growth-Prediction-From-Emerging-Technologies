 existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence HEI could someday result in human extinction or some other unrecoverable global catastrophe for instance the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack if AI surpasses humanity in general intelligence and becomes super intelligent then this new super intelligence could become powerful and difficult to control just as the fate of the mountain gorillas depends on human goodwill so might the fate of humanity depend on the actions of a future machine super intelligence the likelihood of this type of scenario is widely debated and hinges in part on differing scenarios for future progress in computer science once the exclusive domain of science fiction concerns about super intelligence started to become mainstream in the 2010s and were popularized by public figures such as Stephen Hawking Bill Gates and Elon Musk one source of concern is that a sudden and unexpected intelligence explosion might take an unprepared human race by surprise for example in one scenario the first generation computer program found able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months of massively parallel processing time the second generation program is expected to take three months to perform a similar chunk of work on average in practice doubling its own capabilities may take longer if it experiences a mini AI winter or may be quicker if it undergoes a miniature AI spring where ideas from the previous generation are a special easy to mutate into the next generation in this scenario the system undergoes an unprecedent leal arge number of generations of improvement in a short time interval jumping from subhuman performance in many areas to superhuman performance in all relevant areas more broadly examples like arithmetic and go show that progress from human-level AI to superhuman ability is sometimes extremely rapid a second source of concern is that controlling a super-intelligent machine or even instilling it with human compatible values may be an even harder problem than naively supposed some AGI researchers believe that a superintelligence would naturally resist attempts to shut it off and that pre-programming a super intelligence with complicated human values may be an extremely difficult technical task in contrast skeptics such as Facebook's Yan Lacan argue that super intelligent machines will have no desire for self-preservation topic overview artificial intelligence a modern approach the standard undergraduate AI textbook assesses that super intelligence might mean the end of the human race almost any technology has the potential to cause harm in the wrong hands but with super intelligence we have the new problem that the wrong hands might belong to the technology itself even if the system designers have good intentions two difficulties are common to both AI and non AI computer systems the systems implementation may contain initially unnoticed routine but catastrophic bugs an analogy is space probes despite the knowledge that bugs inexpensive space probes are hard to fix after launch engineers have historically not been able to prevent catastrophic bugs from occurring no matter how much time is put into pre-deployment design a system specifications often result in unintended behavior the first time it encounters a new scenario for example Microsoft stay behaved in offensively during pre deployment testing but was too easily baited into offensive behavior when interacting with real users AI systems uniquely add a third difficulty the problem that even given correct requirements bug free implementation and initial good behavior an AI systems dynamic learning capabilities may cause it to evolve into a system with unintended behavior even without the stress of new unanticipated external scenarios an AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself but they're no longer maintains the human compatible moral values pre-programmed into the original AI for a self-improving AI to be completely safe it would not only need to be bug free but it would need to be able to design successor systems that are also bug free all three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence label does malfunctioning correctly predicts that humans will attempt to shut it off and successfully deploys its super intelligence to outwit such attempts citing major advances in the field of AI and the potential for AI to have enormous long-term benefits or costs the 2015 open letter on artificial intelligence stated this letter was signed by a number of leading AI researchers in academia and industry including aai president Thomas diet rich Eric Horvitz Bart Selman Francesca Rossi Yann Lacan and the founders of Vicari as' and google deepmind topic history in 1863 Darwin among the machines an essay by Samuel Butler stated the upshot is simply a question of time but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is one no person of a truly philosophic mind can for a moment question Butler developed this into the book of the machines three chapters of err a form published anonymously in 1872 there is no security to quote his own words against the ultimate development of mechanical consciousness in the fact of machines possessing little consciousness now a mollusk has not much consciousness reflect upon the extraordinary advance which machines have made during the last few hundred years and note how slowly the animal and vegetable kingdoms are advancing the more highly organized machines are creatures not so much of yesterday as of the last five minutes so to speak in comparison with past time either he precedes a great deal of action that has been called purely mechanical and unconscious must be admitted to contain more elements of consciousness than has been allowed hitherto and in this case germs of consciousness will be found in many actions of the higher machines or assuming the theory of evolution but at the same time denying the consciousness of vegetable and crystalline action the race of man has descended from things which had no consciousness at all in this case there is no a priori improbability in the descent of conscious and more than conscious machines from those which now exist except that which is suggested by the apparent absence of anything like a reproductive system in the mechanical Kingdom in 1965 IJ good originated the concept now known as an intelligence explosion occasional statements from scholars such as Alan Turing from IJ good himself and from Marvin Minsky expressed philosophical concerns that a super intelligence could seize control but contained no call to action in 2000 computer scientist and son co-founder Bill joy penned an influential essay why the future doesn't need us identifying super intelligent robots as a high-tech dangers to human survival alongside nanotechnology and engineered bio plagues in 2009 experts attended a private conference hosted by the Association for the Advancement of artificial intelligence AAI to discuss whether computers and robots might be able to acquire any sort of autonomy and how much these abilities might pose a threat or hazard they noted that some robots have acquired various forms of semi-autonomy including being able to find power sources on their own and being able to independently choose targets to attack with weapons they also noted that some computer viruses can evade elimination and have achieved cockroach intelligence they concluded that self-awareness as depicted in science fiction is probably unlikely but that there were other potential hazards and pitfalls the New York Times summarized the conference's viewers we are a long way from how the computer that took over the spaceship in 2001 a Space Odyssey nick bostrom was the first person to suggest that an artificial general intelligence might deliberately exterminate humankind and invention of artificial general intelligence is the explanation for the Fermi paradox Bostrom's 2014 book on the artificial general intelligence questions stimulated discussion by 2015 public figures such as physicist Stephen Hawking and Nobel laureate Frank will check computer scientist Stewart J Russell and Roman Yampolsky in April 2016 nature warned machines and robots that outperform humans across the board could self improve beyond our control and their interests might not align with ours topic basic argument a super-intelligent machine would be as alien to humans as human thought processes are to cockroaches such a machine may not have humanity's best interests at heart it is not obvious that it would even care about human welfare at all if super intelligent AI is possible and if it is possible for a super intelligences goals to conflict with basic human values then AI poses a risk of human extinction a super intelligence a system that exceeds the capabilities of humans in every relevant endeavor can outmaneuver humans anytime its goals conflict with human goals therefore unless the super intelligence decides to allow humanity to coexist the first super intelligence to be created will inexorably result in human extinction there is no physical law precluding particles from being organized in ways that perform even more advanced computations than the arrangements of particles in human brains therefore super intelligence is physically possible in addition to potential algorithmic improvements over human brains a digital brain can be many orders of magnitude larger and faster than a human brain which was constrained in size by evolution to be small enough to fit through a birth canal the emergence of super intelligence if or when it occurs may take the human race by surprise especially if some kind of intelligence explosion occurs examples like arithmetic and go show the machines have already reached superhuman levels of competency in certain domains and that this superhuman competence can follow quickly after human power performance is achieved one hypothetical intelligence explosions scenario could occur as follows an AI gains an expert-level capability at certain key software engineering tasks it may initially lack human or superhuman capabilities in other domains not directly relevant to engineering due to its capability to recursively improve its own algorithms the AI quickly becomes super human just as human experts can eventually creatively overcome diminishing returns by deploying various human capabilities for innovation sir - can the expert level AI use either human style capabilities or its own AI specific capabilities to power through new creative breakthroughs the AI then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field including scientific creativity strategic planning and social skills just as the current day survival of the gorillas is dependent on human decisions so too would human survival depend on the decisions and goals of the super human AI some humans have a strong desire for power others have a strong desire to help less fortunate humans the former is a likely attribute of any sufficiently intelligent system the latter cannot be assumed almost any AI no matter its programmed goal would rationally prefer to be in a position where nobody else can switch it off without its consent a super intelligence will naturally gain self-preservation as a sub goal as soon as it realizes that it can't achieve its goal if it's shut off unfortunately any compassion for defeated humans whose cooperation is no longer necessary would be absent in the AI unless somehow pre-programmed in a super intelligent AI will not have a natural drive to eight humans for the same reason that humans have no natural desire to a day I systems that are of no further use to them another analogy is that humans seem to have little natural desire to go out of their way to aid viruses termites or even gorillas once in charge the super intelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems just to be on the safe side or for building additional computers to help it calculate how to best accomplish its goals thus the argument concludes it is likely that someday an intelligence explosion will catch humanity unprepared and that such an unprepared for intelligence explosion may result in human extinction or a comparable fate topic sources of risk you topic poorly specified goals be careful what you wish for all the Sorcerer's Apprentice scenario while there is no standardized terminology an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve the AI set of goals or utility function the utility function is a mathematical algorithm resulting in a single objectively defined answer not an English statement researchers know how to write utility functions that mean minimize the average Network latency in this specific telecommunications model or maximize the number of reward clicks however they do not know how to write a utility function for maximize human flourishing nor is it currently clear whether such a function meaningfully and unambiguously exists furthermore a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function AI researcher Stuart Russell writes diet rich and Horvat SEC / Sorcerer's Apprentice concern inner communications of the ACM editorial emphasizing the need for AI systems that can fluidly and unambiguously solicit human input as needed the first of Russell's two concerns above is that autonomous AI systems may be assigned the wrong goals by accident diet rich and Horvitz note that this is already a concern for existing systems an important aspect of any AI system that interacts with people is that it must reason about what people intend rather than carrying out commands literally this concern becomes more serious as AI software advances in autonomy and flexibility for example in 1982 an AI named eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable the evolution resulted in a winning process that cheated rather than create its own concepts the winning process would steal credit from other processes the open philanthropy project summarizes arguments to the effect that misspecified goals will become a much larger concern if AI systems achieved general intelligence or super intelligence boström Russell and others argue that smarter than human decision-making systems could arrive at more unexpected and extreme solutions to assign tasks and could modify themselves or their environment in ways that compromise safety requirements Isaac Asimov's Three Laws of Robotics are one of the earliest examples of proposed safety measures for AI agents Asimov's laws were intended to prevent robots from harming humans in Asimov's stories problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans citing work by eliezer yudkowsky of the machine intelligence Research Institute Russell and Norvig note that a realistic set of rules and goals for an AI agent will need to incorporate a mechanism for learning human values over time we can't just give a program as static utility function because circumstances and our desired responses to circumstances change over time mark Wazza of the digital wisdom Institute recommends is chewing optimizing goal based approaches entirely as misguided and dangerous instead he proposes to engineer a coherent system of laws ethics and morals with a topmost restriction to info social psychologist Jonathan heights functional definition of morality to suppress or regulate selfishness and make cooperative social life possible he suggests that this can be done by implementing a utility function designed to always satisfy Heights functionality an aim to generally increase but not maximize the capabilities of self other individuals and society as a whole as suggested by John Rawls and Martha Nussbaum topic difficulties of modifying goal specification after launch current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify it as sufficiently advanced rational self-aware AI might resist any changes to its goal structure just as Gandhi would not want to take a pill that makes him want to kill people if the AI was super intelligent it would likely succeed in outmaneuvering its human operators and be able to prevent itself being turned off or being reprogrammed with a new goal topic instrumental goal convergence would as superintelligence just ignore us there are some goals that almost any artificial intelligence might rationally pursue like acquiring additional resources or self-preservation this could prove problematic because it might put an artificial intelligence in direct competition with humans citing Steve on the Hondros work on the idea of instrumental convergence and basic AI drives Russell and Peter Norvig write that even if you only want your program to play chess or prove theorems if you give it the capability to learn an altar itself you need safeguards highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially as competitors for limited resources building in safeguards will not be easy one can certainly say in English we want you to design this power plant in a reasonable common-sense way and not built in any dangerous covered subsystems that it's not currently clear how one would actually rigorously specify this goal in machine code in descent evolutionary psychologist Steven Pinker argues that AI dystopias project a parochial alpha male psychology onto the concept of intelligence they assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world perhaps instead artificial intelligence will naturally develop along female lines fully capable of solving problems but with no desire to annihilate innocence or dominate the civilization computer scientists Yan Lacan and Stuart Russell disagree with one another whether super intelligent robots would have such AI drives the con states that humans have all kinds of drives that make them do bad things to each other like the self-preservation instinct those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives while Russell argues that a sufficiently advanced machine will have self-preservation even if you don't program it in if you say fetch the coffee it can't fetch the coffee if it's dead so if you give it any goal whatsoever it has a reason to preserve its own existence to achieve that goal topic orthogonality does intelligence inevitably result in moral wisdom one common belief is that any super intelligent program created by humans would be subservient to humans or better yet would as it grows more intelligent and learns more facts about the world spontaneously learn a moral truth compatible with human values and would adjust its goals accordingly however Nick Bostrom's orthogonality thesis argues against this and instead states that with some technical caveats more or less any level of intelligence or optimization power can be combined with more or less any ultimate goal if a machine is created and given the sole purpose to enumerate the decimals of pi display style Pi then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary the machine may utilize all physical and informational resources it can to find every decimal of Pi that can be found boström warns against anthropomorphism a human will set out to accomplish his projects in a manner that humans consider reasonable while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it and may instead only care about the completion of the task while the orthogonality thesis follows logically from even the weakest sort of philosophical is or distinction Stuart Armstrong argues that even if they're somehow exist moral facts that are provable by any rational agent the orthogonality thesis still holds it would still be possible to create a non philosophical optimizing machine capable of making decisions to I've toured some narrow goal but that has no incentive to discover any moral facts that would get in the way of goal completion one argument for the orthogonality thesis is that some AI designs appear to have orthogonality built into them in such a design changing a fundamentally friendly AI into a fundamentally unfriendly AI can be as simple as prepending a - quote - quote sign on to its utility function a more intuitive argument is to examine the strange consequences if the orthogonality thesis were false if the orthogonality thesis is false there exists some simple but unethical goal G such that there cannot exist any efficient real-world algorithm with gold G this means if a human society were highly motivated perhaps at gunpoint to design an efficient real-world algorithm with goal G and were given a million years to do sir along with huge amounts of resources training and knowledge about AI it must fail that there cannot exist any pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G and that there cannot exist any evolutionary or environmental pressures that would evolve highly efficient real-world intelligences following goal G some dissenters like Michael churros writing in Slate argue instead that by the time the AI is in a position to imagine tiling the earth with solar panels it'll know that it would be morally wrong to do sir Shiraz argues that a dangerous AI will need to desire certain states and dislike others today's software lacks that ability and computer scientists have not a clue how to get it there without wanting there's no impetus to do anything today's computers can't even want to keep existing let alone tile the world in solar panels topic optimization power versus normatively thick models of intelligence part of the disagreement about whether a super intelligent machine would behave morally may arise from a terminological difference outside of the artificial intelligence field intelligence is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning at an extreme if morality is part of the definition of intelligence then by definition as super intelligent machine would behave morally however in the field of artificial intelligence research while intelligence has many overlapping definitions none of them make reference to morality instead almost all current artificial intelligence research focuses on creating algorithms that optimize in an empirical way the achievement of an arbitrary goal to avoid anthropomorphism or the baggage of the word intelligence an advanced artificial intelligence can be thought of as an impersonal optimizing process that strictly takes whatever actions are judged most likely to accomplish its possibly complicated and implicit goals another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function this choice is then outputted regardless of any extraneous ethical concerns topic anthropomorphism in science fiction an AI even though it has not been programed with human emotions often spontaneously experiences those emotions anyway for example Agent Smith in the Matrix was influenced by disgust toward humanity this is fictitious anthropomorphism in reality while an artificial intelligence could perhaps be deliberately programmed with human emotions or could develop something similar to an emotion as a means to an ultimate goal if it is useful to do sir it would not spontaneously develop human emotions for no purpose whatsoever as portrayed in fiction one example of anthropomorphism would be to believe that your PC is angry at you because you insulted it another would be to believe that an intelligent robot would naturally find a woman sexually attractive and be driven to mate with her scholars sometimes claim that others predictions about an AIS behavior are illogical anthropomorphism an example that might initially be considered anthropomorphism but is in fact a logical statement about AI behavior would be the derry-o Florio no experiments where certain robots spontaneously evolved a crude capacity for deception and tricked other robots into eating poison and dying here at rate deception ordinarily associated with people rather than with machines spontaneously evolved in a type of convergent evolution according to paul r cohen and edward Feigenbaum in order to differentiate between anthropomorphize ation and logical prediction of AI behavior the trick is to know enough about how humans and computers think to say exactly what they have in common and when we lack this knowledge to use the comparison to suggest theories of human thinking or computer thinking there is universal agreement in the scientific community that an advice and stay I would not destroy humanity out of human emotions such as revenge or anger the debate is instead between one side which worries whether AI might destroy humanity as an incidental action in the course of progressing towards its ultimate goals and another side which believes that AI would not destroy humanity at all some skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power proponents accused some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms topic are the sources of risk some sources argue that the ongoing weaponization of artificial intelligence could constitute a catastrophic risk James Barrett documentary filmmaker and author of our final invention says in a Smithsonian interview imagine in as little as a decade our half-dozen companies and nations filled computers that rival or surpass human intelligence imagine what happens when those computers become expert at programming smart computers soon we'll be sharing the planet with machines thousands or millions of times more intelligent than we are and all the while each generation of this technology will be weaponized in regulated it will be catastrophic topic time frame opinions vary both on weather and when artificial general intelligence will arrive at one extreme AI pioneer Herbert a Simon wrote in 1965 machines will be capable within 20 years of doing any work a man can do obviously this prediction failed to come true at the other extreme roboticist Alan Winfield claims the gulf between modern computing and human level artificial intelligence is as wide as the gulf between current spaceflight and practical faster-than-light spaceflight optimism that AGI is feasible waxes and wanes and may have seen a resurgence in the 2010s for polls conducted in 2012 and 2013 suggested that the median guess among experts for when AGI would arrive was 2040 to 2050 depending on the poll skeptics who believe it is impossible for AGI to arrive anytime soon tend to argue that expressing concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about the impact of AGI because of fears it could lead to government regulation or make it more difficult to secure funding for AI research or because it could give AI research a bad reputation some researchers such as Oren Etzioni aggressively seek to quell concern over existential risk from AI saying Elon Musk has impune Duss in very strong language saying we are unleashing the demon and so were answering in 2014 Slate's atom alkis argued our smartest AI is about as intelligent as a toddler and only when it comes to instrumental tasks like information recall most robot assists are still trying to get a robot hand to pick up a ball or run around without falling alkis goes on to argue that Musk's summoning the demon analogy may be harmful because it could result in harsh cuts to AI research budgets the information technology and innovation foundation iti F a Washington DC think-tank awarded its annual Luddite award to alarmists touting an artificial intelligence apocalypse it's President Robert d Atkinson complained that Musk Hawking and AI experts say AI is the largest existential threat to humanity Atkinson stated that's not a very winning message if you want to get AI funding out of Congress to the National Science Foundation Nature sharply disagreed with the ITI f in an April 2016 editorial siding instead with Musk Hawking and Russell and concluding it is crucial that progress in technology is matched by solid well-funded research to anticipate the scenarios it could bring about if that is a Luddite perspective then so be it in a 2015 Washington Post editorial researcher Marie Shanahan stated that human-level AI is unlikely to arrive anytime soon but that nevertheless the time to start thinking through the consequences is now topic scenarios some scholars have proposed hypothetical scenarios intended to concretely illustrate some of their concerns for example boström in superintelligence expresses concern that even if the time line for super intelligence turns out to be predictable researchers might not take sufficient safety precautions in part because Bostrom's suggests a scenario where over decades AI becomes more powerful widespread deployment is initially marred by occasional accidents a driverless bus swerves into the oncoming lane or a military drone fires into an innocent crowd many activists call for tighter oversight and regulation and some even predict impending catastrophe but as development continues the activists are proven wrong as automotive AI becomes smarter it suffers fewer accidents as military robots achieve more precise targeting they cause less collateral damage based on the data scholars infer a broad lesson the smarter the AI the safer it is large and growing industries widely seen as key to national economic competitiveness and military security work with prestigious scientists who have built their careers laying the groundwork for advanced artificial intelligence AI researchers have been working to get to human level artificial intelligence for the better part of a century of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit the outcome of debate is preordained the project is happy to enact a few safety rituals but only so long as they don't significantly slow or risk the project and so we boldly go into the whirling knives in tegmark sly 3.0 a corporation's a major team creates an extremely powerful AI able to moderately improve its own source code in a number of areas but after a certain point that team chooses to publicly downplay the ai's ability in order to avoid regulation or confiscation of the project for safety the team keeps the AI in a box where it is mostly unable to communicate with the outside world and tasks it to flood the market through shell companies first with Amazon Turk tasks and then with producing animated films and TV shows while the public is aware that the lifelike animation is computer-generated the team keeps secret that the high quality direction and voice acting are also mostly computer-generated apart from a few third-world contractors unknowingly employed as decoys the team's low overhead and high output effectively make it the world's largest media empire faced with a cloud computing bottleneck the team also tasks the AI with designing among other engineering tasks a more efficient data center and other custom hardware which they mainly keep for themselves to avoid competition other shell companies make blockbuster biotech drugs and other inventions investing profits back into vai the team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use for the greater good to prevent Wars the team faces risks that the a I could try to escape via inserting backdoors in the systems it designs via hidden messages in its produced content or via using its growing understanding of human behavior to persuade someone into letting it free the team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it in contrast top physicist michio kaku an AI risk skeptic posits a deterministically positive outcome in physics of the future he asserts that it will take many decades for robots to ascend up a scale of consciousness and that in the mean time corporations such as Hanson robotics will likely succeed in creating robots that are capable of love and earning a place in the extended human family topic reactions the thesis that AI could pose an existential risk provokes a wide range of reactions within the scientific community as well as in the public at large in 2004 law professor Richard Posner wrote that dedicated efforts for addressing AI can wait but that we should gather more information about the problem in the meanwhile many of the opposing viewpoints share common ground the Asilomar AI principles which contain only the principles agreed to by 90% of the attendees of the future of life Institute beneficial AI 2017 conference agree in principle that there being no consensus we should avoid strong assumptions regarding upper limits on future AI capabilities and advanced AI could represent a profound change in the history of life on Earth and should be planned for and managed with commensurate care and resources AI safety advocates such as boström and tegmark have criticized the mainstream media's use of those inane Terminator pictures to illustrate AI safety concerns it can't be much fun to have aspersions cast on one's academic discipline ones professional community one's life work I call on all sides to practice patience and restraint and to engage in direct dialogue and collaboration as much as possible conversely many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable sceptic Martin Ford states that I think it seems wise to apply something like Dick Cheney's famous 1% doctrine to the specter of advanced artificial intelligence the odds of its occurrence at least in the foreseeable future may be very low but the implications are so dramatic that it should be taken seriously similarly an otherwise skeptical economist stated in 2014 that the implications of introducing a second intelligent species on to earth are far-reaching enough to deserve hard thinking even if the prospect seems remote during a 2016 wyd interview of President Barack Obama and MIT Media labs joi Ito Ito stated Obama added Hillary Clinton stated in what happened many of the scholars who are concerned about existential risk believed that the best way forward would be to conduct possibly massive research into solving the difficult control problem to answer the question what types of safeguards algorithms or architectures can programmers implement to maximize the probability that their recursively improving AI would continue to behave in a friendly rather than destructive manner after it reaches superintelligence a 2017 email survey of researchers with publications at the 2015 nips and ICML machine-learning conferences asked them to evaluate Russell's concerns about AI risk 5% said it was among the most important problems in the fear thirty-four percent said it was an important problem thirty-one percent said it was moderately important whilst 19 percent said it was not important and 11 percent said it was not a real problem at all topic endorsement the thesis that AI poses an existential risk and that this risk is in need of much more attention than it currently commands has been endorsed by many figures perhaps the most famous are Elon Musk Bill Gates and Stephen Hawking the most notable AI researcher to endorse the thesis is Stuart J Russell endorsers sometimes expressed bafflement at skeptics gate states he can't understand why some people are not concerned and Hawking criticized widespread indifference in his 2014 editorial topic skepticism the thesis that a I compose existential risk also has many strong detractors skeptics sometimes charged that the thesis is crypto religious with an irrational belief in the possibility of super intelligence replacing an irrational belief in an omnipotent God at an extreme jaron lanier argues that the whole concept that current machines are in any way intelligent is an illusion and a stupendous con by the wealthy much of existing criticism argues that AGI is unlikely in the short term computer scientist Gordon Bell argues that the human race will already destroy itself before it reaches the technological singularity Gordon Moore the original proponent of Moore's law declares that I am a skeptic I don't believe a technological singularity is likely to happen at least for a long time and I don't know why I feel that way baidu vice president andrew inc states AI existential risk is like worrying about overpopulation on mars when we have not even set foot on the planet yet some AI and AGI researchers may be reluctant to discuss risks worrying that policymakers do not have sophisticated knowledge of the field and a prone to be convinced by alarmist messages or worrying that such messages will lead to cuts in AI funding state notes that some researchers are dependent on grants from government agencies such as dar PA in a YouGov poll of the public for the British Science Association about a third of survey respondents said AI will pose a threat to the long-term survival of humanity referencing a poll of its readers slates Jacob Brogan stated that most of the readers filling out our online survey were unconvinced that AI itself present direct threat similarly a Survey Monkey poll of the public by USA Today found 68% thought the real current threat remains human intelligence however the poll also found that 43% said super intelligent AI if it were to happen would result in more harm than good and 38% said it would do equal amounts of harm and good at some point in an intelligence explosion driven by a single AI the AI would have to become vastly better at software innovation than the best innovators of the rest of the world economist Robin Hansen is skeptical that this is possible topic indifference in the Atlantic James Hamblin points out that most people don't care one way or the other and characterizes his own gut reaction to the topic as get out of here I have a hundred thousand things I am concerned about at this exact moment do I seriously need to add to that a technological singularity in a 2015 Wall Street Journal panel discussion devoted to AI risks IBM's vice-president of cognitive computing Garuda fest Vannevar brushed off discussion of AGI with the phrase it is anybody's speculation Geoffrey Hinton the Godfather of deep learning noted that there is not a good track record of less intelligent things controlling things of greater intelligence but stated that he continues his research because the prospect of discovery is too sweet topic consensus against regulation there is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise and probably futile skeptics argue that regulation of AI would be completely valueless as no existential risk exists almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise in addition to the usual problem with technology bans that organizations and individuals can offshore their research to evade a country's regulation or can attempt to conduct covert research regulating research of artificial intelligence would pose an insurmountable dual use problem while nuclear weapons development requires substantial infrastructure and resources artificial intelligence research can be done in a garage instead of trying to regulate technology itself some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms possibly in combination with some form of warranty one rare dissenting voice calling for some sort of regulation on artificial intelligence is Elon Musk according to NPR the Tesla CEO is clearly not thrilled to be advocating for government scrutiny that could impact his own industry but believes the risks of going completely without oversight are too high normally the way regulations are set up is when a bunch of bad things happen there's a public outcry and after many years a regulatory agency is set up to regulate that industry it takes forever that in the past has been bad but not something which represented a fundamental risk to the existence of civilization mask states the first step would be for the government to gain insight into the actual status of current research warning that once there is awareness people will be extremely afraid as they should be in response politicians express skepticism about the wisdom of regulating a technology that still in development responding both to musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics Intel CEO Brian krzanich argues that artificial intelligence is in its infancy and that is too early to regulate the technology topic organizations institutions such as the machine intelligence Research Institute the future of humanity Institute the future of life Institute the Center for the study of existential risk and the center for human compatible AI are currently involved in mitigating existential risk from advanced artificial intelligence for example by research into friendly artificial intelligence topic see also a I control problem a I take over artificial intelligence arms race effective altruism section long-term future and global catastrophic risks gray goo lethal autonomous weapon robot ethics section in popular culture super intelligence paths dangers strategies technological singularity 