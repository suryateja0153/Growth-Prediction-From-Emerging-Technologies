 You're not going to want to miss this episode of The AI Show where we learn how to recognize our own wake word on a microcontroller using TinyML. It's awesome, all the instructions are in there, you're not going to want to miss it. Make sure you tune in. [MUSIC]  Hello and welcome to this episode of The AI Show where I've got a special guest and colleague to talk a little bit about recognizing words on a microcontroller using TinyML. How you doing, Jim?  I'm good. I'm good. How's it going?  Tell us who you are and what you doing my friend?  So I'm Jim Bennett, I'm a Cloud advocate working the same team as you. I mainly focus on IoT devices with a particular focus on students, makers, and that kind of fun area things.  ML on IoT is like a match made in heaven, so why don't you to tell us what you've been working on?  So I've been playing with a lot of small IoT devices. Now, obviously, there's IoT devices come everything from your big edge machines, the tiny little microcontrollers, and I've been playing with a lot of smaller ones. [MUSIC]  I've actually been playing with some devices from a company called Adafruit. Now they specialize in students and makers. They produce everything from little tiny tissue IoT chips like this up to quite fun devices, like this one here is the PI badge. It's got a screen and buttons, and what have you designed for kids to build games. One of their recent additions is a thing called an EdgeBadge. I'm just going to zip over to my device camera here, and you will see my EdgeBadge. So this is an Adafruit device that's designed to run AI models via a microphone, and it's got a whole of buttons and other bits and pieces on there. It's one of a new range of devices that they're working on. So we go back to me. Now, before we talk about this device, you just want to cover from the history of IoT a little bit, so IoT is the Internet of Things. So we have the Internet and you have a thing. So traditionally you would have some device, a sense, and it'll be sending data up to the Cloud. That's the Internet side of things, and you would then do analysis on that data up in the Cloud. So from an AI perspective, for example, you'd build an AI model, you would then run this on a GPU in the Cloud with data from your IoT devices. You have like a factory, for example, gathering vibration data for a machine, goes to the Cloud, and that looks for anomalies to identify if the machine is going to break. Now that was great at the time, but since then, IoT has kind of exploded. So we've got more and more devices, more and more data, and that leads to the big problem around things like bandwidth and cost. If you've got one device sending data to an AI model in the Cloud, that's fine. If you've got a factory with a million devices, that's a lot of data you're sending, a lot of bandwidth you need, and it's very expensive running the models over this data, especially when a lot of the time this data doesn't change. So it's been a push to what we call IoT Edge. So you train a model up in the Cloud, you take advantage of all the GPUs in the Cloud to train these models. But then you would download them to a device running locally and run the models locally on your data. So one example would be you'd use a board like this. This is an NVIDIA Jetson Nano, its got a GPU on this board. It's a $100 board, and this could then run your IoT Edge modules, not up in the Cloud, but actually in your factory themselves to analyze the data. So you think if you've got a million sensors, that's data you can run locally and not sent to the Cloud. If you're doing vision work, for example, you don't want to be streaming full-frame audio up to the Cloud. That's expensive on bandwidth, you can then run that locally. It's also great for the consumer space, you think about Home Assistant. You don't want your Home Assistant constantly streaming your voice to the Cloud to analyze for when you say the wake word. When you say, Hey dingus, do whatever, you want it to be listening for that wake word locally rather than streaming your entire audio stream up the Cloud because it's obviously very, very bad for privacy. So Edge would start off with these powerful devices like this Jetson Nano, which has got a 128 core GPU and four Gigs of RAM, and quad-core processors, and runs for low S's, and that was great, but they're big and they're expensive. So the next iteration of this is what we call TinyML. The idea of can we take an AI model and run it on a device that consumes less than one milliwatt of power. Someone defined TinyML. I think it was a book published by O'Reilly where they defined TinyML as running a neural network on less than one milliwatt of power. So that's where you can take little small boards like the SP32 chip for example. These are devices that are in their sense, dollar kind of price range, you can run these of coin cell batteries, and you take one of these boards, and you actually run a neural network on these devices. So that's what I've been playing with, and that's what this edge by Edge device does. So I'm just going to zip over to my screen, and this is the gauge bytes device here from Adafruit. Now this is not a powerful device. This device has got 120 megahertz processor in it. Now I used to have a Pentium PC back in the late '90s that had a 120 megahertz processor, so these are not powerful devices. This has got a 192 kilobytes of RAM in it. Now I had a ZX Spectrum back when I was six or seven, and that had a third of the RAM of this device. So we're talking 35 years later, we're running with three times the router for Spectrum. So these are very low power devices, but these can still run these AI models. In this case, this has got a little tiny microphone here, and that can pick up audio, can run it through this neural network and actually do wake word recognition and respond to my voice. So the current idea of this actually comes from a TensorFlow sample. TensorFlow, they published this example about 30 years ago actually, and it contains everything you need to train a neural network using a number of words, and they've actually got an open source data set, I think about the two gigabytes of training data inside there, covering a numbers of simple words. So yes, no, up, down, 1, 2, 3, 4, 5, 6, 7, 8, 9, etc. So I'm taking their example, I'm using that to train my model. Now you can always build your own, you just need to get a lot of audio files. This particular one has got a 105,000 audio files, but you can build your own one. So if I wanted to get my EdgeBadge to respond to a different make word, I could do that. If I wanted to build, say, a home assistant, my own personal Seth that I had at home, and I could ask questions of, I could then just record my voice saying Seth a few thousand times, get other people to record that voice, take that audio file, and I could use this to try and log that [inaudible]. That'll be so cool. You just say Seth and something wakes up.  Love it.  We have to invent it, Seth. I want to take this model, I wanted to train this up in Azure. So that pass code to your IoT Edge where you train the model up in the Cloud, take advantage of all that GPU power that the Cloud has got, and then run that on my Edge device. So as always, I spun up my Machine Learning Studio workspace in the Azure portal, it come the first step of getting going, and then I bumped over to the Azure Machine Learning Studio, I started writing my code in here. I spun up some completes, I need something to run this on. So I created some compute to run this. So I spun up a machine here. There we go. So I spun up a machine here to run my Notebook on. Then I just installed a Jupyter Notebook. So I uploaded my Notebook into here. This is essentially the same one as the TensorFlow demo with just a few tweaks to you to correct the version of TensorFlow that it was using. This Notebook, I run it in Jupyter Notebooks, I find it the nicest way to run this. This Notebook, it's pretty simple to do. You tell it the words you want out of the range that it's got. You tell it how many times you want to train, it then take your case, it recommends doing 18,000 training steps. Install Tensorflow. There's a particular TensorFlow example script that you can run that will actually train the model. Then when it's done, then I need to make it into a TinyML models. So this is where things get different from the standard train I/O model. I can train a TensorFlow model normally, but these are large. Tensorflow models can be fairly large. The types of data that they hold are usually 32-bit floats, for example, or even larger numbers that makes them quite big. But what I'm doing here, is I'm actually making this a lot smaller. I'm actually converting it to what's called TensorFlow Lite. So TensorFlow Lite is a lightweight format designed to make very small models. One of the original use cases for this was on mobile. So if I wanted to run an I/O model on, say, an Android device, I would use a TensorFlow Lite model. So it's smaller, it's designed to run using low power. You lose a certain amount of accuracy compared to the full TensorFlow model that runs in the Cloud, but it's a lot smaller, so you can do a lot more with it. So I get my models, it's a nice small model. Then just in this Notebook I'm printing out the size of my model. So if you notice what I've got here, 18,288 bytes. Less than 20K for a neural network to do wake word detection.  Fantastic.  That is tiny. Hence the name TinyML. This is not about TensorFlow Lite, which is lightweight, this is tiny. This is running on devices that don't have much storage. Then what's cold as well is I can take this model and export it as raw binary data in a C++ file. So I can then use this inside C++ code that I used to program my device. So I can literally just copy and paste this code and that gives me my neural network that I can load using the TensorFlow C++ libraries. So what I do now is I'm using Visual Studio Code for this. So Visual Studio Code can actually program microcontrollers. Behind the scenes it can connect to the Arduino IDE, which is a standard tool for programming microcontrollers, especially Arduino-based ones. There's extensions for VS code that I like to do this, and this will take my code, compile it, and then push that to the board via the Arduino IDE. So what I've got in my code here, I have got my neural network. I've literally copied and pasted the code that was spat out by my Notebook into my C++ code here. When I compile this, this would be compiled down nice and small, and then I have got a standard Arduino style sketch. If you've done Arduino programming before, you recognize that from Arduino style sketch. This has got the same code you get in any Arduino file, and this then connects to the microphone on the board, listens to the microphone, and then runs what it hears through the neural network, and then determines what word I'm saying. Now I've trained this one up on stop and go, and it works really well for stop. I'm currently having a few problems with the go word simply because the data set that Google has provided was trained by Americans. As you can probably tell by my evil villain style accent, I am and British. So it's not so good with my particular accent. But all this means is I need to get better data. I just need to keep retraining my model. So I'm just going to zip to my device cam here, and let's see an action. Stop. There we go. So I said the word stop, it picked it up and it showed that on the screen. I'll try go, no promises, no promise it work. Go, go, go. No, it's not going to work. So I need to retrain my model. I need to get it better. There it goes, finally caught up. So I still need to improve my model, continuously improve this. Obviously, this is one downside to running these models on device is that I don't have the data to continuously improve my model. So I would need to then build something in to capture these audio files, send them backup to Cloud for retraining at a later date. So that is how I've managed to get a wake word running on my microcontroller using TinyML.  That is amazing. Now, can people go and find this somewhere? How can people go look this up and maybe try to replicate this?  Yes. So I've got a link here that pops up. This will take you to a GitHub repo. I'll just flip over to that GitHub repo now. This has got everything you need to do this all yourself. So all the instructions here, all the code, everything you need. It tells you what to buy, to buy this EdgeBadge device. You can also run this on other devices as long as they've got a microphone. So if you want to run it on a different pie badge instead, you can just plug an external microphone, or on a standard SB 32 device, again, you can, just take the microphone. Then all the instructions are here on how you can train the model, set you up for a free Azure account, spin on ML Studio, get the Notebook there, choose the words that you want to train for, compile up and run it on your EdgeBadge. Then if you want to train this to build your own, Seth, where you want to train using this for Seth word or any other name that you want, any other thing you want, then there's instructions in the TensorFlow repo which tells you what format your audio files need to be so that you can then build up the audio and train it on your own thing.  Well, this has been a fantastic, my friend. It's really fun. We'll put the links below by the way, hopefully, you'll be able to try this out. Thanks so much for being with us, Jim.  Thank you. It's a lot of fun.  Thank you so much for watching. We're learning all about how to recognize words on a microcontroller using TinyMl, how to peak over because I wanted to say it the right way. Thanks so much for watching. We'll see you next time. Take care. [MUSIC] 