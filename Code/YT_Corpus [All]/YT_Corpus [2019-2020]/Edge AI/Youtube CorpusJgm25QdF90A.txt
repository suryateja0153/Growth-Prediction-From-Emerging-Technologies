 good afternoon everyone welcome to the Google i/o session of introducing Google coral my name is Bill wet I'm a program manager at Google's coral team on behalf of all my team members warm welcome to all of you and many of you watching online around the world thank you for joining us before I start talk about the coral product let's take a quick look of the industry trend which demonstrate to us why coral will be important to us the growth of the smart devices in the so-called IOT the inhale of the things has grown tremendously over the past number of years and it represents the one of the biggest growth opportunity in the years to come for many industry industry trend forecast over the internet such as this one you can see you can see many of those similar on the internet the growth of the non IOT devices such as PCs laptops mobile phones tablets and so on in the next five to six years we'll grow from about 11 billing installed units globally to around 12 billion over the next five to six years time span representing about 14% growth rate however the IOT smart devices the growth rate from the current install base about eight billion units worldwide will grow to 21 Billings in the same period representing a much larger growth rate of more than 150% so this really tells us where the growth opportunity and opportunities for innovation for all of us for developers around the world where that where the lies ahead namely they are in the smart devices so as smart devices users continue to grow the industry interests in smart devices in terms of the innovation and the development will continue to grow there are several key factors who will drive this trend continue forward because number one due to the increase the interest around the world in terms of AI machine learning the advancement of research in AI continue to grow over the last several years in fact the number of research papers has published in the last few years in machine learning is more than the total number of such papers in the last decade combined so mo AI capabilities will make the application of AI machine learning our devices have been more practical feasible as machine learning models become law accurate faster performance better industrials will have interested to continue use them so as more devices at the edge require machine learning we really need to have a solution to bring the machine learning on to the device at the edge we need a technology especially a hardware to bring the machine learning accelerating that capability right on the device so in a nutshell smart devices grows demands bring machine learning to the edge to the device and we need a solution for that so let me introduce to you what Google has made for you for developers around the world to answer this industrial demand what we make that possible introducing coral from Google it is a new exciting technology platform to enable developers all around the world to build on device machine learning accelerating AR applications with ease and with high efficiency so with this introduction instruction session what do you Orlan is about what coral product line offers what a machine learning capabilities you can build with the coral platform in the technology and third what are the use cases in terms of deploying machine learning with applications in lot of industry use so I'm gonna go through all these with my instruction here coral is designed as a make it easier to bring machine learning on the device make it easier for you to develop applications it is a platform to make prototyping to production with high speed with efficiency it is a platform to develop on device machine learning it offers a component of group of components of hardware components to bring unique high performance machine machine learning capability right onto the edge devices it also offers a complete set of software tool chains to enable you to develop applications in AI machine learning with ease in addition to that coral also offers a set of ready to be used machine learning models for you to quickly deploy onto devices so a lot of you may curious why this call Coral how that has anything to do with AI well if you look at the nature of coral in the natural world coral really do represent a community of vibrant teeming with life inclusive very open community right the living organism together they working together in a contribute to a common good and that's exactly what we inspire want to make a a a platform for the industry for all of you so corals inspiration and the mission is to provide a vibrant platform for all of you to collaborate in the developer applications bring AI applications to the device we want enable developers everywhere so turning our ideas to business solutions from idea to prototyping to large-scale production deployment with ease and simplicity and finally we want to encourage everybody joining a community-wide effort to contribute and learn and the sheer machine learning models together so that's where the name of the coral came from so why do we want to talk about the benefits of the machine device so let's take a quick look what are the key points of machine learning on device in terms of a benefits number one it is a high performance benefit because everything is calculate complete computer on the device right on the device locally there you do not need to send the data back to the cloud right so high performance everything on the local allow you to do things a lot more efficiently also it's very important very key in many applications you want the data to stay on the device especially in a business application scenarios some user privacy data cannot be sent to a server cannot be sent to the cloud we want a technology to enable you to make it a solution available to your customers and also because data stays local every information all the information coming from the sensors right on the device can be accessed and used to compute for your machine learning results right there rather than you have to send the data to the cloud to the server to computer and send data back right so this is another way to look at it's much better performance because all the thing every data is locally available next it works offline there's many scenarios the Internet of Things the smart devices they may or may not have internet connection in fact in most cases they don't have a cloud connection so you still want to have a machine learning capability in that scenario and the offline own device machine learning capability will make that happen for you in the lastly it is about much more power efficient lot of edge devices are small they don't have large power supply and they require a high efficiency in terms of computing computation on the device because you don't have to send data back to the cloud and a force you save the bandwidth you said you saved the power to send the data through a Wi-Fi or network so the benefits of machine learning is very strong device machine learning very strong so let's take a quick look of the coral probe the line what do we offer to the industry core bra line offers both how do components and software components on the hardware side it offers Development Board and which is a single board computer allow you to prototype develop your applications also kora offers number of sensors to allow you to build applications using the sensing data in terms of imaging video environmental sensors make those available as a part of the imported data to your application and also in a software side we provide a complete set of software to chain from offering system to SDK to machine learning modules allow you to easily use them and quickly build your application in addition to that we provide a detailed comprehensive set of contents in terms of documentation examples online guide data sheets etc all of those we made available online at the coral website to all of you so now we know what the coral Prada suite contains let's take a look little bit more detail in terms of the hardware components so kora hardware product we offer a suite of hardware to you in terms of prototyping in developing applications the first one is called coral dev wall as you can see on a picture here in retails - for $550 it's a single board computer with operating system on board and machine learning capability right on the device the second one is a USB key what we call USB accelerator it has the edge TPU machine learning acceleration chip right on the device you can put this USB into any Linux machine and be able to bring machine learning capability right onto those devices and I have those with me and I want to show you the relative size dimension of that so this is the coral dev board you know it's very small as you can see it is a single board computer with all the necessary input output so the connectors on that this is the USB key it's even smaller it's just typical to any USB key you want to use so these are the two current computer platform you use to developer applications in addition to that we also offer a couple of sensors as I said right to take a sensing data from the field for your machine learning application to consume number one is a five megapixel autofocus camera what we call choral camera second we just released a few days ago a new environmental sensor which is this one here as you can see very small and it has a digital display on that it allows you to take the input from temperature humidity and light and so on these input sensors you can use that and build it for your applications now with these you do the prototype but when you deploy these devices into large skill production environment we allow you to enable you to take the Sun module from the dev board in other words this piece of circuit book has snap off you can embed it into your product and this is a four-volume largest skill deployment and the individual unit price as you can see for about a hundred fifteen dollars and we offer discount for volume deployment very soon in the next quarter we will offer a PCIe connector a basic connector which you can plug into any pcs or industrial pcs and industrial devices that accept a PCIe connector to allow you to bring machine learning capability into those devices so those on the Left are what you need to do prototyping for your machine learning application the one on the right the one on the right is for large-scale deployment of course the cameras in the environment of sensors you could use for both prototyping and the largest skill deployment so they stay in the middle all right let's talk about the Google HD view this is the center the core of this platform pre machine learning capabilities on to the device so the HTTP you is a small application specific circular chip that Google designed specifically for optimizing machine learning on the device it is designed to take transom for low-light machine learning modules and the supports eight bit tenzin flow model and running our device with a high efficiency it all consumes only two as power and it runs very fast the picture you see in the middle represent its relative size to a penny so it's a tiny chip and with the sound module of this size you can easily embed it into many many devices so how fast is this a GPU goes well in a general term what we publish online the speed the performances speed of the adjective you runs computation at about 4 trillion operations per second so in general trend it's the Four Tops if you will but you may ask well how fast it actually runs machine linear model take a look at this comparison table we benchmark cutoff very common most used of machine vision machine linear models such as mobile net inception running the dev board or the HTTP usb on both of these running against powerful CPU or in bed the CPU powerful desktop CPU such as dziena 64-bit CPU or the arm CPU in the embedded world as you can see this table in comparison the machine learning capability speed of the CPU runs only a fraction of the time necessary compared to the desktop CPU and in bed a CPU so it's much much faster now some of you say well okay this is just benchmarking numbers we have we example real-world example you can show me and I say yes let me borrow example from one of our beta users who posted this in our coral beta online forum he said I'm building an app monitoring online traffic in real time you know watching 10 20 cars in real time I just take the mobile net out of a box without much drinking and they used the coral product I being able to achieve about 48 frames per second performance that is very comparable to a certain frame per second using HCT GTX 980 GPU the CPU a similar gear now for those of you who build game machines you know to build a gtx 980 GPU plus the cpu you're talking about what $500 to $1,000 cost the gear but who is the coral dev board it's only 150 you'll be able to achieve the same results right so this really tells you the cost in a performance advantage of the coral product so now let's talk about a demo little bit more since most of you are engineers you want to see the technical spec of it let me go through a little bit so first of all the dev board as you can see in the picture or in my hand it is a prototype development board allow you to directly develop on board and device machine learning capability in applications it's a full computer and as CPU it has GPU it has online memory and also runs a Linux operating system on this device it uses a modular design what we call some as a win system or module design meaning again you can snap off this some circuit board and deploying to your own product by module design make it easier from prototype to deployment it has many Isles connectors allow you to connect to many accessory accessories during the development make everything really really easy so here's a picture of the temple as you can see it contains all the necessary I will pause for you to connect the two devices such as HDMI to to connect it to display USB connectors to connect two cameras keyboard monitors and so on as well as Ethernet connectors it also of course has Wi-Fi and Bluetooth to connect to internet sort of Wi-Fi ok let's talk about a little bit of technical spec in detail the CPU and use a HP quad-core arm CPU chip as you can see the cortex is very fast a 53 is a very high-speed CPU chip it also supported GPU and it has a onboard crypto chip allow you to secure to connect to Google cloud it has a one gig onboard ram as well as eight gig flash memory so enough space for you to deploy your application it is supposed to Wi-Fi bluetooth and takes a five volts a standard power supply in terms of connectors is supposed to both USB two and a USB three speed connections we support a both USB type-c and a type-a connectors under all of you in the video category as you can see it supports all the a/v necessary connections especially the full size HDMI 2 connector for a full 1080p video display it has a micro SD card allow you to bring more software on board has a gigabit network support and it has a GPIO for a ping for I will connection it is supposed to a special version of the Debian Linux what we call mendel because it's specialized to support edge to functions and the machinery models is supported most of the common vision machine learning models such as mobile net inception which works great on small devices are mobile devices I want to talk about special especially this is GPIO connection for many of you who are makers you're used to making projects using Raspberry Pi right Raspberry Pi has this 14 GPIO connector with developed ball dev board you can do the exactly the same thing it's comparable to Raspberry Pi 40 pin GPIO so for all the things you have done in the past connecting through external lights switches using GPIO ports or using the posts with modulation to control step motors everything you've done in the past with Raspberry Pi you can do with the devil okay very very easy so how do you use their board in terms of development in the deployment conceptually it's very easy as I explained right I'm sure you already seeing so during the prototyping you use it therefore enable all these connectors you can connect two switches sensors temperature gauges whatever you want you can connect to a monitor you can connect to a keyboard you do develop them right there because it's operating system running on the device when you're done you take off the sound module and unplug from the table and you can buy many many some modules as I said as I said right you can deploy the sound module into whatever the product you are making such as they say smart refrigerator smart washing machine depends on the application you develop so this is really easy it's a prototyping and deployment in one being a product in a product package okay all right we talked about there boy let me also touch briefly the second product which is the coral USB accelerator which is this little thing here again it's a small USB key you can plug into any USB slot on any Linux machine it has a onboard adjective you bring the machine learning capability right onto any device you plug into and also supports not only the Debian Linux but also the raspberry Linux which is similar or the same one used on Raspberry Pi so you can plug this and take it as key and work with in Raspberry Pi so it opens up a bit more opportunities for you to do the development so the quarterbacks are advantages in number one as you would imagine why Prine own device machine learning into many more machines okay you can plug into a laptop if you want if the laptop runs a Linux special version of Linux and you can plug into a Raspberry Pi it's compatible with many hardware what the Raspberry Pi supports in the past so not only pcs for the laptops Raspberry Pi in the industry assistance there any box supports a USB plug you just plug this key and you will be able to deploy your machine learning applications so the choral USB accelerator is a low cost convenient way to experiment and building prototypes India okay so we talked about hardware let me also talked about the software side the complete suite of to chain what Carl provide to you to enable you to do machine learning applications so software components let's take a look from components level what the Kouros software pieces and the hardware work together so on the core dev board not only Linux machine if you will right at the bottom layer you have the hardware and we have the Linux demand a Linux running on top of that they talk to the operating system talks to hardware in Coralville we develop a c c++ library api direct access to the operating system and the hardware and so this allows you to have direct access to everything the operating system be able to control for you let's say you have a machine learning model it's a test flow model test flow line model we provide a a GPU compiler allow you to compile take a test flow line model compiled into a binary format that is comparable to the agita view so if you have an application you can access the CC plus API and running the machine on your model right on the device and access to the layers of hardware however we realize that many many machine learning programmers like you you have been using Python so coral software also provide a Python library or Python SDK it is a high level wrapper allow you to using Python programming language to easily access all the features I were just talking about right be able to access machine learning model be able to access hardware and you can write a Python code to i/o control and so on so this is a complete environment with multiple components working together that we put in the product for you to enable you to do AI development so python api is it's very simple for many of you who have programmed in a Python we publish this on the core website by the way the basic classes of all the price on API that you would use for developing machine learning applications I would say pay attention to the middle to those are probably the one that you would use the most will use object detection one is object classification and the base engine based car base class for them is one called classification ji hoons quality detection engine so it's very simple the last one you see here what we call the ink reading RNG it is a something for transfer learning which I'm going to talk about you a few minutes it's something allow you to efficiently develop a customized machine learning model and the Python API library we provided also support that so let's take a quick example how would you use the Python code actually to interact with the machine learning module there are models that we supply so if I say I want to develop a program using the object detection model in Python code you will simply initialize the engine using the detection engine that base class the base class of the group members the the member functions you use and to care to the data and initiate and talk to the Machine linear model so you initially ranging here you also need to load that the so called a label file because if you want to track a bunch of objects you want to identify the movies labels you loaded the label file and then let's say you want to feed the Machine linear model object is your model with the image you load the image file and of course for most of you have a play with machine machine learning models envision you know you can identify object in your photo or you can identify object in a video stream in that case it will be a vector of tensors here I'm using a simple example of just using image and there the code to interact with the machine linear model is very simple the simple line from the engine mm per class you just say detect with image and you pass the parameters to this image and the return results the years will come back from this call you can use it such as draw a bounding box specified color of trolling abandon box relates to the stuff that you are detecting so it's a very simple we supply a group of machine linear models for you to use Koro include them in the products we would put them on a website they are free for you to download they are pre-compiled Tenzing for low light models there for you to use they can be readily run without any further company and you just simply download them into the hardware the edge GPU module the Python module it is already installed on the dev board so you don't need to do anything you're ready to use your Python programming code the example I just showed you you can do that however if you use the USB you want to put on a Linux machine you will need to manually install this python module the price on AP is are pre-installed as I mentioned right so that's what you needed to do I do want to mention though a lot of those models we supply for free online for you to use those are for non-commercial use only that means if you want to build a model let's say you want to sell for money that you would need to make your own model rather than use open source a free model that is a non-commercial use okay the supreme models including the categories of is amazing image classification object detection as well as a one called weight in print that's again use the for transfer learning and I'm going to talk about in a minute so the here are some examples of the models that we make it available for you online and here are the image classification models as you can see we support pretty much all the popular image classification models from mobile net to inception and the different versions of mobile net different versions of Inception the difference between them are the type of objects they are be able to identify for example if you want to develop an application to tell difference between of different birds or different plants you will pick the corresponding model to use okay with that me run a cup of demos for you first them I'm gonna show you is a object detection model and the second demo I'm going to show you is a object classification model so with that videographer please switch the display to the camera here so on the table here as you can see I have this demo built with this conveyor belt okay I'm simulating a real-time traffic over here there's a camera points to it as you can see on the on the screen the camera feed into this coral devoir in real-time it identifies the number of the objects right it shows it's a car and also shows a so-called confidence score how confident the model sinks it is an automobile but on a conveyor belt as you can see I also have a people or pedestrian but what I'm gonna do now is I'm gonna wink make it this movie okay so I'm gonna turn on the power and I'm gonna crank up with a speed to make that running now in real time world as this moving you take a look on the screen the machine learning object detection is continually happening okay it continuously identified the correct car or pedestrian or there's a traffic light as well right so you can see the coral our performance is very high be able to do that as the scene going now I want you pay attention to the low left corner on the screen or up left the corner of the screen shows the frame per second speed it runs last time I saw it was like a 50 to 70 frame per second it is very high speed performance now if I crank up the speed and make a good little bit of faster you can see the machine learning in terms of object identification capturing it's still going up right it continues to be able to identify automobile in this fast-moving environment so this is really demonstrate the power of object detection running right on this coral device okay so that's the first demo thank you yes you can clap I hope you guys be able to make a lot more interesting applications just select that bring the power of coral into your imagination into your innovation all right next one is showing a object classification so over here I've a another tempo and the output of that display please switch to the output of this camera so what I'm going to do I have several food item on a table and I'm going to let this camera identify the different type of an object so let's say I put a hamburger over here and as you can see on the upper left corner and try to identify the object and with some confidence score depends on the light condition hopefully it will see a Hamburg yeah you need to aim at the right angle and wizard right let's try this doughnut they say don't know it showed up and doing okay we can also try a sandwich okay lastly I'm gonna try something exotic let's say sushi okay so this is how you could make object object classification walking by simply running one of the object classification model random device again none of these are connect to internet none of the data is being sent to the cloud or any server everything competition happening right on the device okay great thank you all right let's switch back to the slides so now we talk about how do you use a Korell supplied pre-compile model to deploy that but what if you want build it something on your own you want to customize your model well this is what a transfer learning jamesy transfer learning it is help you saving time in terms of building your own model and basically it takes a pre trainer model that is compatible with HTTP and you only take that for your release to task by using your own customized the data in a concept a neural network has many layers deep of neurons ok if you want to train this whole model in fact I heard from one of my colleagues who develop a model from girl level up takes more than 4,000 CPU GPUs to trigger a vision model and it takes days however if you instead of training everything you only need to modify the top layer this will transfer learning concept layers because the lower layer those neurons are trying to detect as it's a different color different shape different light conditions they can be used for you to help you identify things that you care let's say you want to build a model identify different Apple ok you don't need to train a whole model you take a classification model you only modify the top layer and by training with your own customized data with many different apples so this is what transfer learning does so the code to do transfer learning also in a Python environment kolo is very very simple - basically you prepare to do a transfer learning you set up a docker container you specify what a model you want to use in this case I'm showing example I'm using a mobile net where you want and if you want to train a few top layers the single command you want to use simply start training and again you give the then the parameter is the name of the model but if you want to train in time order you can do that too is you add one more additional flaga say training home auto flag true ok so once you render code the in a console on your system basically the concert will showing you the progress of retraining in terms of the steps it takes and in terms of the number of how much time it takes so it's very simple for you to do that in the environment in real Linux environment so with that let me do another demo for you it is called the teachable machine we are going to publish this it's open sourced for you to do the same in the near future but basically what you see here I'm gonna show you I'm gonna teach it make a machine to remember things so the video camera videographer we need to have an image of this so on the desk here what you see it is actually it was built based on this USB with a Raspberry Pi so right so more than 10 board you could use the Raspberry Pi to build your application so here with this little bit of demo and it has a camera points up and I have some different objects ok so if I hit the button to take every time I hit a button take an image let's say of this hammer it remembers several images of this Hamburg now if I take a different object and take a different photos different group of photos of the second object it remembers the images and I'm going to take ice-cream on the green button and lastly maybe I'll take her this this donut with the red button so what happens in the background here the program runs tweeny transfer learning take your existing object class your model button replace the last layer where's the image just to just take it okay now watch this if i put this hamburger back the yellow light pizza it remembers if i put a dye screen the green light turns on I hope you can see on the video yes and if I take this donut the blue light turns on right now what in that moment ago I trained with this green ice cream right the green light if I put a yellow ice cream you know members too because this machine learning model more than just the color and also identify the shape because this shape and this shape is different the model is a fast smart enough to know the difference between the object so again this is one of the examples you could use to building things with the capability of classification right on the device without internet and build even with a small USB key like that right very powerful stuff thank you ok let's switch back to the slides so the ramification of this is huge right imagine in the industry environment you want to identify things you want to tell a good widgets from a bed we're just seeing assembly line for example you don't have time to train the assembly line the order a sorting machine you could use a transfer learning and learn the different object on the fly just like that so it's very powerful you can use this in a building just end a list of applications using such capability alright so we talked about tracer loonie we talked about building your own customized model let me get a bit more detail of how do you use the coral models ok so we talked about we supplied a set of pre-compiled model for you this is actually case number one user case no more it's very simple you simply take download we supplied to you you don't need a compiler game you simply download it and put on the device okay the second scenario is you take an existing model pre-trained however you use the transfer learning to customize it with your own data however after you've done that it's not compatible yet with the coral board you need to compile so you use corals supply the coral compiler and you compile it the net result of the tensorflow live file you download to the edge view hardware color hardware and you'll be able to run there now I want to say for right now the coral compiler is only released on Google cloud platform but very soon we will make this compiler a standalone executable make a downloadable downloadable my internet and for you guys to use so the user case is you want to build the entire model by yourself okay this is like you really want to customize issue the existing model doesn't satisfy you and you can do that too so in that case you will need starting with the Tizen flow and a beauty model from there so let me talk about the steps involved with there the workflow of creating your own customized model is the following case the flow model is your honor it's a 32-bit floating-point model right and that is now usable for coral because coral device require tensorflow light because this one is on the edge need a very little memory so test flow model wouldn't work you take the taste and flow model you convert it into number one step you convert into training with a quantized version so there's a training process called contest a why we're training you'll convert the or test flow model into a quantized Kinser flow model okay so basically converted to a bit of floating-point base to 8-bit integer based model after that you export this model with Tizen flow test flow model into a tensile flow of frozen graph which is typically it data PD PBF file but this file is not usable either okay it's not quite ready to be deployed on coral what you need to do next step is you need to convert this thing in a transom for low light model and with a tensor for low light converter and after that you compile using the Kinsey flow HTTP you test for all compiler and making into a binary that's compatible with the HD view and in the after you done that you deploy so this is a process or a flow you will use in your environment so to build we talk about how this platform provides you to building a applications we said at the very beginning we want coral to be a platform of an ecosystem for everybody together so really this is a platform for you to use to innovate and share with the community globally altogether with that I want to show you example one of our retail partners called a gravity link they build this app a very cool app you can use your mobile phone download app directly into the Colo temple and you can find more details of this link below or you just simply simply search model play at a Google Play you can download the inner track so this is the idea of we want our developers contribute to this ecosystem building tools beauty models building applications share with the industry and this is what a coral ecosystem is for okay with that let me ending by saying what are the potential areas you could developer for AI right look at this there's a consumer electronics of course appliance there's a lot of opportunities for you to developer their industry warehousing a monitoring the device is monitoring the the where the the assembly line this is another area robots robotics both the industry and consumer is a a field automobiles Auto automotive industry is also a field and as all of you here at the Google i/o keynote medical medical application medical devices is another area and finally education education aids and research you can use machine learning i'm device machine learning using coral that you can innovate so there's a lot of things you could do right and all the information I've talked about today they all summarized at our Cora website if you remember anything remember this it's called coral dot with Google Dhaka our documentation our samples models everything's there so there's more references I put in my slides you can look at later there's a reference to the mando Linux reference to the tensorflow lie how do you do our contest we're training and all these information is very important at one go out on Stack Overflow we have a tag you can join the online community participating discussions and other developers crashings or look at the answers that I watching monitoring right we'll help each other using this online community and I want to give a shout out we have a coral code lab for those of you who would like to experiment using coral at i/o you can go there today our Deb real team colleagues are helping everybody going through the code lab so with that a quick summary and the call to actions after you hear today number one review our core product learn more about tensorflow like using cobalt to experiment and then beauty our own building your own customized models and finally a building model from wanna level up we want all of you taking coral platform putting your imagination put your innovation bring a eye to the industry to the consumers everywhere so with that on behalf of our coral team thank you all very much coming to this session thank you [Music] you 