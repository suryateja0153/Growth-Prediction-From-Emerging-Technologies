 hello everyone my name is sean holiday i'm the senior director of customer success at plays super excited to be here and greatly appreciate the invitation from the minds lab team over the next 10 minutes or so i'll provide a quick overview of blaze a little bit information about our technology talk about our relationship with mindslav provide a demo and then a quick wrap-up at the end let's get started at blaze we're all about providing a new type of computing and specifically a new type of computing for ai and machine learning workloads our founders denneker k satiki and val all stepped away from long careers at intel where they worked in the gpu architecture division today we have about 87 million dollars in funding over 300 employees worldwide with our headquarters in el dorado hills california we also have several offices around the globe a great list of investors and our second generation gsp silicon at blaze we have three key market assumptions that drive us number one is ai compute workloads are ubiquitous across the edge enterprise and cloud every business looking forward will have be positively impacted by ai technology number two ai compute workloads are 100 inferencing at the edge and this is where blaze's primary focus is today as you move a little bit deeper into the network about eighty percent of the compute will be inferencing about twenty percent training uh for any enterprise and cloud number three all compute and networking hardware manufacturers will require integrating scalable native or plugable ai acceleration capabilities into their hardware so at blaze we do three things really well number one is we've developed a new compute architecture called the graph stream processor this is focused on providing much more efficiency than traditional cpus and gpus that are being in utilization today and i'll talk a little bit more about that later number two the far left we've we take this gpu architecture and we integrate into a number of different hardware platforms we have an embedded sum in the lower right hand corner of the picture that is meant to go into an embedded applications then we have two different accelerator solutions that go into traditional servers industrial pcs and such in the center what brings us all together is our software that blaze about 70 percent of our employee resources are focused on software we have two different versions of this number one is our picasso sdk which is a traditional coding environment for people who have experience in coding and then we have ai studio which is a visual coding or code free environment for business professionals that may not have a staff of data scientists and programmers so the heart of blaze products is our gspr processor our gsp processor offers a total of 16 tops comprised of 16 cores all of this at 7 watts average if you look at some of our comparisons against gpu and cpu solutions we utilize about 50 times less memory bandwidth we offer about 10 times lower latency and 60 times better efficiency overall when looking at inferencing person infrastine per watt per dollar we're also we do all this while remaining 100 programmable so you have the flexibility to program our device to run pretty much any network uh that you need to so key tenants of our products of our gsp architecture is faster execution time lower memory bandwidth lower latency higher utilization as compared to cpus and gpu cores and overall lower power so as i mentioned earlier we have three different key products i'll start off with the left hand side first these are our blaze pathfinder embedded modules these are small modules about the size of a credit card that are meant to go into embedded applications into a robot into a camera into a last mile delivery vehicle and such these are full standalone have arm cores to run your applications as well as our gsb architecture for your ai workloads second hardware a product line that we have is our blaze explorer accelerator platforms we have a traditional pcie card which is half high tap length meant to plug into servers we also have a very small form factor called an eds ff form factor that's meant to plug into 1u server form factors to provide acceleration as i mentioned earlier at the heart of what blaze does and where the vast majority of our resources are is in our software we have two key uh software tools available for developers number one is our picasso sdk as i mentioned earlier this is a tool meant for uh developers for data scientists people who are are used to uh working and developing their own code um we offer the capability to work with tensorflow fibe cafe ai frameworks we also offer the ability to program and see c plus plus openvx our picaso sdk has a very useful tool called netdeploy and a lot of times developers will receive a optim or a model from their data scientist maybe it's done in floating point 32 floating point 16 in order to have the highest accuracy possible but it's not something that's easily deployable to a low-cost low-power piece of hardware so we have a tool called net deploy that automatically converts your network say from a floating point 32 into an 8-bit int it provides automatically does the quantization the optimization pruning adding sparsity whatever it may be to compress the network while maintaining your overall latency and performance requirements if you're a small business a you know a medical professional financial professional and you need to add in or take advantage of some of what ai can provide we have a code free environment called ai studio it's a very intuitive um gui driven or graphical user interface driven software application that allows you to import a data set and then interact with our software basically telling it what you would like to do ai studio will will take that information automatically build your network and deploy it to our hardware without having to write a single lighted code so and we have two different development platforms picasso sdk for traditional coding environment and ai studio for a visual code free environment so where do blaze products go today um well if we look at um where inferencing happens i mentioned earlier 100 inferencing is at the edge and then as you move into the enterprise it's about an 80 20 split between inferencing and training so blaze products today go into really one of four categories number one at the edge this is at the point of data acquisition so integrated into the camera with with imaging sensors connected integrated in on or around the robot but our pathfinder products really go in applications where you need a small standalone compute platform but you also have extreme latency requirements maybe functional safety requires you to have latencies at sub 100 milliseconds for example these are our perfect applications for our pathfinder songs take one step deeper in the network and go to number two and this is where you enter the aggregation point so maybe the networking switch or server where all of your your camera streams come in this is a great place to add in ai technology again it may not be as latency sensitive in this application but allows you to deploy ai capabilities out close to your applications especially if you're in providing applications where you're bringing in video streams from multiple different types of cameras say around a city environment like what mine slab is doing in this type of deployment in number two you can leave your existing cameras in place and do all of your ai inferencing at the network edge as well without having to replace your cameras categories number three and four are more traditional server-based applications where you would plug in our explorer products an accelerator into a standard rack mount server could even be an industrial pc in this case as well but these are more traditional applications and it starts really becoming more of an enterprise application as this equipment may be in some sort of enterprise wiring closet or small network operations center so let's talk a little bit about our collaboration with mindslab i've had a great opportunity to to meet the mind slab team face to face several times and we've been working together for well over a year and we're super excited to see their applications actually up and running today on the blaze hardware so moving forward you know we we see a really long and fruitful relationship between blaze and mines lab as we look at uh whereas they look at integrating their software and services onto our current hardware and our next generation hardware products coming up so again really excited about our opportunity to work really closely with mindslab in the region so let's talk a little bit about an application and this is very similar to some of the applications that the mines labs team is focused on this particular application and i'll show a demo video on this as well is a smart city application where we have several cameras around the intersection monitoring people tracking cars recognizing license plates and license plate numbers for toll tracking and so forth so in this particular application we could have anywhere from five to ten hd uh power over ethernet cameras connected uh we're running three independent neural networks um at about 50 frames per second uh each and then we're monitoring human detection human pose and position automotive detection license plates overall intersection safety and so forth so now let me actually show a live demo this is very similar to the application but this is a multi-camera object detection demo so we'll go ahead and run this and i'll pick up when it's over [Music] this is a 5 camera object detection demo running on the blaze explorer x 1600e small form factor accelerator platform here you see five cameras in a smart city type of application running five different yolo v3 object detection neural networks in order to detect objects such as people cars motorcycles buses and traffic lights these five models are required to be processed on the five camera streams independently something that is impossible to do in traditional batch based architectures like gpus and vpus each stream is processed independently on the blaze gsp at 50 frames per second with less than 20 milliseconds latency each allowing the complete 5 camera object detection application to run in less than 100 milliseconds latency to achieve true real-time processing on the edge this multi-stream application runs as independent streaming data flow graphs on the place it looks like we had a little bit of a hiccup there i'm sorry guys let me go ahead and restart the video for you this is a five camera object detection demo running on the blaze explorer x 1600e small form factor accelerator platform here you see five cameras in a smart city type of application running five different yolo v3 object detection neural networks in order to detect objects such as people cars motorcycles buses and traffic lights these five models are required to be processed on the five camera streams independently something that is impossible to do in traditional batch based architectures like gpus and vpus each stream is processed independently on the blaze gsp at 50 frames per second within less than 20 milliseconds latency each allowing the complete 5 camera object detection application to run in less than 100 milliseconds latency to achieve true real-time processing on the edge this multi-stream application runs as independent streaming data flow graphs on the blaze gsp to achieve 30 times higher system efficiency compared to edge gpu or vpu solutions yeah so that was a great application so we're at the end to wrap things up so again i really appreciate the time that mines lab has provided and i hope you guys found the information i shared on blaze helpful to wrap up we do three things really well number one is we've developed a graph stream processing architecture this is our soc that goes into a number of different hardware products for embedded and accelerator applications and at the heart of this it brings it all together we have our ai software suite comprised of our picasso sdk for traditional coding environment and our ai studio for those who want to use more of a visual code free environment so i really again appreciate the time from the mindslab team and if there's any questions or follow up reach out to your mindslap representatives they know how to reach me would be more than happy to have a an opportunity to talk with you answer any questions provide more information so with that said have a great day thank you very much 