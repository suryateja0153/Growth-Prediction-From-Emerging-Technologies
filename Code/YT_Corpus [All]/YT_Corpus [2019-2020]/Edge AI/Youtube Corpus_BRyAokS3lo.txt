 - [Narrator] Welcome back to UpTech Report series on AI. (light music) In this episode we continue our conversation with Alicia Klinefelter, research scientist for Nvidia. Alicia is an expert in her field, she has a PhD in Electrical Engineering from the University of Virginia. Since joining Nvidia, her focus has turned towards high performance hardware, including machine learning circuits and systems. In this episode we start off by asking her what is her view of the AI revolution? (music increases tempo) - [Alicia] You know, I think of it a little bit differently then maybe someone who is more on the algorithms, or kind of theoretic side. For me I see it more as a revolution of finally having enough compute power. Essentially to do a lot of these complex algorithms that you know, have been limiting this revolution for years. 'Cause, I mean, a lot of the algorithms and the you know, the underlying mathematics of machine learning have been around for decades, since the 1950's and really what kind of revolutionized these things is, you know, finally in the mid 2000's to late 2000's we've been having this, you know, this enormous, progression of compute power that has enabled a lot of us to finally kind of implement these algorithms on a larger scale. - [Narrator] What key factors do you think played a large part in enabling this revolution? - [Alicia] I actually, I think that usually when you talk to a lot of people in the space, they kind of, you know they'll highlight a handful of things that have kind of enabled this AI revolution in the last five to ten years in particular. I mean, the first one people will say is compute power, as I already mentioned. Which I think is really important. But, another one that people will site is kind of open source, open sourcing of everything. Whether it's the software or the models, or, just any infrastructure around machine learning. Open sourcing that is so important because in order to generate a complex model essentially it just takes so much of like a end to end stack that you would have to develop yourself, that if, I mean it would be so time intensive to do that on your own that if these open source tools weren't there then no one could do anything very quickly. - [Narrator] You mention the impact of compute power and open source, can you give a bit more insight on compute power and progression? - [Alicia] You know, usually, a lot of hardware engineers will talk about Moore's Law. Which is this law that is existent from Gordon Moore, from Intel, you know, when he founded it, you know, 50, 60 years ago, which basically is that yeah compute power I mean, or you can basically fit as many transistors in a dye, you know, or like double the amount of transistors on a dye in a period of 18 months, every 18 months. And that's held totally steady and allowed the miniaturization of all of our electronics for the last, you know, few decades and it's, you know, it's steadily kind of moved along but of course, in the last, you know, five years probably, you know, we've started to see this stagnation finally, of Moore's Law, you know, you just, you get into specifics like Intel you know, who is often released, you know, since they have a foundry they usually are releasing new technologies every two years. And usually we describe those technologies at something we call their feature size. Which is really indicative of the size of the transistor getting smaller and smaller and smaller. And, you know, and they kind of have stuck at this one node as we call it, at ten nanometer, which sounds incredibly small but they kind of haven't been able to move beyond that for the last few years. And we've seen other foundry's as well the other main one, TSMC in Taiwan, you know, we kind of see them have struggles as they move down to the single digit nanometer scale. So we're starting to see this massive stagnation in technology in terms of how much we can really scale it. And the one thing to note, you know, when we scale these technologies to smaller and smaller and smaller, we also see benefits of scaling these other hardware parameters, such as voltage. And with, voltage goes down and frequency can go up, meaning your performance can increase. So that means that you get more performance for less power because power is a function of voltage. So basically when we start to see that slow down then we basically, you know, we have to find more creative ways to get to what we want to get to. We can no longer rely on the scaling, this technology scaling we've kind of relied on for so long with Moore's Law. So that gets a little bit tricky because right now, you know, we're kind of, you have to kind of think about it as, you know, we have what we have in terms of technology and in terms of the underlying silicon that we use to create these chips. And so we have to start getting really creative with different types of, like architectures on chip to implement these algorithms and find creative ways to implement these neural network topologies and hardware to basically minimize the power or get more specialized with whatever you're trying to do to keep power down. 'Cause, as I mentioned before, there's this constant trade-off that exists between hardware flexibility and energy efficiency and you're always trying to find that sweet spot. So I think, you know, what we're going to see in the next couple years is you're going to see hardware that's much more dedicated for the function it needs to be for, so that you can basically minimize power for a very specific function. So I think we'll see a lot more of that and edge computing is very dedicated chips that are very energy efficient and I think we'll go from there. - [Narrator] What does the future of hardware in the relation to AI look like? - [Alicia] But for hardware, you're actually, what you're starting to see a massive boom of right now, especially in the last few years, especially when it comes to start-ups, is your seeing a lot of people that are trying to push the power envelope, or I guess reduce it, because GPU's traditionally have been known to be very power hungry. You know, multiple watts essentially. And that's not necessarily good for what people are now referring to as "learning on the edge". Or, it would be something such as edge computing which we usually refer to, I mean, it's a really broad term, but usually people mean a power constrained application, like putting a chip on your cell phone to do learning. People even extend it to do, like, automotive learning. But, it could be any device that might be a little bit power constrained, compared to say, something on a server in Iraq somewhere. That can be, you know, use ten's of watts of power. So, so now we're starting to see this huge boom of start-ups that are trying to do this low power inference and training. We're doing this kind of inference on the edge processing. And, we're starting to see people do very dedicated, what we call hardware accelerators for this, and when we say accelerator it's exactly what it sounds like. Instead of having something very general purpose like a GPU that could compute any number of algorithms based on how you program or configure it, a hardware accelerator is very very dedicated in its function. And the benefit of that is that it's very energy efficient if you wanna do something, but then, of course, it's not as flexible. So we're starting to see this really big boom of kind of how to find the perfect trade-off between those two things. And that's what a lot of companies are looking for right now and what a lot of start-ups are doing. It's how can I make something flexible enough in order to future-proof myself, hardware wise, for new algorithms, because the field is moving so quickly, there's a new, you know, machine learning algorithm you need every single month but, still, how do I keep it low-powered enough that, you know, people are going to actually want to buy it and it's going to be a competitive chip. So that's kind of the big boom we're seeing right now. - [Narrator] What's the plus of having machine learning on the edge? - Big, at least I think one of the big, I guess positives of doing machine learning on the edge that we're gonna start to see soon is actually related to security. So I think, you know, having, so usually I guess with the typical model now, you know, if you wanna do, if you're phone, if we're using the voice example again, if it wants to learn something about your voice as it stands right now, you know, maybe if you have an android phone it collects that data and it kind of sends it out to the cloud, and then to do the actual compute, and then it gets sent back to your phone and then kind of that information might get stored locally. But there's kind of this loop that exists from your phone out into the cloud and back and obviously you can imagine how there could be some security issues that exist in that loop. So, if you have something that's very security critical then you, if you could actually do all of that processing but just on your cell phone without having to go back out and come in then you'd basically be keeping your device a lot more secure, your data more secure in particular. And I think a lot of people see that as being a big benefit now that people are more concerned about data security, I guess, societally. So I think that'll be a huge, huge benefit. But I think that's the main benefit people are thinking of, so (laughing). Alexa's a great example, and is, it's definitely an edge device but, you know, it's a, an interesting edge device 'cause it's actually connected to main's power. So, you know, it's not necessarily battery operated you know, it is power limited basically by you know, the connection to the wall, of course, but I think, you know, it does give you a lot of flexibility there if you really wanted to beef up security and basically do all of your inference locally essentially without having to go out and come back in. But, yeah, it does help. This is where I might dig into the hardware details a little bit, you know, it's really interesting 'cause I mentioned before that, you know, compute power is scaled to enable this revolution. I mean, I definitely think it's the best way to scale up very quickly. Again, thinking just more from a hardware perspective you know, because as of right now as the network's get more complex, they're not getting any less complex, and as we handle more data essentially to train them, then we really need that compute power that might exist more on the server side to handle that. So, for right now, I think it's the most practical way forward in order to do complex workloads, but, you know, moving forward, we can maybe, maybe scale down (laughs). - This was just a taste. Stay tuned as we share the full deep dive interviews we had with each one of our panel of experts, and our upcoming episodes, focused on specific topics that will transform the way you think about artificial intelligence. All this on UpTech Reports new series on AI. (upbeat music) 