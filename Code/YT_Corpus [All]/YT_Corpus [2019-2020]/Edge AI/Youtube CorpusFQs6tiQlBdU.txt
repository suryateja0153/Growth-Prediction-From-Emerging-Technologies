  I'm David Shaw, and this is "AI News." When we talk about AI at the edge, it can mean many things. As a developer, you generally have two options where you can deploy your applications-- in the cloud, or somewhere where the cloud is not feasible. Why is this important? Take autonomous driving, for example. There are an array of sensors that need to be used in parallel so the vehicle can drive safely. Time is critical for both the sensors and the data. The right data analyzed at the wrong time is effectively the wrong data. That means computation must be performed locally, on the edge. In a case like this, you can't expect the vehicle to be constantly connected. Consider the case for smart cities looking at smart parking. If a city employs hundreds or even thousands of cameras to determine if a parking space is available, then the cloud would have to process huge amounts of data per second to determine if and when a parking spot is available. In this case, the edge could be the pole where the camera is located. Today, there are few companies that offer cloud-based speech recognition and natural language processing, or NLP. But what if you need to do NLP without connectivity or with sensitive data like health care data? In this situation, a cloud-based solution is neither practical or realistic. This is where AI at the edge comes in. Take a look at this article to learn how to use NLP Architect by Intel AI Lab. NLP Architect is an open source Python library that allows developers to leverage deep learning frameworks to support a range of NLP tasks such as intent extraction, word chunking, reading comprehension, and many more. In the article you'll learn how to use NLP Architect to build a foundation for a conversational agent that could run completely offline. It walks you through setting up the environment, installing and updating NLP Architect, running the intent extraction demo, and creating your own HTTP client. Check out the links, keep developing, and stay tuned for the next episode. [INTEL MUSIC] 