   [MUSIC]   [ANTONIA] Hello, and welcome, everyone, to this Unite Now talk. My name is Antonia Forster, my pronouns are she/her, and I'm a technical specialist for XR here at Unity. XR means extended realities, and it's an umbrella term that covers AR, augmented reality and VR, or virtual reality. I really specialize in these industries, and how they can benefit different businesses like some of the ones that you are working in. I'm just going to share my presentation, so hopefully you'll all be able to see everything just fine. Everyone see that okay? Everyone can still hear me and see me? Lovely. Great, okay. We've also got some videos. I'm just going to make sure I optimize for video as well. I've never done that before, so optimize for video. In fact, I'll optimize for video later when we have the video running, because I know that that can affect the quality of my static slides, so I'll do that in just a moment when I share a video with you. Welcome, I hope today that I'll be able to show you some use cases for AR, augmented reality in industry. What I really want to impart upon you during this talk is to give you some ideas of content that you may want to create, and also to really enable you to create content yourself even if you have no coding experience. I'll be showing you how to make an AR demo using Unity MARS, our new creation tool that makes it really straightforward to start creating AR content. Here's how the talk is going to be structured today. First, I want to give you some context of how extended realities, so VR and AR both are being used in industry, whether that's architecture, automotive, or construction. And at what stages of the product life cycle that can really bring the biggest benefits. Then I'd like to briefly introduce you to Unity MARS, our mixed and augmented reality studio. Then we're going to create a simple AR application together. I'll walk you through the entire process, from how to obtain MARS, to how to create an application, and then how to deploy that on your phone. In my case, [to] an Android device. And then after that, I'd like to show you some case studies of how augmented reality, very similar to the demo that you're going to create, can actually be used to bring big benefits in industry. That's the structure of our talk today, And this is the app  that we're going to make. Let me just optimize my screen sharing. Hopefully, you can all see this clip playing now. This is the app that you're going to able to make yourself by the end of the talk. It's very simple. It spawns these arrows onto image markers, but you can use this technique to spawn any image you like onto any real image marker in real life. You could bring in your own 3D model, perhaps a character, if you work in animation, or perhaps a scan of an engine, CAD data or even a whole building. Of course, those image markers can be on a wall, or they can be flat on a table as well. I'm going to make a few assumptions for the purpose of this talk. Let me just switch off my video clip sharing. For the purpose of this talk, I'm going to assume that you are familiar with the Unity Editor, that's our core engine and platform that everything else is built on top of. However, you won't need any coding experience. We're not going to create any code, nor am I going to give you code to use, this is going to be a code-free experience. I'm also going to assume you don't have experience creating augmented reality applications, and you don't have any pre-existing knowledge of Unity MARS. Now I did see in the chat that some of you do already work in virtual reality or in augmented reality. You may have experience with this tech stack, this workflow, or you may have experience with a different tech stack or workflow. We are going to go quite slowly when we create the demo. If you are already an experienced app developer, then you'll find it very simple to follow along. But for the purpose of that demo, really, I'm going to assume that we're quite beginners in terms of AR app development. How can we used XR in Industry? Well, when you're creating a product, you can use XR to visualize prototype and iterate really at every stage of the product life cycle. The difficulty in my job, given that I work with XR across many different industries, is that every industry has a very unique and specialized workflow. It has very unique pain points. We really customize our solutions, but what I have found in common across all those industries is that XR can be used throughout the product life cycle for a number of different use cases. For example, during the design and creation phase, we can use XR to iterate quickly and immerse ourselves in the environment that we're creating. If you work in automotive, you could sit inside the car that you're designing and developing. If you work in architecture, you could use virtual reality to walk around the building, or you could use augmented reality to see the building in its real-life location. That allows you to very quickly see changes that you'd like to make and to iterate. It reduces your reliance on prototypes and on their associated cost. Before and during manufacture, XR has different benefits. It can be used to train staff on things that are perhaps too costly or too dangerous or just impossible to train someone on in real life. For example, if you work on a factory floor, or you have employees that do, you could train them on how to operate machinery in an assembly line before that assembly line has actually completed construction. If you work in the construction industry, you could use XR to see the progress of a building in real-time to see different phases of its development. And then towards the end of the product life cycle, XR can be used for monetization. You can place your product in the hands of a client or a consumer. You could show them the product in a virtual showroom in VR, or the consumer could view a product in their own real environment using augmented reality. They could visualize and configure it and make changes in real-time, and that leads to a higher conversion rate and a higher probability of purchase. Now, one of the difficulties with mixed reality and extended realities is that they can put up a barrier to entry. They can seem daunting or technologically, not the simplest thing to get started with, because adopting new cutting-edge technology isn't always very simple. Perhaps you don't have developers in your organization. Or perhaps you do, but they don't have experience with augmented reality. Perhaps you are the champion for augmented reality in your business, and you have ideas and apps you'd like to create, but you just don't have experience perhaps with AR or perhaps with coding at all. Well, that's where Unity MARS can really help. MARS is our mixed and augmented reality studio. It's an authoring environment with specialized tools and a workflow for creating intelligent AR experiences. But what does that really mean? Well, it's a set of tools and windows that is built on top of the Unity Editor. It gives you the power to prototype, to test and quickly iterate and deliver your AR content very quickly and easily. Personally, I've found it incredibly useful, a great success, and it's really cut down the time it takes to develop AR applications. Now MARS is built on top of AR Foundation, which is Unity's base AR platform. And AR Foundation is, itself, an abstracted layer, which exist on top of ARKit, which is Apple's, or the iOS AR platform, and AR Core, which is Google's AR platform. What that means for you is that you don't have to develop multiple programs for multiple devices. You just develop one single program, change some configuration details, and then you can deploy it to any device, up to 20 different devices, in fact. And those may be mobile devices, may be tablets or may also be head-mounted devices as well. Although there are some different design considerations for head-mounted devices, too. It really cuts down on the time it takes you to build and iterate. It allows you to concentrate on the content that you want to develop rather than getting bogged down or slowed down by the authoring process. MARS is an additional product on top of the Unity Editor. It's available for a 45-day free trial or a $600 per year subscription. I think there's no better way to demonstrate MARS than to simply use it. Now I'd like to walk you through the entire process of creating an image tracking app. And I really mean thoroughly end to end, so everything from how to obtain Unity MARS, to use it, to how to deploy it. Again in my case, on an Android phone. First of all, how do you obtain Unity MARS? Simply browse to unity.com/product/Unity-MARS and click Try for Free. Click the Unity MARS free trial and continue to purchase. Then sign in with your Unity ID. You'll be prompted to enter some organization and payment details. I'm going to fade to black here so that my payment details aren't visible. Then you'll see this confirmation page. Now you just check your emails. The Unity MARS installer will be emailed to you. Just click the link to download it. Now I'm going to build a project. To do that, we open the Unity Hub. For this project, we're using Unity 2020.1. If you don't have this version, just click Add, find the version of Unity you'd like and click Next. I'll be building my app for Android, so I'm also going to select Android Build Support. I'll agree with the Conditions, and we're done. Now I'm going to start a new project, click Project, New, and select the version of Unity we've just installed. Name the project whatever you like, and hit Create. Now we see the familiar Unity Editor. To import MARS, click  Assets Import Package Custom Package and select the installer that we received by email. Click Import. We now have new Window options under Window MARS. Select the MARS Panel and dock it wherever you'd like. This panel contains Presets that make it really easy to get started. We need two more Windows. Under Window MARS, select Device View. Again, dock it anywhere you'd like. This view shows us a simulated AR device like a phone camera. It reflects what the user will see. Right-click on Device View and select MARS Simulation View. This view shows the entire simulated Scene rather than from the user's point of view. Now right-click the Hierarchy window and select MARS MARS Session. This makes some adjustments to the camera. You're ready now to create augmented reality applications. At this point, if you click Play in the Device View, you can move around the Scene using the WASD keys and holding down the right mouse button to look around. This simulated garden is just one of many environments that we've included for you. You can use this drop-down menu here to choose between several simulated environments. Choose the one that best reflects the real environment you will be using in the app. For this demo, I'm going to use the factory. We're creating an image tracking app, so we need to add the images that will act as markers. To create a folder, right-click in the Asset window... [LAUGHS] I overguessed myself there... and hit Create Folder. We're just going to call it "Images". In your normal File Explorer, find the images you'd like to use and just drag and drop them in. Now I'll create a new folder which I'm going to call "Marker Library". Again, right-click, Create Folder, and name it. Within this folder, right-click and go to Create MARS Marker Library I'll just hit Enter to keep the default name. With this image library selected in the Inspector window, click Add Image. Use this button to select the image, give it a name and a size. 1 in Unity is 1 real-life meter. These images are about 25 centimeters large. I'll do the same again for my second image. Now we need to hook up the MARS Marker Library to our Session. To do that, select the MARS Session in the Hierarchy and use this round button to find your MARS Marker Library. Now, we need some digital content to spawn on the image markers. You can find lots of content in the Unity Asset store. You can access this by clicking Window Asset Store. In 2020, this takes you to an online portal. Or, you can browse straight to the website: assetstore.unity.com. You can search the Asset Store using keywords, or you can search by category. You can also filter by price. Once you've found the asset that you'd like to use, click this button which should say Download or Buy, and then open in Unity. This will take you to the Package Manager where you can import your new asset. Of course, it's possible to import your own personal assets to visualize, maybe you have a 3D model or account data or a diagram. Unity MARS also comes with some simple prefabricated objects that you're welcome to use. You can find them inside  Assets MARS Content Shared Prefab Now we'll start making our app. To create an image marker, we just click Image Marker in the MARS panel. And then choose which image we'd like it to track. I'm going to stick with number 1 here. Now click Image Marker again to create a second marker, and for this, I'll choose Image 2. Now I just drag the Prefab object I want to spawn onto the image marker I want to spawn on top of. We can't see these in the Simulation View, but we can see them in the Scene view. I'm going to zoom in for a closer look using the middle mouse button. Right now, the image markers are on top of each other, but that's not a problem. I will move them apart just to view them, though. These arrows look a little big, so I'm going to select the arrow, not the Image Marker, and change its scale to "0.3" or 30 centimeters. I'll do the same with the other one. Right now, both arrows point downwards relative to their image marker. But I want the first arrow to point left, so I'll rotate it "90" degrees on the Y-axis. I want this arrow to point right, so I'll rotate it "-90" on Y. I also want the arrows to float slightly above the image marker. I'll make the Y position "0.05", so they're floating 5 centimeters above the image. If we zoom in, we can check that this has worked. You may have noticed we can't see these arrows in our Simulation and our Device View. That's because they act like the real world. The arrows aren't spawning, because the image markers don't exist in this environment. These icons here indicate that no matching image marker was found in the simulated environment. To add one, click Synthetic Image marker in the MARS Panel. I'm going to drag it by the arrow Gizmo to adjust its position. And then select my desired image from the Marker Library. Now we can see the arrows spawning in our synthetic environment, just like the real world. And we can see the Synthetic Image in our Environment Hierarchy if we scroll right to the bottom under Simulated Markers. We can also see that this icon has changed to show that the first marker has been detected. But the second marker still does not exist in our environment. I'll click Synthetic Image marker again, to add the second marker, adjust its position, and choose my desired image from the library. These images are currently on the floor, but I'd like to see how they look on the wall. I'm going to rotate them by "-90" degrees in the Z-axis and "-90" degrees in the X-axis. I'll adjust its position so it's flush with the wall, and I'll do the same for the other image marker. Now if we click Play in the Device View, and we walk the user around the position of the arrows, using the WASD keys and the right mouse button, we can see the arrows spawning as we'd expect. We can even see the arrows spawn 5 centimeters away from the image markers as we designed. Now we're ready to deploy our app to a device. I have an Android phone, so I'm going to configure my project to deploy to that specific device. To do this, I'm going to go into Window, and find Package Manager. Ensure that this dropdown is looking for packages in the entire Unity registry. This shows all of the packages we could download. Use the search bar to find "AR Foundation". Click Install or Update. If this button says Up To Date, you already have the latest version and you don't need to worry. Next, search for "XR Plug-in Management" and do the same again. Finally, because we're using an Android phone, we must find and install the "AR Core XR Plug-in". Our final step in Unity is to adjust the Build Settings. Click File Build Settings Android If your Android device is plugged into your computer, it should appear in this drop-down list. Select your device and hit Switch Platform. Now click Player Settings. There are lots of optional settings here, but I'll just show you the essentials. Under XR Plug-in Management, ensure the Android tab is selected and that both Initialize XR on Startup and ARCore are checked. Now under Player, find the graphics APIs. If you have Vulcan, remove it. But ensure you do have OpenGLES 3. Find the minimum API level and set it to API level 24. Uncheck Multithreaded Rendering and ensure your scripting backend is set to IL2CPP. Finally, under target architecture, uncheck ARMv7 and select ARM64. We're now ready to send our app to our device. But to stop it from being blocked, we need to unlock Developer Options on our phone. On the Pixel 3a, these can be found in the Settings tab. Find About Phone, scroll to the bottom, and tap on Build Number about seven times. This will unlock Developer Options on your phone. You can find these in Settings System Advanced Developer Options. To allow your app, just switch on USB debugging. Now, back in Unity, we just have to click Build And Run. Name your app whatever you'd like, and click Save. When your app finishes building, it will automatically launch on your device. And this is what the app looks like. When it finds those image markers in real life, it will spawn an arrow. If we look from the side, we can see it spawns the arrow about 5 centimeters from the plane, just like we saw in our Simulated Environment. The arrow on Image Marker 1 points left and the arrow on Image Marker 2 points right. And that's it. That's how to build an augmented reality app with no coding in about 30 minutes. Now, that was something of a whirlwind tour. I hope that was helpful for some of you. Don't worry if you couldn't quite follow along in real-time. I did speed up the download process quite significantly. But you can always follow along with the recording. You can see that MARS allowed us to create AR content completely visually. We can even drag and drop content where we want it. If we didn't want to use an augmented reality image marker, and we just wanted to spawn our AR content on the floor or on a table, you can actually drag and drop your Prefab object straight into the Simulated Environment and that's it. MARS will figure out the rules of that interaction for you. It really is very simple to use. Because that Simulated Environment view reflects real life extremely accurately, it means that we only had to deploy to our phone once we were actually finished with our app development. If we wanted to see how it looked and make lots of changes, historically, traditionally with AR app development, we would have had to deploy to our device every time we want to test the application and make a change. That really is a very time-consuming part of the process. One of most powerful things about MARS is really how it speeds up that development process so much. It's one of the reasons I really love using it. It's also very easy to build on top of MARS, it's built like Lego bricks. Maybe instead of arrows, you want to spawn a user interface or CAD data or some model that is relevant to you or your business, that's very simple to do. You can also add functionality and extensibility, such as interaction, so you could have the content animate or change. You could have a model explode and expand when a user comes within a certain distance of it, or presses a button. You could have instructions that scroll with the scroll bar. The possibilities are endless. You're only limited by your imagination. It also has built-in systems for rules. If you wanted grass to spawn all over the floor, or you wanted a certain model to spawn on any surface that was the right size, maybe any table that's between half a meter and two meters large, MARS has the ability to write those fuzzy rules for you. It has that logic built in so you really don't have to code that. You could just tell it to do those things via a simple user interface. Now I'd like to show you how Unity's AR capabilities, including MARS are being used already by industry leaders to really solve problems that would otherwise be impossible. In the automotive sector, Toyota uses Unity's AR solutions across their product life cycle. One challenge they faced was showing the functionality of internal mechanisms of the car after it's been assembled, especially when those mechanisms require the car to be in motion. Previously, it would have been difficult or maybe just impossible to show that kind of functionality, Now using Unity and the HoloLens 2, users can move around and inspect a car that is virtually in motion, and in augmented reality, they can see the mechanisms working internally, making that challenge now completely simple and straightforward to do which previously, would have been impossible. Now one way to localize that content to the car would be to use image markers, and as we've just seen using image markers and MARS is really very simple. You could also use MARS to simulate the showroom or the factory floor and the car. That would allow you to easily iterate on changes, such as deciding when should the content spawn as the user moves around the car. AR can also be used for very highly effective sales and marketing. In 2017, IKEA used Unity to create the IKEA Place app. This is a product visualizer and configurator. It really puts the product into the hands of the consumers. They use their phone to scan around their environment, and it detects planes, so walls and floors, and it spawns content of an appropriate size inside their local environment. They can then configure, change aspect or colors of the product, and that leads to a higher conversion rate or a higher probability of purchasing. Using Unity MARS, you could easily replicate the success of this app. You could create content that is generated under a particular set of conditions. For example, the app could generate picture frames or clocks on walls, cushions on tables and chairs, and bigger furniture like sofas when it finds a large expanse of floor, depending on the size and the orientation of the planes that the app detects. AR can also bring huge benefits in terms of service and maintenance of a product, as well as training of staff. ABB is a supplier of industrial machinery and software. And their field operators have to service expensive and dangerous machinery, which means they have to undergo or previously had to undergo a very time-consuming and costly training before going out on site. Once deployed to the field, it's difficult to track the allocation and the procedures which have or have not been completed. This tracking was done on paper and had to be processed afterwards. That resulted in delays and also communication problems. To resolve this, ABB used Unity to develop a novel software solution, which allows any field operator to follow digital instructions in AR while they're servicing the machine. It allows any operator to become an expert without expensive time-consuming training. It also means that the procedures can be tracked, and the data sent back in real-time, so it reduces any delay or communication problems. Now ABB's image tracking actually works, or rather, is spawned based on image markers just like in the demo we've created. You could create an app like this, using our earlier demo as your jumping-off point. I hope these examples, and your own experience with MARS is giving you a taste of what's possible. If you'd like to build on your knowledge, we have a whole host of learning resources available. If you don't already have MARS, you can obtain it via this link, its product page, and I've also linked in an e-book that I think it's a great example of other industry use cases, mostly XR use cases. If you're interested in seeing more evidence and statistics, for example, on the return on investment of these technologies, I'd check out that e-book. If you'd like to learn more about MARS and its capabilities, then I would highly recommend these past webinars. And finally, I've linked our software development guide if you want to jump in and extend your demo. Finally, I want to thank you for coming to my talk today. I really hope you found it helpful. If you have any questions for me, hopefully, I'll be able to take some live now. But also if I'm not able to answer your question live, that's no problem, you can follow up with me on Twitter, if you have any questions or feedback about the talk, and if you have questions about content and this demo, please do post it on the Unite Now Unity forums where myself and my colleague will endeavor to answer all your questions. Once again, thank you for attending this talk and have fun creating. Thank you very much. I'm going to take some of the questions. This is a very interesting question: "Can markers be used to augment an entire room?" In theory, yes. What I would recommend is that markers instantiate or spawn content that's quite local to the marker, because if the marker leaves the view of the camera, the camera will struggle to locate that content. What I would say is that MARS is fantastic for adding content over an entire room because it has a built-in Planes Visualizer. You simply click one button in the MARS Panel, you can visualize all the planes, and then you can set up a rule to generate content, say, grass over the entire floor that it can see. You don't actually need to place image markers over your environment, it can just dynamically spawn that content as the user moves around. That's how I would recommend doing it, however, you certainly could place several image markers around a room if you wanted to spawn lots of different content in different parts of the room. Ah, there are some questions I don't know the answer to yet. "Does MARS have a student license?" I don't believe so, but I will endeavor to find out for you. But that is a very good question. It does have a 45-day free trial though, which is quite a long period, and I would definitely recommend signing up for that if you're interested in experimenting with it and seeing if it's worth the cost, and you can set that to not auto-renew, if that's something that, as a student, you can't justify. Oh, this is a very good question: "Do all users of the app have to have their phones in Developer mode?" I don't believe so, no. Personally, I've actually only deployed mobile applications to my own phone. I'm not certain on this, but once you've actually developed an APK, which is Android's format for an application, you can push it out to the Play Store where it has to undergo some checks, and then your users can interact with it. Certainly, for something like the IKEA Place app, it does not require your users to have their phones in Developer mode. It's just so that you can deploy straight from Unity and build straight to your phone, it speeds up that process. Another content marker type question: "Can we generate content relative to the initial marker?" I'm not quite sure what you mean by that, Rubimba, so please do give me some more details there, but if you mean... oh, I think I understand. If you mean there's an image marker, can you generate the content like to the left or to the right or above or below the marker? You probably wouldn't want to do below, yes you can. It's very simple in the same way that I changed the position of the arrows. I moved them up slightly, you could just generate content on the left of the marker, that's completely fine. What I would say is, again, if the image marker is out of view of the camera, then you might struggle to locate that content, so I wouldn't put the content like two meters away from the image marker because then you'll struggle to keep both the marker and the content in view. But that really depends on the context of your application and the size of your application as well. If you're creating something for a billboard, probably you can generate content some distance away from a marker. There's some really great questions here. "Filmmaking in AR and VR?" I really like this question. If you don't already, I would follow Ben Radcliffe on Twitter, he's my colleague, and he specifically focused on media and entertainment, so he has great experience in the film industry. I think augmented reality and virtual reality have a very interesting role to play in media and entertainment. Right now, they're being used to bring the characters to life in the local environment. MARS was used by Dr. Seuss and used to create Dr. Seuss IP by a company called Sugar Creative. And that was actually an augmented reality reading book, so you would open up the book, it had quite a young audience, it would show the user the letter, and then it would start instantiating or spawning Dr. Seuss' characters around the room. For media and entertainment, IP is king. Generating your content in the environment of the user is really interesting. VR is quite a different use case. One of my past roles, I used to work in a planetarium, so I have experience working in a 360 dome environment. And VR film is, of course, 360 spherical film. And really when you're creating that type of content, you have to have different considerations than a flat film. For example, the user has six degrees of freedom. They can move in three directions, and they can look in three dimensions, or if it's a still experience, where the user doesn't move, which is more common, they have at least three degrees of freedom, they can look in any direction. That's quite different to traditional film where we do know the user's attention is focused on the screen. You really have to think carefully about things like spatial sound, lighting, and how to draw a user's attention. It's really interesting. I think there is fantastic potential in those spaces and there have been award-winning virtual reality films and content. But you really have to approach it with a different mindset. And in that case, augmented reality and virtual reality have quite different use cases, I would say. "Can MARS handle things like occlusion?" That's a very good question. And, "Can hardware, like Intel RealSense cameras, iPhone 12 Lidar be used to provide or boost that capability?" Yes, it can. MARS has two different ways of doing occlusion. It could be a planes-based. That's based on the planes that it's detecting. Or it can be depth-based. Planes-based occlusion would do something like if you generate a content on the floor in your user's room, and then there was a table in the way, what occlusion means, if everyone is not familiar, is like blocking or hiding. Augmented reality content is drawn on top of the camera feed. By default, if, for example, in the demo we made, if I wave my hand, the arrow would appear on top of my hand, because I didn't add any kind of occlusion or hiding. Objects in front of the augmented reality, a virtual object will still appear behind unless you add in that capability. I would recommend planes detection, which is very simple to do in MARS. I think it's one or two clicks to implement it. And that means that tables and especially cubic objects or walls are very good at occluding the object that's behind them. However, this doesn't work as well for something like cars or people walking in front of the camera, or any very dynamic fast-moving content. For that, you can implement depth occlusion, which means anything that's closer to the camera that's in front of the space of that virtual object will occlude it. In terms of Lidar, Lidar works fantastically well with MARS, I'm going to get an iPad Pro with Lidar very soon, so I'm very excited to test this out. But I've seen MARS being tested on Lidar-capable devices, and what it really helps with is detecting the planes. If you don't have Lidar and you're scanning an environment which is a very pure white environment, so like a white hallway with absolutely no texture or color, it can be difficult for the camera to obtain the information necessary to start placing AR content, which is why image markers can be really useful in that context. But with Lidar, that's no problem at all. You can scan a pure white environment with almost no visual markers, and of course, Lidar is like radar basically. It will detect the depth, so yes, Lidar really helps not only with occlusion, but also with the planes detection, particularly when you don't have many visual markers. Yeah, loads of questions. Okay, so someone has said, "I would love to create custom simulation environments from meshes, from 3D scans, as I haven't been successful with planes extraction." That is something I am currently investigating, and I'm exploring at the moment, so I know it is possible to scan your environment, and then to create that as your Simulated Environment. Right now, I can't tell you how to do that. I'm currently working on learning that so I can teach you. But yes, that is something that we have the capability to do. That simulated factory or room or backyard that you saw in the Simulated Environment View could instead be your actual custom environment. And so you could test content in a location-specific application. Maybe you want it to work in a specific monument outside, and it's not practical to keep going there to test it. Or right now, because of the pandemic, we're working from home, so perhaps you want to test it in the office space, but you don't have access to that. That's something that you can do, you can bring in your own custom environments and then test in those, which is really a powerful application. There's actually two questions about that. Now I know what to create my next tutorial on. It's really good to know actually, if you have any feedback about things you'd like to try and do, what you like to use MARS for, what features you'd like to see or learn how to use, please do let me know, because I'm very interested in tailoring my content to really solve your problems. Because I work across a number of industries, every industry has different pain points and needs different solutions, so I'm very interested in hearing your specific use case and helping you to deploy the solution that works for you. Oh, "Can we use real objects as markers like a chair?" That is doable, but slightly more difficult. I'll see if I can create a tutorial on it. One of the reasons that's challenging is an image marker really only has a few dimensions from which you can view it. You could see that when we moved off to the side, the image tracking started sliding very slightly. With an image marker, you'll always get the best tracking if you view it face on. However, you can change the angle and the tracking should continue. Obviously with a 3D object, there are so many different views of that object that you have to have a much more intelligent tracking. Potentially, yes, but it'll be more challenging. What I would say is something like a can, so you've got a cylindrical object with an image on, it's going to be much more straightforward, because it's still really image tracking wrapped around a tube versus something like a chair, which really you can view from any angle and it doesn't necessarily have any specific visual markers on it. That would be more challenging. Just reading some of the questions, there's some very good ones. There's a good question I don't know the answer to: "Can AI be used in the same way as visual markers? Could you, for example, detect a specific car and occlude it by another car?" That's very similar to the question about using a 3D image as a marker. I don't know of an example where a specific car was detected and occluded by another car. I'll endeavor to find out for you. I'll make a note of it. I can't think of a use case where that's happened. One of the challenges there is that machine vision, machine learning, often looks for unique feature points, and so cars have a very similar structure to each other, and so it might be quite difficult for the machine vision or AI to recognize one car as opposed to another. But that doesn't mean it's impossible. I'll try and find out. Oh, I like this question, this is quite different. I'll take this question. "What would your tips be for someone who, in this case, is a graduate student or someone who isn't strong in coding who wants to focus on if they want to get into the AR and VR industry?" This is a great question because I don't have a computer science background myself. I actually come from zoology animal behavior background, and I actually taught myself Unity before ending up working at Unity. And I have a specific interest, of learning AR and VR. What I would say is that Unity is a great tool to use, because you don't need to have that coding experience initially. It is useful later, particularly when you want to develop really custom logic and really sophisticated things. When I started learning MARS didn't exist. I had to code any sophisticated logic. I would say now using MARS, you can get away with using the rules and the conditions, which we didn't look at in this demo, but I'd like to show in a future demo. You can set up rules and conditions just visually, using the user interface. You actually don't need to code. You could become a VR and AR, you could create VR and AR applications and be a developer to some degree without actually knowing any code. I think that's the real strength of Unity, is that, you can get that feedback and get that application working nearly immediately. And then if you do decide to jump into the coding, that unlocks a lot more options for you as well. Yeah, so I would say experiment, try things out, don't be afraid of doing things wrong, also just to see what's possible, just to use it in different parts. I've been the person who sort of championed AR and VR in every industry I've been in until ending up at Unity. If you can be that champion within your business and show that it's really applicable and relevant, that really goes a long way, because once you start using it as part of your day job, like you learn super quickly. This is a good question: "Is the Oculus Quest AR capable of using Passthrough?" Really, if I was creating an augmented reality application, I would recommend either a dedicated AR device for a phone. Because using VR Passthrough, you can get some really interesting results. But you might run into some unexpected roadblocks, and it wasn't really designed with that in mind. The Quest is fantastic for virtual reality applications, which we did touch on in the talk, so you could use VR to train staff for example, and in fact, the Quest is widely used for training in a number of different industries, from energy to aerospace. VR applications are really widespread as well, but I don't think a headset like the Quest is really extremely suitable to AR capabilities. I would recommend something more like a HoloLens. Or if you don't have the resources to obtain something like a HoloLens, there's actually, I just found out about, there are these head-mounted... I don't know if I'm allowed to plug products, but there are head-mounted devices, a bit like Google Cardboard but more sophisticated, where you can mount your phone and you can create these AR applications either using your hands or using dedicated controllers and they're very, very low cost, like under $50, £40 cost. It is really becoming increasingly possible to get a dedicated AR headset, if that's what you'd like to develop for. Ah, okay, this is an interesting question: "What are the limitations for marker tracking if the marker is in motion?" Someone knows what I'm talking about, yes. If you look in the chat, some of the attendees are talking about the device. I actually don't know whether I'm allowed to mention which device it is, but it's from Kickstarter, and I think it's great, I'm really all about democratizing knowledge and bringing augmented reality to everyone, because lack of access to devices was really one of my biggest obstacles for a very long time. If you can get your hands on low-cost AR devices, I recommend doing so. Sorry, back to my question. "What are the limitations for marker tracking if the marker is in motion tracking speed? Can you track a marker on a race car?" I don't think so, I don't think you can, I've never tried. But no, I don't think you'd be able to track a marker with that much speed. However, if I was tracking something that moved that quickly, I would use a more internet of things approach. I would use a tracking device that sends out its location, and then you can stream that data into Unity. One of the powers of Unity is that it's a real-time engine. You can create things that are reacting to the real world, you're not pre-rendering your content. You could have a race car moving at certain speed, you can have a device on it that was measuring its speed or its location, its direction, coordinates, whatever you would suit. And then you could use that data, you could visualize that data in whatever way you want it. I don't think image markers would be best suited for that example. I would use a more dedicated device that is... measuring the position or the speed, depending on what your application is. Another question here: "Can you apply markers on the floor?" Oh, just realized I'm still sharing my screen, I don't have to do that. I can see the Q&A much easier now. That's much better. "Are there any plans to ID moving objects and spawn onto and track like with a dog?" That's a really interesting question. And I really want to know what the application is here. MARS, I didn't actually show you this, but MARS comes with these templates. For example, if you want to create a tabletop application where the spawned things are on a table, if you wanted to visualize a product on a table, like a bag or something, it also has a miniatures template that allows you to spawn miniature contents. Say you wanted an entire building or factory to spawn on a table in miniature, it has a template for that. And also it has a face tracking template that's really useful for any kind of face tracking applications, so things like filters or even try-before-you-buy jewelry applications. I've been creating one of those as a kind of test. And that's tracking moving content. However, tracking a moving object like a dog would be extremely difficult to do visually. Part of the reason for that is dogs are extremely varied. In fact, in computer vision, the dog/cat problem is often used as a common example of very difficult machine learning problem to overcome; training a computer on whether something is a cat or a dog because there's so much variation within those two populations. I would say that, in terms of plans to ID dogs specifically, I don't know of any. However, Unity does have a whole department for machine learning and AI, and we have something called, Unity Perception, I think it's called that. Don't quote me on that. And one of the things that they've been working on is the ability to identify, for example, content in a grocery store. You could look at a shelf, and it can identify all of the different products, and that could really help with things like price comparison. Or it could really help someone visually impaired, for example, to conduct their shopping and know which product they're looking at, because obviously in a tactile sense, you can't tell one cereal box from another. There are some really amazing capabilities with Unity Perception or Unity's machine learning, which is a little bit separate to what I work on. But I don't know of any specifically that are tracking dogs, but I will let you know if I find any. In the case of a dog, again, I would use... if you're trying to track the position of the dog, I would maybe use a marker or something like that, like with the race car example. I guess it really depends on what you're trying to achieve in your specific application. Let's just see if I've got more questions. Someone's put their hand up, but I don't know... Ah, Unity Perception. Thank you, Jerome. Jerome is my colleague, he's saving me from calling it the wrong thing. Yes, I was talking about Unity Perception, and there's a link in the chat for the user who asked about computer vision, and if anyone who's interested in more about what's possible with computer vision. It's not just image tracking markers that you can detect, it's all kinds of complex data. It's really interesting to see what's possible, and I'm blown away every day by what people are creating with Unity. I'm really excited to see more adoption in the industry and see what people create, because I think we're really on the turning point of that now where it's really starting to explode and it's exciting to see some of the crazy content that people come up with. Loads more questions. If I don't get to your question, I will try and answer afterwards, either in the Unity forums or via my Twitter. If you have any further questions about this talk or about my career journey into VR and AR, or any feedback on this talk, then do reach out to me on Twitter, because I really appreciate feedback, and I love engaging with people who are working in this space. Great, I think that's it for now, I don't think I'm going to be able to take any more questions there. I really hope that this has given you a jumping-off point. As I said, this talk will be recorded and will be made freely available. If you do want to follow along with that demo, but you weren't able to obtain the software in time because of course, I did speed up the download part, then absolutely do find it and follow along. Create your demo and share it with me on Twitter, I really want to see what you've created. Of course, instead of the image that I've used, you can use your own image, you could use a painting or poster in your house, you could use your company logo. I would say use something that has a lot of details, because if it's very simple or symmetrical, it may not work as well, so use a more complex image like a poster may work quite well. Yeah, and share with me the content you're creating, any question or comments or feedback that you have at all. And I guess that's it. Thank you so much for coming. I really appreciate it, and I'll answer all of the questions I haven't managed to get to, either on the forums or via my Twitter. Thanks again, and have a great day, have fun creating.   [MUSIC]   