 greetings travelers my name is rofi du i'm a research scientist at google today i'm going to present our open source magic depth lab real-time 3d interaction with steps maps for mobile augmented reality augmented reality has gained mainstream popularity on mobile devices with thousands of ar apps such as pokemon go snapchat and ikea place these apps are typically supported by google's ar core or apple's air kit to place virtual objects including flat physical services often invoke experience as a reaction to detected ar markers however is direct placement and rendering of 3d objects sufficient for realistic ai experiences not always for instance without awareness of ruling virtual content looks like it's pasted on screen rather than in the real world with depth map and agreement where rendering the virtual cat in argentine reality looks much more realistic than the former one here correct occlusion helps ground content in reality and makes virtual objects for you as if they are actually in your space next we wonder how can we bring these advanced features to mobile air experiences with google's er core depth api available on millions of android devices we wonder is there more to realize them than occlusion what about service interaction realistic physics and path planning we are used to these concepts in typical 3d games why not in ar our phones as well next please enjoy depth lab encapsulates a variety of depth-based ui or ux paradigms including geometry aware rendering real-time visual effects such as rain snow flooding and re-lighting and surface interaction behaviors such as surface splatting real-time mesh generation and physics simulation depth lab is available on a large number of android devices with a single rgb camera we compute live depth map by the depth from motion algorithm introducing craft asia 2018. instead of learning that from a stereo pair of camera we use that from motion providing stereo images across time as the phone moves through the environment however this presents additional challenges compared with traditional stereo well traditional stereo has full control of how the cameras are arranged with a wide camera baseline stepping from motion cannot control how the user moves the camera also cannot guarantee vw overlap velocity match motion blur auto focus and essential users likely to move very little they want to arbitrarily move the mobile phone so the important considerations include which keyframe to choose and how to maximize information generated from each frame keyframes are chosen from a pool to give the best keyframe to give the best possible depth for the current frame oftentimes the keyframe is just a few centimeters away from the current frame then we use stereo matching algorithm providing a sparse and noisy depth representation that needs to be filtered interpolated and smoothed we call it raw depth in the upcoming ar core the depth image is matched for every camera image processed by ar core at 30 hours our application can acquire the latest depth image for the current frame for more details please check out our siggraph asia 2018 paper that's promotion for smartphone ar currently our step 16 image format allows for depth sensing up to 8 meters and the best steps measurements are between 0.5 meters to 5 meters however even with real-time test map there's still a large gap between the raw death data and the typical expertise of mobile application developers who are not experienced in handling depth data so in that lab we process the raw depth map from aer codex api and provide customizable and self-contained components for mobile ai developers to build more photorealistic and interactive ar applications to explore the design space we conducted three brainstorming sessions with a total of 18 participants and proposed a total of 39 aggregated ideas these participants include researchers engineers and us designers who have worked on ar or vr related projects we listed these ideas in the supplementary material please feel free to scan the qr code we'll also show the qr code in the end when developing that lab we architect and implement a set of data structures and real-time algorithms for mobile area developers we generated three kind of data structures depth three depth mesh and depth texture definitely stores steps in a 2d array of 16 bit integers on the cpu which is typically 160 by 120 pixels and above the second depth mesh is a real-time triangulated mesh generated directly from the depth map we'll introduce later how we generated the depth mesh third depth texture which is basically a gpu texture decoded from the depth array and interpolated to larger resolution based on the data structures we classify our def lab components into three categories local steps surface steps and thin steps local steps uses the data rate to operate on a small number of points directly on the cpu for example by converting between the screen space with the world space space depth lab provides a 3d oriented cursor the cursor orients according to normal vector of the physical surface and details about its distance to the ground and to the camera computing usable normal maps of low resolution and the core steps maps can be very challenging a simple cross product may yield noisy or invalid results due to depth discontinuities holes and outliers in the estimated scene in depth lab we provide two real-time api to compute a more stable normal map in real time on both cpu and gpu the basic idea is just to simple a touring or even forwarding neighborhood to get a smooth normal map here is the comparison before and after we do the averaging on the normal map our algorithm can effectively smooth the normal map while keeping a reasonable accuracy for developers and in that step we provide a library to cast and reflect virtual literally to leverage the smoothest normal map it looks like that we can automatically plan a 3d path for the editor that avoids a conclusion that can avoid a collision with the statue by making the editor hover over the obstacle statue particle effects can also be achieved through depth map for example each ring job tests for hits to hit with the physical environment and relative ripple upon occlusion in our work we forego surface reconstruction and directly represent environmental depth measurements as meshes this technique relies on a densely tessellated chord in which each vertex is displaced based on the projected depth value after initializing the depth no additional data transfer between cpu and gpu is required during the render time making this method very efficient with service steps we implemented physical collider note that the creation of a mesh collider happens at a lower resolution on cpu runtime however we only perform the mesh glider generation each time when the user throws a new dynamic object into the ar thing instead of running it at every frame this operation is computationally expensive and not continuously needed as the physical environment is mostly static when you are throwing the objects an advanced example is color balloons thrown out to the physical surfaces with texture decals the balloons will explode and wrap around surfaces upon contact with any physical object such as a corner of a table 3d photo effect is also included in depth lab with textured projection mapping techniques when the user clicks the 3d photo capture button we save the current camera image depth image and camera parameters given the screen center we first resolve the central vertex in the real world based on the current depth map next we generate a frozen depth mesh based on the camera's world position we can compute the directional vector from the camera to the central point of the screen next with a cross product we compute the plane perpendicular to the directional vector finally we rotate the camera and recompute the projection matrix based on the camera's position and project the cache texture to the depth mesh directly and you can see the reproductive texture looks like a 3d stereo image effect as for this steps we introduce a built-in depth guided anti-aliasing algorithm to reduce the artifacts due to the low resolution depth map the motivation is that closer depth pixels are typically larger so with simple neighboring pixels but we might simply when simply living pixels we use a larger kernel size for pixels with smaller depth values due to the compute constraints on mobile ar we recommend interactions with thin steps to be implemented fully on the gpu with compute or fragment shaders as for real-time lighting methods based on brdfs such as phone or lab version models usually require a normal map which can contain artifacts around object boundaries in low texture regions in our approach we compute the photo intensity at the points when the rays intersect with physical services please refer to the paper for more detail imagine a recasting from the light source to the target pixel the intensity of the photon will decay when traveling a laundry the intensity is greatly reduced when heather obstacle and the pixel should look much darker with more obstacles or laundry with a little scattering we can achieve more interesting effects such as relating with sun beams here is the sand depth lab also enables wide aperture effect which can be focused on a world anchored point unlike traditional photography software which only anchors the focal plane to a screen point depth lab allows users to anchor the focal plane to a physical object and keep the object in focus even when the viewpoint changes for example just one tab on the flower it will automatically keep in focus no matter the user approaches closed or approaches far of course depth this steps components also included algorithm rendering as we introduced earlier and fog effects next we build a minimum variable application to profile a typical usage of that slab to evaluate the performance of key depth app components we made the application with a point depth example which is oriented radical a service depth example which is a depth match generation and the perfect soul depth example which is a virtualization of depth map we run our experiments in five locations each running five minutes and our simple application runs at over 100 frames per second while surface depth is the most time consuming part in the second test we evaluated the performance of the real-time rewriting one of our most computational expensive technique according to the results we recommend four to eight samples per way to deploy our realizing model on pixel 3 or comparable mobile devices in the third test we evaluated the performance of the white aperture effect and recommend a kernel size of 11 to 21 for real-time performance we share depth lab with both internal and external partners for example depth lab components are used in snapchat line of play game in the mirror app scene viewer and more as for limitation depth lab is designed to enable geometry aware ar experiences on phones with and without hammer flight sensors however we have not yet explored more in the design stress of dynamic steps in future we envision live steps to be available on many iot devices with camera or depth sensors in the future each pixel in the depth map could be associated with a semantic label and help computers better understand the world around us and make the world more accessible for us finally we open source the desktop library on github to facilitate future research and development in depth aware mobile experiences we believe that this library will allow researchers developers and practitioners to leverage the base interaction to build normal realistic ai experiences on regular smartphones finally i would like to thank all my collaborators including eric max lucca evo json joe jose josh nuno sharam adash kong and david for their hard work behind the scenes and thank you everyone today for listening to my talk any questions are welcome thank you 