 [Music] we're used to a are at our fingertips giving us useful and fun experiences your content though often looks like it's pasted on the screen rather than in the world what if aircore could supercharge your camera bringing the world into your phone by giving you color and depth at every pixel that it sees we've squeezed all of our software magic into the new AR core depth API so it with just a single moving camera we can give you 3d understanding of the world on over 200 million Android phones this means that we can start properly occluding objects like our 3d tiger but occlusions are just one aspect of immersive air experiences when your camera understands 3d space your content can realistically collide with the world or even just stick to it instead of bouncing around and your characters can move around your space naturally on any terrain or you can transform the world around your users by using particle based effects such as snow piling up in your surroundings or splashing rain basically blending the real individual and if your phone has an active depth sensor all of these deafspace effects can get even better it's all about you know trying to pull in as much information from the real world into your digital world so you can really help these augmented objects feel grounded in the real world you know you're looking at things like light estimation environmental HDR and now I see depth is just kind of one additional layer to really have those objects feel like they exist in the real world with houses view in my room feature we want to help people make much more confident buying decisions by letting them preview products in their own space before they make the purchase Google solving a really really challenging technical problem here and the greatest part of it is that it's actually available to a really wide range of Android phones without any specialized hardware so it really is something that we can bring to mass more to help people have a much better preview experience we partnered with the studio media monks to explore how we could turn your physical environment into a creative campus with depth by better understanding your physical space we're able to turn it into a new world into a different dimension allowing us to hide characters behind objects and furnitures enabling new types of interaction even being able to paint what's wrong you thanks to augmented reality if we really look at what a unity developer to combine with the a our core technology for depth can do is you drag in a component you easily pull in that data and you can start easily make sense out of it and also have it combined with all the other systems immunity if it's particle systems or if it's navigation masters for characters or it's just occlusion to make sure that things blend in well those are the core pieces that we need together with it will make easy and accessible for unity developers and we need developers like you to help us build the future these experiences are just scratching the surface of what's possible with depth if you have an idea that you'd like to build using the depth API fill out our call for collaborators forum in the description below we're looking for a few more developers over 2020 to help us build out a new wave of the AAR experiences thank you so much for joining us we can't wait to see what you build with the air core depth API [Music] 