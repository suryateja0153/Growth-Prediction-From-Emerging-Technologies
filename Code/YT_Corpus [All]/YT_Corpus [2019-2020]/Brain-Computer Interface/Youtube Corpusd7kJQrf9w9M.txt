 so welcome thank you for being here it's great to see to see such a crowd here I'm Dianne Lipscomb my name is bjorn zhang Shido on behalf of once i would like to welcome you this event as part of the university select building on distinction lecture series it marks the launch five years ago of the surging plan that Brown has now in place I think the the purpose of this lecture series is to highlight exceptional research that's done at the interface of different disciplines and that showcases this research in this series so it's particularly timely to have this that at this time in months time the Chi Institute and the data science in incidence it initiative are going to move into the same space the former Brown office building one 64 inches Street which we are always very very excited about they're going to be on the third and fourth floor we have a lot of shared space and we envision a lot of interactions between the two units and a lot of exciting research coming out of this and I that's above the bookstore 164 angel and you know I've often thought what would be the most wonderful place to have my office and it would be above a bookstore and so we're gonna be up there the machines and everything will be gone soon and you'll be able to walk back and forth along to history yeah so John said I think it's really exciting for us when I first spoke to Rick you know about moving out of Stimpson which is the home of the of the Institute right now next to the Athletic Center and we were talking and and he had mentioned that there was the possibility that this space would be available and we brainstormed a bit and we thought how brilliant it would be to have the data science initiative and brain science kind of located in the same building and like many things that Ric has done over the last few years he made it happen so thank you it was just a brilliant idea to bring kind of an academic focus into that fantastic kind of central place on campus and so we're indebted to you for really making this happen I do also want to mention a couple of other people and I don't know if I see them here but Leah van Wei has been brilliant in helping us through the process of the building holding our hands saying it's gonna be okay it's gonna be wonderful and really really producing and helping us to have this absolutely beautiful space which you will all be invited to so finally I just also wanted to mention as beyond said we're really going to work as a unit the point of being together is to really enhance collaborations you're here today of the existing collaborations that we have across these two disciplines and I think you'll find their research incredibly exciting but that's just the beginning and we really think that being located in the same space is just going to continue to enhance and and bring you innovative and creative research on to campus and finally I just also wanted to say that it's not just data science and brain science Kearney Institute going into the building but also CC MB computational molecular consent of a computation molecular biology run by Sohini Ramachandran and also the Annenberg Institute so it's gonna be a data intensive data heavy data driven research and and science going on there so thank you all for being here do you want to say anything else mention so I wanted to just bring up we'll get the show going and introduce Cornelia Dean you'll be the moderator and we're absolutely thrilled that you agreed to do this you barely need introductions but you will see everybody is listed in here with a great description but Cornelia is a visiting professor at Brown science writer and the former science editor for the New York Times oh thank you so we are here today to learn about three fascinating areas of research all of them combining cutting-edge investigations in neuroscience with advances in data science to me these projects embody the potentially enormous advantages we can derive from true interdisciplinary research there are still plenty of people who deride interdisciplinary research as resulting in collaborations that do not dive deeply enough into any of their component parts as you will see from the presentations you see in here today that is not necessarily the case today we will hear from Matthew Harrison of Applied Mathematics Stephanie Jones of neuroscience and Deema am so of cognitive linguistic and psychological science Sciences on three projects that reach across disciplinary lines to produce research with potential to change lives in our format each of them will speak for about 10 minutes and then we will have a discussion in which speakers will take questions from the audience we ask you to hold your questions until the discussion period in the interest of time I will skip the usual descriptions of people's academic histories and so on all of which are in your program and just get straight to the heart of the matter so our first speaker is Matthew Harrison associate professor of applied mathematics at here at Brown among other things his research focuses on applications relating to brain the brain computer interface and technology that can enable people with paralysis to communicate and do others so here is Matthew [Applause] and welcome everyone so today I want to tell you a little bit about one of my collaborations and it's my collaboration with the BrainGate group here at brown the the brain great group involves an enormous number of people who made possible the research that I'm going to tell you about and I just collaborate with them so they develop brain computer interface technology to improve the lives of people who are paralyzed or who have lost limbs and so one of the types of technology that they develop is they work on a sensor that can be implanted on the surface of the brain under the skull and the pattern of brain activity recorded by this sensor can allow people to control devices with their thoughts so here's an example of a paralyzed woman using the device to type on a computer you can see the device that attached up to the top of her head she's unable to move any of her limbs but she is able to control a computer cursor with her thoughts using his brain gate technology and so she was asked a question how do you encourage your son's to practice music and she's typing when they started their lessons they were almost three and they took to playing violin and cello very fast and I had a sticker system and so on so in this next video you'll see a man with paralysis controlling and experimental artificial hand for the first time the hand there it's connected to the brain gate technology and even though you're going to hear him saying open and close the artificial hand is not voice-activated it's controlled with his thoughts using the brain gate technology Oh so here's an example of the type of signal that the brain Gate sensor records every now and then you'll see a big sort of spike of electrical activity and that's a brain cell firing close to the sensor and the sensor simultaneously records dozens or a hundred of signals like this and a single participant generates an entire hard drives worth of data every day and that's only increasing as the technology improves and so at the heart of the brain gate technology our mathematical and statistical algorithms that translate these brain signals into commands that a computer can understand now of course the brain is a giant mystery nobody understand how it works and the computer that's receiving these signals from the sensor has no idea what they mean or how to use them at least at first so in this next video you're going to watch in real time as the computer learns how to interpret the brain signals recorded by the brain gate sensor and the person a person with paralysis is trying to move the cursor to the highlighted you'll see the cursor in a minute that's what they're instructed to think about when they play this game so you'll see they're trying to move it there comes the cursor it's not really working and so the computer begins with zero knowledge and consequently the algorithm doesn't allow the participant to control the cursor in a meaningful way but over time as this game goes on the computer discover statistical patterns in the brain data that allow it to predict how the user is trying to move the cursor so the person is trying to move it after a while it times out and goes to a different target and all the while the sensor is recording brain data and it's presuming that they're trying to move to the right location so that allows it to relate brain data to movement intention and slowly you'll see the algorithm prove and the person can begin to control this game so my collaborations with BrainGate have focused on how to improve the statistical learning algorithms at the heart of this technology and this video is showing one of the algorithms that I helped develop along with doctoral students in neuroscience and applied math and biomedical engineering and so they're persons a minute into playing this game and after a minute or two they have pretty good control already and it will continue to improve so once they've calibrated using this game and the algorithm understands how to interpret their brain data then they can move on to more interesting tasks I'm typing on a computer so let me just spend a little bit of time trying to give you a sense of how the algorithm learns to translate brain signals so the sensor is collecting brain data but it can also guess what the person was trying to do so this is symbolizing like a movement direction or an intention because it knows that the person is trying to play this simple game of moving the cursor to the highlighted dot and in the next instant of time it gets new brain data and a new movement intention and this goes on and over time it starts to build a dictionary so to speak of brain data movement and tension pairs now when it gets new brain data once it has this dictionary and let's say the person is typing it's not playing the game anymore so it doesn't know where the person is trying to go then what it can do is it goes back to the dictionary that it's built and it tries to situate this new brain data somewhere in the pattern of all the brain data it's collected to fit it into that dictionary and then once it's done that it can use it to predict what direction the person was attending to go and then it moves the cursor a little bit in that direction or moves the robot arm and of course underneath the hood of all this there's a lot of math and applied math and data science and a lot of it to make it work fast enough to to be abused in this brain computer interface technology and I think really a lot of continuing opportunity for applied mathematicians and data scientists to further improve the algorithms at the heart of this truly revolutionary technology that's being developed by the BrainGate group right here at Brown so thank you so our next speaker is Stephanie Jones associate professor of neuroscience among other things her work integrates brain imaging and computational neuroscience to study how the brain works both in people healthy people and in people with problems with the brain so thank you and again welcome everybody very glad to have the opportunity to describe to the community a new software tool that we've been developing to study the human brain signal and you'll see when I described the development and applications of this tool that it integrates many aspects of both data science and neuroscience so I want to begin with the question that we were trying to address with this new tool and as Matt just shows you we can get really powerful and predictive information from the human brain when we record directly from the surface of the human brain but these are highly invasive procedures and they're only performed in select patients that have a very specific need and so what I'm going to do is I'm going to move the discussion to signals that way we can record non-invasively outside of the head and the question that we asked is can we develop a technique to interpret these non-invasive human brain signals and both healthy behaviors and in pathology sorry my slide got cut here that must be something about transferring to this computer so we'll just go there but that says and pathology okay so one of the most powerful ways that we can record human brain signals non-invasively is with a technique called EEG or electroencephalogram now EEG is very powerful because it measures the electrical signals of the brain with very timer and very fine temporal resolution on the order of milliseconds similar to what you saw and the date of that match showed and it's very easily to use and it can be done in awake behaving children including infants and babies so I'm showing you here an example of a well known signature of activity that's recorded with EEG when you put an electrode over the part of the brain that represents the visual cortex and you close your eyes you see this large amplitude rhythm that emerges and when you I'm sorry when you close your eyes when you open your eyes that amplitude of that rhythm dampens now a EEG is our old technology it's been around for almost a century the first human EEG recordings were done in the early 1900s by a psychiatrist named Hans Berger and at that time they didn't have computers and so they had to store the data as drawings on paper now here we are in the 21st century we have massive computer power we have the ability to store vast amounts of data we have now open science that allows us to share this data worldwide we have all these new machine learning and other algorithms that are allowing us to classify different brain States and we've reached a time where we have these vast data streams and data mining capabilities and now these EEG signals are associated with almost every healthy and pathological brain state there is because they're so easy to obtain but there's a big downside to EEG and what we're recording is considered this macro scale signal and it's still very difficult to infer how are these signals generated by the underlying neurons and neural networks within the brain and this network level understanding is really critical if we want to know why these signals correlate with information processing or if we want to be able to target treatments when they're disrupted in neuro pathologies with either pharmacology or other therapies now at the same time over the past century we've also had a revolution in our ability to study signals within the brain with electrophysiological imaging and genetic tools in animal models and sometimes in humans as you saw so we're now having this unprecedented level of cellular and network information and again these signals are correlated with nearly all healthy and pathological neural dynamics but what we need is a translator that can connect these macroscale signals that we can record outside of the head and these vast data streams we're collecting in humans with the underlying network level information that's creating these signals now this is the perfect job for our technique called computational neural modeling this is where we simulate the electrical activity of the neurons on the computer and we can have specificity both at this cellular and network level and at this macro scale recording level and this has been the primary focus of my research program it again builds from a long history of science back in the 1950s Alan Hodgkin and Andrew Huxley established that you could write the electrical activity of a neuron with a mathematical equation now back then they had to solve these equations by hand and so you could only study really reduced representations of neural circuitry now we have computational power that Alexus simulate large-scale networks and large portions of the brain and so what we've been doing is we've been taking advantage of this computational power in developing a large-scale network model that we've turned into a user-friendly software tool for clinicians and researchers to start to develop and test hypotheses on the cellular and network origin of their data so to give you a sense of how this oh we're calling it human neocortical neuro solver which is quite a mouthful but that's because the underlying neural model that word simulating represents the outer layers of the brain called the neocortex and it's these outer layers that contribute to these EEG signals that we can record outside of the head but I'm just going to refer to it as HNN for simplicity so to give you a sense of how this is used in practice I'm gonna describe an application of HNN where it was applied by Tom canet in a group at Mass General Hospital to interpret abnormal brain dynamics and children that had autism spectrum disorder as compared to typically developing children and what tall and her group did is they had these children come in and they gave them a twenty five Hertz vibrotactile stimulus to the middle and index finger and they recorded their brain activity and what I'm showing you here is they applied what's called an inverse solution technique to the data to isolate the contribution to the signal from the hand representation in the brain it's in an area called the primary somatosensory cortex or s1 and so what I'm showing you here is the electrical activity from this hand area of the brain and what they saw is that this is during this twenty five Hertz vibrotactile stimulation during the steady state period there were some subtle differences and the typically-developing versus autistic children they performed another analysis where they looked in the frequency domain the frequency domain measures how fast these signals oscillate in time and here they saw clear differences in these two groups such that the autistic subjects had a stronger response at the stimulation frequency in a weaker response at this higher frequency around 50 Hertz and they had a hypothesis about well what might be the difference in the brain circuits of the children with autism compared to these typically developing children based on prior research and what they hypothesized was well perhaps in these children that have these tactile sensitivities there's an increase in the communication from the periphery into this part of the brain we call that a feed-forward pathway and there's a decrease in communication within the brain through the pathways that we call feedback pathways and so what they wanted to do is to take our model simulate these differences in the neural network and see could this simulation account for the differences that they saw in their data and so I'm going to briefly walk you through how you do that in the model so here again is their data when you start the HNN software this is the graphical user interface that appears on your screen and what's shown here is a skin schematic representation of the underlying model sorry was that me that was you guys it represents a patch of cortex and so we're simulating this little patch of cortex in the brain it also has cellular and network level information including the local connectivity and that these pathways of information flow into the local part of cortex through these feed-forward and feedback projection pathways and so what they could do is they could simulate this vibrotactile input by giving activation through these feet forward and feedback pathways which I'm showing you right here is an envelope of the activation curves that drive this local network and this is the responding electrical signal that the computer calculates and they can compare that to their data and what they did was they tuned the model all the parameters are in the model are tunable they tuned it so that they have an accurate representation of the typically-developing data and then they wanted to test their hypothesis if they increase the strength of the feed-forward input and decrease the strength of the feedback input will that accurately account for the differences they're seeing their data and so they go into our model and they change the strength of the feed-forward input and make it effectively stronger and they change the strength of the feedback and make it weaker and then re simulate the electrical output of this network and what they saw was they did see similar differe but in particular when they went into the frequency domain they found that this was able to accurately reproduce these signature differences that they saw lending support to their hypothesis that what's happening in these autistic children is perhaps there's an increase in the feed-forward and decrease in the feedback now because this is a model you can go in and change any parameters you want and so they could test an alternative hypotheses perhaps and these autistic children there's a degradation of the communication across the brain and so these signals don't come in very clearly but they're more dispersed and so we can simulate them to come in more dispersed and here's the envelope of the input and you can see now that it's more broad and dispersed now the electrical signature is very different and when they look in the frequency domain it looks nothing like the data that they collected and so they can rule out this hypothesis and so in this way you can generate all sorts of families of responses and compare them to your data to see which is the best fit to account for the differences in the brain of these autistic children now on one thing I'm not showing you is you can also go in and look at the circuit and cellular level detail so you can look at the spiking activity of the cells and this gives you a way to now test your hypotheses with things that you can record inside of the brain with animal models and so it's this by directional hypothesis development and testing tool now we've applied the model of HNN to study other signals including lots of healthy brain signals we looked at mechanisms of threshold level sensory detection we've used it to study the origin of spontaneous oscillations in the brain and changes in these oscillations with attention and aging most recently we've applied it to study the impact of non-invasive brain stimulation we can give electrical current stimulation to the brain and study what is the corresponding result on the neural dynamics now our goal is to make this globally accessible so we've made it freely available with an online distribution it's an open source software that's available on this website HNN brown dot e-d-u this is the homepage of our website and you can see this various you items including installation instructions for various platforms we have tutorials on how to simulate some of the most commonly measured signals and under the hood manual that describes the underlying network and some of the theory behind the development of the model as well as a user form the applications of this model have been going on for over a decade and I show you some examples of that but it's a brand new software it came out earlier this summer we've had a lot of interest but we're eager to get the community involved and to apply it to weave vast data streams that are out there so with that I want to thank you and thank my collaborators this is developed by a multi-institutional team two people in particular in my lab samne morton and dylan daniels have been essential to the software developments in the website developments thank you our third speaker is dima am so associate professor of cognitive linguistic and psychological sciences and her work combines measures of behavior genetics and brain activity to study attention and memory in infants and children okay thank you all so much and thank you to the organizers of this wonderful event so I'll start by saying that the work I'm going to show is an example of the types of methodological the type of methodological progress that happens when artificial intelligence meets developmental science so this is Thomas era and his team and this is these are some of the members of my team and it's really the collaborative work between the two groups that I'm going to tell you about so I study human brain and cognitive development I study the most rapidly changing machine in the universe which is that human infant to childhood transition and you know my goal is to understand not just the the development of the human brain and behavior but also all the environmental variables that shape this developmental course and how that changes across the lifespan the the bottleneck that the issue that we have in developmental science is that human behavior is really complex and it's very it seems very noisy and very very difficult to capture and it at the same time it's include the the way the human infant develops into the child is incredibly elegant and it's very systematic but capturing that it has been very elusive and rather than sort of continue to tell you how how systematic it is but also how complicated it is to capture I'm going to show you some videos [Music] so this is a this is a newborn obviously about a month old not quite a newborn and you might think yeah there's not much going on over there but actually if you play closer attention there's a tremendous amount of hand-eye coordination that could be coded and accounted for mathematically that can be quantified there's a lot of tongue protrusion that's naturally not all that random and there's a lot of symmetry in the tongue protrusion there's a lot of eye movement slash slash hitting that you see with the hand-eye coordination and there's symmetry in the arm movements and the legs and the reason that I know that is that I've had brown undergrads code and recode these data as part of my developmental lab and they found a lot of wonderful patterns in that so not just a few months later things start to change you can still see evidence of hand-eye coordination but now it's changed pretty dramatically trunk protrusion is still a behavior symmetry and the leg and arm movements is slightly different there is now babbling and he looks up he engages with the person taking the video he engages socially and not too long after that all the same variables that I've mentioned get paired with speech and social engagement that's substantially more sophisticated so now he's capable of an incredibly difficult coordination task at the same time engaging with a caregiver and moving through space and this is all happening in a matter of months and this is him this past weekend paying it forward developmentally using all that amazing learning that hard-earned learning prowess to teach another baby so the complexity and richness of this developmental change is really not again paralleled in the human lifetime in this first year two years three years and it's striking and it's rapid and in fact we've been really great as a developmental science and neuroscience community at understanding say how memory changes how motor skill changes but we haven't been really great at understanding how all of these variables come together to form early patterns of change that might be predictive of subsequent and again that's because so much is happening so rapidly in so many systems at the same time so what if we could quantify this developmental course what if we could capture that along both behavioral and neural dimensions and what if we do that over time instead of testing the child once what if we capture the complexity of human development over and over and over again what if we sample longitudinally so here I'm showing you for example this is hypothetical data but what if we could do this where we've got motor development attentional development social-emotional development eye movements memory social engagement I can make cortical development I can make a list that's endless and what if we could track this over time in a way that's quantitatively unbiased and in a way that is sort of observant of the complexity of human developmental change are there patterns in this course that we could discover very early on that had we understood them better would have predicted risk is there something here in this very early period that would have predicted the differences that might be observed by the time the child is 10 or 11 and knowing that could we have done something slightly different in order to sort of steer that developmental course perhaps even preventive action very early on so how do we do that this is like as I said it's been elusive and so I'm talking to my colleague tomas sarah who i've already mentioned is my collaborator on this work and he has been sort of working in artificial intelligence computer vision myths machine learning deep network analysis and he says well look the the DNN the deep neural network analysis of video data has been revolutionary in a variety of different domains in our society you know this is this is the this is your GPS being able to track different like you know stimuli in in in the in the visual field this is what goes on on your phone with facial identification you can play games this way so what's happening is that there has been this optimization of video data that has made it possible for areas like driving to take advantage of being able to automatically code different object stimuli movements in the environment what if we could do something like this with capturing the complexity of human development and that's where idea for the that's where the idea for the smart playroom came from so the smart play room is right next door it's just a play space it looks like a preschool or a classroom it's not meant to be something that children come into and feel overwhelmed by it's meant to be quite simplistic but it's but it's different in that it's wired with a variety of different cameras that allow us to do some of the analyses that I just showed you we're changing many other domains before so it's wired with so this this baby is wearing an eye tracker so you'll see little targets in the next few videos those moving targets are exactly where children are looking as they move through the space that's they're literally their change in their eye movements we also put connects cameras actually these are Xbox cameras that can help build these 3d representations of the body that can help us track the body through space and also just track the the trajectory of the child as they perform different tasks at the same time objects in the room are are coded for their identity for their features for their color and for their location in an automatic fashion the video feeds that we take while children play or engage with a caregiver or do tasks or do memory or attention or motor skill or bounce balls whatever it is that we're interested in these video feeds are paired with the tools of computer vision which is a great application of machine learning it requires a great deal of early annotation hand annotation hand coding but then ultimately the end of the annotated data are used to develop algorithms that make sense of the image that learn to code in an automatic unbiased rapid fashion per child per frame what's going on in that scene so here's a here's a child playing with one of our experimenters and what you can see here is that what we're coding for is is she happy is she sad or ha eyes open as her left eye closes a right eye closed and this is happening as she is interacting we can do these for this for both individuals and actually looked at social contingency over time and it the automatic fashion we can look at attention and learning and their development in this way so that little target is the child's eye track as they scan the scene yeah this is a visual search task it's a visual attention and learning and memory task so we can study learning and memory in a very ecologically valid naturalistic way we show them an object to look for just as an example and the child goes searching as they do that there's an automatic coding of all the stimuli as well as whether the child looked at it so that we understand how are they building a memory of their spatial layout how does that differ across children across individuals across age and then do they use that those fixations that they looked at those stimuli do they use that as a memory subsequently when they go searching for those stimuli so that's just when we can look at attention in memory we can do much more sophisticated things by say building models of the visual scene that children are looking at so this is a model of literally just the amount of color flicker intensity orientation and motion information what that means is that a lot of children and especially developmentally children their visual systems don't look like yours in mine so I'm looking out at you and some of you are wearing bright colors that's where I'm going to go that's because my visual feature detection of color may be particularly sensitive children have differences in that and that developmental change they actually drive looking patterns that are quite different so we can model that and then we can look and see what they actually use in the visual environment we can also look at motor development so what I'm showing you here in just a moment she's bouncing a ball but in a minute we're gonna see her skeleton and what that allows us to do is is not only track where she's going because we can do that in an automatic way but we can also track is she you know is there a motion for bouncing that we can follow what does her gait look like so in cases where development maybe atypical with respect to motor patterns we can track that and detect something subtle perhaps not available to the naked eye very early on and I think what's really critical is that this can all happen at the same time we're not just looking at memory we're not just looking at motor development we're looking at the entire whole complex child and so you can see that we have these different feeds and different views of the child this is paired now with cortical data so we actually have them wear a new device a wireless near-infrared spectroscopy device as they navigate this space so that we can also get cortical activation data on topics of interest as they do it we can get heartrate as they do this we can get galvanic skin response if they're getting anxious or not we can pick that up with changes in sweat basically and so this is something that we find incredibly exciting and a really really interesting way to start thinking about capturing the complexity and elegance systematically of human brain and cognitive development so here the particular excitement is that we're getting the precision of computerized data collection but with the ecological validity of naturalistic observation we're getting measures that are pan cultural that are not specific to a race or a class this is child's play this is children changing from their own baselines over time allows us the opportunity to think about putting a smart playroom in Guatemala or a smart playroom when a Syrian refugee camp and actually being able to do the work in a way that we feel is truly faithful to the to the premise of child development for its own sake not things that we think they should be doing or we should be doing and ultimately I think the most exciting thing is that this allows for discovery I don't know what I don't know but if I can look at patterns and existing data and use those patterns to discover something about early human development I think that's making a lot of progress and so with that I'll say thank you [Applause] Bheema a one-bar interesting to talk in with this large amount of data basically it's not a high polish dreaming experiment or you're trying to discover one in the last point of your slide know but it's such a such a rich data and you don't know where to start no Aditya has happened in many places of sign I want people to go to the to the wined a where to start what are your thoughts about how I going to mine this data and it's so amazing so so large a great question so we are taking both a hypothesis driven approach so we do actually have very specific questions and an approach specifically that allows discovery the hypothesis driven approach is often in a particular age group are the the discovery approach really allows us to as we're collecting these data classify use classifier analyses to look if I can bring in a baby at one month and then up six and then at nine and then at twelve and I bring in hundreds of children and then I call them back and I say did anything is there anything going on and we learned that a particular child developed a particular problem we can go back and look and see whether there was a pattern very early on so it is discovery but it's discovery right now at this stage where we have some handle on what the outcomes look like in the future if we learn that there are specific patterns and children that say might have severe delays with motor development then that's something that when we notice that in the future is something that we can identify very early on and discuss with the families thank you anybody else so my question listening says there's a question where sir can you can you go up to the microphone this is for documentation all your projects are in some sense small data projects in the sense that the number of subjects is relatively small and I just wondered whether you would each talk a little bit about what your thinking is if one had big data in the sense of large numbers of subjects that are quite varied so I mean even in professor Harrison's case I wondered how much variation there was in even those brain signatures for a given action but imagine that one had a sample of a thousand subjects there's bound to be a lot of variation I just wonder how you all think about that and whether you think that would make things enormous ly more difficult or it would rather confirm the patterns you think you see with small amounts of data yeah that's a great question so in the in the brain computer interface technology the you know when they put an electrode in the brain they they know sort of generally what region they're putting it in but they have no idea sort of exactly what neurons the sensors are going to pick up it's not even clear that that makes sense because every human's brain is likely very different particularly at the level of individual neurons and even from you know day-to-day that sensor even though it's very tiny is very large relative to a brain cell and it might wiggle around or move or maybe the brain changes so this calibration that you saw where the sort of algorithm have to figure out what the brain is this correlation between brain and behavior or brain and intention that has to happen again and again and I think so there's definitely I don't think that there's the possibility that with let's say you know thousands of BrainGate patients that you're going to ever get to the point where you can just you know pop in a sensor and based on all the other subjects it will know automatically you know what this person is thinking there's going to have to be some sort of calibration phase but I do think that big data and the in the way that you're talking about sort of many subjects does have the potential to improve this technology because there's likely lots of statistical regularities even though everybody's brain is different the that can sort of be they can facilitate this process of calibration for instance understanding how variable neurons are are just trying to identify what are the features that that really matter when you're looking at this brain data and mathematically that might show up in things like choosing the widths or the shapes of different kernels you know that you somehow you can you can move learn something from different patients to improve one but right now the sort of technology is not there I will also say that this does feel like big data even though there's only one subject right because you know a single participant is generating you know roughly a terabyte a day and so there's a lot to get through and mine so yes thank you for the question it's a great question I'll say a couple things in regard to a EEG one is because it's so easy to record and because we now have open science there's a tremendous amount of data out there that's available for everybody to look at and to start to analyze and the other thing about EEG which is often viewed as a downside but I see it as a plus side is that it's called this macro scale signal in like large and Samba 'ls of neurons need to be doing the same exact thing in order to record it outside of the head now because of that there are limited signals that we record outside of the head we typically see things like ERPs event-related potentials or low-frequency office oscillations and very stereotypical frequency bands and so there's a lot of consistency across subjects we see similar patterns of activity in a subject that comes in one day or for weeks later and so that gives us really some targeted points that we have confidence that we'll see even in a very large data set some commonalities and we can apply these artificial intelligence algorithms to them and classify one of the different states the other thing is that it's really dictated by the structure of the neocortex this outer layer of the brain which has very characteristic features throughout the brain and across brain areas and across species and so that also limits the space of signals that we have to work with outside of the head because there's only so much you get from the structure of the cortex nonetheless is very meaningful information as you saw in the autistic example that there are vast differences in these disease states and across attentional states and such we are actually moving towards and we wanted to develop the space and are in sort of now testing in the hundreds but ultimately we would like to be testing in the thousands to get at that individual variability we think it's important it's not noise it's it's signal hi thank you for your presentations in terms of professor Harrison's models it seemed like the the the deep neural net or whatever algorithm you're using it had to know that it was deciding which direction to move the cursor with the smart playroom it seemed like yet very specific features that you were trying to track such as right eye movement left eye movement where the focus of the child was so I'm wondering where does the contextual subject expertise end and where does the machine learning begin so in the in the video that I showed where the person was playing the little dot game the the yeah so let me start over so the the type of machine learning that we were using their needs it would be for those of you who know familiar with this it would be called supervised it needs brain data movement pairs in order to build up that mapping between brain data and and movement this would be the training data and of course the person is paralyzed they can't move and so the the sort of clever innovation that BrainGate sort of came up with is to just ask them to pretend to play a game and and just hope that they're trying to move the cursor in the direction and so that gives you this sort of second variable the movement intention and so so you collect those pairs in a supervised way during this sort of calibration phases you're basically collecting training data and now the video that you saw the it was simultaneously collecting training data and improving its prediction using sort of Bayesian techniques so that you can sort of incorporate data on the fly but eventually the training period ends and then the dictionary is fixed and the algorithm is just always predicting what the next movement is and it's not really collecting new training data although they're thinking about ways to to do that sort of looking back in the data and saying oh this person went to the letter a and that's probably what they wanted to do and so we could get more more training data I don't know if that does that answer your question or yes so the question was if that chip was put into a different part of the brain would the model know that it's not looking at the right part of the you know something different and the the the answer is it depends if it was still in this particular you know the the algorithm that I showed if it was still in the calibration phase it would begin collecting new dictionary data and if it collected enough of it it would sort of discount what it had learned before if it was already in the let's I'm gonna use it phase and you just took the chip out and moved it just wouldn't work you know it would it would be seeing brain data it would think it was coming from you know it meant one thing it's like switching from English to German or something they just wouldn't understand what to do now it's possible that the the participant is also a learning machine that perhaps they could eventually adapt and be able to use this technology maybe but it would take it would be much longer and and sort of much more you know they wouldn't be thinking I think so intuitively about about moving and maybe it wouldn't even work at all is that a better answer yeah is the son with the smart play room right now we the the distinction where machine we're sort of the human experimenter ends and where machine learning begins is we still are identifying the behaviors that then get hand annotated in the video feed and then those hand annotations are used to train the the machine learning algorithms to be able to detect those patterns in now completely novel data sets so that you can do that in a way that's sort of quantitative and unbiased and across a variety of different patterns and behaviors anybody else so I guess if we'd have no more questions we can move on to the reception portion of the evening is that what you're thinking okay okay thank you very much [Music] [Applause] 