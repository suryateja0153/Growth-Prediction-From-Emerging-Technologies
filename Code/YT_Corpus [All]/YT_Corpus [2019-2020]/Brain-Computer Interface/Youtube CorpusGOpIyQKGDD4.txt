 you [Music] all right hello everyone it's my great pleasure to introduce Winkle who was with us here this summer to do a BCI project on multi-sensory attention wink was a PhD student at Carnegie Mellon and he will talk about brain computer interfaces today all right Thank You highness so my name is Rinku everyone knows me so I've been spending three months on this project the effort of decoding multi-sensory attention from electroencephalography or in short EEG and the try to think of how to apply that information into a brain-computer interface so today's agenda is I will start with some introduction of the background like what is a EEG or is a brain computer interface or PCI in short and I will go to the literature review and also my research objectives and I will talk about my experiment my equipment and then how I process and analyze data and finally sort some results and with ended with conclusion and future works so eg is a measurement of electrical signal and where does that signal come from is from the neurons so we have millions or to billions of neurons there were brain and the way that they communicate is so between each a pair of neurons they have is seen that and they use some chemical signal at that smaller gap so whenever there is signal coming to the synapse they will release some neurotransmitter and that signal will be accumulated at the receiving end of the second neuron and thus this thing that will carry on and for long distance communication between the neurons or neural networks the the modality of the signal is actually it actually got a signal so that's why we can measure something like show signal along the scalp where we are having some brain activities so in order to measure some electrical signal from the brain the most the easiest way is to open it up so you asked a neurosurgeon to do a surgery on you you have this craniotomy autonomy and you place a sensor there directly onto your brain and there are some electric electrode arrays so they can measure the potential change on your scalp on your cortex so with this method that we call it ACOG electro cortical Grassi so we have very good signal quality so it's very high signal-to-noise ratio and also because is electrical signal so the temporal resolution is pretty high like could be a few thousands of course and since we know where we put the sensors the spatial resolution is we also very high this is workable only if we agree that we want to open it up but I guess most of us don't want so an alternative way is to measure the electrical signal along your scalp so we put electrodes into these because eg catch and we place them on our scalp just like a swimming cap and we put gel into either of those electrodes so this method is not invasive we don't need surgery to start with the temporal resolution is the same as the previous one because they are both electrical signals however if you if you think about that so in order for the signal to transmit from deeper in the brain to the scalp it has to go through couple layers of tissue brain tissues and also the scalp and the skull so all of them form form a low-pass filter so the spatial resolution is not great is in centimeter level or a few centimeters and also all of these tissues add extra noise to the signal there could be some endogenous like circulation or some function controlling our breath so which means the signal-to-noise ratio for EG is pretty low or depending on what kind of senior you are looking for for the one that I was working with as even lower than 0tp which means the same now is actually a weaker than the noise so what we do usually for for for a neuroscience study is to meet thinking about these low signal to noise ratio single trial doesn't mean anything to neuroscience to study because we may hardly see anything a single trial level the way we do neuroscience with EEG is to collect multiple trials of a single condition and take the average of them with the assumption that the any brain signal corresponding to this stimuli is actually time locked so if we take the average of other trials we have then probably the noise will cancel out each other as the signal itself will stand out and an alternative way of for collecting eg data as you can imagine so the whole brain is a conductor and if we can measure the signal here probably we can also measure signal here so an alternative way is to place some in your electrodes with conductive materials contacting your ear canal and here probably some signal which has me from the brain to here and could be captured by this in your EEG electrode and there are a lot of advantages of using that for example it is unobtrusive it's just like an ear plug and also every time you put it in is at the same spot comparing to the surface EEG I mean depending on how you place the cap the the the electrode position may actually change from time to time but for this why it's very consistent across sessions however the signal to noise ratio is even lower in this case and since we only have a pair of years so there are only two sensors we can use compared to the previous option for the surface you can have 24 64 or even 2 56 channels so we lose those special information we're using in in their eg lectures so how does it related how is that related to BCI so brain computer interface by definition is a communication or control system that allows real-time interaction between our brain and any device we have so here the keyboard is real-time which means we have to operate our single trial level it's not collecting multiple files and take the average and there are already several applications of BCI systems so for example the e cog could be used in surgery like neurosurgery for my from medical purposes and also it can monitor I mean EEG can monitor our stress level or emotion change over time and our focus is that this interactive component of the BCI we want to communicate with any device we have we're going to translate what we think into an action or a message so they already have some applications in this field of study for example to use your mind to control a wheelchair to go forward or turn or turn or there are some interesting studies to use eg to play a game to control the soldier to move in the field so these are interesting previous BCI projects that we already have and in terms of what kind of paradigm we are using for e for PCI here is a summary of the existing wise so the most popular two are using motor imagery and some external stimuli stimulation so motor imagery means you you try to imagine the movement of your body parts for example you sit here or stand here without really moving you think about your removing your hand and that will generate some new signal or some some power decrease in me ban which is around 12 Hertz hate to tell hers and that could be captured by the EEG and we can use it to to control to control system and the performance of this one is pretty good so at the last column there is a information transfer rate in the unity of beats per minute it means how much information you can transfer to the system within one minute and for this motor image every one it could be as high as 20 bits per minute however the weakness of this motor imagery paradigm is in these weeks or even months of training before you can really use it just think about it if I ask you to pay attention to your left hand and try to imagine it so it's moving how hard it can do that I mean how faithful you think you are really thinking about it instead of just you know red it's just some with some random thoughts or whatever so it really takes efforts to in order to use this one which makes it a little painful to to be applied another one is to consider it's also in under the category of motor imagery so where people try to imagine the continuous movement of one single body part for example you can you think about removing your hand and that information it could be interpreted by the BCI could be used for cursor control for example you can think about you are you're moving your mouse in your mind and the cursor could move according to your to your mind however the performance of this paradigm is only good in ink in ACOG in eg sync know that there's a very poor decoding and we don't have actual some some actual ITR information available for EEG and for the paradigm that we are using external stimuli a huge amount of work actually focus on using visual stimuli and the reason is because our cortex is sensitive to visual input so the visual response is actually much stronger than the others and and we can use for example steady state visual evoked potential where we present some flickering objects and if you stare we can play several flickering objects in the field and you and take a flicker at different frequencies and you can stare at one of them and that kind of information the frequency information could be shown at the back of your brain so we can place some sensor there and try to express extract that frequency information to decide which while you're paying attention to we can also use some some paradigm called visual p300 where we we already know that there are some certain type of brain signal corresponding to the presence of some infrequent events for example if I give you a string of green objects and all of a sudden give you a red one the response corresponding to that red one is an infrequent event and that waveform could be captured to use in BCI applications and the good that the advantage of using video attention is the the performance is pretty good like would be comparable to the motor imagery and we don't need training so you can you've asked you to stare at one point you don't need extra training to do that however there might be some fatigue due to the flickering if you stare at one point it's just flashes and you'll be easily tired after a while and also if you want to use your visual world to do some other tasks in this kind of visual based a paradigm you're not able to do that so there are also some work corpang into these other sensory for example auditory Archer is used using some auditory stimuli and here people apply similar concepts as in the steady-state visual evoked potential they they pay attention to a pure tone with certain frequency and just by attending to that frequency your brain responds at that particular modulation frequency will be increased and we can detect that in the EEG signal however this is not very reliable and is not common commonly used and based on the previous works the average or even the best result could be one point five which is one twentieth of what the other modality could offer and the same goes for the using tactile like vibration at the stimuli the performance is also very low however in this project my focus is on the last two so yes so I have to think about some clever ways to to make it more engaging or to make it easier for the subject to and also easier for the BCI system to decode so let's see what happened so as I said the one of my research objectives is to use audio or tactile stimuli to be the BCI system that is functional and also since for a lot of the paradigms that I introduced they are using steady states responses which means you play a certain for example audit auditory stimuli at certain frequency which is pure tone and you just passively sit there try to pay attention to that I think maybe there are some better ways for example if you can integrate is some interactive bade task based paradigm to the BCI system and see how the engagement that the increased engagement of subjects could help the BCI to decode our signal and the lastly it's if the first one actually works I have to compare whether if there's any difference we're using audio or tactile stimuli so here are some literature review the first part is about decoding auditory attention so when we are paying a attention to different auditory objects in the in the world we can use different cues for example we can use if we know where the sound is we can use our special attention I can narrow down our attention will spend to that narrow a direction and you have to do that we so our brain elicited some alpha power and if we are paying attention to our left side the ipsilateral which is the left side of your brain will have some increased power increase alpha power and the same I mean and when you are paying attention to to the right the right side alpha power will also be increased and here's some unpublished work of my thesis project and this is the average of like 30 subjects and over 40 trials precondition to have a multiple condition which is that is the average of a huge number of data that I had and that would give us very beautiful pattern in that but on single trial level it is not very faithful and here is some functional MRI work so we can see deeper in the brain and we know exactly where the effect is and actually when you are paying a special attention the parietal lobe of your of your brain as well as some sensory motor this is actually pre central sulcus it's a little bit in front of your central your centerline here will be activated when you are paying a special attention and also for the steady-state response there was also a work which is which nicely shows this increase this is a attention modulation in an assr so we are paying attention to for example in this case it's 37 Hertz pure tone the the the 37 compose the 37 Hertz components in your eg signal will be will be higher than when you are not paying attention to that and the same for the you are paying attention to 43 Hertz however this is also an average of 10 subjects over like several minutes of recordings so on the other hand if we use tactile the vibration as the stimuli what will happen so there was a study when they applied a vibration I had fingers of the subjects out there they were holding this vibrator in their hand and they were and the vibrators were vibrating at different frequencies and they were instructed to pay attention to either left or right and the result shows that when you are paying attention to for example 21 Hertz in this case the the the power of 24 heures at a particular electrode will go up and the same for if when you are paying attention to the 18 Hertz one and the effect shows it is mostly located at the parietal lobe and also the frontal lobe and again this is the result which is the average of like 16 second trial over 40 trials so in summary all of these evidences in the EEG signal are actually very weak single trial level we can see those effects on average level but if we can want to do some real time stuff using each one of them alone may not give us some very good results and that could possibly explain why the in the information transfer rate is very low for these two types of studies and so with those in mind I want to introduce my work or my idea of how we can improve it so in most of the previous studies we ask the subject to sit there listen to the pure tone and they there are two different frequencies are being played yeah hmm you try to pay attention to that that's the only task that they have to do I feel like well at least for me because I've been working out easy for a while I've been subject for others that is multiple times and if I'm the subjects sitting there it's very easy for me to lose attention like after maybe the first second or or so I can focus on the sound but after two seconds my attention is wandering around this jumping back and forth between the two stimuli so I feel like there must be some better ways that we can engage our subjects for example if we can embed some tasks in the in the whole experiment we will require a response from our subject maybe they are they are pressured to pay more attention than and maybe we can use that to decode whether they are paying attention to left or right and also in order to in that a task we have to have better stimuli design for example if I just play a pure tone there's no task there's no way we can embed a task in that pure tone so we need to think about a way to design our stimuli better so that you can carry some more complex stimuli features and you may also be easy for our subjects to focus and so here is my design in general so I will use modulated signal because it has been shown to be a key feature for a steady-state audio and the tactile studies and I used modulated signal to create a stream of musical notes or vibrations pauses so and I can change the carrier frequency in the middle of this string so that I can form some pattern of the Optimates for example in Audis it's just like maladies like to to to to to to to to so you can have some more complex feature in the acoustics and we can specialize the sound and and the vibration and for example I can play different sound through two different earphones and also play different vibrations on your two wrist and in terms of the interact did the interactive component of this whole experiment I asked the subjective focus on one stream and later I asked sepia to respond with what what is a pattern of that attended stream so this is general idea how I engage the subjects with sort of game-like design so here is a brief introduction of what stimuli I use so I use modulated signal and for the left stream is always with the modulation frequency at 37 Hurst and the camera actually may change depending on it may change in the middle of the stream and the left stream is always is always longer I mean in terms of each it should note so the left stream is always 400 milliseconds and the right one is always with higher modulation frequency and with a shorter length so they may sell something like this so this is the left string when the pattern of the melody is going up and stays there and for the other stream so it's shorter so it feels quicker and it has lower carrier frequency and it the module at the carrier frequency actually goes up and comes back so it's a zigzag pattern and I can play them together [Music] and the instruction is to pay attention to either left of that the left string or the right string and try to figure out the pattern of that particular attendance string so without spatialization is really hard made for someone but with specialization is actually a pretty easy job as an analogous to the audio stimuli we also have tactile stimuli which has the exact temporal feature so the length of the left stream and the right are always matching with audio stimuli and the difference would be the modulation frequency and the carrier frequency and here since our our hand or our wrist is not very sensitive to the carrier frequency when they are going up or down we may feel a change in the carrier frequency so the task is not to identify an up-or-down pattern here is just a switch which means it goes changes and stays there or it changes and comes back so and we can combine the two together when we are playing the audio and also playing the vibration together and we can synchronize the onset of those notes and the vibration pulses and the subject would choose to pay attention to more to the sound or more to the vibration or together to both of them and the initial idea is that the vibration may help us to focus better because well there there's a complexity in our nerves that I mean neural nerves and also the tactile ones they may fuse at some higher level however according to our feedbacks is not a case during the experiment because people may be distracted by the tactile vibration and unfortunately it's just like a divided attention or the selective attention and for the overall of the experiment we have three different modalities - auditory or long tactile alone or next and we have three different conditions so a 10 left turn right or no attention so we can do some 3 bit classification and we have 24 hours per condition and the order of those are randomized and for each trial we have eight seconds which I give as a total of around half an hour or even more so for each trial in the very beginning I present a visual cue to tell people which way to pay attention so it could be a left-right or no attention and after one second those cue disappear on the screen and becomes a dot and the subjects to focus on that dot and then up to half second as start to play the sound or vibration or both both of them and here is the period when the subject pay attention try to identify the pattern of the stream and after half second of the offset of the stimuli the subject was instructed to give an answer by using the keyboard and up another half second we will present a feedback being a green dot which means you've got it correct or it's a red one means you've got it wrong so you know what the answer of the pattern that you are paying attention to so they have two strings going on and they have different patterns so you kill the subject pay attention on the live yeah so in the very beginning and then you are yet you ask him or her to tell you that he paid attention of the leg no the the pattern of the stream happening on the left it could be going up or down or zigzag so it's just a way of measuring how about how well they are paying attention to the string or not and so there are some possible neural signatures we can find in the EGC No so for example as I said if there are if the subjects were using special attention we make special try to log alpha power increase and if they are paying attention to the stimuli itself for example sound there may be some steady-state auditory response and the for the tactile one if they are paying more attention to the the vibration there will be some somatosensory steady-state response NTV if they pay attention more to the timing of the onset of the out ball there may be some p300 so we don't control for that we give the freedom to the subject to do whatever they want using whatever strategy they want to focus better on destory and I kind of hope that this could make it much easier for them to do the task and also may give us better responses in the decoding so the EEG measurement system that we are using is Ambreen chain smarty and it has 24 channels which is also jailed based so we place gel every time we used that system and the december rate was 500 Hertz and in the mean time we also use these in-ear electrodes which was made in-house by our harbor team and we put conductive cloth and their tips so they directly contact our our ear canal and the temporal rate was to 250 Hertz and in terms of the vibration we use vibro types of actuator which is in essence is just like audio speaker but it has a lot of power so you can feel the vibration when you are using it and some are smiling because the the is kind of hard for the subject to feel the difference in the vibration and it could be shown later in the behavior results and this is the overall set up and so we collected data from 12 subjects and and we use the 24 Channel EEG system with any reductions and we play vibrations through this this actuator which we use a bandage to attach to the wrist and in this we can free the subject from using their hand when they want to do some tasks which may have better application usage when you we want to move on to the real life BCI application so we got the data how did we analyze it we got the raw EEG and we do some simple bandpass filter and we EPOC those continuous EEG signals into different fragments using the onset of this stimuli so we give this 4.1 second window and chop them out and use it as a feature that we want for further analysis and we reject several epochs based on their maximum value because sometimes the the signal may not be very reliable if they may go off charts so we can remove those from further analysis and for the EEG for surface EEG we use at the independent component analysis or ICA which is the standard Center way of using of removing eye blinks from the EEG signal and after ICA we got this pre-processed EEG which is supposed to be clean and then we calculate the special power of of each segment with using a three-second window with 90 percent overlapping and that becomes the feature that we use to train and test this LD a model for classification and for classification we use a three-way three-way and because we have three different conditions and we use with the the classification for each modalities individually here I'm using is 8 fold cross validation with a thousand times so I first divide the whole 12 we have the the whole data set we have into 8 fold and use seven eight of them to test to train the model and use the other one for testing and after 1000 times we it can give us a trial level classification result so results what do we have so as I said this is the behavioral performance and each column represents each modality and each line represents a single each subject and we can see obviously the tactile task is really hard so everyone got some lower than the other two results for the behavioral so it's really hard for them to identify the pattern in a tactile maybe because this is this the race is not very sensitive to those frequencies so it's really hard to tell and yeah so for the EEG when we are trying to decode attention we we got these are very interesting results so first of all the same layout here so each column represents each modality and each dot and inch line represents each subject and we can see the average of the three modality is actually comparable and a lot of subjects actually so here since we are using a three way classification that the absolute chance for is one third the however since we suffer from this small sample problem we cannot claim that our decoding is above chance if it's a higher than one third we don't have infinite number of samples to support that there was a paper to tell us how to calculate the significant chance floor which is a function of how many classes we have and how many samples we have and for our particular study there's significant transfer or is 43.1% which is the dashed line here so we can see there might be three or four subjects whose easy data is not really class available and it has to be consistent across different modalities however we got some amazing results for some subjects for example for this red one it's above 80 for all three modalities and it is a 3-way classification so the the information transfer rate could be as high as close to more than 15 bits per minute which is actually comparable to other like visual based paradigms even and here is the results by modality and I can also arrange the results by subject and sorted them based on their maximum decoding and for the the ones at at at the bottom you know it's very hard to catch if I whether they're paying attention to the left right or no attention and it seems to be consistent across different modalities and for those who could score high in in what one of the modalities the classification score is supposed to be also high for the other two so we have this within subject consistency in terms of what kind of modality you are using what are these we can go to what is the issue later but for the noise level there's a way to to to control for the noise by measuring the impedance of the electrodes before we start to study the whole kremlin so we place droughts in until we we guarantee that every single sensor is actually working and the impedance is below 10 kilohertz to tank it all so in that sense the noise should be comparable yeah ok so and some may wonder does the behavioral results like how good we can perform in that in those tasks correlate with classification results I thought about that too but it's really not the case so for example for some acid here's a plot of each dot is an individual subject and the axis is their behavioral accuracy in terms of how they can how many trials they can get it right out of the all the traffic they have and the y axis is the classification which is how well they catch that the model is working it's not linear and it might be nonlinear correlation but it is really hard to tell and he's ready for example in tactile at this particular behavioral accuracy we have very contrasting classification like Y is even close to chance and the other is 90% so it's really hard to make any sense out of this kind of correlation so I would say the relationship between these two are actually very weak and it's a quick summary we found that actually the the tactile attention part is really very hard compared to the others and the attend the attention decoding tends to be tend to vary a lot across different subjects but within subject they are very consistent across different modalities so if you do well in tactile you also do well in the sound or in in the mixture of those and I said as I said the relationship between the behavioral and the classification is not very predictable and I also wonder if the subjects are really paying attention throughout the whole trial because we have a four second task so I want to eliminate the possibility that some subject may lose attention over time and yeah so here's the result and pull and I am showing the average of all the samples at at the same time window that I use so basically I did decoding at each time point and average all the sample from that time point together and show this trace so this is the two-second window after the onset of this Damini it turns out the decoding across time is kind of stable within subject we don't see a very clear job maybe in particular subjects yes the decoding tends to drop after a while but it is not the case for most of the subjects so I guess this is a kind of a measurement of how or evaluation of how our subjects is paying attention or get engaged during the whole trial and another interesting question would be so if I come here today I do the task I have derived this model if I come tomorrow we a week later can I use the same model on the new data to classify if I'm attending to left or right so this is the sort of robustness across time and we asked a few subjects to come back again after one week and to do the same task with random a different order of the trials and here I'm showing this particular subject who did whose EEG is very cat available and this is the results for training a testing the same session and here this the same for the second session and the last column is showing the results when I trained on one session a test on the other and vice versa so it turns out for this subject at least so using a different day I mean using different days data to train the model could also predict pretty well the second day's data a vice versa and here's the result for another subject and it is it's a cat available because everything is above chance but not as good as 80% and the same thing happened again in the second session and if you use the cross session data to to validate we found well it has to stay at roughly the same level did there's a job in the mixture or in mix modalities probably the subject was using different strategies when he or she was doing a task and here is another subject who's eg data it was not very class available in the first session and he wasn't very class available again for the second one and the cross session validation also shows like it's barely classifiable and what does it tell us so if I put them together you will see so if someone's data is classifiable in one session is tend to be attend it pretends to stay at the same after a week and which means if the model we did arrive at one session it could be used for some subject if they their data is already as available and it is not the case if they are not so here's the summary of the this section so the decoding accuracy tends to be stable throughout the whole trial so the peep already paying attention throughout the whole four seconds and the the model that we trained with in one session could be used in a multi-day visit and also the performance the classifier is kind of consistent across multi-day within each subject and next I'm interested to know what kind of features contributes to that classification so are they really physiological relevant features or they're just simply some noise or artifacts so here I'm using a neighborhood component analysis to calculate the feature contribution to the classification so this is basically a feature selection algorithm and I can pull I can take an average of all the features in alphab and for this particular subject and render it on this hat model and as we expect it the parietal alpha is contributing more to the classification which follows our previous expectation and we can do the same for cross day validation so here is the results for this particular subject and I put multi-session multi-day session side by side for comparison and I extract features from the modulation frequency of the audio signal and the monitoring frequency of the tactile sing vibration and also up the alpha band that we were looking for and we can see the white distribution across different sessions are roughly comparable for example in the alpha band is always the parietal region and for the audio that could be some in the frontal lobe and some in the temporal lobe and for the tactile it's also some where as we expected if we compare this that went to this previous study this is overall the results of multiple subjects and we can see that actually a single trial level for the future selection so it gives us some evidence to believe that what we are classifying with is actually the same physical near a physical neuro neurological relevant features and so not just artifacts or other noise and the future weights are kind of similar across multiple a multi-day sessions which means the the the set of features we get from the first day session could be also be used in the second day so that was the results for the surface EEG we also measured in there EEG and here is the pre-processing and analysis pipeline so the only difference between this and the surface EEG is the missing of the ICA a component because we only have two channels here we did the same kind of analysis and here's the result for the classification it's much lower than what this surface eg is offering so a lot of the classifications are actually around chance floor but a very interesting observation is the classification for textile is actually seemingly better than the other two and I can pull the results for each subject together and this is same ordering as we did before if you can remember in the previous plot this is for the surface EEG the ones on the left is very high in classification in this plot it seems to be the opposite like the one the data that that was not as available for the surface EEG tends to be more cat available in energy that we can take a subtraction which between the two and we can see actually some subject some subjects in there EG outperforms their surface EEG this is a very interesting observation and it could be partially explained by how the how our brain actually foes so there are a lot of drivers and suckers in our brain and the structure of our brain defines whenever there is a source in the brain in which direction the dipole is is pointing just imagine there's a battery there in your brain and it could so you could point two different directions depending on how your brain is folded so maybe in some subjects that dipole or that battery is pointing to somewhere parallel to a surface so the signal other surface is not actually great for identifying which way you're she's paying attention but that may point directly to that in there in your electrons so the capture means that information could be captured by the in ear elections of course this is just one speculation we need of course needed further NASA's due to confirm it so the summary for this section yes the the classification results for in your EEG tends to be lower than that of the surface for most of the subjects and we saw this negatively correlated trend between in ear and surface ages so what we learn from this project so first of all to integrate auditory and and tactile stimuli into a more interactive task face paradigm is actually workable at least as shown in our data and the results of our in terms of the information transfer rate is actually compared may become comparable to some of the visual based systems as much better than what what was previously done so the performance of the proposed BCI system as I said may outperform some previous studies and is comparable across different sensory modalities so it could be similar when you are using a tactile or using a audio stimuli and the performance actually varies a lot across subjects and there could be explained by some anatomical structures in different subjects and the performance of BCI system tends to be robust over time which is that the results of the cross-validation over multi-day a visit and we found this interesting negative correlation between energy and surface eg so in the future we can do a lot of things to to investigate what's going on and how we can improve this BCI system so we can do some feature with a dimensionality reduction by a features selection and we can use near-net in fact actually we already tried to use neural net with already existing structure try to do the classification and but the result is varying across different subjects for some subjects there's a huge increase in the decoding accuracy but for some is just like slightly or even lower than the simple Lda so we need more time to work on that end and we can try to integrate spatial information into classification because we have this nicely distributed eg say 24 channel EEG so maybe the special information of each sensor could tell us more and we can work on the individual differences to explain what's going on for each different subject and of course you know in my paradigm I'm out only using some simple modulated signal there might be some better test work or even game designs to to better the stimuli so that's also where we can work on I want to thank all of you guys Hani's for the other help and then I received especially our signal processing and you are very nice and the cameraman for thank you for being there and I want to thank a Casey who might form and not on the cam now so he joined the team in the middle of the project but he contributed a lot in providing very insightful ideas about what we should do what we shouldn't the BCI project and Nick and Hakim we work out the BCI together and we collected data and all together and for Nick we even share ubereats together so it's very very nice experience and we want to just thank you van for giving me the opportunity to share to study to learn and to contribute and David especially Dimitra they helped me with hardware or software aspects of my project a lot and also organizing the whole event like in Bainbridge which is very memorable and I want to thank our Harbor teams Becky and John SK helped me a lot in building the whole vibration system and also to testing differences eg systems and Teresa Todd and Patrick don't you build a lot to the heart to the blood mechanical part of mice project and the other Christian Andy add me high and Mike in the in the BCI team and Mike helped me to figure out what kind of vibro tactile actuator I should use I want to thank all the other interns our team Raymond tansy and Ben Fabian Sahar and I Lisa real Iranian so for all the support I can't say what kind of support by the office of pork and Dickie for taking me home every night January home so and the special thanks to my wife Nancy who is in Boston along with my daughter I can't do anything without her support so thank you very much and about how many questions remain [Laughter] I just realized it's three cause hmm because when I was doing the experiment I was treating a certain class that were right when you said no attention I was just trying left right something else mm-hmm [Laughter] actually if it is randomized like you didn't pick one side all the time it should be fine for this kind of stuff actually we should not discuss that information to be honest better yeah so just left yeah like in here so this is a binary classification between each pair of the conditions left or right and well the results are comparable to three-way I think in general this is 60 something which might even better than the three-way I have but this is a binary so a binary 60 is not comparable to 60 what strategies do you have you know what strategy good thought we didn't do that though maybe yeah I think you can correlate yes yes that could be a good way to tip yeah let's go deeper around individual differences yeah have you tried to fuse surface and linear electrodes oh and use the data together not ready because I personally trusted surface eg more because we have a good control of the that the noise as I said we have this impedance measurement for in year we also have one but it's not very reliable all the time so if I fuse them there would be like different noise level for two different yeah kind of signal so I I was hesitating about doing that but sure because we've seen those comp pattern so maybe that's the thing that was being captured better in one than the other yep so probably experience on if so see people have given different sorts of tasks so the less people to do decorative items are they it's also well what we were saying like people are using even another time attention task and then should it vary between the sound did not have taken those tests thank you very neat game mmm but for that I'm not so sure actually because in our study we control we were only using the tactile and I'll do for this particular sort of task so there's no guarantee that it could be translated into other games as well but if the the whole theory about PCI literacy like how your brain folks and how the signal is being presented at the surface if that that that theory is actually valid maybe for a single subject if his signal is classifiable in one task it's also it tends to be more classifiable you know that task so this is my my I do yeah are you aware of any work with some of these should be called attention from audio or tactile where there is no spatial separation good question yes so there are some studies showing that the the the sound is actually not specialized but they are different in pitch that there's a high pitch or low pitch and in my teachers work actually we are working on male and female speakers so the details a different pitch as well and I don't they don't do another the result yet but there could be some differences in that yeah they're a major change in terms of I hope not because I remember my kids it was much less conductive it like we saw the penis was very high so I'm surprised that you train on one day and on the other it was at all well and it was working [Laughter] so yeah we I mean we tried our best so on a general note I think one satisfying result from this whole thing was that even when certain if they interesting to be at least interesting result that the behavioral performance didn't predict the classification means that even if you felt that you couldn't do the task at all like contact that one was was quite difficult for many people even if you thought you couldn't do it at all because you're paying so hard paying so much attention to trying to do it the signal is actually pretty strong sage and I should pick up when you focus on the left so the classification tasks the rising falling versus the zigzag thing that was there just to give subjects something to about you to to do yes yeah yeah and and the frequency that the frequencies of interest were different enough that if you didn't sort of do that you would have seen that distinction anyway well if we don't ask them to respond is that what you mean well no you would have but you would have seen the SS yeah if they truly pay attention I think so okay think so but to ask them to answer the question is another way to guarantee that they are ready it might be when you think that frequency is for the the secondary note and that visitor harmonic of the other of the first the vibration or for the sound both for both there's no I chose prime numbers so there shouldn't be any so when you when you go to the second note and that sequence in significantly different that's an unrelated frequency modulation that's that's a margin they say modulation yeah yeah that's the one you're actually yeah just to - yes the fact that even people who not do the task very well could still have like a specialization SSSP you'd be there but what about the opposites are you also just so if there were people who could do the task normally they should be a good pacification in there so I the opposite makes sense but is it also the case like people well the task they also perform well well yeah in the in the keyboard so in the behavior result way back there so here we have some people who whose eg is classifiable we're high but they couldn't do the test for a while we also have who could is very sorry was the opposite yeah like very well here but I mean the thing that also kind of vehicle pretty well so there's one I get to your question there's one for any classification is around chance that the last dot on the right yeah okay it's a round chance even though they're actual task performance was 90 yes I think in the fact that we have dots at each corner gives us all the combinations we we can speculate so yeah yeah that brings back to the idea of the the strategy I was suggesting so maybe some people like found good strategies to make the difference between implied right and left but still it's not like a eg classified area because the strategy with what they modulate their attention that what would make the eg classified moon that's right apologies if you give you broke this out already but that you show what happens when we have the inner electrode and the tactile condition just by itself that this one the interaction protects our approach the a little bit above channels for most of the subjects trying to square this with Nikolas of his talk where he was struggling to get these informants in the air electrode Mike can you compare your results with is yeah we can do that offline but I think I guess I guess one one difference is the task the task really helps to focus your attention so it seems that the results they are now that when when when you force people or you give people something to latch on to it it also makes their signals from or more consistent I should have done the correlation with the behavior as well so here right yeah which oh yeah so out of all the sensory that you have a recap if I I asked you can you give me five of them what's best for some alone fix is that someone else yeah in the feature selection part these information could tell us which one contributes more to the classification for example for for sure we need some sensor at the price level or maybe also somewhat some some sensors at a temporal or they can your temporal lobe so yeah this might also be something that we could potentially look at when we train a neural network see the neural network figures out combinations of senses that maximize performance in my beard and actually the network that we used a part of the idea was that it has a very simple structure so you can theoretically trace back to do exactly that and when you need the ica for processing means oh did you look at what those components were ready to I did I so what was like the main component you like have an interpretation of it for example for some subjects who has very strong alpha power it tends to have some component in the parietal lobe okay and some for example with a stronger ERP it should be somewhere in the center I saw those components causal you mentioned them in the in fear and troubles you could've ica have enough sensors so do we have something similar for the cap don't do I see a does a performance drop and certainly to be close enough to the in-ear good question so I started with not using ICA for the surface EEG so I but since the eye blinks is very strong in the age if not in the frontal channels I have to remove the front of sensors from the classification and that performance was it is lower than adding the ICA pack just like around like six to seven percent difference and a suggestion of maybe you should also try someone to replace I see a PCA because in which one is to separate the sources that maybe take the ones that most of the information like organ by both like you do the sausage you take only those that are make sense even for it for your study yeah yeah sounds no way I mean well I didn't do very complicated prepossessing so it just shows whatever is a the city of the artist all right [Applause] you 