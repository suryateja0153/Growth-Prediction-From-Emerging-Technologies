 Hello, my name is Tobias Hollerer from UCSB's computer science department. Thank you for having me here today. I'd like to talk to you about the world as computer interface, how will humans stay in control? In computer science, my discipline, is an exciting, challenging, and accelerating field that impacts the world every day in countless ways. Computer scientists are involved in creating technology, systems that are useful in a wide range of industries including medicine, communications, manufacturing, entertainment, business, and last but not least, science. Computer science leads to new technologies that can change the world, such as the personal computer, the internet, cell phones, social media, much more. Also new discoveries in science and engineering, new possibilities for social science and humanities, and creative collaborations with the arts. But since the early days of computer science, there has always been the warning and danger, computers will take over, take our jobs, replace us as humans. What I would like to argue today and make a brief overview for is the question of how humans can stay in control. One way that I really believe in is, but that's pretty much true for every field, is that we need to entice the young creative minds so they are interested in these technologies to really do the right things for the field in the future. I think we also need to take a step back and see what do we expect from technology and how can we achieve it? My own area is human computer interaction as a sub-field of computer science, so how we use computers, how we solve problems with computers. I run a lab called the Four Eyes Laboratory, where the four eyes stand for the four I's in imaging, interaction, and innovative interfaces, but also for the goggles that we have to wear, still for our work in virtual and augmented reality, which is one of the areas that the lab is interested in. The lab was founded with Professor Matthew Turk in the early 2000s and now we have a range of great faculty in the area of visual computing that are affiliated with the lab. We cover a wide range of technologies in the lab, all geared towards what human computer interaction is the future of using computers. In order to do this, let's look at the last two success stories in technology adoption, each of these paradigms was really successful in terms of user adoption. I'm personally a kid of the personal computing era where computers actually made it into our households for the first time, then we had the worldwide web in the mid-1990s forward, and social computing from 2000 on, cloud computing from about 2006 on, and the mobile revolution from about 2007 onwards which was when the first iPhone came out. You may ask what is next on this because 2007 is already 13 years away? As my take on it, I think there could be an era of reality computing where instead of carrying your smartphone around, you actually have a personal mobile wearable computer that overlays computer graphics or computer information on top of the physical world via augmented reality, which is one of the research areas that my lab is interested in. But really for the sake of this talk, it is secondary what this technology is that would come next as a paradigm. I happen to believe and work towards this particular technology, mobile augmented reality, but others are possible. You'll see that there is a lot of lip service for augmented reality at the moment, for example, Tim Cook, Apple's CEO, sees AR as really profound. He says it's going to change everything, AR has the ability to amplify human performance instead of isolating humans. He is a huge believer in AR, they put a lot of energy on AR, they're moving very fast, he said that in an interview. You see that pretty much all the big tech players are investing heavily in this technology. I just want to show you just to highlight a few possible application areas. One is the industrial or commercial application possibility, for example, for maintenance, planning, and remote communication. A PhD student of mine Steffen Gauglitz together with Professor Matthew Turk, actually kicked off a startup company, Caugnate, a few years back and they got to the technology for remote AR maintenance annotations to productization within two years. These are examples from current exciting start-up companies such as spatial here, for example. In education, which is obviously dear to my heart as a professor here, there's lots of application possibilities, the technology is not quite there yet for full deployment in everyday classwork but you have things like being able to actually see constellations in the nighttime sky directly overlaid in your vision, one really good way to learn about astronomy or the stars constellations. You have lots of other opportunities like that. In telemedicine, really important topic right now, augmented reality can help a lot. Then it already has created a followership for Social Media and Communication. If you look at this example here at the bottom, which is actually from a 5G commercial from Deutsche Telekom, it can be used for communication, for translation purposes. Because that case didn't actually showcase augmented reality, you can use the camera of a mobile device to actually overlay and change written signs and materials very easily with augmented reality as well. I will come back to this application area later in this talk. I just want to show you a few examples that just show what I mean by augmented reality. Here's my student Yun Sun Chen a few years back and he's drawing annotations on top of a printer. This was filmed by a second head worn display, he's wearing a Microsoft HoloLens, and you see how this device, which is a completely self sufficient wearable computer, can overlay and even allow interaction with computer graphics that you use to annotate the physical world. You can do that even from a distance. Here's an application of follow-up work that we did where we drew arrows to physical objects in a wider environment. If you walk up to these boxes that I annotate here from a distance, you will see that these arrows point directly to the box I wanted to annotate, so in this case, a maintenance worker or a security worker could actually highlight things that have to be checked in a large factory hall and then could check it off so that their colleagues would not need to go to that particular spot again. This also works outdoors, here's a demonstration where we have two HoloLenses, actually collaborate together and sighting down, getting basically an adequate model of a particular point, in this case, the lamp that you see there in the background. Through triangulation, you can actually pinpoint with an interactive interface where that is. Just a few examples. I want to say that in spite of all these possibilities, there is a danger that technology can take over. Take the case of us relying on maps, very heavily on smartphones. I think we all love being able to navigate in unknown environments, but over-reliance on mapping software can actually make us lose spatial skills that has been shown. I'm interested in the question, could some smart technology maybe make us better at spatial reasoning while they give us these prosthesis help? As another example, artificial intelligence today already makes lots of decisions for us. Like in some cases, even hiring decisions for companies, firing decisions, legal decisions, and there's clearly dangers of algorithmic biases, of nuances of human contexts being lost. Here my take would be AI could inform and improve human understanding and decision-making, but the human could actually stay in control. Here I'm coming back to the translation example. Translation services are great, they can help us communicate. That's already technology that allows us to have life translation of two people speaking in two different languages while they speak, but now think about a blind reliance on some of these technologies will actually prevent us from ever learning the language itself. Again, I'm interested in the question, can technology actually help us learn a language? Maybe while starting with translation, while you don't know it yet? It would make us better as a human because we are keeping skills, so we're learning skills that are still there even when the technology receipts and it's not available anymore. This is a particular project that goes dead route. This is my PhD student Brad and Wayne and others. The application example is basically one of vocabulary learning in your natural environment in your home. Some people actually without technology, would put a post-it notes onto objects and then label them with the foreign language that they want to learn, and then when they look at them, they actually get a reminder of what that object is called in the foreign language. This fastest act of foreign language vocabulary learning, and you can do that very nicely with augmented reality because you don't have to clutter the physical environment, you can overlay it directly via your augmented reality headset and you can also now personalize it. So maybe the system can even predict or know by the way you're looking at an object, if you already know the word in the foreign language or not. We did the first study to gauge the possibilities of such an approach. We basically had learning of vocabulary in the Basque language, language that nobody in our user group here at USSP knew before. We went through this exercise and showed it in augmented reality associated with physical objects compared to a normal flashcard app, a common way to learn vocabulary. We saw seven percent improvement on same-day tests for the memorization of these words. What is even more promising, four days after this inviting the same people back, they would have a 21 percent improvement on the delayed test, so remembering the words better in the AI case. To take this further, we belt built a system that goes towards automating this process. We would need to recognize objects in real life, in real time and in order to be able to tell if a person knows this word or not, we would use eye-tracking and maybe EEG equipment that was like brain blood flow measurements. Here's an early example this was actually an undergraduate project by JB [inaudible] in 2017, where he took a convolutional neural network that would recognize objects. It's called YOLO, You only look once. It coupled it with the HoloLens. Now the HoloLens, seeing a normal physical environment, would get its views sent over to a machine-learning server. There you have the identification of the objects, get sends back to the HoloLens, and you see object recognition in your life. Sometimes it goes wrong as you saw before, the chair was not exactly categorized correctly, but you get the idea, it works pretty well. But this doesn't only on a 2D basis, frame by frame, and we want to have 3D understanding of the physical world. So with my students and colleagues, that was the next step to actually aggregate these 2D observations into 3D object detection that annotates the physical, world. Now we can have some objects, most objects recognized it in real time. We can use that now for the augmented reality language learning application. While we're working on that, we're also experimenting with how if you have such an always on augmented reality device, would you switch back and forth between different applications. That is what this particular continuation of that project is concerned with. This project used a little bit of machine learning, and so I want to talk a little bit about the potential and dangers of machine learning. I'm not a researcher in Machine Learning and I will only recap some of the results in this field from my perspective as a human-computer interaction researcher. A lot of high profile, really well working examples and applications of machine learning. If you haven't seen it, you can go to web pages such as this person does not exist to get lifelike rendering of a person that is not a photograph, but the assembled from lots and tons and tons of other observations and photos of human beings. We all know the success story of AlphaGo first time that the best player in the world in goal was actually beaten by a computer. Just happened in the late 1990s. We have the example of automated driving, self-driving cars which use a lot of different technologies in the area of machine learning, RNNs, CNNs, reinforcement learning and others. But we don't really understand it and so here I'm recapping results that came from the field of adversarial machine learning. Good Fellow and colleagues in 2015 showed us really compelling example that if you know the machine learning algorithm and you perturb the results or the input to your algorithm just a tiny bit, so you take this picture of a panda and with the algorithm, it gets recognized as a panda successfully at about 58 percent confidence and you perturb it just a tiny bit. What happens here is the least possible representable unit gets added on to this input image, so that to humanize it looks exactly the same. 0.007 times this noisy image got added on to this and this was specially crafted knowing the algorithm and suddenly the panda is recognized to 99.3 percent confidence as a gibbon. For your reference, these are givens and while they are also fuzzy, they look quite a bit different than pandas. The thing that got added on the noise pattern got classified as this nematodes was probably just the closest thing to noise in the set of images used for training. This shows us it's really not clear what exactly was learned because we could shift what the algorithm says an image is by some addition that's even invisible to human eyes. That's very dangerous because now you could hack real life systems if you just know what was used to train them. I called and colleagues sent, 18 followed up on this work and they did these additions to street signs and they actually were very successful in fooling the algorithm to believe that the sign was something else. For example, a stop sign was recognized as a 45 miles per hour speed limit sign and right turn sign was actually turned into a stop sign. This can be really dangerous and they were just a few stickers or additions in simulation, they got to 100 percent wrong classification, so it's very scary actually. You could just say fine, machine learning algorithms are not perfect they'll get better over time. What's the big deal? But as long as an attacker knows or can reverse engineer the learning algorithm in use, they can now successfully sabotage it. As long as we don't fully understand why some machine-learning works, we can't shield against it. Also in some other areas, our trust in automation often goes too far as you can see here [BACKGROUND]. What a missed opportunity that is, this is great technology. The technology to actually drive a car autonomously seems to work fine. Nothing happen in this case but it is still very early in this development of these technologies and we just saw how easily such algorithms can be fooled. What happened here is that this technology was talked publicly as automated driving while it was internally or in the manual just categorized as adaptive cruise control plus some better features. The irony and the sad thing about this is that this exact same great technology, if it were just differently rolled out it could be in use now for improving people's driving instead of endangering other people with reckless driving. Because you could use the same technology to actually tell you if you're doing something wrong as a driver or drive more carefully or drive more defensively. It could be used for bettering the human performance. We're very interested in this topic and so keeping humans in control but being informed by the technology seems to write path. My student, James Schafer did all dissertation on this topic and we looked at how people actually take advice from artificial intelligence in more detail. Question is, do people take advice from machines? In order to test this, we crafted a range of scenarios. One of them was this online trust game, the dynasty dilemma which is basically for those of you who know it, N person, multiple person, prisoner's dilemma. The scenario is that multiple dynasty eat out and agree to split the bill beforehand and they know that they will actually meet together multiple times in the future. Now they have a decision to make between, we simplified it A, cheap like an item with good cost quality ratio and one expensive item that is little bit less optimal in terms of cost, quality ratio. When eating alone, the best choice would be hotdog. In this case, if you eat with others and they decide to choose hotdog because you're splitting the bill, you might get better value by choosing the lobster by getting subsidized by your friends. However, you will dine out with this group in determinate number of times so you should not lose trust. In the end game, it is best for everyone to converge on a hotdog meal although it's little unrealistic but everyone ordering lobster would lead to the worst-case scenario for everyone. We have agent in here to give you advice of what you should do based on historic decisions. We varied some variables like explanation, automation and figured out if people actually would go by the advice of the artificial intelligence recommender system or not. Lessons learned from this work is, just because people say they trust the AI doesn't mean they actually follow the artificial intelligence advice. Persuasion is very critical. It's also the case which is very well known in psychology of human overconfidence. If you don't know much about a particular area, then you may not know enough to say that you don't know the material. You're not expert enough to gauge how much you actually know. Only when your competence rises and when you become actually a very expert, you suddenly realize how much you really know and only a true expert can say of themselves that they have high confidence of the expertise. As a danger and showing explanations to self-confident users, we found in that their situation awareness might be negatively impacted and so appealing to emotion may be the only avenue to accommodate the over-confident and change their minds and let them make the right decisions. As conclusion, as a big picture of what I would like you to take from this talk. In order to figure out how humans stay in control. We need to understand technology very well. We need to come from a very informed position. Because we, the people who actually designed the technology, the people who understand the technology, and also the safe keepers of technology, and need to make sure that it is usable for the rest of humanity. The same time. We need to also understand humans very well because we need to understand ourselves. We need to understand how people actually will be able to interact with new technologies. Will they take advice from these technologies while they learn new human capabilities from it? This needs to be taken, this knowledge. Knowledge about technology and knowledge about humans and combined with the goal of improving humanity. Not just building technology, that is their stand alone for solving certain problems. But we want to use technology so that we get additional benefits as humans that stay with us even when the technology goes away. Build technologies that intrinsically improve us as humans and if I got this point across to some of you who are starting in this field and are excited about the technologies, and you like built a few systems that try to achieve this, then I have more than achieved what I wanted to do here today. Thank you very much. 