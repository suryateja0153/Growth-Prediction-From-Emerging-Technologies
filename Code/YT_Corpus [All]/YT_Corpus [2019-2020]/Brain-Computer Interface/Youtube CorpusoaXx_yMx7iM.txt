 So I think many of us have had this nightmare scenario: You wake up and you're completely paralyzed, you can't even speak. Sadly, this is a reality for people with stroke, spinal cord injury, or degenerative disease. My name is Sergey Stavisky and I'm a neuroscientist trying to help people with paralysis by developing brain computer interfaces. This technology reads out patient's movement intentions directly from their brain. In the Neural Prosthetics Lab here at Stanford, we study how the brain controls movements. And we apply what we learn to build medical devices that bypass the injuries of people with tetraplegia, that is, people who cannot move their arms and legs. Our lab has already developed a system that lets our clinical trial participants type on a virtual keyboard by attempting to move and click a computer mouse, even though they can't actually move their hands. I've been working to extend this capability so that our participants can make more complicated movements with a robotic arm. That project has been going well. But about a year ago, I made a surprising discovery that has opened up a whole new line of research. For people who can neither speak nor move, the need to communicate is even more urgent than being able to reach and grasp. So, what if, rather than detecting patients' attempted hand movements so they can type, we instead directly figured out what they were trying to say? More specifically, I wondered if we could decode the neural commands that would normally move the muscles of the jaw, lips and tongue to produce speech? Our group certainly isn't the first to think of this, but we have something that few other research groups do. We are part of a clinical trial in which people with paralysis volunteer to have little chips with 200 tiny electrodes placed in their brain. Most previous studies of speaking have used coarser measurements, from electrodes placed on top of but not inside the brain. Recording from within the brain lets us observe the activity of individual neurons which I predicted would provide a more informative signal. Our participants can still speak, meaning that I could ask them to talk while I recorded their neural activity. The catch was that our electrodes are in the so-called arm and hand area of motor cortex, the part of the brain known to control arm movements, not speaking. It was a long shot to go looking for speech-related activity there. But as far as I could tell, no one had actually looked for it using the more detailed view that our implanted electrodes provide. When I went back to the lab after the very first day of this study, I was shocked to find that neurons in this area responded strongly during speaking. Not only that, but they responded differently depending on what was being said. This allowed me to train a machine learning algorithm to identify which of ten syllables was spoken with about 85 percent accuracy. I also found that several signature properties of how groups of neurons coordinate their activity during arm movements were also present during speaking. For me this was particularly exciting because it meant that my previous training in studying these neural ensemble dynamics might be directly applicable to this new problem that I'm now working on. There are really two angles to this story. The first is a scientific discovery. We found that a part of the brain that was previously thought to control the arm is also active during speaking. A big unanswered question is whether this is the result of brain remapping due to these people's tetraplegia or whether this is also the case in able-bodied people. At this point, we just don't know because there's no reason to make the same measurements in people who aren't paralyzed. Still, the fact that we can see this activity in our participants gives us an unprecedented window into studying the neural underpinnings of speaking -- which are otherwise notoriously really difficult to study because this is a uniquely human behavior. But this isn't just a scientific opportunity. It's also an opportunity to prototype medical interventions which could be used down the road to help patients who cannot speak. Here I've taken a step towards this by showing that these signals allow us to do a pretty good job of identifying what a person is saying. Now there's still a long road between this result and a fully functioning device that lets someone fluidly communicate using a chip in their brain. But it's encouraging that we could do as well as we did in what is kind of the wrong part of the brain, using just 200 electrodes. There are new technologies on the horizon that will hopefully allow us to put many thousands of electrodes into other brain areas that are more involved in speaking. Looking ahead, I'm excited to work both on developing this technological capability and on the clinical applications of restoring arm movements for people who cannot move their arms and synthesizing speech for people who cannot speak. 