 right thank you for the introduction my name is Anthony Chen I'm assistant professor from UCLA we're excited to talk about this research called minuet multimodal interaction with the pin kind of things is Robert's collaborated between several Institute's CMU UCLA Jie research Google and Apple so I want to start by telling you a story about beloved user user Richard so one day Richard arrives home named Woodstock so I picked up of several Amazon boxes in front of his house and because the starving wants to turn on the lies so it can go through the hallway so he says turn the light and minaret interprets his command and detect its location and he turns on the lies next to it Richard so he can safely walk into this house only to find that the cat has not done what the house pet so he has to command the room by putting at the Roomba at the corner of the living room it said trinkets Park so the Roomba follows the command and arrived at crime scene and clean the knock down knock over plant so Rita was tired and he wants to enjoy and relax a little bit so it points at the music player next to the TV and says play the music which plays his favorite Paley's of songs so it enjoys this music suddenly his phone rings and he's his boss so he's very nervous he picks up the phone in the meantime trying to weight down and the music player turns the volume down so cleanser it's no big deal that or somebody tries to reschedule the next day's early morning meeting so just show you a Nereo of how our user Richard interactive is minuet by a combination of voice command and gestures with inventive things so before I tell you what's under happening under hood we need to take a step back and look at this vision of Internet of Things or IOT Internet of Things it's becoming increasing your pictures your everyday living environment so these IOT devices they are usually spatially distributed in a home and workplaces which means that it needs to be some centralized ways to interact with them after manifests as different apps in our mobile phones however there is a problem of scalability this chart shows that in 2022 we'll have almost double the number of IOT devices from how to 2016 which means increasingly there'll be more more more and more cumbersome for user to install configure and retrieve an increasing number of apps that control this vast number of IOT devices so how can I do better well we all know that voice' systems are becoming increasingly popular when they kind of solve part of its problem allowing the users just talk to one device and which mean two faces many other IOT devices however there's still a problem let's say I want to turn off these two lights how can I tell my Alexa to do that left and right light so maybe some of you might thought I should have named these two lies maybe I don't know comment generic and then I can just tell their names and which is pretty much a state of the art you really need to have specific names or groups associated with your IOT devices if not you want to use voice to specify the spatial information such as turn out the light to the right and above the couch you can see it's very cumbersome and unnatural the problem is voice itself lacks spatial expressiveness so that's one of the problems you are facing right now a lack of expressive means to interact with IOT devices that are usually spatially distributed in our environment the immediate solution I can think of is well when we just point at those lights that seems to be a nice solution and indeed it has been explored in several previous papers using gestures in lieu of explicit Bates control over voice however there is still this problem of ambiguity look at this figure one of the scenario paper you can see sometimes IOT devices can be so densely distributed that pointing might not be able to nicely tease out which one exactly the user is pointing at where they are very close to each other so what can we do now we can turn to mountain model interaction which kind of combines the best part of both worlds voice and gesture it's quite promising especially look at earlier systems a while back put that their system or quick-set both of which nicely combined these modalities however mostly in virtual environments is still yet underexplored to a neighborhood kind of multimodal interaction with IOT devices in the real world so that's our goal long to explore the design space of mountain model interactions that enable expressive control of spatial issue reality devices so before I introduce you to this design space I'd like to take a look at our prototyping platform in order to enables exploring such interaction techniques we in neighbor with the proper age who chose to to build this sensing platform using ultra wideband sensors it's traditionally used to localize people or items in large spaces like warehouses or department stores we repurpose this kind of platform using several UWE anchors three or four anchors are sufficient to localize the using Department where a user would wear a UW tank as in our case as a race worn device the idea is that if we know what it uses at and we can add an additional sensor for example I am you to be happy which way uses pointing and then we infer what the users supporting it by pre-registering IOT devices in the environment so this video will show you how whoops our system works and from the visualization you can see you can track the user's location as well as the arrow and because we pre-register the three lems as brute arts we can infer which one the user is pointing at and then in combination with a microphone we can combine pointing and voice command which together allows us to enable and explore this large design space for interaction techniques so I'm gonna walk through each of these design sub space one by one but first we can looking at the two dimensions you can see that or weapons during at least two two steps first insulating a IOT device and next specified interaction with the IOT device by just opposing the two modalities voice and gesture along these two steps neighbors to create a large design space the first area of interaction techniques is pretty much similar to the existing approaches of voice assistance were voice selects IOT device and then voice proceeds to interact with it the people in our system can make is there we can add contextual awareness so as an example example show you earlier which is able to use his location in Priestley with your echo system to turn the lights only best to him or near it so for example in this figure the lights will be only turn on in the kitchen but not in the family room next to the kitchen for example we will show you another example [Music] so very simple idea just adding location wellness to the command voice commands and then using gesture as a means to interact with IOT can further the neighborhood's tooth for example to specify a continuous value so as opposed to a discrete body about au uses continuously specify the value of the body and now if we combine voice and gesture together at the interaction step who can afford richer ways of carrying how IOT devices should Harper's accomplish tasks such as the Roomba cleaning a specific spot okay so we arrive at the second row of our design space so what can we do with gesture selects and voice and tracks because in our system every user is whirring a separate we spend with the WWE Tag IMU and microphone we can use this Cara sticks to achieve some sort of use identification so example with a presenter point to a projector which then displays one of these light acts and then as an audience member raises his hand as a question the presenter can point to that specific audience member and the two devices can establish some sort of handshake agreement allowing the audience member to have temporary access to navigate to a specific slide where he wants to ask a question but yes just select and gesture interacts previously we mentioned that there is this ambiguity problem so letting additional disambiguation gestures we can try to mitigate this kind of ambiguity so the example of the the user leaving the lab but only wants to turn off his light not his lab mates who do working on his project so in this end be rationed gesture can further swipe left or right to specify which one of ambiguous lights the user was to turn off and as you can see in this video another world way to disputation is just to add an additional voice command for example turning off the left one now finally where we can again combine force and gesture and interaction step we can further specify for example as user waving down his hand whether he wants to lower the volume or the brightness of the TV so this allows user has finer control of exactly how they want to interact and control an IOT device we perform a technical evaluation to see how where our system can tracks and sense uses location and gesture we constructed a simulated environment trying to make a living room out of our lab space ten participants interacting with five appliances on this four by five meter grid we measure localization accuracy of poi 33 meters and only one false positives in a total of 45 minutes of non interaction periods and something that we found is quite interesting is the pointing accuracy is not so much determined by the the sensors but is rather human limitation so imagine average users pointing it's off 9 degree by the of the target by 90 degree which means that we can use this value as a heuristic to guide our ambiguity and set selection so the idea is we'll use 90 degree as a threshold to first narrow down a set of ambiguous IOT devices the user might be selected and then we further ask you to within a small set of devices to disambiguate and pinpoint exactly which one they want to interact with we also get a some quarter feedback first of all it's not surprising to see that uses about pointing and devised in that way this natural and intuitive most users commented that minuet compliments distinct voice assistance allowing them to perform easily object classification without having to man or group of each single IOT device something that's a little bit surprising what's that the first order of consideration by the participant is not so much the expressiveness we thought about but rather the appropriateness of modalities participant for the minuet provides them with more interactional options when it comes to interacting ILP for example one commenting that compared with unless I don't have to talk if I don't want to but still being able to control the device so by having me aware as a complimentary system the user can choose which modality the thing is the most appropriate at a specific context the samurais are presented minuet a marking mother interactive systems for intend of things by leveraging the Utah V localizing and system in together with ayam you sensors and a microphone we can enable a large design space of interaction techniques allow users to to use a combination of voice and gesture to interact with IOT devices special distributing our environment with a I want to conclude my talk and happy to take any questions we we use an IMU sensor an amuse in the hands of in the nation of accelerometer gyroscope and compass right mm-hmm that's correct so in our system the microphone is only on when we detect the onset of a parting gesture and you had to be honest is not exactly at the onset is one or two seconds there is a 100 seconds of buffer so we only process the user's command one or two seconds before the onset of a gesture so this would allow a user maybe to say the command a little bit earlier prior to point it because we don't expect the user to point and then exactly at the same time or after that to say the command no our onset jocheonsu detection is very robust only one false positive over 45 periods of non interactive activity no it's fun on set is this motion of pointing at a specific object in environment the questions it's like three genes and then there's the recognition I think one effective way is to show immediate feedback we thought about maybe adding an LED module to one of each IOT devices or adding an laser pointer at the users wristband which can provide a feedback loop guiding the user how to adjust the point but there's this concern of in the first solution there is an instrumentation cost and in the second solution there is this may be privacy appropriate is you don't want to shoot the laser pointer all the time never want to try with something so this is something who is I still exploring my future work your future work [Music] 