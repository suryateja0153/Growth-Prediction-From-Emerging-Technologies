 Let’s imagine we look at a scene through a window. If we look through the window from different angles, we see different sides of the scene, which gives us the impression that what we’re looking at is three-dimensional. But what is it that we’re actually seeing? What happens is that light reflects of the scene, goes through the window, and then propagates further to reach our eyes. It is this field that reaches our eyes that ultimately determines what we’re seeing. So if we remove the scene, but manage to reproduce the exact same light field in the window plane that the scene would have produced, then we still see the exact same scene, and we get the same impression of three-dimensionality that we would otherwise also have. So by reproducing the light field in the window plane, we can reconstruct a three dimensional scene. But what do we mean with ‘the light field’? Let’s for simplicity assume that the light is monochromatic, so it consists of a single color. Then to describe the light field in the window plane, we need to specify for each point in the plane how much light there is, and in what direction that light is traveling. The way we describe the amount and direction of the light depends on what sort of light we’re using. If we’re using incoherent, or natural light, then we can describe the field using the ray model. At each point the rays indicate the propagation direction of the light at that point. If we’re using coherent light, such as laser light, we have to describe the field using the wave model. The direction in which the light at a certain point is propagating depends on the orientation of the wave fronts. Normal light detectors, such as cameras, only detect the amount of light that reaches each pixel. By looking at a photograph, you can see which pixels captured a lot of light, and which pixels captured little light. However, a regular photograph or a regular tv- or computer screen doesn’t reproduce the direction in which the light at each pixel was traveling. That’s why normal photographs or displays always look two-dimensional. So how can we control the direction of light at a certain point? One way to do it is to put a small microlens at each point. If we put a point source somewhere in the focal plane, then we can control the direction of the outgoing light by controlling the position of the point source. If we’re using coherent light such as laser light, there’s another way to control the direction of light, namely by using diffraction gratings. If we send in light through such a grating, multiple diffraction orders will be produced. The direction of the zeroth order is the same as the direction of the incoming light, but the first order and minus first order are diffracted by a certain angle. This diffraction angle depends on the period, or pitch, of the grating. If the pitch becomes smaller, then the diffraction angle becomes larger. So by locally varying the pitch of the grating, we can control the direction of coherent light. So now we know how to control the direction of light, but how can we detect the direction of light? To do that, we can consider the same methods that we used to control the direction of light, but reverse the direction of propagation. If we send light through a microlens, then the location of the spot changes depending on the angle at which we send the light through. So we can detect the direction of the light by looking at the position of the focal spot. In the case of coherent light, we can detect the angle between two beams of light by having them interfere with each other. The smaller the angle between the two beams, the larger the pitch of the interference pattern. By reproducing the light field we can reproduce three-dimensional scenes: if we look at the field we see the same thing as if the scene were actually there. When using natural light, the light field can be reproduced using light field displays. When using coherent light, the light field can be reproduced using holograms. If we record information of the full light field digitally, then we can use this information to calculate how the field propagates to different planes. This means for example that we can digitally refocus a photograph after it has been taken, or correct for the aberrations in the imaging lenses. If we use coherent light, we can create images without using lenses: if we measure the full field some distance away from a sample, we can computationally backpropagate the field to the sample plane to see what the sample looked like. Now let’s look in more detail at how light field imaging works. Suppose we have three objects at different positions. We use a single lens to create an inverted image of this scene. In a regular camera, we have a simple detector consisting of multiple pixels that each detect the light intensity at that point. For the object that is in focus, all rays emitted by a single object point will converge at a single point in the detector plane. Objects that are closer will be focused behind the detector plane, and objects that are farther away will be focused in front of the detector plane, so these objects will be out of focus. If an object is out of focus, the light intensity will spread out over adjacent pixels, while an object that is in focus has a sharp image. A regular detector only detects the intensity and color per pixel, but not the direction in which the light is propagating. To detect the direction of the light, we put a microlens in front of each pixel, and subdivide each pixel into subpixels. Now let’s focus on one macropixel. The microlens creates an inverted image of the field at the lens plane. The red light that goes through the microlens comes from the bottom half of the imaging lens, while the yellow light comes from the middle part of the lens. So from this measurement we can infer the direction of the light rays that go through the microlens. We can consider a single subpixel, and find its corresponding location on the imaging lens. We now know that the light captured by that subpixel went through that point of the imaging lens, and it went through the microlens of the particular macropixel we’re considering. So we can draw a line between these two points, and from that we know the direction in which the ray of light was traveling. We can repeat this procedure for other subpixels, and this way we can reconstruct the entire light field, consisting of rays of different colors propagating in different directions at different points on the detector. Once we have measured this information, we can extend the rays to calculate what the image would have looked like if we had put the detector at a different distance. We can digitally move the detector back and forth to put different objects in focus, or even slightly tilt the detection plane. In a similar way that we detect light fields, we can also generate light fields. In a regular display, each pixel radiates more or less equally in all directions. If we divide each pixel in subpixels and put a microlens in front of it, we can locally control the direction of the light by lighting up different subpixels. Now let’s turn to holography, where the direction of coherent light is recorded and reproduced. If an object is illuminated with coherent light, then the field in the plane of the window can be described by a complex-valued function U. To characterize the field, we need to know the entire complex-valued function, which consists of the amplitude (the amount of light at each point), and the phase (whose gradient indicates the direction of propagation). However, a conventional detector only measures the intensity of light, which is the square of the amplitude. How do we measure the phase information? The answer lies in interference. The squared modulus of a pure phase function will contain no information about the phase. However, if we add another field with which it can interfere, then the intensity does contain phase information. The first variant of holography is called in-line holography, which was proposed by Dennis Gabor in the 1940’s, and for which he was awarded the Nobel Prize in 1971. In this method, we have an incident field, that is perturbed by a weakly scattering object. It is the interference between the unscattered field and the scattered field that yields the phase information. The intensity is given by the squared modulus of the total field. If we expand this expression we find four terms. Because we assume the scattering to be weak, the squared modulus of the scattered field can be neglected. Now let’s suppose we record this intensity, and then use this recording as a transmissive sample, which is called the hologram. That is, if we illuminate the hologram with a plane wave, then the transmitted field amplitude will be the same as the recorded intensity. The transmitted field consists of an unscattered field and a scattered field, which together form a field identical to the field produced by the original object. Therefore, if we look at the hologram, it will be as if we’re looking at the original object. In other words, a virtual image of the object is formed. However, there is also an additional term that is equal to the complex conjugate of the scattered field. This field will create a real image of the object on the opposite side of the hologram, which is called the twin image. If we virtually extend this field to the location of the original object, we find that when an observer looks at the hologram, they will see a virtual image of the object, but on top of that they also see a defocused object due to the twin image, which degrades the quality of the holographic reconstruction significantly. So in order to improve the hologram, we should ask: how do we get rid of the twin image? The solution lies in a technique called off-axis holography. Again we have an object that we illuminate with coherent light, so that a complex-valued scattered field is generated in the window plane, and we want to reproduce this field with a hologram. To do this, we let the field interfere with a plane wave at a certain angle, that is, an off-axis reference plane wave. By having the scattered field interfere with an off-axis plane wave, there will be phase information in the recorded intensity. The intensity is given by the squared modulus of the sum of the fields, and we can expand this expression to find four terms. We use this recorded intensity to create a hologram, and we illuminate it with the same off-axis reference plane wave that we used to create it. The transmitted field is given by the intensity pattern multiplied with the plane wave field. Writing out the product gives us a zeroth diffraction order term, which propagates in the same direction as the reference wave. Another term gives a diffraction order, which exactly reproduces the field as produced by the original scene. Therefore, if we look at this field, it is as if we’re looking at the actual three-dimensional object. The final term gives another diffraction order, which corresponds to the twin image. Whereas in inline holography the fields of all the terms overlapped, in off-axis holography the different fields propagate at different angles. Therefore, they are separated, and if we look from the right angle, we’d only see the field that the original scene produced. So by using an off-axis reference wave, we solved the problem of the twin image. Let’s try to understand on an intuitive level how an off-axis hologram works. Our goal is to record and reproduce at each point in a plane, the amount of light and its direction. If we consider the wavefront of the field, then locally we can approximate the field as a plane wave propagating in a certain direction. To measure the directions of these plane waves, we have them interfere with a fixed reference wave. The larger the angle between the two plane waves, the smaller the period of the recorded interference pattern. If the recorded interference patterns are illuminated with the reference wave, the transmitted light will consist of a zeroth diffraction order that goes in the same direction as the incident light, and a first diffraction order that goes in a direction that depends on the period of the interference pattern. So by having the field interfere with a reference plane wave we created an interference pattern with a varying pitch, and by illuminating the interference pattern, we create a first diffraction order with a varying diffraction angle to reproduce the original field. We now have a method that can successfully reproduce the field that is scattered by a scene, so that to an observer it appears as if the three dimensional scene was actually there. The reconstruction assumes that the hologram is illuminated with monochromatic light, so light with a single wavelength, or single color. What happens if we change the wavelength (so the color) of the light that we use for the reconstruction? Let’s suppose the hologram is created using a reference wave that has a wave vector k. The length of k is given by 2 pi over lambda_0, where lambda_0 is the illumination wavelength. The phase function that describes the reference wave in the hologram plane is determined by the illumination wavelength and the sine of the angle of incidence. Therefore, if we change the wavelength of the light we use for reconstruction, we multiply the hologram with a phase function that is different from the one that was used to create the hologram, and therefore the reconstructed field is tilted by an amount that depends on the reconstruction wavelength. So if we were to view the hologram using white light coming from a certain direction, different colors of light will reconstruct the hologram at different angles, and we see a blurred reconstruction. How can we make a hologram that gives a good reconstruction when viewed with white light? One type of hologram that can be viewed with white light is called a reflection hologram. Instead of recording the intensity in a single plane like a regular off-axis hologram does, a reflection hologram records the intensity in a volume, thereby creating a volume grating. A volume grating uses interference to filter out a certain color of light. There are many other examples of filtering colors through interference. For example, soap bubbles look colorful in sunlight due to interference inside the wall of the bubble. Some animals display colors using microscopically structured surfaces that cause interference. Gabriel Lippmann used interference effects to produce color photographs, for which he received the Nobel Prize in 1908. In dielectric mirrors, thin layers of different materials are deposited in such a way that due to interference only certain wavelengths are reflected. In acousto-optic tunable filters, acoustic waves create periodic compressions in a crystal lattice so that only certain wavelengths of light are diffracted. To create a hologram that filters out a certain wavelength of light using interference, we do the following: we have a field reflected by the object, which we approximate as a plane wave with wave vector k_o. We have a reference plane wave of the same wavelength that has a wave vector k_r. We let the object wave and reference wave interfere inside a volume, where we record the intensity. The intensity will contain an interference pattern whose wave vector is the difference of the object wave vector and reference wave vector. The interference fringes will be perpendicular to this wave vector. If theta denotes the half-angle between the object wave and reference wave, then the length of the wave vector of the interference pattern is equal to two times sine theta times the length of k_r or k_o. The period of the interference fringes can then be found by dividing 2 pi by the wave vector, which results in the illumination wavelength divided by two sine theta. Now let’s see what happens if we shine light on such an interference pattern. Let’s assume that after developing the hologram, the interference fringes act as reflective surfaces. If we illuminate the hologram with monochromatic light at an angle alpha, then each surface will reflect the light at the same angle alpha. Now let’s see when these reflections will interfere constructively. Constructive interference will occur when the path length difference between the two reflections equals an integer multiple of the wavelength. Using trigonometry, we find that the path length difference is given by 2 Lambda sine alpha. Equating this expression to an integer m times the wavelength of the light used for reconstructing the hologram, gives Bragg’s law. If we only consider the limited wavelength range of visible light, we only need to consider first order diffraction, and we can choose m to be equal to 1. We write Bragg’s law in terms of the wavelength at which the hologram was created. We then find that when we illuminate the hologram with white light at a certain angle of incidence alpha, only the wavelength that satisfies Bragg’s law will be reflected. Because only a single color gets reflected, we don’t observe a color blur when viewing the hologram in white light. Another way to solve the problem of color blur in holograms is by using a rainbow hologram. The idea here is that if we view the object through a horizontal slit, and the color blur is in the vertical direction, the different colors don’t overlap. Suppose we’re again viewing a scene through a window, but now we reduce the window to a horizontal slit. If we look right through the slit, we can still see the entire three-dimensional scene. The reason why we can still perceive depth while looking through a horizontal slit, is because our eyes are separated horizontally. Each eye therefore sees the scene from a slightly different perspective, which is called parallax, and our brain interprets this as depth information. If we move horizontally we see the scene from different viewpoints, but because of the horizontal slit, we cannot see the scene from different vertical positions. In other words, by introducing the horizontal slit, we still perceive three-dimensionality through horizontal parallax, but we eliminate vertical parallax. This is acceptable because our eyes are separated horizontally, so we perceive depth primarily through horizontal parallax anyway. Remember that ultimately, it is the field in the slit that determines what we see. If we can reproduce the same field that the three-dimensional scene would produce, we will see that three-dimensional scene, even if it isn’t actually there. So suppose we have created an off-axis hologram using a vertically tilted reference wave, that reproduces the field in the slit. If we illuminate the hologram with white light, the colors will separate vertically, because we used a vertically tilted reference wave to create the hologram. This means that at different heights the field in the slit will be reproduced in different colors. So at a certain vertical position, we will see the scene in a certain color. If we change the vertical position of our eyes, we don’t see the vertical perspective change, but we see the same scene in a different color. But how do we make the hologram that reproduces the field in the slit to begin with? First, we create a regular off-axis hologram by having the field reflected by the scene interfere with an off-axis plane wave, and recording the intensity. If we illuminate the hologram with the reference wave from the other side, we create a real image of the scene. We introduce the horizontal slit, and then create an off-axis hologram of the transmitted field using a vertical reference wave. If that hologram is illuminated by the reference wave from the other side, it will create a real image of the field in the slit, which is what we wanted. So in summary, in inline holography we obtain phase information by letting the scattered light interfere with the unscattered light. This gives a problem with the twin image, so in off-axis holography, we let the scattered field interfere with an off-axis plane wave to eliminate the twin image. If such a hologram is viewed with white light, the reconstruction will be blurred due to the different colors of light. Color blur can be eliminated using a volume grating which filters out a single wavelength of light, or by reconstructing the field in a horizontal slit while having vertical color blur. We have seen how holograms can be used to reproduce the full field; phase and amplitude. But even though there is phase information present in the hologram, we cannot see the phase information like we could see amplitude information in a normal photograph. But sometimes the field that is transmitted by a sample contains relevant phase information, so in addition to having a regular intensity image of a sample, we may also want a phase image of the sample. For example, suppose we have a transparent sample of varying thickness. It’s very difficult to obtain meaningful information from a normal picture of a transparent sample; you’d almost see no contrast. But if we could take a picture of the phase of the transmitted light, you would see quite a lot of information. Suppose that we illuminate the sample with coherent light that is in phase: so all the peaks coincide, and all the valleys coincide. Inside the sample, the light has a reduced wavelength that depends on the refractive index. When the light exits the sample, the light has undergone a phase shift that depends on the thickness of the sample. If the sample has a varying thickness, then the transmitted light has a varying phase shift. So the transmission function of the sample is a pure phase function, and it gives direct information about the thickness of the sample. If we have a sample whose transmission function is a pure phase function, and we take a normal image of it, then what we measure is the squared modulus of the transmission function. Because we only measure intensity, all phase information is lost, so in the image we don’t see any meaningful information about the sample. One way to create an image using phase information is Zernike phase contrast microscopy, an invention for which Frits Zernike received the 1953 Nobel prize. To reveal phase information in the recorded image, the unscattered light is phase shifted before it re-interferes with the scattered light. If we illuminate a phase sample with a plane wave, a part of it will continue as a plane wave in the same direction, while other components will be scattered at different angles. Mathematically, we can write the unscattered field as 1, and the remainder corresponds to the scattered field. If we assume the phase shift is small, we can Taylor expand the complex exponential to first order, which allows us to approximate the transmitted field as 1 plus i times the phase shift. If we insert a lens, the plane waves will focus at different points in the focal plane. The unscattered plane wave will focus in the center, while the scattered plane waves will focus at other positions in the focal plane. Mathematically, the field in the back focal plane is approximated as the Fourier transform of the field in the front focal plane, so the unscattered field becomes a delta function, and the phase function is Fourier transformed. Here comes the crucial part of Zernike phase contrast microscopy: the unscattered light is phase shifted by 90 degrees. Mathematically this means that the focused unscattered field is multiplied by i. We now introduce a second lens to perform another Fourier transform in the image plane. The intensity we measure there is given by the squared modulus of the field. The resulting expression has a term that is linear in the phase, and one that is quadratic in the phase. When we Taylor expanded the transmission function to first order, we assumed that the phase shift small, so the quadratic term is negligible. We find that by phase shifting the unscattered light, the phase information appears in the image intensity. By comparison, if we hadn’t phase shifted the unscattered light, there would be no linear phase term in the intensity, and since the quadratic term was assumed to be negligible, there would be no phase information in the image. In practice however, the sample wouldn’t be illuminated with a single plane wave, but with multiple mutually incoherent plane waves from different angles. If we have a ring-shaped light source in the front focal plane of a lens, then the sample that is in the back focal plane is illuminated by plane waves from different angles. In the back focal plane of the first imaging lens a phase shifting ring is introduced to phase shift the unscattered light. One can also insert a gray filter to reduce the intensity of the unscattered light, since the unscattered light is usually much stronger than the scattered light. And to have better image contrast, it is better to have approximately equal amounts of scattered light and unscattered light. This type of microscopy is commonly and successfully used to reveal phase information in the image. It makes it possible to see samples that are otherwise transparent without having to artificially color them with dyes. However, the phase information that we see in a phase contrast image isn’t quantitative: it doesn’t tell us the exact phase shift at each point in the sample, and so we cannot reconstruct the full, complex-valued transmission function of the sample. In the following we discuss several methods to retrieve the phase of a field quantitatively. One way to measure phase information quantitatively is by using a Shack-Hartmann wavefront sensor. Just like in a light field camera, it locally determines the direction of the light by using a microlens array. If you have an incident field with a smoothly varying wavefront, then you can locally approximate the field as a plane wave. Depending on the orientation of the wave, the microlens creates a focal spot at a certain position in its focal plane. So by checking the positions of the spots, you know the local slope, or gradient, of the wavefront. And by stitching the slopes together, or integrating the gradients, you can find the entire wavefront. While this is a straightforward method to find the shape of the wavefront, the resolution is quite low, as it is determined by the number of microlenses you can fit in the sensor. Also, the wavefront has to be smoothly varying. If the wavefront oscillates over the area of one microlens, the method doesn’t work anymore. For this reason, Shack-Hartmann wavefront sensors aren’t commonly used to create phase images of intricate samples. Instead, they are used in adaptive optics for real-time aberration correction. For example, suppose you want to look at a star in the sky with a telescope. The star emits light, but when the light reaches the Earth’s atmosphere, the wavefront gets distorted due to turbulent airflows. If this light is used directly to create an image of the star, it would be of poor quality due to the aberrations. So we want to correct for these aberrations before recording an image, but we have to keep in mind that the aberrations change continually over time because the air in the atmosphere is continually moving. We can use a deformable mirror to correct the distortions in the wavefront. This corrected wavefront is used to create a clean image, but we also have to continually check whether the applied aberration correction is still correct, and update it if necessary. To do this, a portion of the corrected wavefront is sent to the wavefront sensor, where the aberrations are measured and used to update the shape of the deformable mirror. A method that can be used to create phase images of intricate samples, is digital holography microscopy. As the name suggests, it uses the same technique as off-axis holography to record phase information, which can be extracted digitally. Suppose we have a sample with a complex-valued transmission function, so it has a certain amplitude, and a certain phase. We want to measure both the amplitude and phase, while in a normal imaging system, you’d only measure the amplitude. So to measure the phase, we let the field in the image plane interfere with an off-axis plane wave. With the camera, we measure the intensity which is basically an off-axis hologram. But because this time the intensity is recorded digitally, we can analyze and modify the image computationally. So how can we use the intensity measurement to compute the amplitude and phase of the sample? If we expand the expression for the intensity, we find four terms. The complex exponentials with linear phase will cause a shift when applying a Fourier transform. So Fourier transforming the measured intensity yields a term that is not shifted, a term that is shifted in one direction, and a term that is shifted in the opposite direction. These three terms are also called the central band and the side bands. We can isolate a shifted term, which corresponds to the Fourier transform of the sample’s transmission function. We can shift it back, and inverse Fourier transform it to reconstruct the phase and amplitude of the sample. However, we do see some noticeable difference between the original transmission function and the reconstruction. In the original transmission function the phase is constant over a large area, while in the reconstruction we see a wildly varying phase. The reason why we see this, is because the area where the phase is constant, the amplitude is zero. And if the amplitude is zero, the phase has no meaning. Mathematically, you can see it by writing a complex number as its amplitude times its complex phase factor. If the amplitude is zero, it doesn’t matter what the phase is. Physically, you can understand it by realizing that the phase describes the shift between two waves. But if the waves have zero amplitude, a shift is irrelevant. Even if the amplitude isn’t exactly zero, it is still true that if the amplitude is smaller, the uncertainty in the phase becomes larger. We can see this by drawing in the complex plane a circle which denotes the uncertainty of a complex number. Because of the finite size of the circle, there is an uncertainty in the angle, or phase. If we take a circle of the same size, but move it closer to the origin (so reduce its amplitude), then the uncertainty in the angle becomes larger. The other difference between the original sample and the reconstruction is the presence of two vertical lines in the reconstruction. The cause of these lines is that the three terms, though separated, still overlap. When isolating the side band, there is still a contribution from the central band that we include in the reconstruction. This contribution consists of high horizontal spatial frequencies, which result in vertical lines when inverse Fourier transformed. We’ve now seen methods to measure both the amplitude and phase of a field, which may be useful when imaging transparent samples whose most important information is contained in the phase. But there is another advantage to measuring both the amplitude and phase of a field: if you measure the complete complex-valued field you can compute how it propagates, and this allows you to compute images without having to use lenses. Let’s recall what happens in conventional imaging with a lens. We have a sample with a certain transmission function. If we illuminate it with a plane wave, the transmitted field will be the same as the transmission function. The transmitted field then propagates a certain distance, which causes the field to look more messy. By letting the field pass through a lens and propagate to the image plane, the features of the sample become clear again. Technically the image is flipped and the resolution is limited by the diffraction limit, but for simplicity we can say that the measured image is the squared modulus of the sample’s transmission function. In the case of lensless imaging we do the following. We again illuminate the sample with a plane wave and let the transmitted field propagate, but instead of focusing the light with a lens, we just let the field propagate freely to the detector. What we measure is an image where the features of the sample have become unrecognizable due to propagation. In lensless imaging, or coherent diffractive imaging, we measure a field that has propagated freely, and then computationally backpropagate the field to reconstruct the field at the sample plane. To calculate how a field propagates, we need to know the full field information, namely the amplitude and phase, but we can measure only amplitude directly. After the field has been backpropagated, we know the full complex-valued field transmitted by the sample. So to perform lensless imaging, we need to retrieve the phase of the field. We have previously seen how phase information can be recorded using holography, so let’s see how we can apply holographic methods to lensless imaging. In inline holography, we have a weakly scattering sample, so that the transmitted field is given by 1 plus a perturbation. This field is propagated to the detector plane, where we detect the intensity, which is given by the squared modulus of the field. Expanding the expression gives four terms, two of which correspond to the propagated field. So if we inverse propagate the measured intensity image, these two terms will give the sample’s transmitted field. So consider the measured intensity pattern. We’re going to computationally propagate this pattern as if it were a field, so to plot the intensity of the field we have to square it again. As we backpropagate the field to the sample plane, we see the features of the sample becoming sharper. If we compare the measurement to the reconstruction, we clearly see the effect of computationally backpropagating the intensity. The features of the sample have become sharper, but the other terms visibly degrade the quality compared to the original sample. To overcome this, we introduced off-axis holography, which in the context of lensless imaging can also be called Fourier Transform holography. In Fourier transform holography we have a sample of limited size. We introduce a small pinhole sufficiently far away from the sample. If we illuminate the sample, the field in the pinhole will act as a point source, which we can mathematically describe as a shifted delta function. We assume the detector is in the far field, so that propagation is described by a Fourier transform. In the far field, the shifted point source will become an off-axis plane wave that interferes with the far-field of the sample, which creates a situation as in off-axis holography. The interference fringes in the intensity pattern contain the relevant phase information. So we have a transmission function which is the sample plus a shifted delta function. The far field, which is given by the Fourier transform, is an off-axis plane wave plus the far field of the sample. The measured intensity is the squared modulus of this field. Writing out this expression gives four terms. If we inverse Fourier transform the intensity, we find an unshifted DC-term, a shifted reconstruction, and another reconstruction that is mirrored and conjugated which is called the twin image. To make sure that the three terms are sufficiently separated, the pinhole has to be sufficiently far from the sample. If we compare the original phase to the reconstructed phase, we again see some differences. As we saw previously, there can be big differences in the phase because for zero amplitude the phase becomes undefined. Moreover, we see that the reconstructed phase for the different letters aren’t the same as the original phases. The reason for this is that physically only phase differences are relevant, and global phase shifts are irrelevant. Mathematically, it can be understood by observing that a global phase factor doesn’t change the intensity we measure. Physically, it can be understood by observing that a global phase shift corresponds to a shift in time, but we only measure time averaged intensities anyway. Recall that the phase is periodic: a phase of pi is equivalent to a phase of negative pi. If we now compare the original phases to the reconstructed phases, we see that indeed there is a global phase shift present in the reconstruction. Aside from holographic techniques, there are also iterative phase retrieval algorithms for lensless imaging. The most basic algorithm works as follows. We have a sample with a complex-valued transmission function, so it has a certain amplitude and phase. We assume that the function has a known finite support: that is, we know that outside a certain area the transmission function is zero. The sample is illuminated with a plane wave, and we measure the far field intensity pattern. This gives us two pieces of information: we know the support of the sample, and we know the squared amplitude of its Fourier transform. Using these two constraints, we want to reconstruct the sample’s transmission function. Let’s see if we can come up with a simple algorithm that performs the reconstruction. We want to find a transmission function that satisfies both the support constraint and the intensity constraint. So let’s start with a function with a certain amplitude and phase that satisfies the support constraint. To check whether it also satisfies the intensity constraint, we take the Fourier transform of the guess. This gives a complex-valued function, whose amplitude doesn’t in general match the measured amplitude. So to enforce the intensity constraint we substitute the estimated amplitude with the measured amplitude, while keeping the phase. This gives a new estimated far field, and by inverse Fourier transforming, it gives a corresponding new estimated transmission function. In general, this new estimate doesn’t satisfy the support constraint, so we can enforce it by setting the field outside the support equal to zero. Now we have a new estimated transmission function, and we can repeat the cycle of calculating the far field, enforcing the intensity constraint, calculating the updated object estimate, and enforcing the support constraint. One cycle of enforcing the support constraint and intensity constraint describes one iteration of this phase retrieval algorithm. This algorithm seems straightforward enough, but if we apply 100 or several thousands of iterations, we see that the reconstruction doesn’t get as good as it should be. So this algorithm doesn’t work very well, and we have to try a different algorithm. Another, better, algorithm works as follows. If we enforce the support constraint in a straightforward manner, we multiply the estimated object with the support function, thereby setting the field to zero outside the support region. What we do now instead is setting the field outside the support region to some non-zero value. More specifically, we define a negative feedback function outside the support region with some feedback parameter beta. If the updated field outside the support region is positive, then instead of making it zero, we’re making it negative. If we apply such an update, then we see that after a few thousand iterations we have a good reconstruction. The two algorithms we just discussed are called the Error Reduction algorithm, or ER, and the Hybrid Input-Output algorithm, or HIO, where HIO is the more effective algorithm for phase retrieval. These two algorithms were developed around 1980. One might wonder how one would come up with using a feedback function to improve the algorithm, or why the feedback function should have the particular form that it has. In 2012 the inventor commented that to find the HIO algorithm, he used the way that Edison used to invent a practical light bulb: keep trying different things until you find something that works. So in that sense, it isn’t obvious how the success of the HIO algorithm can be explained from fundamental principles. However, we can analyze the algorithms in more detail to gain more insight in them, and to get clues on how to improve them. The ER algorithm, although less effective, is more straightforward, and therefore easier to analyze first. One way to interpret the ER algorithm is in terms of projection operators. We have two constraints, and we want to find the function that satisfies both constraints simultaneously. So we can define the set of functions that satisfy the support constraint and the set of functions that satisfy the intensity constraint. The function that satisfies the two constraints simultaneously is the intersection of these sets. The ER algorithm alternatingly applies the two constraints, so you start with a function that satisfies the support constraint, and by applying the intensity constraint you project the function onto the intensity constraint set. This can be described with a projection operator P_I. After that you enforce the support constraint, so you project the function onto the support constraint set, which is described with a projection operator P_S. We see that one iteration of the ER algorithm consists of applying the two projection operators to the estimated function. By applying multiple iterations we move closer to the intersection, which is the solution to the phase retrieval problem. However, we have already seen that actually we don’t reach the right solution, so what is happening? The reason that the algorithm stagnates at a wrong solution is because the intensity constraint set is non-convex. In the pictorial representation that we just saw, the intensity constraint was convex. What this means is that if you take two points in the set, and you draw a straight line between them, then every point on that line lies in the set as well. However, the intensity constraint is in fact non-convex. So if we pick two points in the set, and we draw a line between them, then there are points on that line that do not lie in the constraint set. If we now go back and forth between the two sets, we find that indeed we get stuck at a point which is not the correct solution. We can illustrate with a simple example that the intensity constraint is non-convex. Both negative 1 and 1 lie in the constraint set where the modulus has to be 1. But if we take a convex combination of negative 1 and 1, then the result does not have a modulus of 1, so it doesn’t lie in the constraint set, so the set is non-convex. Using this framework of constraint sets and projection operators, let’s see if we can understand what the more successful HIO algorithm is doing. In the ER algorithm we updated the object estimate by enforcing the intensity constraint, and then enforcing the support constraint. In the HIO algorithm we do the same thing inside the support constraint, but outside the support constraint we define a feedback function with some feedback parameter beta. Let’s choose beta equals 1 to slightly simplify the expression, and then expand the brackets, and then collect the terms. To see how we can further rewrite this expression, let’s consider the reflection operator. To reflect a function around the constraint set, we can first project it onto the set, check the difference between the original function and the projection, and then add that difference to the projection. We find that the reflection operator is given by twice the projection operator minus 1. We can now consider the following expression, substitute the expression we found for the reflection operator, expand the brackets, and simplify the expression to find that this is the same operator that is used for the HIO update. So we find that whereas the ER algorithm uses projections to update the object estimate, HIO for beta equals 1 uses reflection operators. This insight was published in 2002, two decades after the invention of the HIO algorithm. Since then, people have been inspired to design other phase retrieval algorithms that use some combination of projection and reflection operators. Furthermore, the HIO algorithm works well on simulated data, but it can become unstable when measurement noise is present, so noise robust variations of the algorithm have been designed as well. So we’ve seen how the ER algorithm can be understood as using projection operators to find the intersection of two constraint sets. Another way to understand the ER algorithm is through cost function minimization. Let’s say that we have an object estimate that satisfies the support constraint. We want the estimate to be such that the modulus of its Fourier transform equals the measured amplitude. Or we can put it differently, and say that we want to minimize the difference between the estimated far field amplitude and the measured amplitude. The total difference is defined as the sum of the squared differences for all pixels. To minimize this cost function, we can apply the gradient descent algorithm. In this algorithm, we calculate the derivative of the cost function to find the direction of steepest increase, and then move in the opposite direction with a step size mu to decrease the value of the cost function. We repeat this procedure until we have reached a minimum of the cost function. To calculate the derivative of the cost function we apply the chain rule, which results in two derivatives that we can calculate separately. The first one we can calculate by writing modulus T hat as the square root of T hat times T hat conjugate. This derivative can be calculated and simplified straightforwardly. We can substitute this result, and then simplify the expression. For the other derivative, recall that T hat denotes the Fourier transform of T that satisfies the support constraint. If we write out the definition of the Fourier transform, we see that the derivative of T hat conjugate to T conjugate in a certain pixel (x,y), is equal to a complex exponential. If we plug this result in the expression for the derivative of our cost function, we find that we can write it as the inverse Fourier transform of some expression. The inverse Fourier transform of T hat reduces to T, so we end up with the following expression. If we assume that the initial estimate satisfied the support constraint, and that we choose a step size of mu equals 1, then the updated object estimate is the support constraint applied to the inverse Fourier transform of the far field, where the estimated amplitude has been substituted with the measured amplitude. In other words, to update the estimated object, we enforce the intensity constraint and then the support constraint. In this scheme, the value of the cost function is reduced with each iteration, which is why the algorithm is also called the Error Reduction algorithm. If we interpret the algorithm in terms of cost function minimization, we can explain the stagnation of the algorithm using local minima. If the cost function has a local minimum, and we use a minimization algorithm that always tries to decrease the cost function value, then it gets stuck in the local minimum even though the actual solution is elsewhere. One way to overcome this problem is by adding a sort of ‘momentum’ to the update so that the estimate can escape local minima. Other methods for cost function minimization have also been explored in the context of phase retrieval. With the ER and HIO algorithms, we can reconstruct an object using a support constraint and a single intensity measurement. There’s another phase retrieval method called ptychography, where multiple far field intensity patterns are recorded by shifting the illuminating spot across the sample. Typically, the sample’s transmission or reflection function is called O for object, and the illumination spot is called P for probe. If we illuminate the object at a certain position, then the exit wave for that position is found by multiplying the shifted probe with the object. The far field is calculated by taking the Fourier transform of the exit wave, and the measured intensity is given by the squared modulus. As we shift the probe across the sample, we detect multiple diffraction patterns which we will use to reconstruct the object. We vary the probe position continuously across the sample and measure the intensity patterns for all positions. From this four-dimensional data set and the known probe function, we can reconstruct the object using the Wigner Distribution Deconvolution Method, which was proposed in 1989. This method works as follows. The far field is given by the Fourier transform of the exit wave, which is the shifted probe multiplied by the object. We can write the shifted probe as a plane wave expansion, so as the inverse Fourier transform of its Fourier transform, evaluated in the shifted coordinates. Now we substitute the expression for the shifted probe in the expression for the far field. We can write out the complex exponential, and define the function f as the Fourier transform of the probe times the object times a complex exponential. With this definition, we find that the set of all far fields is the four-dimensional Fourier transform of f. From this it follows that the four-dimensional Fourier transform of the autocorrelation of f gives the set of measured far field intensities. The reason why this is so, is because a product in Fourier space gives a convolution in object space. So if we take the squared modulus in Fourier space, then in object space we convolve with a flipped and conjugated function. By flipping the sign of the integration variable, we obtain an autocorrelation integral. So by taking the inverse 4D Fourier transform of all measured intensity patterns, we obtain the autocorrelation of f. If we substitute the definition of f in the autocorrelation integral, we find that we can write the double integral as the product of two separate integrals. These integrals describe the Wigner distributions of the object and the Fourier transform of the probe. So from the set of diffraction patterns, we can calculate the product of the two Wigner distributions. Assuming we know the probe, we can divide out the Wigner distribution function of the probe to find the Wigner distribution of the object. There are two ways to reconstruct the object from its Wigner distribution function. One is to take the Fourier transform with respect to x. Taking the integral over x gives the Fourier transform of O times a linear phase factor. Taking the integral over x prime gives the Fourier transform of O evaluated in k plus capital K. By picking small k fixed and choosing capital K freely, we reconstruct the Fourier transform of O, from which we can straightforwardly find O itself. The other way to find the object from its Wigner distribution function is to take the Fourier transform with respect to K. Performing the integral over K gives a delta function, and performing the integral over x prime gives the object reconstruction. So we’ve seen that with a single intensity measurement and a support constraint, we can reconstruct the object using an iterative reconstruction algorithm, but it may stagnate at a wrong estimate, and it can be unstable in the presence of noise. On the other hand we have seen a method that requires multiple intensity measurements, and gives a reconstruction non-iteratively. The major downside of this approach is that the probe needs to be scanned continuously across the sample, so you’d require extremely many measurements. So the question is: can we combine the best of both methods? That is, can we come up with a method that suffers less from stagnation and noise sensitivity than single-shot CDI, but also requires fewer measurements than the Wigner Distribution Deconvolution Method? This very question was asked in the early 2000’s, and it led in 2004 to the development of iterative ptychography. In this method, an object is scanned with a probe in steps that have considerable size, but are still small enough to guarantee overlap between probes at adjacent positions. With the set of intensity patterns, and knowledge about the illumination spot, the object is reconstructed iteratively. Several years later, the algorithm was analyzed in terms of projections and as a cost function minimization scheme, just like the ER and HIO algorithms from single-shot phase retrieval. These analyses led to the very important insight that from the measured intensities patterns, both the object and probe can be reconstructed simultaneously. This was a significant step forward, because usually the probe isn’t quite exactly known in advance, and it could result in a poor reconstruction quality if it’s not corrected for. Another major development in the field of ptychography was the invention of Fourier ptychography. This method uses the same mathematical algorithms, but they are applied to a different practical problem. Whereas the original proposal for ptychography was intended for lensless imaging, Fourier ptychography aims to extend the functionality of a regular optical microscope. Suppose you have a rather large sample, with small features on it that you want to examine. You can put the sample under a microscope and zoom in to see the small features. But because you zoomed in, you only see a small part of the entire sample. You’d have to mechanically scan the sample around to cover the full field of view. You could also zoom out to increase the field of view, but then you wouldn’t be able to see the small details anymore. So there is a trade-off between field of view and resolution: by zooming out you have a large field of view but a low resolution, and by zooming in you have a high resolution but a small field of view. In order to have both a high resolution and a large field of view, you’d have to zoom in and move the sample around. Fourier ptychography provides an alternative method. You can get both high resolution and a large field of view without having to mechanically move the sample around. Moreover, you also retrieve the phase of the sample, as opposed to a regular microscope which only measures amplitude. The phase information of the sample may by itself be of interest, but it also allows to correct for aberrations introduced by imperfect lenses. The method works as follows. If we illuminate a sample, then in its far field we have the Fourier transform of the transmitted field. A part of this field is transmitted by the lens, which forms a low-resolution image with a large field of view. If we change the illumination angle, the Fourier transform is shifted, and a different part of it passes through the lens to form an image. So if we have an array of point sources beneath the sample that illuminate the sample from different angles, we scan the Fourier transform of the transmitted field across the detection lens. For each angle of incidence a low-resolution image is recorded, and with the set of images, we can reconstruct the entire Fourier transform using the ptychographic algorithm. And from the Fourier transform, we can directly find the full complex-valued transmitted field with a high resolution and large field of view. Because we use a fixed array of point sources we don’t need any mechanical movement, and because we retrieved the phase information we can also perform aberration correction to correct for imperfections in the imaging lens. Note the similarity between regular ptychography and Fourier ptychography. In regular ptychography we can scan a sample across a fixed probe, and measure the intensity of the far field, which is given by the Fourier transform of the transmitted field. In Fourier ptychography, the Fourier transform of the sample is shifted across a fixed lens transmission function, and the intensity in the image plane is measured. Because of this similarity, the same reconstruction algorithm can be used for both applications. Other developments that have been made in the ptychographic reconstruction algorithm include: the ability to correct for errors in the probe positions; performing the reconstruction using partially coherent illumination; and reconstructing thick samples that do not have a single well-defined transmission function. To summarize, we’ve seen that to fully describe a light field we need to know both the amount of light, and the direction of light at each point. For incoherent light, the direction of light can be measured or controlled using microlenses. For coherent light the direction can be measured using interference, or controlled using diffraction gratings. The full field information can be used to reproduce three-dimensional scenes, to correct for aberrations in imaging systems, or to obtain relevant information from transparent samples. With the advent of digital cameras and the increase in available computing power, computational phase retrieval is finding many practical applications. The first iterative algorithms that use a support constraint and a measurement constraint were the Error Reduction and Hybrid Input Output algorithms. The Error Reduction algorithm can be understood in terms of projections on the constraint sets, or as a cost minimization scheme. The more successful Hybrid Input Output algorithm can be understood in terms of reflections around the constraint sets. The sample can be reconstructed using a single intensity measurement, but the algorithm can be sensitive to noise or suffer from stagnation. Another method for phase retrieval is the Wigner Distribution Deconvolution Method where the illumination spot is scanned across the sample and multiple far field intensity patterns are recorded. The sample is reconstructed non-iteratively, but very many measurements are required. A combination of the two methods was found in iterative ptychography, where an iterative algorithm reconstructs the sample from multiple intensity measurements obtained by moving the illumination spot across the sample. This method can be used for lensless imaging, but also for extending the functionality of optical microscopes. 