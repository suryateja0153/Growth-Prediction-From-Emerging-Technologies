 Hi, I'm Don Engle. I'm from the University of Maryland, Baltimore County (UMBC). I'm giving a talk on our "Visualization Research and Facilities", and I'm going to divide the talk today. First just an introduction of myself- because I have a couple of different hats and I want to explain why I'm here and what I hope to get out of the opportunity I have to present to you (and hopefully the opportunity you have to hear this talk);  and then, I'm going to talk about the institution, just to give you an overview of who we are (because you might not know); and then, I'm going to talk about simultaneously our research and our facilities. A lot of what we're doing that I think is particularly exciting- especially for an external audience who we might maybe be able to partner with- is related to our unique hardware and corresponding software that we've set up. Both because you could come to our university and use it with us, or because you could replicate things we've done and we could learn from things you've done in improving what we have. Most of what we're building is meant to be built in such a way that others can reproduce it. So without further ado, here's an introduction of myself. I am Don Engel; I serve as UMBC's assistant vice president for research. Through that role, I lead our office of research development, which is the part of the research leadership team that is working on building collaborations- helping faculty find external and internal partners. We manage our internal seed funding. To a certain extent, we manage our shared research infrastructure, our core facilities, and and we help faculty think about where they might apply for funds. Also, some of our funds come from NOAA, for which we're very grateful, and we'll talk about that in a moment. I also have a 20% appointment by salary with the Division of Information Technology. A lot of that relates to my data science work, that is in large part tied to my work with our visualization facilities; but also relates to my work with our high-performance computing research facilities. I am a professor as well- I have an affiliate appointment, because my primary appointment is the leadership role. I have appointments in three departments in each of our colleges: Department of Computer Science and Electrical Engineering, Department of Physics, and the Imaging Research Center. So in the Venn diagram of things that people know in those disciplines, I'm the intersection, not the union. I'm just that little area in the middle of overlap; maybe between all of those things, as a specialist I lead a research group which I call the "assistive visualization and artificial intelligence lab". The Venn diagram, again of these three units is best understood as a lab that is a lab lab of each of these entities because I of each of these entities. There are seven students and one full-time research staff person working with me in the lab, but we are working as we are improving on our facilities and as we are doing projects of interest with faculty and students from across the university and lots of external collaborators. So, we'll talk about the projects in a moment, but I want to make sure you know about UMBC. We are, by most measures, the third-largest doctoral University in Maryland- but the largest, Hopkins, is larger by a factor of two in research dollars than the second runner-up nationally. So, we're much smaller than the first two. The other one is College Park, which is the flagship of the university system. We're a a separate university in the system. We are-and this first one is per capita, it's the only per capita rank that I want to mention - we're #17 nationally and #48 globally for citations per faculty member, so our researchers are producing impactful work at high volume. We're number nine in "Most Innovative Schools" in U.S. News and World Report. We are number eight in "Best Undergraduate Teaching"; many of our undergraduates are engaged in research. We have a lot of internal programs to make sure they're getting involved in research, although we also have extensive masters and PhD programs. Perhaps most surprisingly, we are 12th nationally, not per capita, for NASA funding. We've been in the top 20, higher and lower than 12th, for years. This is the most recent survey, which is FY19, and I want to highlight that since I'm speaking predominantly to a NOAA audience, that most or a large fraction of which is atmospheric science research or earth systems research. A good chunk of our total NASA dollars come from our Cooperative Research Centers that we have housed at Goddard. Outside of those activities, the NASA research centers- which are research faculty who have affiliate appointments but are mostly in these centers-the largest set of fields that we have, as a sort of describable unit, are information science and data science disciplines. Now, this is important to note, as we're considering the relationship between NOAA and UMBC and what I think our relationship could be. We're only about twenty-eight miles away from NOAA. UMBC and  NOAA in Silver Spring is a half hour away, if you take the more direct route, so it's not hard to get between our two campuses. We're doing a lot with NOAA's support already. These are a subset of the awards that UMBC was funded from NOAA in just FY19; two of them are Cooperative Agreement Centers that are multi-university, and UMBC is a part of them. The rest are more traditional grants, but most of them have a atmospheric component that is supporting and collaborating with atmospheric scientists. Some of it is aquaculture. So, I mentioned that UMBC is about 30 miles from NOAA. For our main campus, if you've ever been to downtown Baltimore, you may have seen this beautiful campus with this canvas roof, which is the Columbus Center-that's managed by UMBC. It houses UMBC and other researchers. The image below is a large fish tank in our aquaculture research facility. We have an extensive set of research activities happening there on closing the reproductive cycle of fish, and other aquaculture research. None of those things that are funded by NOAA are heavily data science or visualization oriented, yet we are doing a lot in those spaces and NOAA is, too. The project which really led me to have the opportunity to be here today is our work with the "Magic Planet" that we have acquired; it's four feet in diameter, but it's in the background. What you see in the foreground are some students and I'll talk about their project in a moment. But, we acquired this magic plan at the spherical display for the purposes of teaching in our Geography and Environmental Systems department, but more recently we've taken an interest in what else we can do with it. We have an internal funding mechanism to help our undergraduates be as involved in research as our masters and PhD students are, called "Co-Lab." That particular opportunity is to bring an interdisciplinary set of students to work together with a faculty mentor through a summer, on some scientific storytelling or other interdisciplinary project. So, these students- working with Ben Daniels, who's in the photo, and Nicole Trenholm, who you'll see in a future slide-created a spherical video with the intent that it be distributed ultimately on science on a sphere which gave us the opportunity to present their work here at the Science on a Sphere Center at Silver Spring Campus of NOAA. There's Ben again, Nicole, and the four students who worked with them. Nicole has an interest in in extending this work further. Nicole goes on expeditions on ships into straits in the Arctic, and we'll be capturing spherical video this summer to show the terrain, the changes in the terrain and we'll use that content for future content production. So, in creating this content, we actually had to entirely replace the software that came with Magic Planet, which slices videos up into single images and then shows them, roughly speaking, as a slide show. We wanted to be able to just take a video and play it directly as a video file on the sphere, and we've created, released and now published in an academic conference. So this work is to be presented at the IEEE Visualization conference in a month or two. The code is now open source and our hope is to make it so that not only can things such as a Science on a Sphere, like the Magic Planet, be more democratized; allow people to more easily create content and download Science on a Sphere content and install it. Also, we can just tweak our software and continue to release it in an open way free way a modifiable way so that people can build their own desktop spherical systems and there's some plans for those online that you can find using micro projectors and just the little covers that go over a light bulb that you can buy in a hardware store the right arrangement of a lens and something 3D printed you could make your own little spherical display for a couple hundred dollars or less so we want to be modifying our software so it will work for a wide range of viewing modalities including magic planet which we've already done other spherical displays and also as you'll see later we do a lot with wall based VR head mounted VR so we're interested in making it easier to consume science on a sphere content produce science on a sphere content spherical content generally in other viewing modalities there's a lot of underlying technical challenges I won't get too deep on it, but I'll mention that they were addressed and solved, in case anyone wants to contact me to discuss them further. We're taking an equirectangular projection and we're converting it into an azimuthal projection, where you have the North Pole at the middle and the South Pole at the outside. But, it's trickier than that, because in the arrangement of Magic Planet, which is exposed here, there's a projector going through a fisheye lens into a sphere that it's lighting from the inside. It isn't as equiangular as a multi-projection, which is to say that the lines of latitude aren't evenly distributed across the circle as you move out. It's a nonlinear distribution. So, we had to reverse-engineer that and figure it all out. The other thing that we created with our interface is the ability to take just a regular sort of PowerPoint remote and control the sphere, provided that you have a computer tucked away somewhere driving it. It will then automatically look for a folder that has whatever content you want in it that it can read image files, movie files, and then will display it on the surface of the sphere so you can just control it directly. Another facility that we have which is less obviously connected to NOAA (but I will tie it in, I promise!) is our photogrammetry facility. This was supported by a National Science Foundation Major Research Instrumentation Grant. You can see the facility here; it's a configuration of 96 cameras that are all wired to fire simultaneously, so you end up with many different perspectives of the same image. Until recently, we've been using commercial off-the-shelf software to process those images. The software that we were using, ISoft PhotoScan, is used by the small community of people who do photogrammetry. It's for creating 3D models of things like virtual reality or animation, but also by the GIS community, which takes many photos over time from a drone to map a scene in 3D from the air. Our cameras are repositionable, and the software pipeline is something that we're re-engineering now. We're working with the Mesh Room Suite, which is open-source- and we hope to heavily contribute soon. One of the most exciting things that we're doing with this is that we're making it so that we can run the photogrammetric reconstructions- it's the process of taking all 92 photos and turning them into a 3D model that we can very quickly run on our high-performance computing and maybe get a much better result because we can put get much more computing power into it in a shorter time. With what we currently get out of a single computer that's beefed up with a lot of GPUs (graphical processing units), the current process requires a couple of hours of an artist's time to touch it up afterward. Even though you get submillimetre resolution on a good day with a good surface, those areas don't process as well on hair or on things where the cameras aren't able to get a good view, like in between your fingers- where things might be occluded. I'm putting some of the text in these slides, so that if anyone wants to download these slides later, you'll have it as a reference. I don't mean to read the slides to you, but I will say that we have 96 cameras and projectors which add contrast to a surface if the surface isn't interesting enough for a computer to automatically recognize features. They can turn into 3D points, if you have a very flat/monochromatic surface. The projectors can then add noise to make it easier to process surfaces into a 3D model, and flashes that go off together. Again, this is mostly for the person who wants to download the slides. They contain much more technical descriptions. We're taking these 3D models that we can capture instantaneously, and after a couple of hours, process into something useful. Then, we using the models in a variety of VR environments. One of our VR environments, which we can use for things besides VR, is our wall. We call it the PI Squared. Technically, it's a partial cave. you may have seen a cave VR environment, which have existed for a few decades. They are rear-projected screens, usually a partial 'cube' (missing maybe a wall, the floor, or the ceiling). You stand in the middle of the 'cube' with 3D glasses of some sort on, which will let you see what's on the wall in 3D. Your head position is tracked, so as you move, what the left eye and right eye receive is updated relative to your head position. You then get what we call parallax objects- what is in the foreground appears to move relative to the background as your perspective changes. With the Cave Two system, which is more competitive and has sort of different advantages and disadvantages compared to the head-mounted VR, we have the next generation of those rear projected cubic caves. They're curved, it's actually a set of regular 3D LCD panels. They're 24 of them: it's a grid, in our case, of four high and six wide. If we had a full cylinder, it will be more screens- but because it's a gradual curve, if you have other users. You can have a really useful multi-user experience in VR. If you have a headset, it's very much a one-person experience. Sometimes, you can do things through a network to combine headsets together, but really, you're looking at avatars of other people. It really dampens your ability to have a deep, meaningful conversation about some scientific data that you're immersed in. By being able to stand next to each other in a wall like this, as opposed to a cave, the secondary users (the ones whose heads aren't being tracked) do see some distortion as the main users perspective changes, but because the curvature is gradual instead of these sharp 90-degree curves, it's not nearly as disorienting. It's much more useful for group VR experiences, but it's also useful for just high-resolution, multi-person data explorations. We were fortunate to host the governor of Maryland and the university president in reviewing various statistics about the university's computer science and data science programs. That's one of my favorite photos of our facility! Here it is, in a more scientific visualization mode. This is a VR visualization of fMRI data, so you're seeing color coding, showing how the water is moving around in the brain. The user here, whose silhouette you can see, has VR glasses which are really just the 3D movie theater glasses where you have circularly polarized -clockwise and counter-clockwise- for the left and right eye. The wall's rows of pixels are horizontally interlaced, which means one eye is seeing all of the odd rows and the other eye is seeing all of the even rows of pixels when you have the glasses on. One of our glasses has these little spheres that stick out of the side, and we have these cameras on the top the ART tracking system that's mentioned here, looking in infrared for these little balls that are very shiny in infrared. So, we can track the 3D position of that user's head. We're fortunate to have a very nice high-end network connection between this and our high-performance computing facility which we're working on leveraging and I should note that this system is built to a spec, that was developed by the by the EVL Lab at the University of Illinois-Chicago. They came up with the original cave concept and the cave 2 concept, and they licensed the cave 2 scheme with an exclusive license to Mechdyne, the company that assembled it for us. Since it was assembled for us, we've heavily re-architected it, and we've been working with Mechdyne with what we've weve done- informing them, learning from them about how to do it better. We stepped away from some of the constraints that were built into it. We made it a single computer operating the entire wall, so now we can just run any software that's built to run 3D visualizations, using what's called quad buffered stereo that would work on a single computer. We can now say, "run fullscreen", and it runs across the entire wall of 50 million pixels as though it's a single computer. In fact, under the hood, it is now is really a single computer. Making that happen for this many monitors was a technical feat. We've adapted UniCave from the University of Wisconsin; I believe that's a Unity variation, a Unity plug-in that allows Unity to work in a cave. Unity is a game development engine that's used for research, as well for creating VR environments. So, here you can see one of our projects that we've done using Unity. This is Marc Murnane. He is a full-time research staff person I mentioned in the lab. He also serves as UMBC's core facilities specialist for the photogrammetry facility, the wall, and a few other instruments. As the camera moves, it is being tracked. In this view, it really looks like the wall. You can see the edges of the individual monitors there, but it really looks like the wall is just a window into this virtual world. So, this is our Robo sim project. We're doing it in collaboration with other faculty from computer science and electrical engineering who work on robotics and artificial intelligence. We've made it so that you can have a simulated robot which thinks it's a real robot. It's being driven through the network by ROS, which is a Robot Operating System. It's the software that's used to drive real robots and it's receiving data from the virtual robot's sensors in the same way that a real robot will be receiving data from the real version of those sensors. We're able to train robots on scenes, and in particular on human robot interaction, when the robot has actually never been physically set up in that scene or environment. We're able to just have a simulated version of the robot do all the learning and then transfer the learning to a real robot. Here, you can see this is an avatar that was captured (Marc is laying in the bed there) using our photogrammetry facility. As someone who is interacting with the robot, the robot turns around, looks at the person we have rigged as a model. So, they see the actual operator in virtual reality moving around, based on the scan that we did of the person beforehand. Another VR facility that we have is our Observational VR room. These pictures barely do it justice! It's a very large space. We've been able to take advantage of the fact that the latest lighthouses, which help a VR headset- particularly the Vive headsets- know where they are in space. You can now have four-and we have ideas on how to make it more than four-cover a much larger area when you used to only be able to have two. So, it's a very large room, and two of the four walls in that very large room are green. We're able to do green screen and have people in the actual scene. This is a different Mark- Mark Torsinki. He's doing things in virtual reality and you can see him in the virtual reality in real time doing those things. Now, we're applying that setup that we've created to a wide range of use-cases. This is a project that we're doing in collaboration with Carissa Chee, a professor in our psychology department. We've scanned our real dining hall, reproduced it in virtual reality, and we have users you can see here. These are the Vive controllers, this is the virtual scene. The person has picked up food and put it on the plate. We have users go through the buffet and see what food they want to put on the plate. This is the classic experiment in psychology with food choice, where you can change the arrangement of a buffet, and people make demonstrably different choices based on the arrangement, or what caloric information is shared. Some states and localities are making policies based on that. What we're doing in that existing model of the "buffet setup" of psychology research projects is that we're comparing how people behave in VR to how they behave in the real environment. When I say how they behave, I mean we're going deep. We've got F-near on people's brains. We're looking at brain activity; we're looking at their biometrics; we're looking, of course at the review port and what they can see at any given time as their biometrics are being recorded. We're building all around the infrastructure to see what we can measure about people while they are consuming information while they're consuming and interacting with a visual scene. It's my dream as someone who's trained as a physics PhD to see this applied to more work on physical science data, the sort of data that NOAA would have. Another thing we're doing with head-mounted VR (thanks to the support of the company that makes amazing software products, called SyGlass) is that we're looking at volumetric data. So, when we do our photogrammetry scans, we have point clouds that we generate from the things that the cameras see from the outside of a thing but those cameras that are wired together to all fire together, so that they're just capturing the outside of me-they don't know anything about my liver, right? But, when you take an fMRI, or a CAT scan, or an industrial CT scan-which is like a medical CAT scan- but it's of an object usually smaller or any number of other what I would call volumetric or voxel-based 3D scans, instead of pixels or little squares making up an image, you're getting a 3D grid of tiny cubes where each cube has a value; it might be a density, or a color, or any number of values per cube. So, SyGlass allows us to interact with that data, manually tag it, and we can have humans in the loop on machine learning. So, humans can be interacting with volumetric or other 3D data and validate or flag things; then, there's a cycle on for training a computer model that can then deal with a much larger volume of data of the same type. In a very general way, the creators of SyGlass came from neuroscience, and a lot of the initial use-cases for SyGlass are for neuroscience. One of the things that I think they've been most excited about in talking with us is seeing what other disciplines and other domains we can develop interesting projects around. I'm very excited about doing things like this in atmospheric, oceanographic, and other physical sciences. We also have a Looking Glass. We are working on this, which is a little bit more early stage. The Looking Glass (the manufacturers call it a holographic or a light-filled display) is really, under the hood, a regular LCD screen with pixels and a lenticular overlay. That's the ridged plastic that you might have on a Cracker Jack box prize or the outside of a 7-11 cup, that gives you either 3D or motion as you move it, or if you put your head in just the right position. But, what this gives us (because the pixels are so small and because it's able to be updated in real-time) is the opportunity to have lots of different perspectives. You have full parallax, full ability to move your head horizontally. Each eye is getting its own image as you move that's correct for your head position, with no head tracking. It's glasses-free, it's tracking free, and you can have multiple users around it at the same time. Well, you can't tell from this little looping video- it's not very large- so getting a lot of people around it isn't plausible. It's pretty low-resolution, but it's highly interactive. So, the hand that you see moving there has been tightly integrated by a lot of developers into the leap motion sensor, which you can move your hand over and it tracks your hand position. So, this is a virtual hand that's mirroring, out of camera, a real person's hand. As they're moving their hand by, in this case the 3D model of the heart, they're able to move a cutting plane- something that's slicing away- so you can see the middle of the data by the position of their hand. In general, there's a lot of things that I hope might come from giving this talk. I hope that you have a better perspective on UMBC. Especially as the assistant vice president, who's responsible in part for making sure people know what we are and what we do, I hope you might think about collaborating with a university generally. Specifically, on this slide, I'm talking about my own labs' research interests. We're very interested in collaborating with anybody who's creating applications, specific publications, or outputs that are for their domain. Because I have a funny set of appointments, I don't need to publish in my discipline. I would love to work with people and help them publish in their discipline. They don't need to be at UMBC- I'd love to work with anybody, collaborate with anybody, and help them publish in their journals. For us, what our journals are, to the extent that we care about doing something other than that, is that we're looking at IEEE-VR, IEEE-VIS, and SIGGRAPH. We were fortunate to publish in all three this year and present at all three. ACM GROUP and ACM CSCW- our conference is about enabling group work, and I'm particularly interested in scientific collaboration, scientific collaborations over large data- maybe with an assistive AI and visualization thrown into the mix. There's some computer vision conferences that we're targeting slightly longer-term with our work in photogrammetry; and in addition to collaborations, we're also interested in finding funding for our lab or for instrumentation. Sometimes, we have collaborators who have the opportunity to ask for money to support undergraduates, but don't have an undergraduate lined up. If they wanted to work with us, and they had the opportunity to just fund an undergraduate? By asking for a supplemental- in this case, NSF grants-you can often just request an REU (Research Experience for Undergraduate) supplement. We are able to put undergraduates to excellent use in our lab; they're having great educational experiences and they're being highly productive their public. We had a student who just finished his freshman year, and has now published two papers already in competitive conferences with us! So, we'd love to work with people, to just see our work adopted by other people. So, please don't hesitate to be in touch! Come visit us at UMBC. We're just a half-hour away from Silver Spring. Come visit us; we have some things like the wall that is the size of a room, and we can't bring that with us to Silver Spring or wherever you'd rather meet. Otherwise, we're also glad to come to meet you or talk to you electronically. My contact information is there (donengle@umbc.edu). I have many people to thank! There's a lot of projects that I've touched on today, and many more that I didn't insert into the slides because we have too many to say and a short talk the only project that really I can't claim any contribution to it all is the the buffet project which is my colleagues also in the Imaging Research Center working with Dr. Chee in psychology but we've been involved in building the overall infrastructure. Mark Murnane is the research staff person who's really made all of our facilities in visualization possible. He's the full-time facilities specialist. Nicole and Ben are the PhD students who instigated and led the undergraduate project to create Science On a Sphere content over the summer that led to them coming to NOAA, and what led to me being here today. So, thank you to Nicole and Ben. The students listed here are the undergraduates currently working with me: Trevor, Max, Caroline, Daniyal, Sarah, Danilo, and Brendan! I can't do anything without all of you. Thank you so much. There's a lot of professors who are working in our facilities and collaborating with us on research, but just to name a few: Lee Boot, Suzanne Braunschweig, Frank Ferraro, Jeff Halverson, and Cynthia Matuszak. They have played very heavy roles in the projects that I had the opportunity to highlight today. I also need to thank our sponsors. We've had six NSF Major Research Instrumentation (MRI) Awards- one to establish the PI Squared (the partial cave to visualization) wall, one to create the photogrammetry facility, three to create our main high-performance computing facility, and one which has just been awarded for creating a new facility for our GPU cluster. We are fortunate to have had an NSF CS 10K Award, the undergraduate research supplement of which has supported a lot of our work. We have an NSF CC IIE Award, which allowed us to upgrade our network, enabling us enough bandwidth between our visualization facilities and our high performance computing facility, so hopefully someday we'll be driving things in real-time from the high-performance computing facility instead of from our local cluster- although, the local clusters are serving us well! We're excited about what that will enable. We've had funding from the Hrabowski Innovation Fund from the UMBC, collab interdisciplinary grant program and in-kind support from IstoViso, who are the makers of the SyGlass tool that I mentioned before; they've allowed us to work with their tools and given us a lot of their time. Thank you to them, thank you to the students, and thank you most of all to NOAA for inviting me to give this talk today. 