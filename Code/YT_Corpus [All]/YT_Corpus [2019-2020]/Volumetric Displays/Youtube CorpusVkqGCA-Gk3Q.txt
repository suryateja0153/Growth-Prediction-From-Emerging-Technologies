 Today we're lucky to have Dr Thomas Wischgoll with us he is our uh resident expert on visualization uh augmented reality virtual reality graphics um parallel processing with with uh graphics cards data science and kind of all things related to visualizing and understanding data and information and he's going to talk to us today a little bit about data science and visualization using xr which includes augmented reality and visual reality kinds of technologies so Thomas thanks very much for preparing a mini lecture for us today I will turn it over to you okay sounds good thanks Mike um I'm gonna share my entire screen um so I'm not sure if I can see comments um if you have an urgent question feel free to just interrupt me verbally so thanks for joining me I wanted to give a little brief introduction to visualization using different display systems um and it's going to be very brief because well you only want to give it a little bit of an overview and so therefore the there's to go into all the details um but in generally in general if you think of visualization uh it follows the usual idea um what people say a lot this uh picture says is worth a thousand words and so visualization is essentially following that concept uh that we're trying to generate images to provide some insight into whatever data it is you're currently looking at and take note of the the plural in the word images because usually it's not just a single image that we're trying to generate a big part of visualization is uh to come up with an interactive system that people can use to explore the data so it's not just a single images image but a sequence of images and in order to derive the visualization usually the process involves a sequence of steps uh that we have to go through different operations to get from the actual data to the final visualization or the system that allows us to interact with those visualizations and in visualization therefore you see the same concept that you'll encounter in computer science almost everywhere is this divide and conquer idea so we're trying to break up the overall problem into smaller chunks that are essentially more manageable so small enough so that you can more easily wrap your head around it and basically address and solve them and come up with these individual components that we then can piece together to form the overall the entire visualization system and from a software engineering aspect the advantage of going that approach using this approach as well is that we can come up with modular components that ideally we can reuse in other visualizations for example if you have something that loads a data set that may be something you can reuse for a different visualization system that you develop later on other projects so then these modules in order to combine them to form the uh the overall system uh usually they're combined in some form of a pipeline so that basically the data flows through this entire pipeline and gets transformed or manipulated in a certain way so that we uh ultimately end up with the uh the final visualization the output image for that matter or sequence of images it's supposed to be interactive and this sequence of data transformations then essentially is referred to as the visualization process this visualization pipeline and the word pipeline is probably a little too simplistic of a term because it doesn't have to be just a linear sequence of modules that be kind of put together uh the pipeline in that sense can branch off or merge so it can be relatively complex in the end depending on what your visualization system is supposed to be doing This shows you a very simplistic example of such a pipeline but I think it gets the idea across what it is we're trying to do so you see on the left side the top side the raw data so that's essentially the data that I don't know somebody gave to you or you generated in some way and essentially the data that we're trying to now visualize so then of course as the first step what we would have to do is import it into memory so that we can manipulate it and do stuff with it so that leads us then to this imported data set in this case for example a polygonal data set bunch of triangles representing some form of geometry and there may be data associated associated with all these different vertices different spots where all these triangle connects of course that completely depends on what data you are you're using right now then the next step is usually an important uh step uh this data enrichment transformation resampling step where we usually try to uh generate additional information that we can then use and feed into the subsequent steps of the visualization pipeline to make the visualization make things look nicer or to enable us to use specific visualization algorithms for example the resampling part the the volume rendering algorithm that I'm going to talk about in just a little bit usually involves or requires you to resample the data set if it's not given in a regular representation that it basically uh expects and for those cases you will have to essentially resample the data set usually something you would want to avoid but in those cases uh can't get around it but oftentimes the data set is already in the proper format and therefore we don't necessarily have to resample so that then of course gets us this enriched data set which is of course still similar to what we had originally this polygonal structure the bunch of triangles that represents the geometry but you can see already that we calculated an additional entity the color just to visualize the data set at this step and I'm not sure with the compression going on if you can see these little blue arrows all around that's also computed in this data enrichment step essentially which in this case is the so-called normal vector so essentially a bunch of vectors that are orthogonal to in this case the outside surface of our data set you see these being computed quite a lot in visualization because from a computer graphics perspective those are needed to essentially make things look pretty uh allows us to render things um to look more natural the way we basically usually encounter things in real life so these vectors these so-called normal vectors are actually pretty useful in in a lot of different examples for visualization I will show you uh the difference in um in the live demo that I I'm going to show afterwards so now that we then have enriched the data and have generated maybe some scalar entities these normal vectors then we can actually map the data into some visual representation that allows the user to essentially interpret the data usually involves some kind of abstraction from the actual data some geometric representation so you can see from this blobby rendering here that in this case we're still using the same data set we still have this spherical overall shape but now we're essentially bumped it out in direction of the normal vector uh based on the scalar entity that we that we calculated so it actually uh changed the overall surface representation to encode the underlying the underlying data so that gives you another example of where these normal vectors can actually be used for and then of course finally we actually render the data to generate the final image which is essentially something along these lines here um it just I had to throw the final image in on the other side to actually showcase the individual representations but that essentially would be something like the final uh visualization of this particular data set of course fairly simple example just to give you something uh to see what this is about so the example I'm gonna showcase today is essentially volume rendering and uh uh it can be applied to a whole bunch of different examples and I'm showing a few uh on these slides I hope you can at least recognize some of those through for webex and all the compression that comes with it the canonical example that initially was used I don't want to say only for that example but a lot was the medical example medical data such as Ct or MRI scan or PET scans or any of those usually have some kind of a volumetric representation which is basically a regular grid that we have data points at the individual vertices of that particular grid so it's pretty much similar very similar to just an um just an image where we essentially have just a rectangular grid with pixel values at each data point uh so it's a simple essentially a sampling of that of a rectangle where we have in case of the image color values and the only difference between the volume is that it's basically the same thing blown up into 3d so we have the gene image at front stacked right behind that is another image then another image and so forth to give you the same idea same concept just in in 3d you can see the medical example shown right here which is essentially the hips and part of the torso then volume rendering was used to that particular particular scan there's a lot of other examples that you can apply this to as well so anything that has this fairly regular structure this regular grid with sample data points at each of those grid cells or quick points if you will for all of those data sets you can basically use this or as I mentioned uh when I talked about the pipeline you can always resample the data to get it into that structure and then apply volume rendering has a little little caveat with resampling um essentially doing multiple interpolations if you do so and that can create artifacts so you have to be a little careful and aware of that but it applies to all sorts of different examples you can see just a few so I have fluid dynamics listed here so the top right corner is lattice boltzmann simulation lattice boltzmann is essentially a simple way of doing CFD type simulations um usually more from a visual perspective but if you use a fine enough grade it works up well for more complex complex things as well so you can use that for for those examples astronomical data right below that so this is essentially a supernova and you can use volume rendering for that so you just pick a scalar entity and we have to pick a certain color scheme I'll talk about that in just a second to essentially make this look good so there's quite a bit that goes into making the volume rendering um look decent uh basic uh in addition to the the concept concept of the algorithm itself uh there's other examples as well so you can see a water molecule the rendering here essentially the the field representation so you can use a volume rendering for that uh you can scan your uh your engines scanner would do for like an engine part or the whole whole engine if you want to um since there's usually metal involved the uh basically x-rays and um metal doesn't really uh work too well together because the x-rays get scattered by the metal but uh since the med the engine is a dead part uh it's not a live human for uh for that matter uh we can usually crank up the x-ray source and uh get these results but same idea as a CT scan essentially that you can use for engine parts and uh so therefore the data that you get is following the same format so you can apply the same rendering techniques there's a whole bunch of different algorithms that you can apply to or that you can use to do volume rendering I'm not going to go into into any detail at this point um my class actually spends a few weeks on just volume rendering alone so it's it's it's quite a bit involved in terms of all the different algorithms but the basic concept behind all of those is essentially what you see in this this simple picture here so we have on the left side a light source that basically sheds out light sun or whatever you want to consider that and then this model right here is essentially the volume that we're trying to uh to render so I actually use volume rendering to show you uh generate this particular image and then originally the data consists of basically just scalar values that represents the particular sampled data at a specific point in 3d space so there's usually no color value or anything associated with it so that's a step which i'm going to talk about in the next slide so we kind of associate some kind of a color value and transparency value to each each of those and once we've done that we have a somewhat translucent blend of of data if you want to think of this conceptually and when we shed light through that then we can uh depending on what transparency values or how we basically define these for all these sample points uh then we can essentially see the accumulated light that basically allows or is allowed to get through this uh this entire mass and then we basically use that to project that onto this particular image so if you think of this as like a very pointed light source sheds out a lot of light and we measure in the end how that how much of that light makes it through and what kind of color it will have after we accumulated all the light passing through all these different sample points based on the transparency value of course and color value for that matter some light will basically be eliminated or cut down or filtered out depending on how you want to look at it and we're essentially looking for the residual value that makes it through onto the screen which then gives us that volume rendered uh image in the end so it's essentially the combination of how much light makes it through plus the projection onto our screen for that matter and so the tricky part is how do we actually associate color values to uh to the individual samples that we encounter so usually that's where essentially these so-called transfer functions come in so as I pointed out before the volume volumetric data is essentially represented as a regular grid and at each grid point we have a data value again similar to an image just blown up into 3d and this grid is just an artificial construct that we use to basically represent our data if you think of the medical example um then we take a scan of I don't know the whole body or a body part and this grid of course if you think about this conceptually has nothing the human body human body couldn't care less about what we superimpose on it it's just our way of representing that particular data and then the particular data values are usually then some kind of scalar value it doesn't necessarily have to for the volume rendering we have to break it down to a single data value so we can manipulate different data types in different ways but ultimately we'll have to break it down to a scalar entity some point or something at least that we can convert into color and transparency values so with a medical example if you stick with that uh typically you see scalar values with ranges um which are basically represented as 12-bit integers so if you get a medical data set such as the CT or MRI scan that's usually how it is how it is represented and then we have to take that scalar entity and associate colors and transparency values to it and again that's what this transfer function is supposed to be doing you see on the right side here that's actually the transfer function I used to generate the volume rendering on the previous image this one right here so it's the exact same transfer value a transfer function to generate that specific image and I'll show you that uh live in uh just a second once I'm well a few minutes uh once I'm done with my slides so essentially the way this is structured is this range here on the x-axis is all the data values that are uh that occur in uh in our data sets so for example the entire 12-bit range of data values that can technically occur and then on this axis we have the transparency value so how transparent a particular data value is supposed to be so at the bottom it's completely transparent so all the light basically passes through and at the top it's completely opaque so none of the light passes through it somewhere in the middle then we get semi-transparent so essentially if you looked up a particular data value right here if you go up then this tells you tells us exactly what transparency value we want to associate with that particular data value so everywhere this data value occurs we're essentially using that specific transparency and then this bottom part here represents the the color encoding the color value that we're assigning to this particular uh data value so same idea here we look up this value here this scalar entity that comes from the the actual data and then we look up here the actual color that we want to associate with it so in this case it's just a color rainbow going from blue to red and we can see these little points here we can actually move those to manipulate the transfer function so that we can essentially change the range of colors how they are associated to the individual individual data values and that's essentially the the key element that we need to convert the color value the intensity values to color and transparency values and then that gets applied to basically every data value and then if I go back to my previous slide we do this shiny light through projection idea to basically map the whole volume onto a single single plane so that we can essentially render it on on a computer screen so then once we are capable of doing that we can essentially play around with this and use this kind of a of an idea on all sorts of different systems obviously we can use a desktop system that's fairly easy pretty straightforward we could use head mounted displays as well like a vive or oculus we can use cave type systems so we see me on the right side here standing in one of them so these cave type systems are it's essentially a full room in this case it's like a 10 by 10 footprint that you can just walk through just like through any other room the difference here is though that as you can see here this image is basically projected on all walls right here so all the walls uh at least four of them are basically projection surfaces and there's a smooth transition from one wall to the next or the wall to the floor so there's a seamless uh image basically showing that popping up in front of you and on top of that the system actually is capable of figuring out where you as the user is wearing the 3d glasses is actually standing so there's two components here so there's the tracking part now you see kind of this purple reddish glow those are actually cameras capturing me uh and calculating the 3d position and then the stereo glasses the system actually shows two images at the same well not exactly the same time alternating in an alternating fashion synced up with the uh with the stereo glasses so that I can actually see different images with my left and right eye so I actually perceive things in 3d when I'm in there and then the system because it knows where my head is located at it can recreate that exact perspective so on this on the simple picture here it doesn't really look very spectacular simply because you don't have that 3d perception the camera just can't convey that and well powerpoint for that matter can't either and but for me standing in there I can actually see this in this case the CT scan floating right in front of me and I see it perfectly in in 3d and because of the 3d tracking because the system knows where my head is and this allows able to recreate that exact perspective uh from my viewpoint uh wherever my head is at the time the data basically now floats right in front of me and I can literally stick my head in there and look around and it'll stay right there um and I can inspect all sorts of different uh aspects of the data so it's actually really cool uh experience if you ever get to Wright State and once we're back open you should uh basically try it out sometimes because uh the image doesn't do it any justice really it's uh it's quite an impressive experience just to see it for yourself and uh similarly we can of course use other displays so this shows here an augmented reality display or device which is a similar or follows a similar idea than a head monitor displays we have basically two screens in front of your eyes the difference is now those are translucent so uh it can basically superimpose um an image onto the real world so you're not completely shut off the real world it just adds hence augmented reality it adds elements to the real world scene so that you can show enhancements or additional information on top of the real world so it makes it a pretty cool interface and I'm currently working on a volume rendering application for that I can't show you this to you because my the surface I'm using here doesn't have the graphics capability to do a live demonstration with that unfortunately otherwise you you would basically be able to see exactly as I'm wearing the headset I can project it onto even through webex um but it needs one of a more powerful ATI or an Nvidia or AMD and Nvidia graphics card for that to uh to work properly which the surface does not have unfortunate but the basic point is once we're once we have a handle on this volume rendering we can do uh use this on all sorts of different displays we can use uh the cave-type display as you can see here we can use our tile displays basically large display um and display it on there so pretty much anything you want to do with it which is really powerful and gives you a lot of uh interactions with this particular data so you can actually see for sure or for yourself what's in there and how it basically looks maybe from a doctor's perspective what's wrong uh and what needs to be fixed in a specific specific patient's case so with that I want to switch over to basically a the demo part so this is a software that is basically capable of doing volume rendering already so it's called paraview you can download it yourself it's basically freely available it's actually open source because it's built on vtk um and all I've done so far is basically just load the data set I haven't done anything else just open the data set um it's not like it takes a whole lot of time but I figured uh that's a really boring part that I don't need to talk I'll show you in the in the presentation so right now it doesn't really show you anything exciting it's basically just just a cube for that matter or that we can look at and doesn't resemble the volume all that much at all and the reason for that is simply because right now the representation that we picked is set to outline which is just the extent of the the overall data as far as whatever we scanned in there it's kind of meaningless but from a computer science perspective well we always care about how data because that's our bread and butter that's how we can apply algorithms and manipulate data so for us it's important if I go to the information part you can see a little more about what the data is actually about uh so you can see that we have data values ranging from minus 1024 to 1640 uh so it gives you a little bit of an idea of uh what data you can expect uh very rudimentary of course and then it gives you an idea of the dimensions of the data set and I loaded this as a dicom file so in this case the dicom files magical scanners for that matter are always nicely calibrated so they know exactly the dimensions of the data how far apart points are for example that's what it shows down here basically this is the overall dimension for the entire data set but this is an actual dimensional unit so it's actually in the metric system metric units um so if I go back to the properties so the interesting part of course is now the volume rendering so therefore let's switch over to actually representing the volume and there we go so now I hope you can see this a little bit so this is uh a similar scan that I showed you uh in the the cave system and on the slide as well uh so this is it's a scan of a heart and in this case a pig heart though not a human heart but this already shows you that or the importance of why we actually need a transfer function so this now shows you basically just the outside of the the scanned part in this case um um well it's basically the way uh the way this was scanned um we kind of like limited to this the cylindrical uh shape that's just part of the scanner section so we selected a subsection of what it's supposed to be storing and usually this outside representation is probably the most uninteresting part within this data because if you wanted to see the outside well we would look at the outside so stick with the medical example the doctor doesn't want to see the skin on on a patient on the ct scan because well if the doctor wanted to examine the skin you or she would just examine the skin because it's readily available you want to see what's inside that's the motivation of why you would do a CT or MRI scan so what we would like to do then is basically tweak these transparency values so that this outside part of all the material on the outside is basically transparent so that we can actually look inside and all we have to do is basically manipulate these points so if I click in here it creates a new data point on our transfer function I can now move this around and now this whole section from the very left to that point that i just created is now completely transparent and now we can actually see what's inside of our data so in this case now we can actually see that it is in fact hard you see a little bit of the heart you see the bones the ribs you see some of the the pulmonary vessels right here so this gives you a lot of insight and the nice thing is the interactivity part with visualization that I can now manipulate this in any way I like to include for example more data I can create a new point right here for example to make the cutoff a little sharper and that essentially allows me to interactively drill down on whatever I'm looking for usually medical data is very well calibrated so that I don't necessarily have to now that I essentially know what kind of tissue values correspond to what uh intensity values but um still depending on because it's overlapped between different tissues oftentimes you still have to tweak things a little bit this also allows you to select kind of like color values or color mappings for example so we could for example use this one to pick a different color value color range you can see now it edit all these dots and again i can move those around to adjust essentially the way it maps the colors to the intensity values um we talked about when we we looked at the the pipeline the visualization pipeline we looked at enhancements or data enrichment phase particularly the normal vectors and you can essentially do the same thing with these volumetric data sets even though we don't have a necessary surface representation I can still calculate normal vectors people use the gradient which is essentially the highest direction of the highest change in material which is usually a good representation of a surface dish type and when I do this now it actually looks a lot nicer now we can actually use computer graphics to make this look much more like a surface or real object so we can see a lot more now and you can play with this a little more and enhance the rendering phase itself used as ray tracing which is a pretty fancy way of rendering things a pretty computationally it's an intensive way as well um so that makes this look even better so you can see now that it almost looks like a real object so you can actually tweak these things in such a way that it looks fairly realistic it doesn't have all the bloody stuff so that probably makes it better from that perspective but it allows you to actually do pretty fancy renderings to make this look really realistic you can even uh adjusted for example uh we can add shadows to which of course radical data that may not make as much sense but it shows you uh from a graphics perspective all the cool things you can do uh so now actually geometry that we're looking at casts shadows onto itself uh it makes it even more realistic from an object perspective or rendering perspective um that's basically what we use if you have a natural scene well physics basically does this type of lighting all the time um automatically that's just uh the way nature is so from a rendering quality perspective there's a lot of fancy things you can do and even this little surface here can keep up with this pretty well considering how rendering or computationally intensive uh ray tracing actually is and uh that was the gist of what I'm trying to get across um I hope you're still with me and uh if you have any questions feel free to ask directly or type them in fantastic that was really interesting while you're getting back I'll start us off with a question so first of all um so uh those of us those of you who are joining us um feel free to type into the chat window if you'd like to ask Dr. Wischgoll a question um or in a second I'll I'll see if anybody wants to just unmute if you've got a really long question sometimes it's that easier just to ask them um verbally so uh really interesting you know it I kind of think about um I never really thought about the fact that in medical imaging you've got returns you've got data for every single area and you've got to mask a lot of it out to get down to the things that you're interested in um and so I can see that that's got uh all kinds of uses for different kinds of medical data is there do you do the same kind of things for other kinds of data is there like um I don't know whether modeling or atmospheric data are there other it seems like biomedicine is probably the main application for volumetric rendering but are there other ones out there that are commonly in use definitely interestingly enough uh volume rendering wasn't developed for medical stuff it was really it was modeled for cloud rendering it's a computer graphic principle because cloud are essentially a volumetric mass with uh different if you sample it with different uh humidity values in that sense and then you use that exact same clouds and that gives you pretty sense ex results because it models the physics behind it right right but yeah you can use it for any kind of atmospheric modeling as well a lot of weather data is projected onto the surface if you look at a radar or image data with radar you could probably get 3d data it gets pretty complex big for that matter have people tried to do volumetric rendering with with non-physical data like if I've got a big bioinformatics data set and I project it into three dimensions and then look for you know with certain settings on thresholds look for globs that show up in those three dimensions does it tell me something interesting about my original data I'm not sure I've seen it with volume rendering I know the uh what was the TRD algorithm they um I've seen uh Derek's student uh use at least image representations um in a similar way to represent the the results that he was looking at to figure out what's basically going on right right usually for this type of data you probably would stick with 2d more because it makes it a little easier to see from just from visual computational or visually processing the data so it makes it a little easier for us humans to interpret the data right let me say one last thing you're welcome to ask in addition to questions about um graphics visualization augmented reality the specific topics I can tell you that um that Thomas is also an expert on using gpus for various tasks and for um and on we didn't talk about it too much today but all kinds of augmented and virtual reality platforms and i'm also happy to answer any questions that you might have about wright state or classes or preparing to do work in graphics what classes in math you need to know that sort of thing so yeah any questions that we can answer for you um while we're all together on the topic of math as you can imagine all the things that talked about they all involve math to various degrees uh so math is actually important the nice thing about all this this graphics and visualization stuff is um from math perspective you can actually see what it's good for I usually start my graphics class with like a refresher for all the math stuff and I try to use uh graphical representations and um oftentimes the students have kind of this aha moment kind of oh this is what this is used for and this is why this behaves like this right right exactly and why yeah yeah graphics is one of those fantastic domains where the math sometimes clicks and you understand you know what what it's there for in the first place and how it relates to reality in a way that actually makes it useful and interesting um and yeah is often I think like an aha moment for students to say oh now this is really interesting math it's not just a set of equations that don't attach to the real world in any way yeah thank you very much Thomas I appreciate it um been very interesting I hope that we'll see you back next week to hear about uh machine learning and health care and thanks everyone for being here yep thanks everyone 