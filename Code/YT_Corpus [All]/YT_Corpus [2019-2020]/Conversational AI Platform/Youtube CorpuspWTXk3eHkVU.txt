 Welcome, everybody, to this Open Learning Talk on the relationships between AI and education with Cynthia Breazeal from the Media Lab here at MIT. In case any of you are wondering, this is a virtual background. I'm not at MIT. I'm in my home office. And we are joining you remotely, but good to be with you today and good to have you here today. I'll introduce Cynthia and the topic in just a second. Just a few housekeeping notes. The format for today will be that Cynthia will give a little bit of an overview of some of the work that she does on AI and education-- that is, the role of AI in education and people learning about AI, as well, kids learning about AI, I think through some videos spotlighting some of her work. And then she and I will have a little bit of a conversation about this work and its impact on both research and learning. And then we will open it up to you all for some questions. There'll be two ways for you to ask questions, if you'd like. The first one is that you can use the Raise Hand feature. And if you raise your hand, we will try to queue up people who would like to ask questions, and we will spotlight you and bring you in so that you can ask your question on camera. But you can also use the Q&A feature, which you can also find at the bottom of your screen to also ask questions there. And then, we will sort of curate some of those questions and be able to call upon those. So those, you're actually welcome to do at any time. We'll do the raising hands later. I'll specifically call on that. But if you want to put things in the Q&A, you can feel free to do that at any time. So with that, it is my pleasure to introduce Cynthia Breazeal, who's a professor at the Media Lab, where she founded and directs the Personal Robots Group. She's also associate director of the Media Lab in charge of new strategic initiatives. And she's spearheading MIT'S K-12 education initiative on AI, which is a collaboration between the Media Lab, Open Learning, and the Schwarzman College of Computing. She's really a pioneer in the field of social robotics and human-robot interactions. Her robots have achieved both academic research impact, as well as mainstream appeal, which I think is what's really interesting about Cynthia's research and body of work is that it has both impacts on the fields of research, as well as on the lives of many people around the world. Her award-winning Jibo robot was featured on Time Magazine's Best Inventions, on the cover, several years ago. She's a AAAI fellow and is a globally recognized innovator, designer, and entrepreneur recognized by many organizations, from the National Academy of Engineering to Technology Review. She received her doctorate from MIT in ECS in 2000. So with that, it's my pleasure to introduce Cynthia and to introduce her work on AI and education. Great. Hello, everyone. It's great to join you. I can see people are joining us from all over the world, so thank you for either staying up late or getting up early, as the case may be, to join us here today. So I know the format, it's intended to be a little more interactive, a little more conversational. I was planning on literally just showing a couple of videos so you could get a better sense of what the nature and experience of these artifacts are that I create. Then I ended up creating a little slide deck, so I guess I am going to do slides, after all. But it's really meant just to give you a little flavor and texture to what I do, so when we get into the Q&A, you have just a little more context for what it is what we're talking about. So I'm going to share my screen and just, again, run through a few slides with some videos, as promised. Can everybody see? Eric, can, maybe, you give me-- Yep. OK, good. So I wanted to give you just a little bit of the backstory and then, yes, we'll absolutely get into the meat of the intersection of AI and education and learning. So as you could probably tell from the introductory remarks that Eric-- and thank you, Eric-- my claim to fame, so to speak, is actually in robotics and artificial intelligence. And so I'm recognized as being an early pioneer in this particular kind of technology called social robots. And so people ask me what a social robot is. It's like, I say, think about R2-D2, right? It was like this dream, the vision of robots that we could interact with in an interpersonal way, that we could collaborate with, that could express and communicate a natural emotion, in C-3PO's case language and all of that. So that's always been the vision. And growing up, I always kind of was given that promise as a child, that someday, we'd live with robots in this way. And of course, eventually, I became a graduate student and realized those robots didn't exist yet. So that really became a lot of my life's work. So this slide is just to kind of set the context for there are a lot of, I think, high-impact cases for a new kind of technology that can engage us in this very interpersonal way, social-emotional way, particularly where there are domains where we know that high-touch interpersonal support, social support, emotional support, is important for people to achieve their best outcomes. And so we can look at a number of big societal, global challenges-- global aging, global health and chronic disease management, personalized high-quality education for all-- and just appreciating that we literally can't train enough people to meet, in many cases in these areas, a growing demand. And even if you could, it's expensive. So is there a way that we can create a technology that can help bridge the gap, that can help augment our human support and care networks in a way that really makes sense and respect our values? So this is just a slide to say so much of what I'm going to be talking about today is really motivated by this vision. But I will be focusing particularly on children in this talk and learning. So, of course, now we live in this very different time where these talking speakers and children now are interacting with AI every day. And I was designing these very sophisticated AI technologies in these application domains of health and education, and it just occurred to me, people, in general, they don't really understand these technologies. And as you'll learn, they can have profound influence on people-- people's behaviors, people's ideas-- in beneficial ways but potentially not beneficial ways. And it became increasingly poignant to me that we really need to help people understand these technologies because the genie is kind of out of the bottle. So I started to transition from AIs that help kids learn, help people learn, to really thinking a lot about AI literacy for all. We really need many more people to understand AI enough to be able to live with it and make decisions around it and shape it. So that's just the preamble for why I'm here, talking about the work that I do, where, because I do a lot of work in social robots and so forth, I just have this sort of deep appreciation of the human experience and how important social and emotional interaction and engagement is for us, as human beings, to engage and to unlock our human potential. The field of AI has tended to be very biased towards the cognitive-- decision-making, decision support. But so much of what we know when we learn about the psychology, social-emotional is extremely important. So this is guided by this vision of if you're going to design technologies that really help engage people in a way that we can unlock our human potential to become who we aspire to be, you need to do it in a way that supports the holistic human experience. So that's the punch line for the nature of work that I do. So whether that is learning with AI-- how does that AI experience support a more holistic human experience for that deeper engagement, or learning about AI-- how do we engage people of various ages around these concepts and societal impacts and so forth that they are engaged, and they can learn about it and discuss it in rich, collaborative ways? So it straddles, I's say, both of those objectives. So I want to just kind of give a quick little highlight of the learning companion work. So these are social robots that engage with young children. We are particularly looking at early childhood education where, in the United States, in particular, we're looking at early language and literacy. And the reasons are very simple. It's like if we look at the United States, we don't have a high-quality national early childhood learning program. And so there are many, many kids-- over half of our children don't attend a quality preschool program and, as a result, don't enter even kindergarten ready to learn. And there's been so much science and data showing that if you don't even start kindergarten ready to learn, it's very difficult, very expensive, to catch up. So earlier intervention-- can we create an earlier intervention that can support parents, support early childhood educators, in a scalable, affordable way? And that is where AI and technology kind of comes in. It's the scalable, affordable way of augmenting, so hugely important national priority in education, at least within the United States, hugely important time for learning. The challenge is how do you create an AI experience that is well matched to how young children learn when so much of it, of course, is around play and collaboration and face-to-face interaction? So it's a very different kind of technology you're going to want to create. And you're going to want to create it in a way that it's not like our devices today where, once your face goes in, you kind of shut everyone out. You want a technology that encourages group interpersonal interaction among many. So social robots became kind of an intriguing different kind of experience, AI experience, that helped support this. And so we've been very inspired, interestingly, very inspired by peer-to-peer learning. So there's a lot of work in AI in education has been on computer tutors, where the computer is the expert. We actually package that more as a peer-like companion because peer-to-peer interaction is very powerful learning. Because there are times when you can teach your friend. You may know more. There's times when your friend may know more than you and can teach you. And then, sometimes, you just explore together. So in terms of this kind of dynamic, it's a really fascinating one. And the model under the hood, the AI model, could be having a lot of expertise. But the experience to the child is kind of like I call the Disney sidekick. It's the fun, emotionally engaging, almost like a companion animal, not judging you but just playing and engaging and having fun with you, but really scaffolding learning behind the scenes. So this is the paradigm that we've been exploring, where, now, how can you take that inspiration of peer-to-peer learning and now have a robot-- in this case, a fluffy, appealing, child-friendly robot-- play educational games, often on a tablet computer, where you have a lot of content and so forth, but the robot plays with you as, again, a peer-like companion and is personalizing to children over time? So now, we're also now living, obviously, in this time of COVID. And just, again, the poignancy of thinking about technologies that can support early child learning in remote contexts is becoming even more like, oh, my goodness, this is really important. We've been deploying these kinds of systems in classrooms. We often work with, certainly, schools, communities, where additional support, engagement, is very beneficial, lower SES, often, communities. And now, we're actually starting to deploy these technologies in homes, as well-- so this is an example of a social robot with a tablet, kind of built together as a nice unit-- but for parents and kids to do together in the home context. So we're thinking a lot about, again, beyond classrooms. Classrooms are important but also home. And especially for preschool, home engagement, the earlier the intervention, the better. So of this was really just getting to the point where I wanted to show you this video-- [LAUGHING] --so you can kind of get a sense of what this interaction really looks like. So again, I mentioned we go into a lot of public schools, a lot of English second language learners, again, the kids who could benefit, obviously, the most from having some additional support in an emotionally engaging way. So this is a case of a little girl. So she is attending a preschool program. So she's kind of in this separate little room that we're able to work with her. There's a learning companion robot. They're playing a game together. So this game is kind of inspired by the game of I Spy. So we call it Word Quest. So there are challenge words. And so these are not easy words. In the example of this game, it's "lavender." And the little girl and the robot collaborate, taking turns to try to identify objects in these scenes that they can kind of scroll through to pick things that match the challenge word "lavender." So that's the nature of the game, and they take turns. So I want to just let this video play so you can see this peer-like dynamic. But also, I don't know how well this video will play as it's being streamed, but there are some real, notable, emotionally resonant moments. And I'm going to talk about them after you see this video. So first it's the little girl's turn. [VIDEO PLAYBACK] - What are we trying to find? - We are trying to find lavender color stuff. - OK. We've got-- - That's not lavender. - --was lavender. - Yes. [INAUDIBLE] is lavender [INAUDIBLE].. Pearl. Something. [DESCENDING TONES] - Sure you will do better next time. I believe in you. - The robot's turn. - Time to perform. Lavender is purple. - Yes, just like my friend loves lavender. Ooh, [INAUDIBLE]. - Flower. - Looks like [INAUDIBLE]. [CHIME] - The flower is purple. - Yes. - It has the color lavendar. - Yes. [END PLAYBACK] OK, so I hope that kind of played well. There is actually a lot going on in that video. So the first thing I hope you can appreciate is just the nonverbal communication that is fluidly happening between the child and the robot. There's a moment where the little girl makes a choice, and she actually gets it wrong. And she looks a little disappointed, dejected. And the robot literally kind of leans towards her, like this affiliative gesture, and she comes right back into the interaction. So even though it's the robot's turn, she is really invested in that process. You can see that the robot is actually, like a peer, it's flipping between scaffolding and allowing the child to demonstrate her expertise. So it's a adaptive peer model. And that's actually learned through reinforcement learning. So this is, again, where AI comes to play. The robot has actually played many of these games with many children to capture a corpus of these interactions and actually is learning a personalized model for how to engage each child as this blend between when to act as a more knowing tutor or a less knowing companion, [? 2T, ?] to give the child a chance to teach the robot, to go back and forth. So again, this is AI under the hood. And we can show-- we've done randomized controlled trials, as we often do to evaluate these technologies-- the tutor-only robot versus the [? 2T-only ?] robot versus the adaptive peer robot, children learn best and retain what they learn more with the adaptive peer. So again, this is an intriguing different take in a different direction, all of this. The other fascinating thing-- I mean, we know already from the human psychology literature, the better the relationship, the better rapport, the better relationship between a teacher and a student or between peers. Often, the better engagement, the better learning. We are starting to see the same evidence in child-robot interaction. So we actually measure the quality of relationship and how that corresponds to learning. So we are starting to see these correlations. And the more positive the relationship, the child tends to have higher vocabulary scores. But interestingly, that's enhanced even more through personalization. So it's a personalization adds this power boost to [INAUDIBLE] the relationship. So anyway, it's really fascinating stuff because it's really at this intersection of human psychology engagement and AI technology. And we're learning a lot. So I'm going to wrap up this part of the talk. But the punch line I hope you appreciate, we're not trying to build technologies that replace teachers or compete with parents. These are fluffy, pet-like robots. But they can engage children at this sort of interaction where there's aspects like a motivating ally, like a friend, there are aspects like a cool technology, inter-connected, whatever, and there's aspects of this companion animal. And this companion animal, this nonjudgmental companion animal, gives this nature of this relationship a very different flavor where children, even if they're embarrassed to make, maybe, mistakes in front of their teacher or their friends, they seem not to be with the robot. And you can't learn if you're not willing to take learning risks. So anyway, it's as if the robots kind of give them permission to do that and feel OK about it. In fact, we actually have the robot intentionally make mistakes in order to model things like growth mindset. How do you show perseverance through these things? And we're seeing children actually socially learn these kinds of attitudes, as well, which we can get into now when we talk about the whole other side of what's the implications of all this? Because the engagement is deep. It's not superficial-- the child just likes it. These are deeper things at work here, and that's what we're starting to understand. So as we started uncovering this, again, I started kind of going, it's really important people start to understand what's going on here so they can be informed users, future designers, of these technologies. Because this is kind of the future that we're living in. So again, we kind of know the story here. AI, it's an exciting technology. It's a disruptive technology. It's changing the nature of work and workforce. It's generating new jobs and opportunities. We want a much more diverse, inclusive group of people being able to participate in shaping this future. That's kind of one big point. The second one is that, as these technologies have been rapidly proliferating, it hasn't been going, necessarily, perfectly smoothly. So we're starting to see examples of these deep learning systems, when they're trained on data sets that encode the kind of biases of society, let alone the decisions these systems are making, continue those biases. And we don't want that. We don't want to be privileging certain groups over others. So people need to understand this. We need to understand how to design some of these unintended things don't happen. So within MIT now, more broadly-- so Eric talked about this-- I'm leading a MIT-wide effort between Media Lab, the Schwarzman College of Computing, and Open Learning, where a lot of committed people are saying, we really want to step up and kind of address this opportunity of AI literacy for everyone so that everyone has a chance to not only be responsible, conscientious users of AI technologies, they can participate in democratic processes and decisions around them, and they can become the ethical future designers of these systems, too, should they choose to do so. So we just want a much more diverse and inclusive group of people to be able to do that. So we've been working on these kind of new, thematic curriculum. A lot of our work has been focusing on middle school, but we've certainly done work in high school and preschool and everything in between. But a lot of our focus has been on middle school. We call this emerging curriculum Responsible AI for Computational Action. So what's kind of new and exciting about it is seamlessly dovetails, obviously, computational thinking with AI concepts, technical aspects of it, ethical implications and design principles of it, and design thinking. So as you're learning about these things, you're also appreciating how it's impacting the world around you and how you could potentially design and make with these technologies, create with these technologies, to help improve your community. So there's a very constructionist, collaborative flavor to everything that we're doing. It's very project-based. Well, so how do you empower kids to create things with AI? You're not going to put a middle schooler on TensorFlow and say, good luck, right? So the team, it's like MIT is the home base for things like Scratch and App Inventor. So we're taking these more advanced AI methods and curriculum and concepts and augmenting these platforms to actually empower kids to use these AI [? techniques, ?] to learn about them and then use them to design projects of their own passion and actually port them-- kind of download them-- to different kinds of platform. So whether it's a browser on your laptop or an Android phone, a robot, an Alexa, it's like these are the kinds of things you want to empower kids to do. So as I kind of just wrap up this quick, whirlwind tour, I wanted to give you an example of an expansion we've done to the Scratch programming language, that we've done a custom, our own version of it, where it's all-around gestural interfaces. So kids can use Scratch blocks that can recognize facial expressions and hand gestures and so forth, so just thinking about the creative opportunity around that. So it's called PoseBlocks. So here's an example of the ways we're kind of expanding these tools and platforms. [VIDEO PLAYBACK] - PoseBlocks is a new toolkit for students to create body tracking and motion interactive projects while learning about artificial intelligence. AI systems like Kinect games, Snapchat augmented reality lenses, and Instagram's AR filters are some of the most engaging ways students experience AI in their everyday lives. And many students have passions around sports and dance that we can use as entryways to AI concepts. The PoseBlocks toolkit introduces a suite of block-based programming tools that students use to conceptualize, design, build, and reflect on interactive physical movement-based projects. To support this, we've developed a suite of new block-based coding extensions for our online editor built on top of the open source Scratch programming library. A new Google Teachable Machine integration allows students to quickly train their own image, pose, or audio classifier models and bring them into their own coding projects to build interactive AI-powered projects of their own devising. Here, we're using Teachable Machine to train a new recycling detector, which can tell us whether we're holding glass, paper, or plastic. And since all model training and execution are done on the client side, and only these model weights are exported, student privacy is preserved. Now we take this model into our project and respond to the model's image detection-- looks like glass, looks like paper. This lets students make projects like these. Three more sets of new blocks allow students to create projects with hand, face, and body tracking machine learning models. These blocks allow you to track body parts, finger points, even face parts, and respond to expressions and emotions that are sensed. That allows projects like these and these. [HISSING] You can make a spray painter with your fingers and your nose as the eraser. Here's a teachable machine project where a bunny runs around when you put bunny ears behind you. Here's one that detects when your posture is bad. [END PLAYBACK] All right, so that just gives you a sense of the toolkit and the kinds of projects. I want to just end this with an example of a project one of our middle schoolers did. So we did a bunch of virtual workshops over the summer where we actually trained teachers, as well as hosting students around this Dancing with AI curriculum, as well as a number of others. So again, within a week, we had kids developing projects like this. Well, I'll let her describe it for you. [VIDEO PLAYBACK] So mine is a book detector. It just tells you about the book and it rating. So if I did this-- So this is really cool. She's basically designed a visual book recommender system where you literally just have to show her system the cover of the book. And it will tell you about it. [END PLAYBACK] The fact that, now, a kid can make something like this in a week is incredible. So again, these tools and these potentials are really exciting. So everything I've talked about today, actually, it's on this website aieducation.mit.edu. You're welcome to peruse it. You can see a bunch of research projects, as well as curriculum learning modules that we're making available, creative commons license. So we want these materials to get out there. So anyway, you're welcome to peruse that. You can email us at aieducation@mit.edu. And we'd love to hear from you. If you want to use it, let us know. Again, we just want to help get this stuff out there. So I will pause there. Thank you. Thanks. Thanks for that, Cynthia. Every time, I learn something new about your work, as well. One thing, I neglected to say who I am. You probably saw it in the announcement, but I'm Eric Klopfer. I'm also a professor at MIT. I run our teacher education program and a group called Education Arcade and head of Comparative Media Studies and Writing. And I work with Cynthia on some of the AI education stuff, particularly around teacher professional development. Let me start with sort of a more basic, fundamental question and then dive into some of the education work. I noticed on your-- I don't know if this was intentional or not-- on your slide, one of your initial slides, showing you different kinds of robots. But the top row was all sort of embodied, had human embodiments or things like that. I think Jibo was one of those. And the bottom were more things like an Amazon Echo and a robot car, which sort of just look like common objects. What is the role of the human embodiment in some of the personal robots' work? Yeah, so, as you can imagine, in the nature of the work, embodiment has been a really important question and particularly around, fundamentally, human engagement and what the impact of consequence of that is. So there's one layer which is, is it anthropomorphic at all, whether it's a robot or a virtual character, versus thee more object-based things? And the other set of questions we get into is digital versus physically embodied, remote, on-screen versus physically co-present. So there have been over 100 randomized controlled trial studies done all around the world doing these comparative studies. So I think the punch line is-- and part of this is based on the data in the studies that we have-- when you want a deeper social and emotional engagement, these cues that can achieve the dynamics of interpersonal interaction obviously lend to a deeper engagement with people. And with that, if you have deeper emotional engagement, et cetera, often, that can lead, if it's a behavior change application, greater adherence to a protocol, or if it's a learning application, potentially, better retention in learning. So we can actually start to see better outcomes for people if the social-emotional engagement is the key to those outcomes. There's nuances there. So if it's a task where that social-emotional doesn't matter as much, well then, the embodiment doesn't matter as much. I mean, I guess that's logical. But it's just to say, across all of these studies, these are the patterns we're seeing. It's not a always one or the other. It's a depends, and it depends on context. When you look at long-term studies, which more and more of what we're doing is trying to look at-- and long-term is like months, say-- there are way more robot studies than virtual agent studies, interestingly, in terms of avatars and so forth. So the data there is just suggesting emotional engagement, particularly over long-term in a relationship, is stronger and more consistent when something is physically bodied in co-presence. I think there are reasons that it makes sense that that's the case. Because we've evolved, as human beings, to interact and engage with the physically co-present world. Everything else beyond that becomes more layers of abstraction and beyond that. I think the interesting thing, however, is there's kind of been this assumption in the field that the more human, the better. And we actually have not seen that. So part of that might be because it's really hard to do human-level expression, et cetera, et cetera, super well. Part of it may be this different kind of relationship. So I talked about this sort of new relationship. We're not trying to emulate or replace a human. We're trying to create this new kind of relationship where there are just different attributes of that relationship that can actually be very beneficial. So if you're trying to learn something new, maybe having something that engage kind of as a Sesame Street sidekick that has this effective engagement with you but non-judgemental actually might really benefit you. Because you're less worried about making mistakes, for instance. So again, it gets into a not always one or the other but the context and the application and what are you really trying to design for. So I know it's not an easy answer. But I think the robots are doing quite well. The studies that are done, often, we can see the more rich and dynamic and the mutual dynamics of the interaction become really important. That's where a lot of rapport and relationship are built. When you talk to an Alexa, it's more like playing chess. You say something, it responds. It's very transactional. That's not like how humans communicate. When I was a graduate student, I read a ton of developmental psychology in how we develop these abilities over time. And they talk about communication as a dance. It's this mutually regulated dance. And that's where a lot of this other stuff really comes from. So the challenge is, I don't think, trying to make something look more human. But building things that can engage us in that dance more richly, I think, are going to be more impactful. And I think it's exciting as a designer. Because I think that means that the design space of what these things can look like and so forth is huge. And I think we kind of get that from the world of animation. We watch Disney animated movies, and you can make a carpet express, right? It's a compelling character in its own right. It doesn't have to be human. So I think those lessons from even these other fields, other sensibilities, other forms of art, I think, are also adding an important lens and contribution and knowledge and insight to how we think about design of these technologies in the future. I see. So let me just say, Cynthia and I will have a little conversation for a little bit. I may sort of incorporate some of the comments I'm seeing coming in. Please keep them coming, some of the questions that you're posting there. Because some of them will overlap with some of the things I was going to talk about. I do see a note here about the sort of uncanny valley of human expressions, which we see both in digital and in real expressions, where if it's human-like but not too human-like, it sort of looks weird and makes us uncomfortable. Let me ask a question about-- there's a Q&A here, and it relates to a question that I had, as well. A lot of parents are sort of skeptical about screen time, about the way the kids spend the time with digital objects and artifacts. And as, maybe, we come into more spending more time with some of these objects, what's the impact on particularly young kids as they interact with some of these things? Are there potentially detrimental things? And if so, how do we design around those? Yeah, so I think this is one of the big questions right now in terms of what's the long-term impact and consequence. And very little, honestly, right now, is known about the long-term interaction effects of these kinds of technologies on people in general. From the work that we're doing and others are doing, we can start to point to implications, both phenomenon, results, behaviors that we see that can be used to both benefit people but potentially also to manipulate people. And we just need to be aware of that. Any kind of powerful new technology or phenomenon can be used by people for good or for ill. So this is no different. I think what's important is for people to understand the implications of this particular kind of technology. I remember, a number of years ago, having a poignant conversation with Cliff Nass. So Cliff Nass, the late, great Cliff Nass, is the founder of this computers as social actors. And he talked about this notion of, in medicine, there's this concept of dosage and side effects. And he's like, we don't have anything like that in human-computer interaction, but maybe we should. And I think he really had a really important point then. And I would never want to advocate, no matter how effective my learning companion robots are for children, I would never want them to be so dominating of an experience in a children's life that they start to miss out on other really critical things to their development. So I kind of think of this balanced diet. There are times when certain kinds of technology-enabled experiences make sense. But rarely is too much of a good thing a good thing for you. So you want that balance. Children need to be playing with real friends, and they need to be going outside. And ideally, they'd be exposed to music and creative pursuits. I mean, you want that balance. I just see these technologies, or these experiences, as a type of food group in that balanced diet. But it's really making sure the holistic kind of nurturance is in balance and makes sense. So that's kind of my philosophy. And I think it's important for people to understand that you always have to use your appreciation and judgment around what, holistically, do we, as human beings, at any age need to flourish and to thrive? And we need each other to do that. I think the dosage comment is important. Because I think oftentimes, we simplistically look for answers like is this good or bad Instead of looking at in what context might it be useful and how much of it and what are the applications that make it useful. And I think that's really important to criteria to think about. We have an audience member to ask a question. So go ahead. Is that me, Randy Davis? That's you, Randy, yes. Hi, Randy. OK, fine. Hi, Cynthia. How are you doing? Good. I thought the responsiveness of the robot to the child's affect was particularly interesting. Having done just a little bit of work in that, I know how difficult it is. I suspect it's particularly difficult with children. They respond very quickly. You've got to read it fast. Is your system really doing that and doing it that quickly? That's remarkable. Yeah, no. So I will tell you, in general, you're right. It is challenging. However, we're also seeing these social dynamic effects. So for instance, the more expressive the robot, people tend to emulate that, and they become more expressive themselves. So even the way you design the robot to get that interaction going can help make it a little bit easier for the robot to do these things. But the robot, it is doing these things in real time. Is it 100%? It doesn't capture everything all the time. But when we do our randomized controlled trials, which we're trying to understand, in the big picture, are children learning better or not, we're seeing positive engagement gains because of it, and we're seeing positive learning gains over it. So I guess the news there is a system doesn't have to be perfect and flawless to bring benefit. So it is above the bar, I would say, at this point, that it can bring benefit. And we're continuing to deploy these systems over longer periods of time in real schools. And we're trying to build the case for it, that this is something that could actually make a difference in the real world. We're at that kind of middle ground, I would say, of research where we've gone beyond the proof of concept. So we haven't done the very large-scale clinical trials, so to speak, to say, OK, I think we really have something that we could try to scale massively. We're in that translational research phase of the work right now. So now is the hard work of working with real schools in different parts of the country and showing consistent benefits that you feel you've actually got something that is worthy of being considered as a-- Well, sure. And I'm not commenting on that basis. I realize the stage that the work is at and what has yet to be done. No question about that. My concern was actually a little more locally focused. What is watching the kid? Has the robot got a camera in it? Is there a camera in the room? Are you getting body codes from a Kinect-like thing? What's your signal? Yeah, so, of course, all of this is done on IRB. We are capturing a lot of data, a lot of data from children. And I'm not trying to put you on the spot with IRB issues. I'm just really interested in how you managed to do it. So there's ASR. And we have a whole set of collaborations around making children-based ASR much more robust. Because as I'm sure you know, ASR for kids isn't great. So the robot is really kind of trying to pick up on key words. It's not able to engage in full, fluid ASR yet. So there's limitations there. But you design the games in the right way and you have the tablet interface, you can get a sense of the intention that's happening, so the robot can do something reasonable. So you have speech. You have touch on the tablet. You have cameras. So there's a camera above the tablet, and the robot has a camera. In some cases, depending on the study, it may actually even have another camera. So they typically are running, like [? AFDX ?] has actually worked pretty well for us-- again, not perfect but real-time response. And those are the main-- sometimes, in one of the studies, if you looked at her, you could actually see she's wearing Galvanic skin response on her. OK, good. So depending on the study, different kinds of inputs. But the robot is receiving those in real time and making decisions real time, based on it. So yeah, vision, sound. Excellent. Somebody in the audience suggested we explain the term IRB. You want to explain it, Cynthia? Yeah, so basically, if you're going to conduct studies with people as participants in your studies, you need to do so ethically. So there's a whole review board that every university, before you even engage people, you need to write an application that includes your protocol, the data that you're capturing, and how you're going to disclose what you're going to be capturing and how with people, so they can give informed consent. So for an adult, they sign it with informed consent. For a child, often, you have the parent sign a form consent, and the child assigns assent. So you often have to translate these materials in a way that you can explain to the child, here's the experience. Here's the kind of things we're capturing, recording from you. You can stop this at any time, whatever you feel. So it's a way of ethically, basically, ethically doing this kind of work. And how you protect that data is a very important part of getting approved, to make sure that it's secure, that it's kept private, that even things like taking photos of children. Often, media outlets would love to have these awesome photos, where you're like, unless that child and the parent give informed consent, you can't use those images. Or they may only give you permission to put it in scientific publications but not media use. So there's many layers of permission people have to grant. And then, you have to make sure that, again, the overall thing that you're doing is ethical. And you also need to compensate people for their time and effort. And you need to make sure that your subjects that you are recruiting are the ones who can actually, reasonably, be the ones who are going to benefit from these interventions, so you don't have the case that there's one group that's taking all the burden of testing it out, and a whole other group is the one who benefits. So you need to make sure that the people who you're working with are actually the group that you would envision would be the ones who are benefiting from what you're developing, too. So there's a whole bunch of stuff. But universities have to abide by it, and it's a good thing. Great. Thanks for the questions, Randy. We're going to head over to another audience member in a second. I'm going to ask a question here that relates to something that someone put in the chat, which is, thinking about the other side of the work, where you're teaching kids about AI, what kinds of decisions do you think that they would make better with the kind of knowledge that you're trying to convey to them in these experiences? Yeah, so as you know, Eric, this whole area of AI literacy or AI education for the rest of us, it's very, very new. And our hopes are that, by having people of whatever age go through these learning experiences and materials that we're offering, first of all, they're just much more savvy about where AI is in the world around them. I can tell you there's a lot of people of many different ages who don't even realize how much they're touching AI every day or AI is touching them. So we want them to be much more conscientious in terms of their use of these technologies. They understand where it is in the things that they're using every day, that they can just be that much more informed. In the case of democratic countries, we want people to be educated enough as adults so that they can help set process and policy around these things, whether they're a manager or a citizen. So in Somerville, they decided-- in Boston, now, right-- they decided to ban surveillance camera technology. So people need to be informed enough to make those kinds of decisions. And then, a big one is we want to empower a far more diverse and inclusive future workforce who can design these technologies ethically. And we believe, I guess, at MIT that you want the designers themselves to also appreciate these ethical challenges and issues and societal implications. Don't just build it and hope someone else will critique it. The more the people who design it are savvy about these questions, I think, the better off we'll all be. And we want a much more diverse group of people, inclusive group of people, to be able to do that. Because the field of AI, it is not diverse or inclusive. And that's a problem. Because that's already another form of bias in the nature of systems that are designed, the nature of solutions that are designed, who uses them. So we know that AI, it is a really transformative technology. And we want to make sure the right distribution of people can really shape it and shape how we live with it. So those are the big, high-level outcomes. In terms of the next level down, there are specific concepts about AI you want people to understand, practices, [? both out of ?] computational thinking design practices, ways of being able to critique, reflect, on these things, to be able to engage in teamwork and discourse around it, a lot of those great 21st century learning skills. These are all the things you want to have as part of the learning development outcomes of what we make. Great, thanks. So we have another audience member with us. Samira is up, if we can highlight her. Yes, I wanted to ask that if the child gets frustrated and the AI is trying to console them, I know some people my age don't really like that. They just want to have some time alone. So will the AI respond to that by just leaving them alone, or will they try to console them and get them to try again? Because I know it can be extremely frustrating sometimes for somebody my age. Yeah, so I think this big question and opportunity around personalization is really, really important. Because you are different from your friends on how to engage in support if someone is frustrated. So there could be a number of ways that you support that. One could be, if you've designed in a natural language ability, you could communicate that to the robot. No, when I'm upset, you've just gotta leave me alone. If you have the right dialogue models, that would be one way that the system could then adapt and tailor to your intent, your desire. Another way, which is more of what you saw in those videos, is the robot's actually learning from experience. So it takes it longer, and it's a little more, I guess, indirect. But that's another way that, over time and encounters, it can learn a personalized policy for you, that you actually do better in the longer-run if it might give you time to simmer down. But in order to make that happen, you have to design a system that that is something that is possible for it to learn. So I think one thing that's really important that I don't think the general public understands enough is how much human crafting there is involved in getting any of these systems to work. It's not as if you just write a generic algorithm and feed it a ton of data, and it just magically learns the right stuff. There is so much human insight and crafting. I was at a presentation where the term "artisanal" from a gentleman from Microsoft Research. These AI systems are highly artisanal, meaning it takes a lot of expertise of human designers to get them to do the reasonable things. And often, this requires a lot of iteration over time. So when we develop these systems, you're seeing a robot that's probably already gone through like 10 cycles of innovation, between early pilots with fewer children and then changing it and then trying something else. And you're seeing it evolve over time. And that, I think, it's just a critical part of the design process and of knowing you can't just design something and lob it over there and assume it's going to work. You really need to have evidence-based iteration. And the more co-design-- I think this is another theme that's coming out more-- the more you engage the users and the stakeholders of your systems in the whole design process, the better chances what you come up with is something that really makes sense for them and is going to be actually useful. So the more you can engage the main, again, stakeholders-- whether the users or the people who might be paying for it, whatever, the people it's going to matter and affect-- the more you can engage them in that design process, probably, chances are, the better the designed product will be at the end of the process, as well. So these take time and a lot of human crafting. Thanks. Thanks for the great question and the answer there. Thank you. Our next question comes from Shigeru. Hi, Shigeru. Hi, Cynthia. Hello. [AUDIO OUT] Oh, we lost him. Oops, we lost Shigeru. Maybe he'll come back. I did see he had a question in the chat. I'm going to assume that he was going to ask some variant of this question, which was, how do you choose the language that the AI device speaks in? Is it standard English, whatever that is, or more vernacular to the particular audience? Can you hear me now? Yes, so we can hear you. Oh, OK. Yeah, so as a linguist, I'm interested in language. And I was just curious as to how you select the language that your AI devices speak. And I have in mind, for example, if you're going into a predominantly minority disadvantaged population, and if your device starts speaking standard English, that could unintentionally insert a power relationship that you may not want. So language can really help with what you're trying to do so. Yes, you are absolutely correct. So it's an important design question. And as a design question, it's also a technical challenge. So when we work in just different cultural contexts, and it could be a different culture within the United States. Like you're saying, if we're going into schools in Atlanta, Georgia and you have this different dialect of English, there's been a lot of studies that show the ability to speak and understand that local, cultural dialect of English is actually really important. Now, you also have the more academic English, as well. But you really do want to think carefully about the interplay between supporting what the child knows and is culturally familiar versus, maybe, what's more, again, academically aspirational. We're starting to do work in other countries, as well, where now, maybe it's UK English versus American English. So it is a really important design decision, and it's also an important technical challenge. Because now, that means that not only do you have to be robust to children's speech but children's speech with dialect. And you need to be able to synthesize speech well enough with a dialect, especially for young children, because you don't want a lot of speech artifact. So anyway, it's just to say that there is a strong interplay between understanding what is culturally and pedagogically and experientially important to do, as well as technically being able to actually do it well enough that you can have the benefits that you want. So yes. [LAUGHING] Thank you. Thanks. Cynthia, there were a few questions I've seen-- I'm going to sort of synthesize some of them-- around the role of AI in education and how does AI in education narrow the gap that we see between socioeconomic groups where, like right now, there's the privileged few who are getting great resources and a lot of people who are not. How do we see AI bridging that gap rather than widening that gap? Yeah, so this is always such a poignant challenge. So we can use the word democratized, and it sounds great. But sometimes even that means that the people who would take advantage of it are the ones who are benefiting more. So you have to be just super vigilant about it. So for us, in our own work, we really are mindful about who are we bringing these materials to, whether it's the AI literacy materials, for instance. This summer, we worked with-- Amazon Future Engineer has a network of over a thousand Title I schools. So we worked with Amazon Future Engineers network to make sure that we were actually training teachers of those schools, engaging students of those schools, to make sure that we're giving access to these materials. We talk a lot with teachers and try to understand things like if we're going to bring these technologies into the classroom, what are the considerations we have to keep in mind so that it is actually accessible? Wi-Fi is highly variable, as we know. Now, getting Chromebooks seems to be more kind of standard issue, so to speak, but just being very aware of what the educational context is so what we're designing and developing can work within those school classroom technologies. We think a lot about-- and Eric, you're an expert in this-- how do we train teachers where they're not going to be experts in AI. So how do we think about the materials? And we've actually designed AI curriculum, by the way, that can be almost no computers at all, almost all discussion, paper prototyping. So we think about these different kinds of materials and different ways of learning that might not even involve technology to help also give access. So we're thinking about all of these things. But Eric, you are very much around how do you address the teacher professional development challenge of this? And it's a big one. Yeah, and this gets back to the presence-absence conversation we had a little bit earlier. It's not just about is this used, is it not used? And you mentioned Chromebooks are becoming more ubiquitous. So the technology itself is not the impetus for the divide anymore. It's the way that the technologies are being used and the way that people are trained to be able to use them. And that's so key, I think, in making sure that this is applied equitably, that we don't just repeat our mistakes from past technological innovations, where we just sort of distribute devices to schools without thinking about the training and expertise that needs to go with that. That's really critical. I'd say the other thing that we're also doing, which has turned out to be, actually, wonderfully effective is that we try to be very mindful for what are the AI technologies or systems that kids are very fluent with? Like, they are part of their lives, like Snapchat or YouTube. And we use those as kind of in-depth case redesign studies. Because kids already have deep knowledge in some cases of these kinds of systems. And it's just very engaging for them. Because they can start to understand why this really matters. And so you can talk about examples where these things could go horrifically wrong, like when you had the auto recommender system, where, suddenly, you're getting conspiracy theories if you let it go long enough. It's just these kinds of things where you just need to know that this is within the realm of possibility, and you've got to think about what does that mean for how you might design YouTube. We love when-- we've done these workshops long enough, and you'd be surprised how many kids want to design a feature on YouTube which basically makes them want to get off of it. Because they acknowledge they spend way too much time on YouTube. So anyway, I think empowering kids so they understand the relevance and the impact is really important. We just did a whole curriculum on generative adversarial network, which is just this new, cutting edge machine learning algorithm. It's when AI can generate these photorealistic faces and videos and things like that. And we talk about it in the context of deep fakes. And you can see the kids' eyes like, oh, that's even possible? And so it really gets them to think about their future with these technologies and how do you know when something's actually real versus misinformation that's being spread? So we're finding, anyway, that when you do that well, the students are super engaged. We come up with these questions of equity. They are so engaged in these conversations. So I'd like to see this become more of just the way that these technical subjects are taught. OK, we only have another minute. I'll try to squeeze one last question in here, which is, there's a late question here around, how are your products useful for children with autism? Or I might expand that generally to different kinds of learners. Yeah, so this is another really, really important question. So I will say that in the field of social robotics-- actually, even from the very beginning of the field-- social robots, as a technology to support children with autism, has always been a very active area of research. So I would say, as a field, absolutely, there's been a lot of attention paid to that particular use case domain. It requires a lot of-- as you know-- it requires a lot of special expertise. And you need to engage clinicians and parents. I mean, there's a whole ecosystem you need to really understand, as well as the nature of being on the spectrum and the breadth of that spectrum, the diversity of that spectrum. We have a project now at MIT where we have the opportunity to think about how do we support inclusive classrooms, meaning you can have kids with special needs in the same classroom as neurotypical learners. And so we have the opportunity now to actually think about, as we design these materials with these different kind of support-- special teachers and so forth-- how can we design a curriculum in a way, or the activities in a way, that it's flexible enough, that can be customized or tailored to address this broader range of learners with different learning needs. So we're at just the very early stages of that. But I think it is so important to be mindful of these things. Great, thank you. We are at the top of the hour, so thank you, Cynthia. Thank you to the panelists who came on, the audience members who came on, Randy and Samira and Shigeru. And thanks to all the questions that came here. I know we didn't get to all of them, but hopefully, we were able to answer some of the ones that were particularly relevant. So thank you for making this a great and interactive event. Great. Thank you so much. This was a lot of fun. 