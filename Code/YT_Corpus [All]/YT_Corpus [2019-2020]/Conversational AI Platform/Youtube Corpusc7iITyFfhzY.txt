 [MUSIC]  Hi. My name is James Tooles, and I'm a Cloud Solution Architect at Microsoft. I'm here today coming from my Brooklyn apartment in New York. Today's episode, The Control Room, we'll cover how you can infuse AI into your media workflows. We'll be going through some of the components used to do this, as well as the technical architecture for a solution our customers are leveraging in the field today. We'll be going through some of the Azure Cognitive Services, but mainly focusing on Azure Video Indexer. I hope you have a good time. [MUSIC]  So we have John de Havilland here, Customer Success Director out of New York. How have you been, John?  Thanks, James. It's great to be here.  I haven't talked to you in a while, and you're always hearing interesting things about our media and entertainment customers and how they're leveraging our technology to solve their business needs. What have you heard lately?  You're absolutely right. We're hearing a lot from our customers around different areas of innovation, artificial intelligence is one of those. From enabling content creators to automating repetitive tasks, through to things like helping consumers find content they're more interested in. How is that going to be driven better with AI? There's a lot of different areas that customers that we're working closely with are exploring today and starting to build platforms out on around helping enable with that because consumers get a better experience. I think net-net, we're going to see a much more improved customer experience as we move forward with greater options and innovation to really help improve the overall offerings that are out there. So it's an exciting time to be walking in this space. As we all know, there's a lot more direct consumer platforms out there, and this is continuing to expand and grow using AI there to make content recommendations through to improving the service offerings, through to just getting more data and understanding what customers want to see, personalizing channels for them, and engaging them more proactively to really help improve that customer touch point and that customer experience, [inaudible]. We're seeing a lot of innovation in that space and some of the pieces of AI are being used to drive that. If we take a look at the Digital Supply Chain and the reinvention of that, we see a lot of companies looking to consolidate and drive down costs there, especially on these wider, globally distributed productions. It's becoming more important to be able to do this faster and with better technology. We see a lot of companies that we work with looking to the Cloud and the scale that their large networks provide to enable some of these global productions, and on top of that, layering in AI here is an innovative way to do network optimizations, to do better routing, as well as also improve and increase the efficiencies in the workflows across the whole supply chain. So it's super important, and we're seeing AI being used today, and there's innovation is being built on top of that for the future too. Finally, another area that's definitely a top interest across many of my customers is around Edge Computing in 5G. We're seeing this lighting up in stadiums, across theaters, and elsewhere in the industry and it's a game changer in many aspects. If you think about it, where AI can be brought to bear here, it can be used both in the Cloud for heavy-lifting, but also I pushed down to the edge, and computing scenarios where we've got AI running very close to consumers to provide those real-time rendering experiences, real-time innovative platforms, and in massive storylines. The capabilities that this can bring to bear with edge computing coupled with the AI are going to drive some very interesting innovative scenarios. Then layer on top of that, things like augmented reality and VR experiences. If you think about that, then the innovations there are going to be really interesting for consumers. So overall, very exciting across the industry and for the consumers consuming that content.  Yeah, John, I can definitely see the value that our services are bringing to our customers. So what do you need to really leverage our tools? Do you need a team of PhD experts in machine learning, or do you need an advanced enterprise level of development team to start leveraging Azure services and AI?  That's a great question, James. It's actually often a misnomer that you need a large number of data scientists or people with PhDs in statistics to build out some of these more complex AI platforms. In fact, Microsoft has really democratize AI to some degree. We've opened up a bunch of APIs through to artificial intelligence platforms on the back-end to allow you to pass in data and get intelligence back about that data pretty quickly with just some development skills. You can take that and integrate it really easily into your existing workflows without having to understand the complexities of the AI under the hood, or requiring you to have massive amounts of compute to train complex models. So I think the ability of the Cloud to really offer these out-of-the-box really helps people get started with some really intelligent and complex AI, without needing to have the gory details under the hood. This is fantastic, because it really democratizes it in such a way that you can have complex solutions built out, and leverage some of this technology today to start really driving innovation. This doesn't mean that you shouldn't then start to, as you mature your platform and look at other use cases, bring in an expert who know the system and field very well to take advantage of some of these data points coming out of these platforms to then leverage that on top of some of the existing things you're looking to build, as well as building newer models and training those yourselves around this niche and customer scenarios to really enable a competitive advantage. But I think it's really interesting to see taking some of these out-of-the-box platforms and combining them together to create newer and innovative offerings without needing to go deeper into training your own models.  That's awesome, John. Thanks for sharing the trends you're seeing in the industry from our media and entertainment customers. It's always interesting hearing the stories, and how our customers are leveraging our technology to achieve more.  This has been great, James. Thanks so much for having me. [MUSIC]  Hi, everyone. My name is Inbal Sagiv. I'm a Product Manager for Video Indexer. I'm going to do an overview of Video Indexer today and I will show you also the new features that we have, so you can even try them out yourself. But before we see the demo, I want you to understand what this is all about. So Video Indexer is the solution within Azure Media Services to extract insights from audio and video file using machine learning algorithms and also integration with the cognitive services. The idea basically is that you can use those different insights to truly understand what is exactly in your video. You could use it for automatic tagging or for transcription to make your content more accessible, and you can also use it to really help with content monetization opportunities and also to enable user engagement. You see several use cases from the customers and from the partners who's using this. Let's take some examples, so those that would like to improve the search experience, they would use Video Indexer to search for spoken words, for images, for celebrities in their movies. Others that would like to use Video Indexer as monetization opportunities, they would target the right ads while viewing the movie. We see also use cases like facilitation of user engagement for those customers who would like to allow users to skip directly to the parts of the video that they want to see. Also, interesting use cases could be like enablement of content monetization, like identify profanity in the videos. As I mentioned, you have two options. You can, of course, run them both. But we support both audio files and video files. So it means that we have setup models which are running on the audio and setup models that are running on the vision. These are the high level flow of the different machine learning models and insights to extracted when a video file has been actually indexed by Video Indexer. As you can see, there are a lot of them. The number is actually growing. Of course, I will not talk through all of them. But let's see an example. So on the top you see the pipeline or the models which are running on the audio device. You'll see that, for example, when we get as an input, the transcoding, then look at the bottom of that. It starts with detection that there is actually a voice activity in the audio. Then, we do what we call the speaker diarization. We are trying to identify who's speaking. That is being, of course, then being then qualified and actually used as an input to create that transcription. Transcription is asset as its own. It means that you can go to Video Indexer and you can download it's transcription. Afterwards, the transcription, for example, is used as an input for a sentiment analysis and so on and so forth. So we have single channels and multi-channel support. Some of the models are output, but some of the models are also input for other models. So you see here, I would say complexity, but also the variety of content that we support starting from face detection and recognition, key frame extraction, topic inferencing, sentiment analysis, language identification, and many others. Now, let's see it in action in a short demo. So this is Video Indexer portal experience. Everything that you see here from the portal is also available through our RESTful API documentation. So you see here the full list of the APIs that we have with a documentation for all their parameters, and how do you suppose to consume those APIs. Of course, that it really depends on you if you have multiple type of videos and you want really to scale, so I wouldn't use the portal for that, but I will do my own automation and we'll do it from coding. But of course, that if you would simply would like to try the capabilities and evaluate them, or you don't have a high amount of a video file, then I would highly recommend to do things from the UI. This is also really great visualization of the different features that we have. Our customers also could benefit from free trial experience where they can simply register with their own username and password, and then to try all the capabilities of Video Indexer. So as you can guess, the very first thing that you would need to do is to upload your own video to be indexed. But I already did that to be able to show you the capabilities of Video Indexer. So what you see here is the gallery. The gallery is mainly used for a deep search or a cross search capability. So let's just preview example, try to find out Satya. So if I'll simply type for his name, I will be able to see his response all the different videos and where Satya appeared with his face, or in the tile of the video, or as a named entity, or maybe in the spoken transcription of the audio of the specific video. So just to demonstrate the capabilities, I've already upload one of the videos which I mentioned. So let's move to the player. The player is where I actually indexed a very specific video. This is where I can deep dive and see all the capabilities that we have. So on the right side of the screen, I see the different insights that I have from this specific video. In the left side, I can see the video file itself. What is nice here is that I always can download the different insight through a JSON file and then I can parse JSON file and see all the different insights available also there. Of course, I can download also the source file, and we support multiple types of transcription like VTT, TTML, and many others. I mentioned that we do a transcription. So if we'll go to the timeline and see here the full transcription of the video, and with only one click, I can simply translate that to more than 50 different language. Also, from here, I can download the transcription file and the insights and the source video. If I'm working, for example, with multiple languages, then those languages can be automatically detected from the video file. Let's say, for example, that I've just figured out that something in the transcription file is wrong, I can always edit that and it will persist my changes for the next time. Let's go back to the insights and see some of the capabilities. The video which I'm using is actually a video of the Cold War and arms control negotiation during that time. So we have, for example, one of the insights as the face detection. So we see here, Paul Nitze who's the former United States Deputy Secretary of Defense. So if I click, for example, on Paul Nitze, I will be able to see where Paul appears in the video. So the only thing that I can do is either click on the different timestamps or just click on "Play Next" and we'll be able to see all the different appearances where Paul is available in the video. For sure, you can recognize Ronald Reagan also in the video file. So I just clicked on his face, and if I click on the timestamp, then I will go directly to see his faces in that video. Of course, there are also some faces which are unknown. I mean, the face was detected but not identified. So for that purpose, we have our own customization option. So for that there is dedicated panel for all the people that were detected within that video. If some of them can be detected manually by me, for example this video, and at least I can can simply change the name and that will be available here for the next time, or I can also upload my own model. That's relevant also for languages and for brands and also for animated characters. Let's go back to the video to see some more of the insights that we have. So I mentioned only one of them. I mentioned there just the faces recognition and detection. But you see here that we also have topics inferencing which actually point on different themes that are relevant throughout the video. These topics did not explicitly need to be mentioned, but as long as they have been discussed and the UI picked them up, we see here the full list of the topics. So I mentioned that video is about the Cold War, which you see most of the video is about the Cold War. But there is also warfare which is been mentioned and discussed within that video. Additional insights that we have here is the visual labeling. So VI also do the visual labeling of all the different visual items that are showing up throughout the video with the timestamps again. So if I go, for example, to a shirt, then I can in a very similar manner navigate to all the places where we see shirt within that video. Same about any other elements that we have, like just giving you another example of trees. It's very simple and intuitive just to navigate between the different labels that were identified. Named entities is the next insight which is available here and just show the ability to identify location. So you see here, Geneva. Let's check the appearances of Geneva. So you see number 1, and then number 2 where this entity is been actually showed within that video file. Another nice thing that we do is the motion detection. So we pick up for across the couch around emotions like anger, fear, joy, sadness, and then all other parts are actually considered as normal. Last one which I want to show you is the scene. So we have a different scenes that are making this video. So you can see all the different times and camera cuts to a new scene and then within each scene you'll actually see the different shots that making a scene. Let's take an example. So enfolds like that specific scene, you see that it has four different shots, and for each and every shot I can identify whether it's a close up, whether it's indoor, for example, this one is marked like an indoor shot, but if I'll move to something else, then you will be able to see that it is an outdoor shot. For all the different shots that we have within the scene, we also know how to extract a different keyframes. It's the most stable and distinct frame within your video and it's actually very important because there are some other algorithms, for example that belongs to cognitive services, which you can test and extract those keyframes and test those algorithms with them. There are some additional capabilities which I will let you explore by yourself, like the multilanguage detection and the animated characters which are really cool. But in general, you'll see that the whole idea here is to let you automatically extract all the metadata with no coding skills with this nice view, or you can alternatively go and do it from the RESTful APIs. We're outlining to add some additional new features and always improving the existing pipeline of features that we have. I hope you actually enjoy that demo. [MUSIC]  In Azure, we have several Azure Cognitive Services, which are AI as a service. Our Microsoft AI engineers and researchers, have done the hard work of developing enterprise AI. Our AI has been tested and benchmarked to have human parody in several domains, including speech, language translation, conversational question and answers, vision, as well as many others. Azure Cognitive Services support a wide number of programming languages, SDKs, REST APIs, and code free support. When using Azure AI as a service, using AI is as simple as sending your data to a given service, and receiving an AI analysis response in the form of JSON. Our customers leverage Azure Cognitive Services, so they can focus on their business problems without having a team of AI experts. Ensuring business applications enable enterprise scale, security and compliance. We have a wide variety of Azure Cognitive Services for various domains in use cases. Today, we will be focusing on Video Indexer, our video cognitive service, and how our customers are empowering their media and entertainment workflows, leveraging Video Indexer. We have the customer with a real business need to understand the videos and automate the classification on these videos. Previously, this was a manual process where human reviewers will have to watch each video, then classify the video to the appropriate category. Doing this for thousands of short videos every day, was a costly and time-consuming process, so of course the customer turned to AI. Here's an overview of the deploy architecture in Azure. I'll take you through each of the phases of this workflow. But at a high level, the customer starts with normal video files of their media content and they upload that to Azure Storage. Consistently tagging each video with a rich metadata leveraging Azure Video Indexer, automating this entire workflow process leveraging Azure Functions as a serverless architecture, and controlling the development lifecycle with Azure DevOps.  James, this is really interesting. I think it seems quite complex though. How complex is this overall?  That's a wonderful question. Generally if you think about solving this problem, you might need a team of computer vision experts to write code that understands videos. Another team to develop the APIs for using the custom machine learning models. A business team to ensure metadata tagging is consistent with business needs. All while leveraging enterprise, production, scale, and security for the entire application. The great thing about Azure is you can use our turnkey services, then the problem really becomes connecting these pieces together to solve your particular business need.  That is fascinating, James. I notice you have a lot of Azure Functions highlighted throughout the solution. Could you explain what an Azure Function is?  That's a great question, John. An Azure Function is really just a compute target and it has a serverless architecture. So that means that you're only paying for the compute cycles as milliseconds of compute time, that your code is getting executed. It's a really great option we see our customers leveraging, for dynamic scale out, and to control and manage costs.  This looks really good, James. Are you going to show us how it's built?  Sounds good, John. Let's go take a look. So what you're seeing is Azure Machine Learning, but we won't be doing any machine learning here. We're going to be leveraging Video Indexer to do all the machine learning and AI work for us. The reason why it's in Azure Machine Learning, is because you can host and run Jupyter Notebooks. Jupyter Notebooks really are a great way of sharing and showcasing code, so it's very useful for webinar sessions like this. Now, all this code is actually deployed in Azure Functions running on the serverless architecture compute. So let's go ahead and start running some of this code. Now all this is Python, so the first thing we're going to do is import some Python libraries. Next, we're going to be using the Video Indexer APIs to request a Video Indexer access token to authenticate. After that, again, we're going to be using the Video Indexer REST APIs, and we're going to upload the video file to Video Indexer for Video Indexers AI to process and extrapolate metadata from it. As part of this requests, we have a number of optional parameters we can pass in, one of them being a CALLBACK_URL. Now, this is important for our use case. Because once the video is done processing, Video Indexer will actually send an HTTP request to this CALLBACK_URL to kick off the next Azure Function, which is part of the workflow and part of that architecture you previously saw. So we'll run this code. In this section, we're really simulating what happens once it's deployed in the Azure Function. So Azure Functions have different triggers you can set, one of them being a blob trigger. So every time a new video file is uploaded into Azure Storage, this code will automatically get kicked off and this workflow will get executed. So in the example of a new blob being uploaded to Azure Storage, will look something like this code being executed. So let's do it. We can see in the output, we create a Video Indexer access token. We uploaded the video file to Video Indexer, and we succeeded in everything. So now that's complete. We actually get a response from Video Indexer once we send that video out. So let's see some of the fields that gets sent back. So we have a unique ID that Video Indexer creates for us. We have a name which we gave Video Indexer to call the video, and the state of the video is it's processing. So let's go check. So here's Video Indexer, and if we refresh, we can see that our video was uploaded, my beautiful faces behind it, and it is processing. So it take some time.  That was great, James. So if I understand correctly, we took videos, uploaded them into Azure Storage, that then triggered an Azure function that would take each of those videos and send it to the Video Indexer API. Now, if I understand correctly, the Video Indexer API is processing those. So could you talk through about what happens next? How do I get the insights out of Video Indexer from those videos?  Once we send the videos to Video Indexer, it can take some time to process. This normally depends on the length of the video, and there are various levers to increase or decrease the processing speed. The Video Indexer API allows for a callback URL. So when we send this new video to Video Indexer, we can send a callback URL, and then Video Indexer will actually send a HTTP request to kick off the next part of the workflow automatically for us, which is great. So our video is uploaded to Video Indexer, and you see completed here. So if we go back to the code, processing the video does take some time. So if you recall, we are able to give Video Indexer callback URL. So when the video is dumping process, Video Indexer will automatically kick this next part of the workflow off, pinging the Azure function deployed on serverless architecture in Azure. So let's see what code gets executed. Here we're looking at a function which again leverages Video Indexer is REST-based APIs to request from Video Indexer artifacts. Now, these artifacts are nothing more than JSON files, and the output of Video Indexer's AI analyzing the video. So we'll execute this code. This next piece takes these JSON files we get from Video Indexer and stores them in Azure Storage for us to use later. Finally, we're going to download a special type of Video Indexer JSON file called insights. Now, Video Indexer insights are great because they're a combination of all the other Video Indexer artifacts in a summarized way. Finally, we have a chunk of code which is going to go and execute all the smaller functions we saw earlier. So you had some different types of Video Indexer artifacts. We have Ocr for object recognition, Face for faces, Content Moderation, both visually and textual, as well as a number of other Video Indexer artifact types. So we'll loop through each one of these and download the JSON artifact. But first, we need a Video Indexer access token to securely authenticate. We'll loop through each one of these artifact types, downloading the artifact JSON and storing into Azure Storage as well as getting the special insights JSON and storing that into Azure storage. So let's execute this code and see what happens. We can see this code gets kicked up and we get our access token. We then go get the Ocr artifact, download that from Video Indexer and store that in the Azure Storage and continue to do the same for each type of artifacts, and as we keep going, we'll see we did well, we failed one. So you can see we failed getting the MultiLanguageDetection artifact from Video Indexer and the reason is that unfortunately, I'm monolingual and so I do not speak multiple languages. The video I uploaded only has one language in that video. So there was no MultiLanguageDetection artifact generated automatically from Video Indexer's AI. If we keep going, we'll see everything else succeeding successfully, and we completed. That's great. So now, we have all the AI data in the form of JSON files downloaded from Video Indexer and stored into our Azure storage.  This is fantastic, James. So now, I think I get how we can extract the insights out from Video Indexer. Looks like when the video is finished processing, it's going to automatically call a function that is going to pull down the JSON, which contains all of the insights. So how do we narrow down the insights that we have? How do we start getting business value out of those James?  Yeah, John. There is a lot of data. Video Indexer is a general purpose video AI service, so it contains more information than most use cases will need. The amazing thing is that each video is consistently tagged with mega data. Automatically, no machine-learning developers were needed or utilized and now the problem becomes we have too much data, which is a nice problem to have because it's an easy problem to solve.  All right, let's take a look at how you built this.  Here we've downloaded one of the insights JSON for the video, a Letter to My Longer Self. One of the videos we've uploaded to Video Indexer. The Video Indexer AI went and process the video and generated this JSON for us, and if we take a look, this video contains some important faces here. So as faces, we have a famous person who is recognized as a soccer player in some details. Going all the way to the end of the JSON, we have over 6,000 lines of information from this one video for this one JSON file. So how do we get the information we need for our business use case? Well, now, it's just a basic ETL process. So there's a number of ways you could do this. SQL, Python. We have pass services like data factory for accomplishing this. I'm going to show you how to do it through Python. So first, what we're going to do is we are going to download those files from Azure storage, so that we can manipulate the information, and these are some basic data manipulations. So again, SQL queries, ETL process, joins, filters, things of that nature. So we're going to do that here and as well here, and lastly, we're going to do a merging of all of the different video JSONs. We're going to merge them together into one dataset. So that's going to be accomplished through this function. Again, if we want to view an example of the JSON with the video we uploaded previously, we can look at that and it's all that data, and again, this is just for one JSON file, one video. So we're going to process that for all the videos, and this is the code that's going to execute that. So here we're going to download the videos locally so we can work with them. We're going to emerge all the data for all the videos together into one data set and then we're going to trim the Video Indexer confidence score. So what the Video Indexer confidence score is it's paired with a feature and it's how confident our Video Indexer AI is that saw this object, heard a Word or captured a keyword topic, things of that nature. So let's see what that looks like. Here's the code it's executing. So it's merging all those insights into a dataset, removing the local files that we're done working with and is completed. So here's our nice dataset. So again, this is from every video we uploaded. So we have things like brands, keywords, labels, and if we just scroll through, you can see a bunch of interesting features that AI metadata tagged for us. So using Video Indexer, all of this data, all these insights, the understanding of what was in this video was processed for us and really our job becomes grabbing the pieces of data that's meaningful for our business use case. So the rest of this really is just going to explore that data. So if we look at all the Ocr Cloud for all the videos, transcripts, labels, name, locations. Again, we can take the confidence score and cut off anything that is below 65 percent. If we want to make sure that the video indexer AI was just as confidence we want to be, we can set that threshold here and we can again manipulate the data in a way that makes sense for our business use case. So if we execute that, we can filter out all the key words and make sure that they are unique. So this aggregated all the unique topics from all the videos and sorted them by count and so we can see that companies was a common topic for the videos we have thus few technology Cloud computing, all of this generated leveraging Video Indexer and Video Indexer is out of the box AI capabilities.  This is great, James. I think I have a good understanding now of the platform you've build out and I'll confess it's not as complex as it first seems. If I understand correctly, video files land in Blob storage, that triggers a flow where those videos are passed into Video Indexer. Artificial intelligence is used to extract insights out of those videos and then those insights are then pushed into Blob storage and then you are able to use a function to extract a subset of those to then enable you to do your business classification and modeling to drive your taxonomy use case that you were looking to solve for. I love the fact that this is serverless too, because that really allows you to scale up and scale down very efficiently and cost effectively.  Yeah, John. The customer has a lot of options now. We have this automated pipeline that kicks off every time a new video is uploaded to Azure Storage and for the business classification piece, we see customers doing anything from looking for common key words or phrases and then applying the business classification there or using this rich data set that's generated with AI and Azure Video Indexer to develop custom machine learning models to do the business classification. We even see customers integrating Video Indexer and so their backend Ma'am and damn systems to leverage all the consistent metadata tagging and AI coming from Video Indexer. I hope this session was useful and informative. You can continue your learning using Microsoft Learn, a free online learning platform. There's a curated AI track, which upon completion, you should be fully prepared to take our AI 100 certification exam. Azure Video Indexer. We can learn more about Video Indexer and it's full capabilities and features, and our Azure Cognitive Services, where it has a wide variety of domain specific cognitive services for your business needs and your use cases. [MUSIC]  Thanks for joining, everyone. [MUSIC] 