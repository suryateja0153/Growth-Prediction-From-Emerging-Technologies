 [Music] everyone welcome to MLA I 100 actionable AI insights using translation and natural language I'm sure everybody has had a night of good sleep and coming here refreshed on the last day so welcome and we're excited to get you started today for our speakers we have myself I'm Sarah Weldon the product manager for cloud translation API and Auto ml translation followed by Olga better go via who is one of our customers VP of language services at we localize and then one of my peers Sudhir of angry product manager for natural language processing API as well as auto auto ml natural language processing and dua I which is one of our new products that we launched this week and finally Karen Casas will closes out a head of mobile engineering with DocuSign who will walk us through some really great use cases for natural language processing with mobile apps here's an overview of the session today we'll keep working through stuff because we have a pretty jam-packed morning and I'll be starting us off with our language insights you might be familiar already today with Auto ml translation that we launched last year as well as translation API which has been around for a little bit longer time with some very powerful hundred language pairs of translations that can be used with mobile applications to give helping to go to new markets between Japan and India or Spain and Portugal this API helps you seamlessly translate between content to help your users connect additionally natural language can do things like entity extraction it can do sentiment analysis and even beyond that so as I said earlier we we launched Auto ml translation last year and it it allows customers to bring their own training data to further customize and improve our existing translation capabilities to tailor fit to their particular use cases and to strengthen it and in their language pairs we have new updates that we have continued to invest in our auto ml platform including investments in our base platform that will help other help our customers by reducing their overall training costs to make improvements in the training efficiency that we've found in observable reduction and twenty to forty percent reduction in training time which reduces the overall training costs additionally Auto ml translation now supports HIPAA compliance which is great news for our healthcare customers translation API as I said has been around for a while and today is really important juncture where we're continuing to advance the features and capabilities to better serve our localization providers as well as streamline capabilities for our developers so today we are announcing new updates with a set of features that will help customers have greater control over their brand data as well as be able to do longer running operations for translations asynchronously so this allows you to do more file translation type capabilities and then finally a translation API v3 which is we're going to be coming out ahead of you to we'll be introducing model selection so for the first time with one API you'll be able to choose whether you want to use the best model fit for your needs you could use our general nmt model you can use your preach your existing auto ml custom models that you've built or you can use you know the model of your choice with our capabilities we'll be starting off today with with batch translations and so today you can now use your GCS bucket take files that you have in a source language say English put that in your GCS bucket in text or HTML is accepted and then place a request to your translation API with your Google project and request the translation to be returned to you in the in file format in a GCS bucket with several you can do multiple languages and you can use a mix of our pre trained translation models or an auto ml model that you've custom trained yourself this allows you to do a single translation API request that refers to GCS bucket and I'm single call you will be able to do multiple files multiple languages multiple models all in a single request and then glossary which we'll talk more in our customer session with we localized you will be able to have greater granular control over things like your brand IP or location names or you know other other specific terminology that is that is tailored to your company if you have a small set of training data additionally to that you want to start with but it is really important to you so you first specific terms you're able to use this with either our pre trained models or our custom models to be able to have control to ensure that the translations are delivering at the right quality and also to reduce the amount of work that your human translators need to do after the translation in machine translation has come out all of these features because they are all part of the same translation API v3 will work together so you can you can use glossary with batch you can use batch with custom models you can use glossary with custom models all through one API and we localized will come up and talk about how this adds value to their customers as an LSP by beining glossary with these various cases to reduce the the overall needs for their company so this is olga barrow gávea vp of language services with localize so i represent one of the world's largest telescope or in the in the US and seventh in the world this is a little bit about us so we have 15 over 1500 employees and damn I should have brought my glasses but ok I didn't so but I don't need to read basically oh yeah I have it all here well I can't see that that well I left my glass but anyways I mean I know my company inside out we're pretty big and what's important we serve a variety of domains and this is where the glossary feature that I'm going to talk about comes in extremely handy because our customers some of us come with huge training corpora of million plus translation units which allows us to avail of our ml model which I'm going to talk about but some of us will come with just glossary or not even that so we need to harvest the main glossary from say internet or open source or translation you automation Association so there is just a tiny glossary at our disposal but a customer would demand domain adaptation or we would work with customers subject matter experts and build a glossary so customization is necessary and I will explain why in a minute so here is the scale of our operation and what I'm going to talk about it's like opinion portal for restaurants today or it's like an automatic automotive company where all they care about is would say is it a crane or is it like whatever so here is we localize for you and this is what we do and you can see like it's life sciences today it's legal tomorrow and it's God knows what day after tomorrow and these are the services that we provide from digital sir and interpretation - which is probably growing field training data for machine learning so here we are and this is where we're gonna talk about glossary stick case study where we're going to compare Google pre-trained models which would be generic models versus Google pre-trained models with customized glossary and then we're going to compare Auto ml models which have trained with customer corpora with a glossary put on top of pre custom models and what the glossary would do for us is allow us to add even more granularity on top of terminology selection and then the languages why did we pick the languages as you can see we have a romance group then we have Germanic group then we have a slavic group and then we have some more representation from the romans group of languages and then we have an asian language so we basically see what the glossary feature what the dictionary feature does for different language groups so obviously we see highly morphological languages slightly less morphological languages and then we see Asian languages which basically operate on the ideograph level so how did we rank the languages how did we evaluate them we have highly trained evaluators that use evaluation for adequacy in fluency and then engine ranking a B ranking based on aro typology and that's a methodology some industries trend industry standard methodology combined with localized proprietary methodology and then we see better quality and user satisfaction which in our case where services providers so it translates into higher quality and subsequently higher productivity which in turn translates for pricing time to market and better final output quality for our customers so why customize a glossary have a look driver driver and driver would provide services as I said will provide services across a variety of domains so do we want driver too when we provide services to a hardware software company obviously not have a look pin pin pin again services across a variety of domains do you want pin 1 pin 2 and pin 3 depending on the domain obviously not now let's have a look at the models see how and that's Google pre-trained model with a dictionary injected on top of it as you can see dictionary pre-trained models with a pre trained model without a dictionary consistently losers - pre trained model with a dictionary and you can see that's consistent across all the languages that were ranked maybe the difference is not as drastic but the difference is consistently there and same goes for the engine ranking across the variety of error types especially with the terminology when it comes to both in domain terminology and customer specific terminology and when we pull our translators what bothers them most as they perform post editing process terminology is usually the biggest culprit because when the translator translates and what the customers are most sensitive to that's usually terminology so higher number when it comes to engine ranking for google play trains nor a pre train models as you can see higher number means which translator is better with the following arrow types again you can see that a higher number means fewer errors and again you see a pretty consistent picture here accuracy and fluency results now you look at Google or ml Google or ml rated for accuracy and fluency and again you see the same consistent picture so engine ranking for google raml again you see the same results whenever the dictionary is injected the translator preference and the ranking is again in favor of dictionary so the summary and next steps we obviously can see that the gains in accuracy and fluency for all the languages using the glossary feature comes on top both on Google with pre-trained models and on Google on Google with our ml so the dictionary injection what does it do in reality whenever you inject the dictionary you refine the terminology whether you are using Google pre-trained models or whether you are using Google or ml and then you're able to refine that and whether you are able to refine the terminology when you use Google or ml yes you train with customer data but still the training data can have some ambiguity some translation segments can translate in certain way some translation segments can translate differently but the dictionary feature is very specific around the terminology now a couple of things of what we're doing at will oka lies we still make sure that we cleanse for named entities such as proper names such as product names such as geographical names such as I don't know location names and then we also make sure that we're very consistent around specifying do not translate and we remove all terminology ambiguity so it's also very important how you massage your glossary is how you massage your dictionaries before you feed them into the Google dictionary feature and then next thing we never just utilize our ml or Google pre strange models they all just by themselves in isolation we always make sure that they are a part of the API integration in our translation management system because both Google pre trains models and Google rml are integrated in our end-to-end global content delivery workflow and that's basically the essence of our partnership so once all these features are in place and once we've expanded the integration to other languages we do see tremendous benefits again as I said we work across a variety of domains and our customers come to us from so many domains and with such variety in how much data they bring to the table that we do see great benefits in this feature and on that note I pass the microphone to sadirah who is the product manager for NLP and she'll continue Thank You Olga hi everyone hi everyone I'm sadirah and I lead NLP at Google for products such as cloud natural language although I'm a natural language and our document understanding AI solution which just announced this week today I'll be covering an overview as part of this session for two of our products cloud national language and auto ml natural language a cloud national language API which is currently in GA supports who various key features a Content classification API is enable you to classify content into over 700 different categories think sports lifestyle entertainment music so on and so forth our sentiment analysis API enables you to detect the prevailing emotion of a writer within a text passage both in conversations as well as documents and we do this at a sentence level at an entity level and a document level sentiment our energy extraction API is enable you to identify commonly found entities on the web such as Google's and organization or beverage is a consumer good so on it support our syntax analysis API is enable you to identify parts of speech at tokens and break down a sentence into its granular granular tokens today we have two new updates on our cloud national language a gie we've added more entities for in voice pacific entity analysis and extraction and we've added new languages to our cloud national language API first our invoice pacific entities think about entities commonly found on receipts such as dates addresses phone numbers companies prices and numerical values we've now loaded this all into a pre-trained entity analysis API available to you as part of cloud natural language this will enable you to automatically convert an unstructured invoice or a receipt in to structured data within a matter of seconds using our pre trained Google model that has been trained on Google data the good part is that this is these addresses are already validated for you by Google Maps so you can easily pipe the address that you've detected for instance on this receipt to the places API on Google Maps you identify let's say you've found a receipt that has an address of a restaurant you can identify the lat/long values of that restaurant and get further metadata on that restaurant or store all of this in structured format into your own database invoice processing becomes very easy in combination with our OCR API you can easily convert all of this unstructured data in the structured data using our entity extraction API s-- second our new languages you may already know that our national language api support multiple languages today we're adding support for Russian and Japanese for some of our more popular api's like dart into the analysis API or entity sentiment API and our syntax analysis api's let me pause here for a bit before moving on to our ml natural language a quick show of hands how many of you have trained a custom national language model quite a few how many of you have deployed a national language model in production a smaller percentage well I hope that by the end of this you'd be as excited as I am to deploy a mattre language model introduction within a matter of hours using our auto ml national language product last year we announced or amyl national language a product that enabled citizen data scientists to train custom natural language models specifically a Content classification model without writing a single single line of code using a simple and hiked simple and easy-to-use interface and the model quality is is inherently high without any hyper parameter tuning involved or any machine learning expertise involved today we're announcing several new updates to our auto ml natural language product so we are now HIPAA compliant our medical customers or healthcare customers can easily support content classification entity extraction as well as sentiment analysis on any medical document formats we're also adding support in addition to our content classification models we're packing in to more models custom models into our ml natural language you can now train a custom entity extraction model using Auriemma natural language as well as a custom sentiment analysis model all you need to do is come into auto emulation language with some amount of labelled data these this label data could be custom classes custom entities or tags you would like to identify on your documents or custom sentiment scores tuned to your own business or domain we enable you to quickly in a four step process that's import your training data click on train train your own custom model for about three to four hours and you get an out-of-the-box custom classification model custom entity extraction or a custom sentiment model that can start predicting so that you can get insights that are relevant and tuned to your business your own domain let me deep dive a little bit into our or AML sentiment analysis feature that we just announced today or another sentiment nabel's you to do domain-specific sentiment analysis on your documents or conversations wise domain-specific sentiment analysis important one of our customers Avalon solutions has used auto ml sentiment analysis to build an empathetic chat bot now this is a stress coating app for workplaces that enables the chat bot to respond with empathy when it detects stress within a conversation so if an employee would type into a chat board it's really hard right now or I'm really stressful work or what one day is going really bad today the chat bot responds sorry to hear that and this is all working on top of all ml sentiments so the alcohol solutions chat bot has auto ml entity extraction story all ml sentiment analysis built-in to detect stress within the conversations so that it can start responding and starting to create a more engaging customer experience with the sentiment analysis feature this can improve the quality of predictions and we've seen that the gin the auto ml sentiment model performs with a higher accuracy or higher precision than a generic sentiment model to detect emotion or sentiment scores that are very tuned to a conversation or a certain business domain and it does not really apply just to a conversational years case think about document specific sentiment scores that may be relevant to detect a sentiment on a financial statement or earnings statements volumes of earnings statements are generated every quarter so we can use all animal sentiment models to predict analyst sentiment towards those earning statements we can print a use or animal sentiment to detect the market sentiment towards news articles or on a lighter note you can also use Auto ml sentiment panel intended to do sentiment analysis on musical lyrics let's say there is a tweet called Delta I was waiting on the tarmac for two hours now this is a totally made-up tweeting sorry Delta you may be familiar with our sentiment analysis API now if we were to pipe this into our sentiment analysis API as you can see the sentiment score is neutral our sentiment analysis API doesn't know that waiting on the runway for two hours is a bad way let's try to apply all animal sentiment analysis to make this better now I've found a data set on Crouch CrowdFlower that is specific to airline tweets multiple Airlines are listed and this is a custom sentiment score from zero to two zero being the most negative and two being the most positive so I've taken this data set and pointed a link to GCS bucket with that data set on auto international language all I need to do is specify that this is the customer sentiment scale that I'm going to be supplying all in my national language zero two to quickly verify that I've my labels in order and click on train now within a matter of three hours I get a custom sentiment model no code involved all I need to do is check the precision and recall look at the confusion matrix check if my sentiment model is confused on all of these three labels and go back and tune my training data if needed look at the false positives and negatives now as you can see our audible sentiment model or airline sentiment model knows that more legroom is definitely good and losing luggage is not that fun and waiting on the tarmac is probably not that fun either so that's sentiment analysis for you our third Auto ml natural language model customed entity extraction enables you to identify domain-specific entities within your document now why would domain-specific entities be relevant to your business imagine you have a document or you're an energy company with oil and gas reservoir parameters that you'd like to extract from within your documents our regular entity analysis API would not be is tuned mostly for detecting web-based entities so it may not perform really well in detecting let's say the density the pressure viscosity values of a certain reservoir in the Gulf of Mexico let's say you have doctor's patient notes that you are trying to analyze and you would like to extract specific symptoms or diseases from within that within that patient note you could be and well the invoice example that we just saw we are able to predict well prices numbers for phone numbers on and so forth but they may be specific dates like billing date supplier date so on of the various variations of entities within those documents that you would like to tune and tag on those documents or other male entity extraction enables you to do just that without any code in wall again all you have to do is provide some examples and we asked for at least 10 mentions of each entity type 100 for a good model so that you can start building a custom entity extraction API that performs really well for your own business specific use cases or your own documents domain-specific documents let's launch into a quick demo hey today we're going to talk about how to build a custom entity extraction model with Google Cloud auto a mental natural language if you've played with the natural language API before you might have seen that it can extract common entities from text like the fact that Google is an organization or that iPhone is a consumer good but what if you want to tag more domain specific entities like types of diseases or the specialized terminology you might find in legal documents or patents for that we'll use auto ml to train and build a custom entity extraction model first we'll need a label data set today we'll build a model that analyzes patents from Google's public patent set using Google visions OCR API we extracted the text from a set of PDFs of patents here we've stored that data in a JSON L file which is the data type that auto ml natural language expects and we've manually tagged within that text the character offsets of the custom entities we want to extract Ottawa mental natural language expects to read our jason l files from a google cloud bucket in addition to our JSON L data we'll make a CSV file that tells it we're in Google Cloud we've stored our data here we specify which data files should be used for training validation and testing the remodel if you don't break your dataset into these three files autum l natural language will do it for you automatically now let's upload our data here in the Ottawa natural language interface we'll create a new data set to import the data we give Auto ml natural language the CSV file we made and then we wait importing data can take a couple of minutes to finish but you'll get an email and it's done once our data is imported we can view individual examples to make sure they were labeled correctly we can check where and the text different custom entities were annotated we can also annotate a new entity right here from within the tool notice that we can also tell how many examples we've got for each label we can begin training a mile by navigating over to the train tab and clicking start training training a model can take up to three hours behind the scenes Auto ml will do a neural architecture search to find the best model it can so now's a good time to take a coffee break they'll get an email when the models finished training when training is finished we'll be able to see our model here in the training tab we can see how well it did by clicking here to see full evaluation here in the evaluate panel we can see the performance of our model on different labels the metrics we'll want to look at our precision and recall precision tells us of the snippets the model labeled applicant how many were really applicants recall tells us of all the real applicants in our dataset what percent were at recognized by the model we can also scroll down to see where the model correctly labelled applicant these are the true positives and we ran accidentally mislabeled Texas applicant or false positives in the test and use tab we can start actually using our model hosted by Google to do that we first have to deploy our model which we take a few minutes next we can try to extract custom entities on a new text snippet there we go here are the custom entities be extracted that's all for now have fun modeling that's excited well that was the demo with that I would love to invite Karen Casas director of engineering at DocuSign who's used Oroville natural language for auto-tagging documents let's find out what he's done thank you thanks for dinner all right Haga hello everyone good morning my name is Kiran I lead mobile engineering at DocuSign and some Pai fun Kea machine learning initiatives you probably know doc assigned today because you used it to e-sign the important moment in your life buying a new house or maybe a new car but today DocuSign is more than a signature company VR and agreements cloud company and helping our customers modernize their systems of agreement part of modernizing systems of agreement we are focused on delivering best-in-class products for preparing agreements signing agreements managing agreements and acting on your agreements as we think about modernizing systems of agreement we are adding intelligence to our agreement process to eliminate and reduce manual actions that's where we partnered with Google for auto tagging while I'm on DocuSign I would love to share a quick story and this this one I love this so this was four plus years ago when I started working with DocuSign one of the evenings I was going back home on BART and I don't know for people who are not local Bart is our local commuter train from downtown back to suburbs so as going back on train this gentleman box over to me and says dude do you work at DocuSign and yeah I do he's like I love DocuSign I just bought a home and I signed everything it was so easy and if you talk to many of my colleagues at DocuSign they will share similar I'll or DocuSign stories and it is such a great honor to work for a product with meaningfully touches people's life and such a huge impact as they are dealing with important moments and issues of their life so that's us that's we whoo-hoo we are at DocuSign now let me talk about the problem we're trying to solve today when our customers send documents out for signing using our web or mobile app they're to manually place tags so what is tagging what is this word tag means tag is a DocuSign term used for identifying form input fields where customers can input data and put and also draw their signatures so let's take an example there's an NDA document that I send out for signing you would expect your customer to input their name the date they signed the document the signature itself all these different input fields are called as tags and as you can see on the animated gif their manual tagging can be done we do it today but it does delay time time to value and it is error-prone our goal is to seamlessly identify tags based on different tag types and categories and place them on the document so that user doesn't have to place a tag move it around make sure it's properly x and y coordinated and it is matching the label for which the tag belongs to so then we thought about solving this problem and by Kiran why why did mobile pick up this problem imagining tagging or placing tags on this form factor it is it is not fun we we definitely wanted to make this process this for my domain it was a big problem and I tried many things we tried OCR we try to OCR with custom code none of them really work at the scale at which dachshund operates and the different use cases my customers had all the different human types I had none of them really really worked so the only way to do this right was using AI and machine learning so we partnered with Google and the first question was why Google I think there are three key reasons I call them out one I believe Google has the best best-in-class product for vision deep learning AI and auto and natural language AI I do think that they have best-in-class products there the second key factor is Auto ml is a very powerful tool which helps us manage or model accuracy in performance and finally partnering Google engineering and product team was a lot of fun I've being a engineer myself I had a lot of fun working with Google team partnering with them because our scale of solution solving this problem could not be done in the first item by ourselves so we needed some help there so essentially we had three different models and I'm gonna walk you through the different architecture how we solve this problem pretty soon but we have three different models first we were we are using vision OCR we are using Auto ml vision object detection model and vision our auto ml natural language entity extraction models and all these three different models we trained with thousands and thousands of documents and hundreds and thousands of tags that's the only way we could get the accuracies we are seeing right now so let me before I actually talk about the architecture let me demo and show you guys how this really works can we switch to demo please so here I have my mobile device and the first thing I'm doing is launching our dock sign app as you can see there's a create button for somebody to create a document as I mentioned before the use case is somebody is trying to create a document so I created a document I'll load a document first I'm from us from my local file directory al pika NDA the document is loaded next thing and typical doc sign world any NDA would require two different people agreeing to an NDA so we call them signers in this case I'm have two different test users I'll use Tamara my first signer Adam the recipient in this case I will use Martin so these are my two different test users I'm using for signing the next thing I want to do is load the document and send that out for signing now as you can see in literally instantly without user having to do any interaction the entire document is tagged all the fields I'm zooming in all the fields are tagged if you look at the bottom of the document both those recipients will need to sign attacked ready to go and all I need to do is long press and assign few fields to Martin and this document is ready to be sent this in so if you can imagine like the entire process of tagging very I had to manually place tags make sure they're line all that is gone so some of you might be thinking that's a simple use case India had like ten fields dude come on really is it that hard so let's take a more complex use case I have another example here so I'm using a u.s. passport form so I don't know how many you but I recently renewed our passport for there at least 350 fields on this document if not more so I'm going to take a u.s. passport form load it for the sake of using a user I'm going to continue seeing Tamera and now I kick off tagging and you can see now this is instantaneous the entire document is stacked the date the check boxes now let's go through the rest of the fields there he goes guys look at look at the beauty of this all these fields are tagged the signature tag is tagged the whole document is tagged and ready to be sent back to the slides please so as simple as the demo was as quick as the demo was that's the beauty of auto-tagging that's the beauty of AI and machine learning you can really simplify and eliminate manual processes that otherwise would have taken a lot of time for a normal users to do so I'm going to now explain how we actually solve this problem and I'm going to start at three different levels at the document level which is simple business for a business owner what does it mean and then I'm going to talk about the architecture and how the models really worked at the core document level let's take an example I'm loading a w-9 form I want to use this for tagging the first thing we do is extract text the OCR or OCR aspect of hey what are the different text components in this document the next thing I do is identify bounding boxes for people who have done machine learning on vision they know bounding boxes is the technical machine learning term for tags and sorry to throw another term on you but they are called as bounding boxes once we get the results back from both of them we we process them to form fill analysis where we apply more contextual meaning of the bounding box with the document for example if there's a sign here and there's a tag next to it it is a signature tag it's not a text field so applying that concept is is the form field analysis we do and then eventually as you see on the screenshot by the way that's coming out from our auto tagging machine learning algorithms so there's a completely tagged a w-9 form ready to be used the next I'm gonna go to a level deeper talk about how it's actually architected a user makes a call to a GCP endpoint we internally call and app engine we need App Engine because I'm not making a straight call to a single model we actually have multiple models who do different work so we want to do some orchestration there and App Engine helps us do that orchestration the first call we make is a simultaneous called to OCR vision model and auto ml a vision of detection model then the idea of those two models is extracting tests and identifying the bounding boxes the results that we get back we feed or to the App Engine App Engine and forwards that request to Auto ml natural language entity extraction model which does form field analysis and processing and those results get fed back to App Engine where we do clean up why we need to do cleanup is there lot of times dad's you overlap if there are two tax very close to each other so there is lot of noise and you wanna eliminate some of those noise and so there's some of that logic is built in our App Engine site it returns as hell back to cloud endpoint and finally we give the results back to the users so it looks it looks a little complicated because for for the scale DocuSign operates we operated billions and billions of documents and transactions we had to we had to use multiple models to come back to come back and provide the proper results that you saw in the demo now let's talk about how do the models really operate so now we understand our document flow works how the architecture well the next step is understanding how models work again this is a very simplistic view internally the models are more complex but just for sake of understanding let's say the vision object detection at the at your right at the very top identified something as a text tag so the category it identified hey I'm looking I looked at the document it looks like it's a tech stack and the OCR model found hey this text tag is between words saying sign in favor of okay now the when we merge the results you see sign there's a text tab and there is sign in favor of so this tag actually is a signature tag so natural language entity extraction classifies bounding boxes and represents them in the context of document text and now the result is you can see the category of those doc the category of the tag is signature so that's a beauty of natural language entity extraction models so I well implementation of this is lot more complicated it does take more time but I'm going to take a step back and talk about what does it mean for users point of view what what does it mean for the business we actually solve three problems at the very foundation we are making agreement processes faster we are eliminating manual steps it becomes easier for customers to use secondly it's once once you apply machine learning logic it's less error-prone it is lot more lot more user-friendly there's more adoption so it's more reliable and like any other machine learning project it is self learning and intelligent as these models are used more as this model are seeing more data and get trained more they just keep getting better over time so that's in in business view we are making a process pass reliable and intelligent finally that's pretty much how doc Stein solved our problem with help of Google and our passive actor so there are two walk us through closing thank you Karen since launch of Ottoman national language we have seen applications across multiple industries from real estate to news media tribal music energy voice of customer analytics and we're extremely humbled by the amount of traction we are seeing thousands of auto ml national language models have been built to classify security documents for GDP our compliance HR files again to track GDP our compliance or to classify certain documents downloads as sensitive document downloads we've seen various applications in news editorial processes we think that we are able to apply natural language to solve various business specific use cases across multiple domains and we're constantly actually excited and sometimes surprised by some of the innovative applications of our technologies our NLP technologies towards solving real business means we'd love to get more feedback from you as you start to try try using auto ml natural which please feel free to reach out to us this is where you can find more information about our products of natural language products or or m/l natural language products and our translate products we also have a session on more advanced use cases and a solution called document understanding ai a deep dive of that will be available at 1 o'clock today in one of my other sessions please feel free to attend that your feedback is always welcome thank you so much for your time today I'll stick around for more questions right after [Applause] [Music] 