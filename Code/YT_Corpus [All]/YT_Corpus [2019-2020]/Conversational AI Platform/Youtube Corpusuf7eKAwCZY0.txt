 [Music] good morning folks Levent basic director product for cloud III business and developer solutions super excited to be here today thanks for coming and joining us here at excel today I'd like to give you a quick overview of our Claudia product portfolio and more importantly actually we're going to have three of our customers come join me on the stage and do a deeper dive into their use cases and talk about how they were using our products to add real value into their businesses so let's get going so I should probably say that your feedback is greatly appreciated so please make sure to form your fill up your surveys after this a quick recap of our mission our mission as cloud III is to empower every enterprise to transform their business with artificial intelligence we believe that machine learning is a fundamental new technology that's going to revolutionize everything in the enterprise and we want to make sure it's accessible to every business on the world and to that end we've been very busy we work on our products to make sure that it's a compressive suite that meets all the needs of our customers and just this year we launched 60 plus products and features on our portfolio serving 26 verticals because we believe that those vertical context-aware use cases and those products are really important to really add business value and on top of that of course you want to make sure that we have a strong ecosystem of partners so you never feel alone in your AI ml journey last but not least we have a still a deep and continuous connection to our data science community within and outside of Google through kaggle which today has more than 3.5 million members so really excited and proud of that all right so as I mentioned customers come to us to use ml in varying levels of needs and requirements and what we find out is that in order to be a powerful platform we need to serve each of these needs and how we do that is that we provide different levels of abstraction layers of abstraction of our products for anyone that might need a specific use case so on the very top here we have our solutions and these ideas solutions are basically plug and play ready to go with our partners so that you can have immediate business value for your specific use case and for your specific workflow without a deep investment of any kind into machine learning one level below that is our api's and our ml which is primarily geared towards our developer friends and again here you don't have to be a machine learning expert but you can still have access to some of the best-in-class tools and api for either with pre-trained api's or with customizable tools like our ml to create a specific model for your needs and now underneath that is of course our powerful AI platform it's a robust and reliable platform for data scientists primarily who wants to create their own models iterate on them collaborate with their colleagues and of course deploy them successfully and all of this of course is built on top of our purpose build infrastructure with GPS and TP use to make sure you get the best efficient performance out of it so I'm gonna go through these one by one starting with our solutions and I won't have to specifically touch on contact center AI so one thing that we see quite often especially from our b2c customers is that they often have this really growing pain in their call centers where they face a trade-off between operational efficiency and efficacy and great excellent customer service well with the advances in language and conversation AI that doesn't have to be the case anymore and with contacts and AI we basically want to solve problem primarily in three levels first with our virtual agent product we want to contain your incoming calls and chat conversations as efficiently and as reliably as possible getting to happier and quicker customer outcomes and if for some reason we cannot contain those calls we want to make sure that we empower your agents in real time with turn-by-turn directions with suggestions and also mapping your entire coops of knowledge your enterprise to their fingertips so that I can they can quickly resolve the customer issue and you know leading to better agent outcomes a better customer happiness and finally we want to give you tools so that you can see what's happening in your call center with our call center analytic tools such as topic modeler so you can see what's happening you can optimize for it and it can feed that back into your call center now I'm going to quickly invite Akash Palmer to stage who's going to talk more about how they were able to use technologies in Marks & Spencer for better outcomes Akash thank you morning everyone I'm Akash Palmer I'm an Enterprise Architect at Marks & Spencer established in 1884 Marks & Spencer is one of British leading retailer selling home clothing and food items in over 1400 stores across 57 countries and through our 50 international websites Marks & Spencer is undergoing a five-year transformation plan everyone in the company is working towards restoring the basics shaping the future and making Marks & Spencer special again so what does that mean to the customer care team I work with in Marks & Spencer this means we have to provide the best customer experience to every collar beat our colleague or our customer and and make sure that experience is memorable so one of the big challenge we had was how do we effectively manage 14 million calls which come into our contact center stores and an head office in the past these calls were managed by completely different platforms with their own IVRS with their own routing and reporting solution and it was expensive plus the experience across them was very very inconsistent so if you as a customer you think it's Marks & Spencer but if we call a contact center you would have a company different experience compared to how what you'll get in the store so a very typical store experience if you had called the store maybe 18 months to two years ago now you'll go to the website you'll pick up the store number of your of your local store you give a call you'll get an IVR and say press one for opening hours Dalian extension press 2 and you know after going through this IVR you probably press an option to speak to someone first person you'll get is somebody on the switchboard they'll try and send you to an extension in store most of the time it will just ring ring ring because the colleague in store is busy dealing with the customer who's actually walked into the store the call will bounce back to the switchboard switchboard will try and that extension this will go in loop till either the customer gets frustrated and hangs up or if you were lucky enough to get the call answered you might and have a query like you know buying something the store colleague will have to then transfer the call to the contact center so a very very inconsistent experience the problem was very clear an IVR which you have to press 1 or 2 cannot really capture hundreds of reasons why a customer is calling us and and then the fact that we were sending a lot of calls to stores which they couldn't even service so we had to come up with a with a solution when we thought about this a very obvious and a very clear solution was why don't we just let the customer say in their own words why they are calling we then understand what they are saying if we find that intent we try and sell serve using automation if we can't we send them to the right place first time so here's what we came up with we call this platform Ava automated voice assistant the fact that ava is the name of our managers granddaughter had got nothing to do with this so just just to make sure and so what what what how does this work so we moved the entry point of all our stores and contact center calls on to the Twilio platform it's a sea pass platform using which you can pretty much talk to any api's you like to this gave us complete control of our calls so now what happens is a call comes in we created an orchestration layer which was within our secure M&S network so call comes in if we take over the control of the call we ask the customer please can you tell us why you're calling they leave in a trance and then we Google speech API transcribe data transfer us we take the text back pass it to dialogue flow data flow will then return an intent at this point we'll try and either do some automation and if we can't we'll have a destination where we can do the call to either the contact center or to the stores so so you know we use the text-to-speech API which was very very useful to make it sound very very human-like and and not too robotic and some you know slight integration with our existing Genesis and Mytyl solutions in stores and contact center overall you know it proved to be a big success and and one of the things which we learned it as you can see the the journey was very very simplified going forward what what happened here was we reduced calls to store by 50% because they would either sell serve they will go out or they would go to the contact center we also replace the did EMF IVR with natural language and this allowed over hundred colleagues who were busy taking the calls in the in the background to be on the shop floor dealing with the customers this was a really great experience to go through and one of the things which we learned was not to get bogged down into making everything hundred was inaccurate you need to let different AI complement each other and work with each other in our early days we were getting some transcriptions from the speech API which were not what we expected it was not word by what what we expected and we had to kind of train the model to make sense of it so I've got a few examples of these transcriptions which were not word by word which makes any sense within M&S have a guess and see whether you know you can you can guess what it means within the M&S world so jamaican order we were like do we sell Jamaican food what kind of Jamaican food we sell you're getting thousands of calls about it it was actually customers calling to make an order and then we just place that into the place order intent it goes to the contact center next time any American order comes it goes to the contact center we started to get calls about Brad Pitt or like we've really won the Oscar here Brad Pitt is calling us suit on actually Brad Pitt this was called about profit which the the int no it's a big business for M&S and we had to send these calls to the profit section in store so next time Brad Pitt calls up you know where he is going to go this one here I did say we want to provide the best experience but we have to draw a line somewhere but this was a colleague calling to find out about their ours and they had to speak to the ops manager to find out when they are working so just to see line you have to play with these and it's not the old world where you have to be accurate about everything just let different AI work together and as long as you get what you want out of it it is perfect so so experimentation rapid experimentation is is the best advice I can give some quick stats around it so this proved to be very very successful we have now so initially we saw customers calling in and there lots of calls were they were just hanging up the call and we're like why are we getting this abandoned calls and then we saw those customer calling back again and we've worked out the customers were not used to kind of saying something as soon as they call they were waiting for this press 1 press 2 and then press something and suddenly they have to speak and then they like okay I don't know what to do hang up the call and then they call up again and like oh yeah I will I want this I want that so over time now 92 percent of the calls when they come in the customer leave a good uh trans for us the transcription rate using the speech API so Google speech API is really really good now we no more bother about getting it 100% correct as well and we just trained the model and then just take it from there the intent model has matured over time so now almost 92 percent of the calls are assigned to the right intent in the contact centers we got rid of reason for contact because we just put the intent as the reason for contact because that is exactly what the customer called about and we know that in their own words overall a very positive experience from the customers oops its future use cases so we have you know as you can see we created a platform which not only solved a problem but it also opened up opportunities for the future so now we know what the customer is calling us about we understand it what we have to do now is to fix the problem solve the problem for the customers so we want to do that through automation personalized self-service we want to also create channels where the customer only channel where the customer can just call in or get in touch with us using any channel they like because now we have the brain pretty much what I call it you can not just push voice through it you can push SMS chat whatsapp any channel can go in and come out with an actionable intent and and overall I mean I wish Google had come up with the the combination to AI 18 months ago because that would have made our life a lot more simpler we are looking forward to experimenting with the new cloud AI features to make every contact our customer has with M&S truly special thank you very much thank you thank you all right that was great to hear about their success story so next up I want to talk a little bit about our developer building blocks and here I'm referring to of course our free train api's as well as our ml so our goal here is very simple we want to provide a comprehend of set of capabilities across four major domains site language conversation structured data to developers so that they can easily use the latest and greatest an ml development and technology in their applications and services and add that layer of intelligence for their purposes and we do that primarily through either our pre-trained api's which are simple API is that you can just call from your applications or services no ml knowledge needed using Google's data of course to train those 80s for common use cases or if you have a custom specific need that needs a custom machine model we of course have our other mouth suite of products that allow you to customize as api's with your own data and the great thing about these capabilities is that as Google once we release that we don't forget about them we continuously drive new improvements into those api's so forest as a speech API convinced it gets better and better you don't have to do anything we just retrain our models and get those accuracies up over time and we of course continue to release new products and as part of that I'm actually very thrilled to announce here today that as of today we have three new products available for general availability for your use our ml vision as well as all ml translation and other a cloud translation API so I can't wait to see what you guys will do with those api's and RMR capabilities now one thing I want to double-click a little bit deeper on is our ml natural language since we see a lot of excitement around this feature basically our ml natural language allows you to create your own custom machine learning model for classification for senton analysis and for entity extraction and the experience is pretty smooth it's a primarily a UI driven experience although we do have api's if you prefer api's we don't discriminate but in our UI you only have to do it just upload your documents your text snippets our ml on the backend trains multiple models before in some of the latest technologies that's available to us multiple architectural types you know feature engineering whatnots picks the best one you get the evaluate which one you like the most or you can just deploy the the best one that our mouth picked and you can actually just deployed immediately to arrest API that's relative scale period so all of this is done through a you are driven experience with no deep knowledge of ml needed so we're of course really excited about this but I actually want to now invite Dan Gilbert up on stage who's going to talk a little bit more about how they use these capabilities and news of your case use cases then Thank You LaVon so yeah thank you my name is Dan I head up the team of machine learning engineers and data scientists that news UK we are a media publisher most of you have probably heard of us so we published titles like The Times the Sun Sunday Times TLS we also include the wireless radio group so includes stations such as virgin and much like our cash in Marks & Spencer and that the times is like an old publication it's been around for more than 200 years and one of our roles is to kind of guide guide the world and the UK through kind of an increasingly confusing time in terms of politics and news and what's happening and increasingly a lot of the content that we produce and we produce a lot of content so dependent it's always tricky to put your finger on a number but somewhere between quarter of a million and half million pieces of content per year we produce across our titles and more and more of that is produced and served and reached his audience online rather than through print and as such we've gone through a digital transformation and much of that digital transformation has been accompanied with machine learning and machine learning and the rise since kind of kind of 10 or so years ago has helped us build better knowledge about our customers how they behave what features of the product they enjoy how to improve the customer experience but despite that a lot of the benefits we've gotten has been through kind of big big data analytics on things like bigquery a lot of the advances from deep learning in terms of things like image recognition have not really helped kind of transformed our business I think the real exception is natural language processing and much like deep learning and image classification kind of seven or eight years ago was going through kind of a moment of massive change we're really in a kind of golden age of natural language processing in terms of the ability to use machines and data to understand the semantic meaning of text and as a news publisher we produce a lot of text so this is incredibly interesting to us and we use it for a range of applications from metadata which I'll talk about today and through to things like automating fact-checking within the newsroom to kind of assist journalists and editors and so metadata is kind of almost like the baseline core piece of NLP that you tend to want to apply when you're in the in a in a content rich business and so we have this article here it's about economics is about UK politics it's about brexit and that metadata is absolutely critical to the success of us particularly in a digital age it helps us build better and new user experiences so when you have a game between kind of quarter and half a million pieces of content over the course of the year that adds up to millions of pieces of content within your archive the ability for it for a user who is kind of short on time to navigate to the content that is meaningful to them is really aided by our ability to put it into collections of topics based on the metadata that sits behind the scenes perhaps most importantly it helps us understand the impact of our publishing decisions so in an age of free news where you can read your news anywhere for free using search ends engines and other other means our ability to understand what it what is a unique to us that our customers value what drives engagement is is ever more important through doing so we can reduce churn by producing content that our readers engage in value more and we can create more relevant advertising so metadata is not new we've had metadata tagging along on content for years I guess what's really really changed is the ability to put that metadata within the same infrastructure as the rest of our data lives to drive better decisions and bed user experiences we've been using Google bigquery for a approximately six years now and Google bigquery is the home of our customer data so kind of people who pay and subscribe to the times the click stream so the behavioral analytics of what happens on our websites and our apps but then most recently the actual content itself so not just the list of content we produce but the actual kind of full body of the text the links or all of the other information but when you're trying to deal with kind of millions of pieces of content and understanding what is working what is not working the metadata is absolutely vital you you can't just rely on the headlines and things like that and so kind of a really transformative moment was bringing in the metadata within to within the same data architecture that this other date and other data sits in traditionally keeping your content data in the same database or data warehouses where your customer data lives would just have been impossible but bigquery is an incredibly flexible tool that allows you to do that and we use google's AI api is to derive a lot of this metadata and i'll talk about a couple of them today so we'll send the content that lives at rest with in bigquery off to the google ml api's but then send that back into bigquery itself so we have this kind of rounded view of the customer the content and the behaviour with our digital products so going back to that this article again so we know it's about kind of bricks it's of bricks it is an entity it's kind of merged into it kind of morphed over time into a topic but it starts out as an entity and things like UK politics economics that is a topic so we're using Google's NLP API is to kind of systematically when we produce new content send it off to the api's take it back we add a bit of kind of business logic and processing on top before surfacing it on the front end of the website but within bigquery we retain all of the kind of the raw data from the API for analytics and data scientists to use but you can only get so far with these kind of I guess out-of-the-box entities and topics types descriptions of 10 as you can probably imagine we've written hundreds of thousands of articles about UK politics over the years thousands of articles about brexit so how do you kind of distinguish between one article about that same topic and another article over here and which types of content really drive user engagement and users to understand the value of our product and so that's where kind of custom kind of metadata comes in and this is a very recent kind of addition to this to our our data architecture so if you take her another article like this earlier in the year we employed kind of a team of very skilled interns to look at our kind of arrange a sample of our content from the previous year and not tag it based on topics and entities but using a kind of a much richer classification system which was developed by a company called KITT in Sweden and what that led to is for a given article like this the ability to kind of get a much deeper understanding of the nature the tonality of the content and then when you run analytics on top of this it gives you a much more kind of nuanced view of the type of content that works well and doesn't work well with our audiences so rather than being restricted just to just to a kind of a broad topic of think in this case kind of music and books you can get a sense of whether kind of the field reporting style of interviewing it kind of performs better with certain audiences than other types but but obviously doing that process once kind of using again some very skilled interns it is not a sustainable ongoing process and we're kind of scratching our heads for a while on how do we kind of like scale this out so that's where kind of Auto ml entered the picture so also ml is again it's part of kind of Google suite of ML API products sitting alongside kind of the natural language API and what it enabled us to do is so whilst we have some very talented kind of data scientists and the team who have kind of deep expertise in natural language processing what this allowed us to do is someone who didn't necessarily have that background to kind of upload this data on the content that had been manually labeled by people within the newsroom and then train models to try and understand whether we can systematically predict which label new content would have and so here you see for one of our so I think overall there were kind of 10 to 20 new kind of custom metadata types within which you'd have kind of ten or so individual values so you'd have many dozens of potential metadata tags you'd want to assign so in this case we uploaded the content with the tags and then without a game without having a deep understanding of how NLP approaches work you can have a look at the performance of the models and so in a case like this things like whether the article was deemed authoritarian or objective the model performed really well what this then means is that we can kind of systematically tag new content as it comes through and is produced without having to have someone manually label that as with all these things there are nuances so if you're expecting the machine to be able to tell you as something is funny or not it is pretty bad it has like a four percent accuracy rate in terms of determining whether a piece of content is humoristic or not so it's not a panacea for everything but it massively advances our ability to apply this much more nuanced metadata to our chronic content on a systematic basis so we used auto ml text classification which really put the hands in the hands of our arc of analysts some of the latest capabilities in natural language processing the commercial benefits really being asked to being able to identify exactly the type of content and parts of our journalism that drive the most engagement supporting acquisition and retention and so kind of making learnings around kind of what type of content works well in home news within business understanding kind of the breadth of content that would interest people things like world news understanding that it's actually local more nuance to reporting rather than informative pieces that work better trying to describe metadata that with traditional approaches would have been very difficult with this new approach that that starts to become possible technical benefits are applying this custom metadata and it's kind of like a kind of a layered cake so the auto ml on top of the the more fundamental entities and topics using the NLP API and putting these NLP capabilities in into the hands of an analyst and the real benefit is and often to kind of I guess the skepticism both myself and also people in the team is the ability to label with a relatively small number of input data so traditionally when you're trying to classify using NLP techniques you may need thousands of examples of a particular label to assign to a piece of content and in this case we've had success where you've had as as few as a hundred or so positive labels for a particular type of metadata when you're in that world it reduces the the kind of the time to do this by hundreds and hundreds of man-hours and what that allows is kind of the the people in particularly with NLP expertise to focus their problems on other new more difficult challenging problems such as fact checking with the newsroom thank you so I'll hand back to lavond thank you I think that's fascinating you gotta watch out for this little interspace here it's easy to fall back great so moving on last but not least I'm going to spend a few minutes on our AI platform with our AR platform our goal is simple we want to provide a robust reliable powerful platform for data scientists to create their own models from scratch to iterate on them on those models to do your data science workflows effectively and efficiently and as part of that of course we offer our this end-to-end code base development environment specifically for AI inside of GCP it's an integrated tool chain from data labeling - of course built-in algorithms to Train and prediction and not only it's integrated within themselves but of course it's also seamlessly integrated with the rest of the GCP products such as data pub/sub data flow and of course bigquery so again our goal is to make it as simple as possible for data scientists to create the great models out there after now one thing I wanna briefly touch on is the need for explainable AI this is a been a rising need especially for our customers in in healthcare in financial services for many reasons first of course with the rise of deep learning you know it is harder to understand why a model is doing what it's doing and why a prediction is is happening with multiple layers of neural networks and the best of course benefit is for the data scientists themselves right to understand that you know or develop a deeper understanding of their models to better debug it to better understand why the mouse doing what it's doing but it doesn't stop there natural also helps you communicate the value of top model to other parties within your company and outside of your company but most importantly it helps you build user trust especially if you're predicting things like credit scores or a cancer diagnostics you want to know how the model works and of course your customers and users want to be able to trust that model so to that end as Google we're trying to build multiple tools here in the space and have already a few things in place but there's more exciting news to come on this so stay tuned for that all right now in my Jacob Eggers to stage who's gonna talk a little bit more about how they'd be using our cloud platform with embarr to help specific use cases and Diagnostics pair Jacob [Applause] okay thank you very much Wow a lot of people welcome I'm gonna take you through our journey in the use of data in healthcare and I was asked to speak about AI in healthcare and I thought that's huge AI in healthcare so I sit in Bear which is a pharma company and you probably know what is the aspirin company and I want to take you through our journey through data and AI so first one of the things bear does is radiology and that sort of got us into this whole arena and field because it's really big data a lot of them are 3d images which I'll show you coming up as well as when we talk about EMR data which is that metadata so as we move forwards here we have a system called router metrics which is our platform that allows us to connect to many different types of data in hospitals and it's very disparate data because some of it is metadata like your EMR electronic medical record and other data are images and often like if you've ever had a CT scan on how many people have had a CT or an MRI a few hands showing up those datasets often are 512 by 512 by 512 their 3d data sets it's a lot of data and through our platform we've currently have about I don't know give or take a few million 26 million patients come through it so you're talking about petabytes of data so as we move forwards on this and I've already mentioned this to you one there's on-prem data that we have to digest and understand in order to apply our models there's the integrity of the data and workflow because different hospitals have different systems we have to integrate with a lot of different and non-standardized systems data access can be an issue because of these non standardized systems and obviously data security is a big piece of what we do because in healthcare data everyone's worried about where their data and that they want to make sure it's secured let me take you through our journey here so when we wanted to develop an intelligent data Lake we used a combination of some work on Google cloud with quantify and this is sort of how we bring our data sources in that's our ingestion layer then we have a D identification layer which you can use google has out an API healthcare API that allows you to not only ingest different data types but work on D identifying that data some of the data we do D identify the imaging data through our own proprietary methods some of the other data that can be very difficult to de-identify like doctors notes and they can write in their notes mr. and mrs. so-and-so's husband or whatever in the estart their personal information you need to remove from your data itself which can be difficult and then from there there's our data mark that we then can use to build inference engines and make diagnosis so as we move forwards here one of the big pieces of work in any sort of machine learning unless it's unsupervised we're not talking about unsupervised learning here is the annotation or that tagging methods as you all know so the tag data in healthcare it's a little different if you have a self-driving auto driving car that you're working on an algorithm for almost any of us can find the stop signs even if they're on the right or left side of the road depending on your US or England and at the same time you can find the people in the cars in healthcare it's a little more difficult you need experts to annotate these things we have a whole platform to allow us to annotate in 3d otherwise we take forever I mean if you had to do 512 images for just one patient one CT scan you'd be there forever okay when our data comes in if we look at let's say 50,000 patients that may turn out to be 2 3 4 million images you really do need to annotate in 3d we have technologists annotate first for Anatomy then we have radiologists and at 8 for the abnormalities and then we overeat quality is a huge issue and it's not so much quality meaning people annotated it wrong it's not like a stop sign where I think all of us would agree it is a stop sign or it isn't in healthcare there's about a 20% discrepancy between one train what one doctor says and another or one radiologist says in another so there's discrepancies in data because there's different opinions so all of these are challenges it's just a quick insight into our annotation platform they're semi-automatic tools that allow you and I'm showing picture here the chest and the heart allow you to segment the ventricles and then you can by hand correct any of that with a 3d painting tools and then you can use clipping tools you're not using scissors along a line here you're using planes to cut things so these are how we actually annotate the data so there's some challenges though and some of them I've already mentioned and want to take a long time here we're not trying to turn everyone here into a data scientist in the next half hour take maybe 45 minutes but in healthcare the data we're looking at from imaging is all 3d and we're also looking at the EMR data which is obviously not imaging data but some of the problems there is people come in all different sizes you have pediatric patients and tall adults short adults you have many different size datasets it's not standardized and on top of that they can come with all different resolutions each Hospital does their own thing each doctor may do their own thing so you have to deal with all these different challenges between disparate data sets so what do you do about that well some of these data sets are so large they can't be analyzed in one inference so you have to divide them up like with cube flow or something else they'll show you examples but you also have to make and this is where data science comes in and where physicians come in some common-sense decisions so some things you want to do you need full resolution meaning you need that original 512 by 512 by 512 data set and there are other things where you can condense that data set down resample it to like 128 cubed and you have to know whether your problem allows you to do that so let me show you an example so if you look at this example here and this is where we're using a 3d u net as you can see on the left side on the right we're looking at a stent and you know like a cardiac stent you can see all the little dots of the wires in the stent at one resolution you can easily make out that it's got wires in the stent as the resolution decreases it certainly turns into like a doughnut you may ahead this morning and your problem may be good enough to just know it was a doughnut and other times we need to know actually where the wires are in the stem if you just wanted to measure how big the diameter is of the vessel the dark part in the middle of the doughnut the the Dunkin Donuts donut holes if that's what you wanted to figure out the low-resolution maybe in office you have to understand your problem before you can decide what tools to use that's my point here so if you need whoops I think we have two slides ahead there yeah if you need the full resolution we do use cube flow and that allows you to do your inference on each of the cubes at full resolution rather than resampling it to lower resolution and on the side here I'm showing you sometimes we use squeeze net on the upper image and on the lower image that's dense net and we've used both to make inferences using the cube flow engine the real challenge is once you've implemented this for each of the cubes how do you put all the cubes together it's like a flock of birds do you need more than two cubes to be positive or three cubes in order to say that your ml engine should give you an inference or a classifier as positive and these are some of the questions that come up the tools will and need to be built in the future and some of the things that we work on so let's just take an example here for I think again I scooped yeah so a case here lung disease classification when working on classifying diseases in the lung again we need to leverage the AI workspace we develop radiology solutions to this that use both the EMR data as well as the imaging data and then we try to classify the pulmonary abnormalities so as you step forwards and you have to have your image ingestion as well as your other data ingestion there's pre-processing there's training and then there's patient level aggregation that's where you have to put the cubes together and finally a classification result let me show you a little bit better diagram of this this is one of the key slides and how we approach things that you bring in disparate data we have a supervisory a I bought or like a inference manager that then manages multiple deep learning algorithms in each of those looks at different features and then the inference engine can decide what AI to run which of the different modules and then make a differential diagnosis at the end and you can have multiple more modules I'm showing a cardiac a pulmonary vascular you can on you could add on other modules for cancer for breast analysis and so forth as the system grows so it's very scalable I just wanted to show you a few results again after you've annotated or tagged your data you may want to do like a segmentation a little different than what you've heard about earlier a lot of that's classification or NLP segmentation here allows you and this is an example of the heart to look at the ventricles individually the atria the aorta the pulmonary arteries in order to be able to make diagnoses of like left heart failure right heart failure and so forth this is one of those engines here built using 3d unit and then the other point I want to bring out is and we're here about AI and we're here about deep learning and we do a lot with it but at least some percentage still requires other algorithms and we still do a little bit of image processing on the back end in order to then reanalyze these inference output so you saw the segmentation masks we then can fit these spheres or in this case ellipse I to the ventricles or to the septum to tell you more about how your heart's functioning so what I want to end with is what is the future imaging in the future of AI in healthcare is very big it's very broad there's a lot of people working in it so imagine a deep learning system where you did have enough data to build kind of whatever you felt like now what if we could make a diagnosis as simple as like a Google search or we could eliminate the diagnostic errors from us data we know we spent over three trillion on health care but almost 1/3 of it we believe is wasted so these are big things that we can bring to improve healthcare for every individual not only in this room but throughout the world and that's really where we dream about using the different tools we talked about today so thank you very much thank you so much thank you all right so today you heard from our you know three customers about their use cases you heard a quick overview of our clutter our product portfolio we have many other customers in Amyas specifically using cloud III some of them are which actually are here today doing sessions so make sure to check them out I want us to do a quick shout out to our growing ecosystem of partners AI journey is it can be simple can be difficult we want to make sure that there's a this growing ecosystem of our partners so that you never feel alone in this journey and you get the help that you need but made the integration maybe other purpose of AI into your workflows and with that I want to thank you hope you enjoyed our quick overview and our customer stories I can't wait to see you guys use our tools and hopefully transfer your business with AI thank you [Applause] [Music] you [Music] 