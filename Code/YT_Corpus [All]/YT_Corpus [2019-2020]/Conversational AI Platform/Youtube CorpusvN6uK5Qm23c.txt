 my name is Charles bear I'm a solution architect here based in this office so I have a talk to give on some surplus and AI ml solutions we have one that I've developed another one that we have developed jointly here as as a part of Google cloud for a solution alright good to go okay so that's what I said yeah an ml so the first one is a project that I worked on recently and this is you know there's a trade-off when when you're looking at to build or to make use of some a I and ml as a part of your application and the point of this project was to make it easy for one of our customers to be able to ingest the whole series of different audio files and these were long ladee files they had a platform that they would accept audio files from and they weren't able to categorize these in an intelligent way or they could categories of subsets of them were they relied on their users to categorize them for them so wasn't optimal for them and so we came up with solution here that was it was pretty interesting so the main problem here again they've got a website they allow users to upload any of their data any of their audio files and for example these could be things like podcasts and then those content would be prioritized and and categorized in different ways tagged make it easier to surface and for other people to find so this was kind of the the main point of it here that the problem with this is that they essentially allowed users to do that so they the the content could have been not properly tagged when somebody uploaded it it may be improperly you know inadvertently or maliciously tagged in different ways and then you're relying essentially and your users to be able to find that for you so this wasn't an ideal situation for them but it's also not an easy problem to solve right because in order to do this you would have to then go through and process all the audio you could easily listen to it we could process it somehow you could write your own I'll wear them to go and process that data and in translate that audio file into text and then and do some things with it essentially there's a couple problems with this you can imagine if you have operate any kind of site at scale you'd have a lot of different West's you're not just going to have one or two uploads you're gonna have thousands potentially hundreds of thousands depending on the populate of your site this particular customer had a pretty big problem with scale they had a lot of lot of audio submissions so you need to be able to do this at scale the second one is performance you need to build to do this relatively quickly or you need to be able to decouple this so you have a whole queue of different files that you're going through and you're able to do the transcription and categorize them appropriately so how do you do this at scale and while still having good performance with your with your system that was the other challenge and the last one is you know the audio format itself is not necessarily conducive you don't you don't have a simple way of understanding you have to do a significant amount of translation from the actual hurts that are recorded and the way that's represented an audio file to actually think about categorizing that and that's because there's are essentially two steps you have to convert that to text understand what kind of text that would be that the context of the text and then make a decision based on that point on the categorization and how what what specific action you would take so those are the main main problems to solve that I had working with this customer came with this reference architecture fancy server and for a reference architecture but the the main point here is if we sort of walk through this we we made use of a couple of machine learning api's that Google provides and in one of those is our speech of text so this is pretty should be pretty straightforward with speech of text it takes in either an audio file or it could be actually you can actually stream real in real time the audio back and forth and get responses but I'll take that audio file and convert it to text so it seems like a relatively straightforward thing to do this is something that you know as as a developer you could decide to do yourself and there are some benefits for from doing yourself for example if you are working in a problem or a domain space where you have very specific jargon or a very specific terminology that means a certain thing so for example financial services might one of those where if you of you're thinking if your recording has very specific terms that means something really only to that your your domain you may actually wanted to build your own model because your audio model may or may not or the the speech of text API may or may not pick that up in this case we're essentially mostly focused on podcasts and things like that which are essentially relatively straightforward we can make some assumptions about not being in a specific business domain so for that case we decided to save a lot of time and energy you've actually developed developing your own model by able to just using the API and it's really straightforward API you send in an audio file and essentially over time it will go through that that audio file and then return to you a set of text which is which is in contained in the audio file built into the platform and that API is the ability to you know essentially handle that long running kind of polling for your free application so for us that made a really good really good case for using that because we didn't have a specific reason to develop our own so for us performance scalability and our our ability to meet the requirements or well-suited to to use the speech-to-text the other one is our natural language processing so essentially if you take you that text that you've converted now how do you categorize that pull out entities or sentiment analysis from that so again you faced with the same choice you can actually you build your own and this is you could build something like this with tensorflow there's many there's a couple of open source ones you could make use of as well so you use someone else use transfer learning to train the last bit and use that as a part of your API for for our purposes what we wanted to do is plot entities as a way to help categorize as well as sentiment when it was positive negative or sentiment relative to individual entities so our our cloud NLP API provides that for us made it really straightforward as an option to you so again we decided to use this against our requirements for scalability performance and be able to meet the business case and so we we made use of that as well a couple other things where the worth note on that I'll sort of walk through this the the glue that stitches this all together is essentially an asynchronous type of cloud function the cloud functions are asynchronous type of you know is very good for building an asynchronous event-driven type of platform because it decouples the the real-time the synchronous requirements from an application and allows it to be driven by specific of events in this case because we stitch several different api's together when the results of those api's became available or we're scheduled otherwise we went through and we were able to - checked and so that actually helps from a scalability perfectas perspective because we have can scale up the number of cloud functions we have we have a large load it will automatically scale to do that it also allows to scale down when we don't have a large load as well so it doesn't require constant running so therefore we can reduce our costs as well as usage so spending a minute walking through the the architecture so we have audio files obviously in this is sort of a step one somebody uploads an audio file and this could be as a part of any of your process but in our case you know we essentially assumed that was that was the entry point we would upload it to our cloud storage and that would be the beginning of our business process here so step number two we have a kind of a handy function within cloud storage that if you put something into a given bucket and cloud storage it will kick off a trigger and that trigger would allows you to then notice and take action on that trigger so our our cloud function is also triggered by by that that file that's being placed in there so every time I files place in their users uploaded their files places as a part of this process it'll kick off this cloud function this cloud function will then go from three to number four and actually call the speech-to-text api so we take that file sends the speech-to-text api it's gonna return a job ID because again we're sending in a file so it's gonna get me a job ID which i can check overtime what we did here in in number five six and seven is essentially once we got that that speech-to-text job ID was out okay we will actually we need to keep checking on that so we would publish it to two pub sub in pub subs another part of the event driven architecture which allows you to essentially build it scale because pubs you can send individual messages and then other computing components like cloud functions or really many of the other components of the platform can listen to those messages as well so when we publish that that message whatever is listening to whatever is in that queue will then be be triggered so what we did is we published that job ID once we got from the speech-to-text api publish that job ID and then every 10 minutes and when this is configurable of course we have something called cloud scheduler it just kicks off very simple pub/sub message or invoke one of your actions the the main point of this is that all of those job IDs that sort of get that get aggregated here from 10 minutes or so in number 5 are then are then essentially picked up by because the cloud scheduler will trigger a cloud function another cloud function in number 7 so essentially that cloud function will check with the speech of text API and say hey are you done with my results do you have my translation for me yet if it does then what we do is we store those results here in number 8 and then what we'll do is trigger two more cloud functions and because again we're storing a file in cloud storage you can you can use those handy triggers so we use a trigger here to kick off the NLP API so then using our NLP API super straightforward we we grab the entities within the speech of text so all of the you know person places and things that are within that that that text we have access to those we also have access to the sentiment relative to those individual items or the sentiment in general for the entire the audio component the other thing that we did though we wanted to do was use something called the perspective API and this would essentially decide whether or not you know the components or the text that we're sending in is toxic to conversations this is something that it's been jointly developed and Google's contribute to from it from the YouTube perspective as well it's a third party API but again just shows you a way that you can integrate your process to then look at the the content you have is there something that is super negative in this in this in this discussion or in this audio cat that you may want to either tag or otherwise know about so when we when we put that file in there both of those cloud functions those cloud functions call the api's and then essentially store the results in cloud storage and then we put a very simple UI which is App Engine app engines is are essentially platform-as-a-service great for running super scalable or even simple web apps you just write the code and deploy it so we essentially build a front-end around that so this is essentially the reference architecture here and I can spend a minute are there any questions on the architecture before we move forward I've been talking for a few minutes see if anyone's here all right ok well I can go through a quick demo of it here I will see if I can go through a quick demo should go through a quick demo ok so again this is see if I can eat this a little bit bigger here all right a little bit better this is a very super simple UI essentially lists a series of files that we've uploaded and then based on the results of those files and again we ran through this through previously because with the speech-to-text API takes about half of the time of the actual audio length to actually process so I didn't want to do this live but you we have the results here from from the conversion let's see if we can load it here there we go okay so again super simple API we have both the full transcript so this is the entire text that we got back and then if these are essentially rating the the types of comments and the individual comments and so you see these are all green in the case that none of them really from you know from a from a negativity perspective provide as any indication that they would be a negative comment what what is interesting here though is that when you click on the individual kind of component that is highlighted here this is provided by the speech-to-text api it gives you the you know what this is this is the the information that we get from the perspective API it also gives you the start and end time which is super helpful if you want to go back and be able to look at that individual content and then from the NLP perspective it gives you a really nice sense of which entities are actually in this individual subtext so you can see you know some of the things that it pulled out of here some of them are useful some of them are not so useful but it also gives you then a sort of a sentiment so if you look at the color coding up here it will give you a sentiment and you can use these for categorization right because these are essentially the types of labels you can use to actually categorize that content that you have so we again built the UI just to surface those and individual components you could built these into the tagging if you were actually building a website okay alright so that's that's it for for the demo any questions on the the this particular architecture or the components itself yeah a platform if not I have one more section to go through as well go through that alright good okay so this is a different solution so the previous one is something that I wrote with with one of our customers and our professional services team this one is actually a solution that has been broadly put together by the product team it combines a couple of different technologies for a very specific applied use case context center API and the point of this one is that I think you've probably all had some experience with the contact center either a chat bot or if you have you know calling with your phone and you know oftentimes those experiences don't don't go terribly well you have to repeat your information a lot so this is a way to help optimize that in two different ways one to help understand and provide an intelligent chat bot functionality which can integrate into your back office system and pull a lot of data from your own systems and make it much more intelligent the second way is to to provide a phone chat agent which essentially a virtual agent which will respond to be able to do the same thing as you could with a chat bot but it's using our text-to-speech API so it essentially provides that and then our dialogue flow if you work with that at all this is essentially kind of like the Google homestyle type of back-and-forth using speech and then and then honing in a specific intention that a user may had via voice and then there's a whole framework that's provided by dialogue flow so if we think about you know this as a business problem the problem is these these conversations that you have with with the contact centers really aren't that great and they're not great for the user but they're also not that great for the company as well so they don't off they often don't provide a great outcome or they require a human to get on the phone and speak with you what otherwise might be a relatively straightforward or otherwise kind of a mundane type of question so in order to enable a better economic conversation there's really three main components which you see here the perceive talk and interact the perceive is really what we there are speech attacks you need to be able to to take the audio that you hear and convert that into text reliably in order to be able to respond to that you also need to be able to take that text that you get back from your back office systems or your intentions in dialog flow and be able to respond to that user in a verbal way and so that is our text-to-speech API and that's how we provide that and the last one is sort of a whole framework around how you have an interaction with via voice or be a chat bot and that that whole framework of doing so and that's what our our dialogue product does so the the main components of where this this contact center API fits in or essentially what I kind of describe here is a customer either phone or chat so this is again the voice or chat and typically where this fits in is where you those individual users are routed into a customer contact center that and in customers have a lot of different or users and companies have a lot of different types of solutions that they use here across a wide range there could be IVR systems they could be you know there's there's many many different systems that are offered by many many different companies but essentially they all have hooks into third parties and external systems to allow them to integrate because that's how the call sensors typically work so where the contact center ai4 from Google Cloud fits in is essentially right here with the contact interface we provide an API that allows you to call and respond and two main functionalities a virtual agent so this is the this is the voice component of this so I talked about you know having the the text-to-speech being able to respond in an intelligent way and in go in using dialogue flow to go back and forth and be able to answer the questions within the context of the conversation that you're having versus just be able to to to answer the specific context with with the answer without any real context so that's the big difference here and what something that's kind of interesting here is agent assist and this is even if you do end up end up with a live user and as a part of the the virtual agent we have this capability which will be in the background and essentially be listening for the car listening to the conversation and the customer or they call her in maybe you ask those questions it will automatically go out to a knowledge base or any third you know whatever type of system you may have to look up information and what's really cool about that is as the as the user may be asking questions the agent says we'll be pulling up information from there from that knowledge base from the from the help center etc and pulling that up there for the agent so they can be better able to answer the types of questions you're asking it's a really pretty cool feature and then what you see here is the backend for filling in this is the integration with a lot of you know the third-party system before they you're an organization system so this could be their account system you want to look up the account look up the hours for the individual location it's a whole series of different api's you can imagine that'll be available that if we expose and if you are essentially your knowledge base as well so if you're looking for information within the context of commonly asked questions this is a great way to integrate this into the context center API okay so I think we we talked through this I mentioned we have a lot of technology partners and the reason that we have partners here is because these are typically things that companies already use and the way that this is kind of understood is a way to and drive intelligence into this or be able to bring your the data you already have in your organization into into the conversation here okay so the virtual agent is the the text-to-speech bit and so this is this uses the same IP API same style API as the speech-to-text in the sense that you send an audio file for for the speech of text you do the reverse right and so it you can do things like send in your text it will it will it will respond in a series of different languages languages so there's 30 different languages not and someone on the order of 180 different voices there's a lot of different components that are available there which is pretty cool the the agent assist that's what I talked about being able to reach out to third-party systems and pull that back and then we have a third part we're just working on now the insights API which is essentially looking looking at all of your your data that you they're essentially generating by running the system and be able to pull some metrics and we'll pull the trends out of that so what does it actually look like if you're if you're using this I so the the first part here is the you know a customer call comes in into a telephony Center and these are usually the some of the partners you just saw in the previous slide Cisco this is typically like a sip type of system and then that gets routed into our cloud speech-to-text and our speech to text is can in real-time take the streaming audio that you have and automatically return the text here in this case it's going to Malaga flow and the dialogue flow as I told you before it's kind of the context of how do you have this conversation and back and forth and you can develop the the dialogue flow flow with this is the specific of your business case here so in this case you have one you've got the virtual agent here so again if this is the the audio component here we will use the the cloud text-to-speech to be able to respond directly to that to that user and stream the audio back to the call center so essentially you know the users interacting with the the IVR but really it's loading this agent in dialogue flow looking up some information and then responding to that user with with voice as well the second part of that is the the natural language understanding here so this is building outs what what kind of context what kind of context and you pull out of what the user is saying so again is using the speech of text is taking that speech to text API and look and looking up the the results of that in in a in the database or other third party you know it could be within an organization you know their accounts etc looking up that data and then responding back and then you have the the full session of transcripts of your speech text so you can have the sort of the back and forth between that customer you can use for for training and additional so I think the the key components of the platform and why this could be difficult in the future or difficult if you didn't have this is this these components right here represent a pretty high barrier to actually providing this this intelligence right so if you already had the data that or the text it wouldn't be that difficult to then go and build in the intelligence for that but if you but the api's themselves provide a lot of value makes it really simple to be able to integrate and so that's the whole the whole point of this here and and so if you've if you use our api's you know between the authentication the the developer experience they're relatively straightforward and you know compared to what you would need to build for a comparable type of architecture you start way ahead though you have some limitations as I mentioned before about you know maybe domain-specific these models are something models that we maintain as Google Cloud and allows you to use them pretty straightforward in a pretty straightforward manner just like any other API which is really the power of the platform here and then the the other component is the dialogue flow in order to build something that has sort of a context and a conversational type of context it takes a lot of effort to be able to do that it's pretty straightforward to write if-then statements or to write switch statements in as a part of your application to handle different components but being able to have the context of the conversation and then use that later as you're answering questions or doing searches is something that really adds a lot of value so if you were to develop that it adds I would certainly add a lot of time so I think those are the three components that as we built this solution seem to add and offer a lot of flexibility offer a lot of value to people as you use them as well again it brings it back to the developer experience so very quick example here from an architecture perspective if if any of you are interested in architecture but generally the same type of flow customer you have various channels that come in and they'll come in either via via chat or via the via voice and this is how we fit in here so nothing I think I really want to highlight here other than you know we integrate with with different partners here all right ok and this is the interesting thing about the insight so if you if you have access to these conversations that you've been having in or that your users have been having with your call center or your virtual agents you can actually do something interesting over time right you can start to notice that all call centers do this they keep track of metrics and then take actionable measurable actions on that so one of the things that we did is for the insight API it essentially provides a framework for use a hook into and this is a very similar type of architecture that you saw from a service perspective we use the whole set of service functions like cloud functions and our speech-to-text api we also have a data loss prevention API so as as our as our chats flowed through here we ran all of the all of the chats through this particular flow again this is server list and we store that in bigquery and then we we essentially expose big queries our server list you know data warehouse if you haven't worked with it before so we use that as sort of the component that the data house component of that and then we built some a whole bunch of different dashboards using an app on top of App Engine but the point here is that we were able to gain a lot of insights into like the the the durations of the calls what types of calls they were the percentage by which you know the the automated or the agent was able to respond versus had to call in a human the extent to which the customer rated as satisfactory afterwards the number of engagement with with with chat BOTS versus you know in person or or voice calls so we're able to pull all of this together and again we built this this is a pretty straightforward it's it's an event event-driven architecture as well and it's also service let's makes it really easy to be able to spin up and spin down and not worry about sort of the consumption or the ongoing costs so that was also a pretty interesting component of that so I think that that's all I had to talk about today mainly the two projects that I worked on from from a service perspective from an FML and AI developer perspective 