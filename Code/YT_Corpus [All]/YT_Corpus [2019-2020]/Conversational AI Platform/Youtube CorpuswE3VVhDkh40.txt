 [MUSIC PLAYING] Hi. Welcome to the session. My name is Dhaya. I'm a product expert with SAP Conversational AI Team. In today's session, we'll understand best practices and tips to build a successful chatbot. At CAI, we have devised a project methodology that involves five steps in building a successful chatbot project. Let's understand what these five steps are. The very first step is to build horizontal coverage. What does horizontal coverage mean? Essentially, horizontal coverage means enabling the bot to understand every request of the user. So when we typically start a project, we start with a typical project scope-- that is, the scope of activities that the chatbot aims to automate for the user. Now, typically, users could ask questions outside the scope of the project. So what happens, how does the chatbot respond to such a request when it is outside the scope? So that is where we build the horizontal coverage to ensure that the bot understands every request of the user and directs or navigates the user toward a social page or a link, or sometimes to an agent. We'll understand more about this later. The second step is to actually build the use cases. Right? So for the use case as defined, we go through the whole process of develop, test, and ensure that the bot is working, functioning well. And once that is done, then we are ready to extend the chatbot for testing with actual end users. And in the alpha testing phase is where we ask users to test in our own environment. And once the testing is done and the feedback is received and all the changes are incorporated, then we are ready for beta testing, which is essentially testing the bot with the end users, but in their environment. And then we repeat the process of testing and receiving feedback, incorporating changes, and fixing errors. And finally, we're ready to go live and extend the adoption to customers globally. And that's the ramp-up phase. First of all, why do we need a good project methodology? There are a few good reasons why we believe a good project methodology really helps in building a successful chatbot project. First of all, it helps you deliver to your intended return on investment. We start the chatbot project with defining, why are we building this chatbot? And then we do a workshop to determine what metrics will be used to determine the ROI. So following a proven project methodology such as this, which has been tried and tested with several chatbot implementations, really helps us to deliver to the intended ROI. And also it helps in delivering a good conversational experience. We talked briefly about horizontal coverage, right? When the user asks questions outside the scope, how does the bot respond? So it has to deliver a good conversational experience in a meaningful way. And also the project methodology incorporates several of the pitfalls and mistakes that could be potentially avoided. And this is based on learnings and best practices from the implementation of several chatbot projects. So the first step, which is step zero, is to choose a use case with a business value and a strong ROI. As I said, we start with asking ourselves, why are we building what we are building? What is the ROI? And how are we going to measure the ROI? That is where we define key metrices. Now let's try and understand, how do we go about building horizontal coverage? Essentially, there are five steps involved in building horizontal coverage. The first step would be to start collecting expressions, gather expressions. In certain projects, you already have a ready log to refer to. And in certain projects, you do not have a log. So that's where we follow the approach of crowdsourcing. Crowdsourcing is nothing but identifying representative users and asking them to converse with the bot and ask the questions that they would like the bot to respond to. We follow this approach with several representative users until we gather a few thousand expressions. Ideally 10,000 expressions, but in certain projects, we start with even 3,000 or 4,000 expressions. So once we gather this huge number of expressions, the next step would be to cluster them logically into clusters, in a way that is homogeneous within and heterogeneous across, based on the user's intention behind the expression. So let us say a user wants to understand anything about vacation-- taking a vacation or leave requests. So we gather all those expressions into one cluster and identify an intent for that. So as I said, all expressions within a cluster or an intent should mean the same user intention, which essentially means that these are the different ways the user would like to ask the same thing, pertaining to, let's say, leave request. So once we've gathered, clustered all these expressions and identified intents, the next step would be to run the training analytics using the platform and check [? for ?] bot performance level. The aim is to reach 85% accuracy. But most times, we could start with anything between 60% to 70%, which is perfectly fine. And then the next step would be to test the chatbot with users and train it further. Now, here, what we do is we package the bot as a concierge bot and ask the users to ask questions that the bot does not understand. Let us say the user wants to ask a question about working from home policy. Then the bot says, hey, I understand that you want to know something about work from home policy. Did I get it right? Thumbs up or a thumbs down. That way, we test the bot and particularly pay focus to the expressions that received a thumbs down. And we use these to train the bot further. So we may create a new intent. Or we may add these additional expressions to already existing intents. So this way, we train the bot continually until we reach 85% performance levels. And once we reach this performance level, then we say that we have completed all the five steps to building the horizontal coverage successfully. And the bot is ready for the next stage of the project. Once we've done that, the next step is to design our use cases. So there are a few steps involved in building use cases. First is the design of the use cases itself. So once we've defined the scope, we build the conversation flows. And then we define the UATs for the bot, which means, the user-bot interaction, how is it going to be? Right? And then once we've defined the UATs, then we actually move into the development phase, the technical design. Right? And then we test and train until the chatbot is responding relevantly to the users. And then comes the actual testing phases. I talked about alpha, beta, and ramp-up phases. Let's try and understand a little bit more about, what do these testing phases entail? In alpha testing, as I said, we start with a well-tested bot which is ready for testing with the end users, but in our own environment. Now, what are the various things that are typically tested by users during alpha testing? The first is obviously the UATs, the user-bot interaction, which we capture at the beginning of the project. And we test it to ensure that the bot is responding exactly the way it was intended to be, whether the conversation flows are working well. And we talked about horizontal coverage. So we also test whether the navigation and the fallback is working well for user requests that are outside of the scope of the bot project. So this is the code-functionality testing. Then we also test whether the channels are working well. So when we start the project, we say which of the channels that we want to deploy the chatbot to. Then we test whether the channels are working well. In certain projects, the projects have the need to respond to multiple languages. So the multi-language testing becomes a key part of the alpha testing. And finally, the performance, which is like, how many seconds does it take for the bot to respond to a user request? Ideally, one, two, three seconds, whatever. So these are the various aspects that are being tested by the users during alpha testing. As I said, alpha testing is done within our own internal environment. Once that is done and feedback is received, there is a whole cycle of fixing the errors. And whatever improvement feedback is given by the users, we work on it before we move to beta testing. So beta testing is essentially the same thing, but we test it in clients' own testing environment. Now, there is a simple, logical flow that we follow. We don't engage too many customers at the same time. We start with one customer at a time. And then we train, onboard, sign them up. And then we ask the customers to test the chatbot. Testing typically happens over two weeks. And after that, we receive feedback from the users. And based on the feedback, we take about a week, typically, to fix all the improvement feedback that we receive. And once that is done, then we move on to the second customer and a set of identified users. They follow the same cycle. We sign them up. We onboard them. We get them to test and then receive feedback, fix the feedback before we move on to the next customer. So beta testing typically engages three to five customers. Again, this would depend from project to project. In certain projects, we may just engage two or three. In certain projects, you may engage more customers. But the more you test and the more things you discover early and fix them, the better it is going to translate in terms of good conversational experience to the users. And then the final phase is the ramp-up phase. Second, the ramp-up plan would vary from one project to another, depending on your entire project duration. Sometimes the ramp-up could take a few weeks. Sometimes it could go up to a few months, depending on how [AUDIO OUT] many total number of customers and users you want to extend your option to. So here, again, we don't go big bang. What we recommend is to start with one customer at a time and a few identified chunks of users within the customer. So let's say we onboard about 50 to 100 users in week one and week two. And then gradually we ramp up to additional users within the same customer organization. And then we extend to all the users within this customer organization, before we move on to the customer two. The second customer is then onboarded the same way, in chunks and clusters of users. And once you've done with one or two customers sequentially, then we identify chunks of customers who can be onboarded. So we identify the next three to five customers and then the next five to six to 20 customers. And we follow the same incremental approach of adopting. And finally, we extend it to all the customers. So as I said, this entire cycle of ramp-up could vary from project to project. But what is extremely important is we have somebody who will monitor the usage of these users, continuously look at the log feed, analyze, and continue-- and use that to train the chatbot. That way, we ensure that the chatbot is incrementally getting trained and is able to perform better and better. So that's the final phase of the methodology that I talked about. And that brings us to the end of the project methodology. It's very easy to get started with SAP Conversational AI Services. And the very first step to getting started is to create your own account. And once you've created an account, you'll find a lot of tutorials and useful product resources that are there that'll help you understand various topics. And also, there are tutorials that'll help walk you through the various steps in building the first chatbot. There are many ways you could stay in touch with CAI team. And there are various links and resources there as well. And yes, there are links to contact our team. Sebastien Beghelli is our Solution Owner and a very useful resource contact point for you all to stay in touch with for any topics on Conversational AI. We encourage you to stay in touch. Also, you could continue your learning experience from SAP TechEd. You can widen your learning experience, activate your free access. There are links here. And also you could subscribe to Learning Hub, solution editions. There are various links [AUDIO OUT] here that that could be useful. And we encourage you to [AUDIO OUT] leverage all these. Finally, thank you very much for attending this session. Please note the contact details of our Solution Owner, Sebastien. For any topic related to CAI, you could get in touch with Sebastien. He'll be very helpful, happy to help you. Thanks once again for attending this session. [MUSIC PLAYING] 