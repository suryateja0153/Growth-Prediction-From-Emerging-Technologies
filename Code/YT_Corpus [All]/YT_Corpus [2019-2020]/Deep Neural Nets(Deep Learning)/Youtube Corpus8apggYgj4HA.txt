 visual context particularly facial expressions play a key role in a distance in the attention of message the speaker on the left is trying to convey a positive meaning while the speaker on the right is clearly listen but visually impaired individuals lack this context so it might have an application that is able to attempt the facial expressions of a subject real time imagine something like this happiness sadness surprise maybe some neutral and I made this real tiny motion detection application for the sum of high-performance competing under praise and nights for innovations [Music] in order to make a machine understand facial expressions we need a way to encode an image and array of pixels in a way that it's emotion is easily classifiable the face images I'm going to be using are 224 by 224 pixels making them fifty thousand a hundred seventy six dimensional objects so instead of pixels I'm going to be using a lower dimensional encoding actually units any facial expression can be reduced to a set of action units which encode combinations of muscle movements for most common facial expressions like happiness or sadness can be encoded using only a handful of action units 17 to be precise in order to make this encoding from the pixels of a face I used two deep neural network a neural network is a computer system inspired in how neurons work but is able to find underlying patterns and data these models are able to learn these patterns by seeing thousands and even millions of examples from which they learn from the data I needed to train the neural network came from for existing datasets that consisted of footage of a spontaneous emotions with the active action units annotated frame-by-frame I went through an array of different processes to improve the quality of this combined answer this involves balancing the frequencies of the classes cleaning noisy labels removing contiguous frames and not meant in the data by applying random rotations sure's zooms and brightness shift to each image before feeding into the network building and training in your network is a very time and resource design process for that reason I reuse the convolutional areas from an existing network called vgg face this cone practice of reducing parts of existing networks that were devised to solve problems in a similar domain is called transfer learning so let me explain what the compositional layers they took from vtt face to a convolution can be understood as a filter that's applied to an image the convolutional layers are just a bunch of this filters acted within panel and in sequence superficial areas income low level concepts like edges or corners but deeper ones become less intelligible as they encode meaning related to the class of a image instead so the output of the last convolutional layer could be thought as the high level features that define a face now all I need is a classifier on top of that that is able to map these features to the active actual units for that I used a bunch of deaths in drop out layers dense layers could be thought a fully connected layers of neurons that fire an output only if their inputs are high enough same Y neurons behave in your brain chopped hot layers as their name implies drop a random fraction of the units in the training process to stop the network from memorizing individual images and instead learn the underlying pattern during the training of an e or network I explored a number of hybrid parameters and settings like Valeri Bure dat the batch size the number and size of dense entrepot layers and different pre-processing of the data now that we got an eel network to infer the active action helix in the face all we have left is to find a way to infer the emotions from them let's start by visualizing how different emotions look in this hyper dimensional space where each dimension corresponds on actually in it I generated these data points by having my model predict the action units from faces to baked in one of the basic emotions the value of each dimension is the probability of the corresponding action in it to be present on the face there are clearly separate clusters corresponding to each emotion which proves the robustness of encoding emotions using a few action units so predicting the emotion that corresponds to a new listen vector is just a matter of looking eyes neighbors see there's new prediction by finding the most common class in its neighborhood we get the most probable emotion I deployed this pipeline to an application that is able to infer the emotions synchronously every fifth of a second and average them over time to foster stability besides the fact of this emotions that ridiculous and action units makes this application very easy to extend as all you need to do for the algorithm s were the emotions is to add a few data points to the neighborhood corresponding to the action units active in them happiness fear anger disgust happiness okay so thrilled thanks for watching and good bye [Music] 