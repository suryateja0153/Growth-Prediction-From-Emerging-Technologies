 so the first question that comes out of course is that whenever you see machine learning or you hear about machine learning it seems to be like this magic wand your boss says put machine learning into your application or if you you hear about startups they put machine learning into their pitch somewhere and it suddenly they become like a viable company but what is machine learning what is it really all about and particularly for coders what's machine learning all about actually quick show of hands have any of your coders yes I oh I guess pretty much all of us right are coders I do talks like this all the time and sometimes I'll ask how many people are coders and three or four hands show up so it's fun that we can geek out and show a lot of code today so I wanted like talk about what machine learning is from a coding perspective by picking a scenario can you imagine if you're writing a game to play rock paper and scissors and he wanted to write like something so that you could move your hand as a rock a paper or a scissors the computer would recognize that and be able to play that with you think about what that would be like to actually write code for you know you'd have to pull in images from the camera and you'd have to start looking at the the content of those images and how to tell like the difference between a rock and a scissors or how would you tell the difference between a scissors and a paper that would end up being a lot of code that you would have to write and a lot of really complicated code and not only the difference in shapes think about the difference in skin tones and male hands and female hands large hands and small hands you know people with gnarly knuckles like me and people with nice smooth hands like Carmel and so how is it that you would end up being able to write all the code to do this it'd be really really complicated and ultimately not very feasible to write and this is where we start bringing machine learning into it this is a very simple scenario but you can think about there are many scenarios where it's really difficult to write code to do something and machine learning may help in that and I always like to think of machine learning in this way think about traditional programming and in traditional programming something that has been our bread and butter for many years all of us here were coders you know what it is is that we think about expressing something in expressing rules in a programming language like Java or Kotlin or Swift or C++ and those rules generally act on data and then out of that we get answers like in rock-paper-scissors the data would be an image and my rules would be all my f ends looking at the pixels in that image to try and determine if something is a rocket paper or scissors machine learning then turns this around it flips the axes on this and we say hey instead of doing it this way where it's like we have to think about all of these rules and we have to write and express all of these rules and code what if we could provide a lot of answers and we could label those answers and then have a machine in fair the rules that map's one to the other so for example in something like the rock paper and scissors we could say you know this is these are the pixels for a rock and this is what a rock looks like and we could get hundreds or thousands of images of people doing a rock so we get diverse hands there are skin tones those kind of things and we say hey this is what a rock looks like this is what a paper looks like and this is what a scissors looks like and if a computer can then figure out the patterns between these and can be taught and it can learn what the patterns is between these now we have machine learning now we have an application we have a computer that has determined these things for us so if we take a look at this diagram again and if we look at this again and we replace what we've been talking about by us creating rules and we say okay this is machine learning we're gonna feed in answers we're gonna feed in data and the machine is gonna infer the rules what's that going to look like at run time how can I then run an application that looks like this so this week we're gonna call the training phase we've trained it what's gonna be called a model on this and that model is basically a neural network and I'm gonna be talking a lot about neural networks in the next few minutes but what that neural network is you know we're gonna wrap that we're gonna call that a model and then at run time we're gonna pass in data and it's going to give us out something called predictions so for example if I've trained it on lots of rocks lots of papers and lots of scissors and then I'm gonna hold my fists up to a webcam it's gonna get the data of my fists and it's gonna give back what we like to call a prediction there'll be something like hey there's an 80% chance that's a rock there's a 10% chance it's a paper and 10% chance it's a scissor something like that so a lot of the terminology and machine learning is a little bit different from traditional programming we're calling it training rather than coding and compiling we're calling an inference and we're getting predictions out of inference so when you hear us using terms like that's where it all comes from it's pretty similar to stuff that you've been doing already where traditional coding is just slightly different terminology so I'm gonna kick off a demo now where I'm gonna train a model for rock paper and scissors the demo takes a few minutes to train so I'm just gonna kick it off before I get back to things so I'm gonna start it here and it's starting and as it starts to run I just want to show something as it goes through so if you can imagine a computer I'm gonna give it a whole bunch of data of rock paper and scissors and I'm gonna ask it to see if we can figure out the rules for rock paper and scissors so any one individual item of data I give to it there's a one in three chance that gets it right first time if it was purely random and I said what is this there's a one in three chance that would get it correct as a rock so as I start training that's the one of the things I want you to see here as the accuracy that like the first time through this the accuracy was actually it was exactly 0.333 sometimes when I run this demo it's a little bit more but the idea is once it started training it's getting that random it's like okay I'm just throwing stuff at random I'm making guesses of this and it was like one in three right as we continue we'll see that it's actually getting more and more accurate the second time round it's now 53% accurate and as it continues it will get more and more accurate but I'm going to switch back to the slides and explain what it's doing before we get back to see that finish can we go back to the slides please okay so the code to be able to write something like this looks like this this is a very simple piece of code for creating a neural network and what I want you to focus on first of all are these things that I've outlined in the red box so these are the inputs to the neural network and the output coming from the neural network so I love talking about neural networks at i/o because of i/o input output and you'll see I talk a lot about inputs and outputs and this so the input to this is the size of the images those are all of the images that I'm going to feed to the neural network of rock's papers and scissors are 150 square and they're three by color depth and that's why you say 150 by 150 by three and then the output from this is going to be three things and because we're classifying for three different things a rock a paper or scissors so what was when you're looking at a neural network those are really the first things to look at what are my inputs what are my outputs what do they look like but then is this mistake thing in the middle where we've created this TF torque eros the layers are dense and there's a 512 there and a lot of people wonder well what are those 512 things well let me try and explain that visually so visually what's going on is I'm gonna what those 512 things are in the center of this diagram consider them to be 512 functions and those functions all have internal variables and those internal variables are just going to be initialized with some random States but what we want to do is like when we start passing the pixels from the images into these we want them to try and figure out what kind of output based on those inputs will give me the desired output at the bottom so function 0 is gonna grab all those pixels function one is gonna grab all those pixels functions who is gonna grab all those pixels and if those pixels are the shape of Iraq then we weren't the output of function 0 1 & 2 all the way up to 511 to be outputting to the box on the left at the bottom so stick a 1 in that box and similarly for paper you know if we say ok when the pixels look like this we want your outputs of F 0 F 1 and F 2 to go to this box and that's the process of learning so all that's happening all that learning is when we talk about machine learning is setting those internal variables in those functions so we get that desired output now those internal variables just to confuse things a little bit more than machine learning parlance tends we call parameters and so for me as a programmer it was hard at first to understand that because for me parameters are something I pass into a function but in this case when you hear a machine learning person talk about parameters those are the values inside those functions that are going to get set and gonna get changed as it tries to learn how I'm going to match those inputs to those outputs so if I go back to the code and try to show this again in action now remember my input shape that I spoke about earlier on 150 550 by 3 those are the pixels that I showed in the previous emulator them here with gray boxes but those are the pixels that I showed in the previous diagrams my functions now is that dense layer in the middle those 512 so that's 512 functions randomly initialized or semi randomly initialized that I'm gonna try to train to match my inputs to my and then of course the bottom those three are the three neurons that are going to be my outputs and I've just said the word neuron for the first time but ultimately what when we talk about neurons and neural networks is that you know there's it's not really anything to do with the human brain it's a very rough simulation of how the human brain does things and these internal functions to try and figure out how to match the inputs to the outputs you know we call those neurons and on my output they're like those three at the bottom are also going to be neurons too and that's what lends the name neural networks to this it tends to sound a little bit mysterious and special when we call it like that but ultimately just think about them as functions with randomly initialize variables that over time we're going to try to change the value of those variables so that the inputs match our desired outputs so then is this line the model dot compiled line and what's that going to do that's a kind of fancy term it's not really doing compilation the way we're turning code into byte code as before but think about the two parameters to this and these are the most important part to learn in machine learning and these are the loss and the optimizer so the idea is the job of these two is remember earlier on I said it's going to randomly initialize all those functions and if they're randomly initialized and I pass in something that looks like a rock there's a one in three chance it's going to get it right as a scissors are as a rock or a paper or scissors so what the loss function does is it measures the results of all the thousands of times they do that it figures out how well or how badly it did and then based on that it passes that data to the other function which is called the optimizer function and the optimizer function then generates the next guess where the guess is set to be the parameters of those 512 little functions those 512 neurons and if we keep repeating this we'll pass our data in we'll take a look we'll make a gasp we'll see how well or how badly we did then based on that will optimize or we'll make another guess and we'll repeat repeat repeat until our guesses get better and better and better and that's what happens in the model dot fit here you can see our model dot fit where epochs epochs epochs depending on how you pronounce it is a hundred all that's doing is doing that cycle a hundred times for every image take a look at the parameters fit those parameters take a guess measure how go to how bad you did and repeat and keep going and the optimizer then will make it better and better and better so you can imagine the first time through your gonna get it right roughly one in three times subsequent times it's going to get closer and closer and closer and better and better okay now those of us who know a little bit about images and image processing go okay that's nice but it's a little naive I'm just throwing all of the pixels of the image and maybe a lot of these pixels aren't even set into a neural network and having a try to figure out from those pixel values can I do it a little bit smarter than that and the answer to that is yes and one of the ways that we can do it a little bit smarter than that is using something called convolutions now convolutions is a convoluted term if excuse the pun but the idea behind convolutions is if you've ever done any kind of image processing the way like you can sharpen images or soften images with things like Photoshop it's exactly the same thing so with a convolution the idea is you take a look at every pixel in the image so for example this picture of a hand and I'm just looking at one of the pictures on the finger pixels on the fingernail and so that pixel is like value 192 in the box on the left here so if you take a look at every pixel in the image and you look at its immediate neighbors and then you get something called a filter so which is the gray box on the right and you apply you multiply out the value of the pixel by the corresponding value in the filter and you do that for all of the pixels neighbors to get a new value for the pixel that's what a convolution is now many of us if you've never done this before you might be sitting around thinking why on earth or what I do that well the reason for that is that when finding convolutions and finding filters it becomes really really good at extracting features in an image so let me give an example so if you look at the image on the left here and I have apply a filter like this one I will get the image on the right now what has happened here is that the image on the Left I've thrown away a lot of the noise in the image and I've been able to detect vertical lines so just simply by applying a filter like this vertical lines are surviving through their multiplication of the filter and then similarly if I apply a filter like this when horizontal lines survive and there are lots of filters out there they can be randomly initialized and they can be learns that do things like picking out items in an image like eyes or ears or fingers or fingernails and things like that so that's the idea behind convolutions now the next thing is okay if I'm going to be doing lots of processing on my image like this and I'm going to be doing training and I'm gonna have to have like hundreds of filters to try and pick out different features in my image that's gonna be a lot of data that I have to deal with and wouldn't it be nice if I could compress my images so compression is achieved through something called pooling and it's it's a very very simple thing it's sometimes it seems a very complex term to describe something simple but when we talk about pooling I'm going to apply for example a 2x2 pool to an image and what that's going to do is it's going to take the pixels two by two like if you look at my left here if I've got 16 simulated pixels I'm gonna take the top four in the top left hand corner and of those four I'm gonna pick the biggest value and then the next four in the top right hand corner of those four I'll pick the biggest value and so on and so on so what that's going to do is effectively throw away 75% of my pixels and just keep the maximums in each of these 2x2 little units but the impact of that is really interesting when we start combining over convolutions so if you look at the image that I created earlier on where I applied the filter to that image of a person walking up the stairs and then I pull that I get the image that's on the right which is 1/4 the size of the original image but not only is it not losing any vital information it's even enhancing some of the vital information that came out of it so pooling is your friend when you start using convolutions because if you have 128 filters for example that you apply to your image you're gonna have a hundred and twenty-eight copies of your image you're gonna have a hundred and twenty-eight times the data and when you're dealing with thousands of images that's gonna slow down your training time really fast but pooling then really speeds it up by shrinking the size of your image so now when we want to start learning with a neural network now it's a case of hey I've got my image at the top I can start applying convolutions to that like for example my image might be a smiley face and one convolution will keep it as a smiley face another one might keep the circle outline of a head another one might kind of change the shape of the head things like that and as I start applying more and more convolutions to these and getting some smaller images I mean instead of me now having a big fat image that I'm trying to classify that I'm trying to pick out the features of to learn from I can have lots of little images highlighting features in that so for example in rock-paper-scissors my convolutions might show in some cases five fingers or four fingers and a thumb and I know that that's going to be a paper or it might show none and I know that's going to be Iraq and it then begins to make the process of the machine learning these much much simpler so to show this quickly I've been putting QR codes on these slides by the way so I've open sourced all the code that I'm showing here and we're talking through and this is a QR code to a workbook where you can train a rock-paper-scissors model for yourself but once we do convolutions and earlier in the slide you saw I had multiple convolutions moving down and this is what the code for that would look like I just have a convolution layer followed by a pooling another convolution followed by a pooling and another convolution followed by a pooling et cetera et cetera so the impact of that and remember first of all at the top I have my input shape and I have my output at the bottom where the dense equals three so I'm gonna switch back to the demo now to see if it's finished training and we can see it so we started off with 33% accuracy but as we went through the epochs I just did this one I think for 15 epochs it got steadily and steadily and steady steadily more accurate so after 15 loops of doing this it's now ninety six point eight three percent accurate so as a result we can see using these techniques using convolutions like this we've been actually able to Train something in just a few minutes to be roughly 97% accurate at detecting rock paper and scissors and if I just take a quick plot here we can see this is a plot of that accuracy the red line showing the accuracy where we started at roughly 33 percent and we're getting close to 100% the blue line is I have a separate data set of rock-paper-scissors that I test it with just to see how well it's doing and it's pretty close this I need to do a little bit of work in tweaking it and I can actually try an example to show you so I'm gonna upload a file I'm gonna choose a file from my computer I've nicely named that file paper so you can guess it's a paper and if I open that and up that it's gonna upload that and then it's gonna give me an output and the output is 1 0-0 so you think I got it wrong it detected it's a rock but actually the my neurons here based on the labels are in alphabetical order so the alphabetical order would be paper then rock then scissors so it actually classified that correctly by giving me a 1 so it's actually a paper and we can try another one at random I'll choose a file from my machine I'll choose a scissors and open that and run it and again paper rock scissors so we see it actually classified that correctly so this workbook is online if you want to download it and have a play with it to do classification yourself and to see how easy it is for you to train a neural network to do this and then once you have that model you can implement that model in your applications and maybe play rock paper scissors in your apps can we switch back to the slides please so just to quickly show the idea of how convolutions really help you with an image this is what that model looks like when I defined it and at the top here it might look like a little bit of a bug at first if you're not used to doing this but the top here remember we said my images coming in 150 by 150 it's actually saying hey I'm gonna pass out an image as 148 by 148 anybody know why is it a bug now it's not a bug ok so the reason why is if my filter was 3 by 3 for me to be able to look at a pixel I have to throw it when not for me to star on the image I have to start one pixel in and one pixel down in order for it to have neighbors so as a results I have to throw away all the pixels at the top at the bottom and either side of my image so I'm losing one pixel on all sides so my 150 by 150 becomes a 148 by 148 and then when I pulled that I have to each of the axes have become 74 by 74 then through the next iteration it becomes 36 by 36 then 17 by 17 and then 7 by 7 so if you think about all of these 150 squared images passing through all of these convolutions are coming up with lots of little 7 by 7 things and those little 7 by 7 things should be highlighting a feature it might be a fingernail it might be a thumb it might be a shape of a hand and then those features come through the convolutions are then passed into the neural network that we saw earlier on to generate those parameters and then from those parameters hopefully it would make a guess and a really accurate guess about something being a rocket paper or scissors so if you prefer an IDE instead of using colab you can do that also I tend to really like to use pycharm for my development any PyCharm friends here out of interest yeah and I fell out of you and so there's a screenshot of PyCharm when I was writing this rock-paper-scissors thing before I pasted it over to collab where you can run it from collab so pycharm is really really nice and you can do things like step by step debugging if we can switch to the demo shooting machine for a moment now I'll do a quick demo of pycharm doing step by step debugging so here we can see we're in rock-paper-scissors and for example if I hit the debug I can even set breakpoints so now I have a breakpoint on my code so I can start with taking a look at what's happening in my neural network code here I'm just like this is where I'm pre loading the data into it and I can step through and I could do a lot of debugging to really make sure my neural network is working the way that I want it to work it's one of the things that I hear a lot from developers when they first get started with machine learning is that there seems to be your models are very much a black box you have all this Python code for training a model and then you have to do some rough guess work with tensorflow being open source I can actually step into the tensorflow code in Python like I'm doing here to see how the training is going on to help me to debug my models and Karmel later is also going to show how something called tensor board can be used for debugging models can we switch back to the slides please so with that in mind we've gone from like really just not understanding what's or bit just beginning to understand what neural networks are all about and basic hello world code to taking a look at how we can use something called convolutions and there's something that sounds really complicated and really difficult but once you start using you'll see they're actually very very easy to use particularly for image and text classification and we saw then how it was in just a few minutes we were able to train a neural network to be able to recognize rock paper and scissors with like 97 98 percent accuracy so that's just getting started but now to show us how to actually stretch the framework and to make it real and to do really cool and production quality stuff Carmel is going to share with us thank you hi so quick show of hands for how many of you was that totally new and now you're paddling as fast as you can to keep your head above water alright fair number of you I'm gonna go over now some of the tools and features that tensorflow has to take you from when you've actually got your model to all the way through production don't worry there is no test at the end so for those of you who are just trying to keep up right now track these words store somewhere in the back of your head that this is all available for the rest of you where you've already got a model and you're looking what you can form or that you can do with it pay attention now all right so Lawrence went through an image classification problem in slides we love image classification problems because they look nice on slides but maybe your data isn't an image classification problem what if you've got categorical data or text-based data tensorflow provides a number of tools that allow you to take different data types and transform them before loading them into a machine learning model in particular for example here we've got maybe we've got some user click streams right we've got a user ID now if we fed that directly into a deep learning model our model would expect that that is real valued and numeric and it might think that user number 125 has some relation to user 126 even though in reality that's not true so we need to be able to take data like this and transform it into data that our model can understand so how do we do that well in tensorflow one of the tools that we use extensively inside of google our feature columns these are configurations that allow you to configure transformations on incoming data so here you can see we're taking our categorical column user ID and we're saying hey this is a categorical column when we pass in data for it and we don't want the model to use it as a categorical column we want to transform this in this case into an embedding right so you could do a one hot representation here we're gonna do an embedding that actually gets learned as we train our model this embedding and other columns that you have can then get directly fed into Kerris layers so here we have a dense features layer that it's going to take all these transformations and run them when we our data through and this feeds directly downstream into a Karass model so that when we pass input data through the transformations happen before we actually start learning from the data and that ensures that our model is learning what we want it to learn using real valued numerical data and what do you do with that layer once you've got it in your model well in Kerris we provide quite a few layers laurence talked you through convolutional layers pooling layers those are some of the popular ones in image models but we've got a whole host of layers depending on what your needs are so many that I couldn't fit them in a single screenshot here but there are an ends drop out layers batch norm all sorts of sampling layers so no matter what type of architecture you're building whether you're building something for your own small use case an image classification model whatever it is or the latest and greatest research model there are a number of built-in layers that are going to make that a lot easier for you and if you've got a custom use case that's actually not represented in one of the layers and maybe you've got custom algorithms or custom functionality one of the beauties of Carus is that it makes it easy to subclass layers to build in your own functionality here we've got a Poincare normalization layer this is represents a Poincare embedding this is not provided out of the box with tensor flow but a community member has contributed this layer to the tensor flow add-ons repository where we provide a number of custom special use case layers it's both useful if you need point care normalization but also a very good example of how you might write a custom layer to handle all of your needs if we don't have that out of the box for you here you write the call method which handles the forward pass of this layer so you can check out the tensor flow add-ons repository for more examples of layers like this in fact everything in Karis can be subclassed or almost everything you've got metrics losses optimizers if you need functionality that's not provided out of the box we try to make it easy for you to build on top of what Karis already provides while still taking advantage of the entire Kerris and tensor flow ecosystem so here i'm sub-classing a model so if I need some custom forward pass in my model I'm able to do that easily in the call method and I can define custom training loops within custom model this makes it easy to do in this case a trivial thing like magic multiplied by a magic number but if for a lot of models where you need to do something that's different than the standard fit loop you're able to customize in this way and still take advantage of all of the tooling that we provide for Karis so one of the problems with custom models and more complicated models is it's hard to know whether you're actually doing what you think you're doing and whether your model is training one of the tools we provide for Karis and tensorflow more broadly as tensor board this is a visualization tool it's web-based and it runs a server that will take in the data as your model train so that you can see real time epic by epic or step-by-step how your model is doing here you can see accuracy and loss as the model trains and converges and this allows you to track your model as you train and ensure that you're actually progressing towards convergence anyone you're using Kerris you can also see that you get the full graph of the layers that you've used you can dig into those and actually get the opt-in tensorflow too and this is really helpful in debugging to make sure that you've correctly wired your model and you're actually building and training what you think you are training in Kerris the way you add this is as easy as a few lines of code here we've got our tensor board callback that we define we add that to our model during training and that's gonna write out to the logs to disk a bunch of different metrics that then get read in by the tensor board web GUI and as an added bonus you get built in performance profiling with that so one of the tabs in tensor board is going to show you where all of your ops are being placed where you've got performance bottlenecks this is extremely useful as you begin to build larger and more models because you will see that performance during training can become one of the the bottlenecks in your process and you really want to make that faster speaking of performance this is a plot of how long it takes resonant 51 of the most popular machine learning models for image classification resonant 52 train using one GPU don't even ask how long it takes with one CPU because nobody likes to sit there and wait until it finishes but you can see that it takes a better part of a week with one GP one of the beauties of deep learning is that it is very easily parallelizable and so what we want to provide is tensorflow are ways to take this training pipeline and parallelize it the way we do that in tensorflow 2.0 is we're providing a series of distribution strategies these are gonna make it very easy for you to take your existing model code here we've got a Karass model that looks like many of the others you've seen throughout this talk and we're gonna distribute it over multiple GPUs so here we add the mirrored strategy with this elite these few lines of code we're now able to distribute our model across multiple GPUs these strategies have been designed from the ground up to be easy to use and to scale with lots of different architectures and to give you great out-of-the-box performance so what this is actually doing here you can see that with those few lines of code by building our model under the strategy scope what we've done is we've taken the model we've copied it across all of our different devices in this picture let's say we've got four GPUs we copy our model across those GPUs and we shard the input data that means that you're actually going to be processing the input in parallel across each of your different devices and in that way you're able to scale model training approximately linearly with the number of devices you have so if you've got four GPUs you can run approximately four times faster what that ends up looking like on ResNet you can see that we get great scaling and just out of the box you're what you're getting with that is that your variables are getting are getting neared and sync across all available devices batches are getting pre fetched all of this goes into making your models much more performant during training time all without changing code when you're using Kerris all right and mirrored strategy with multi GPUs is just the beginning as you scale models as we do at Google for example you might want to use multiple nodes in multiple servers each of which have their own set of GPUs you can use the multi workers mirrored strategy for that which is going to take your model replicate it across multiple machines all working synchronously to train your model mirroring variables across all of them this allows you to train your model at faster than ever being ever before and this API is still experimental as we're developing it but in tensorflow 2.0 you'll be able to run this out of the box and get that great performance across large scale clusters all right so everything I've talked about so far falls under the heading of training models and you will find that a lot of model builders only ever think about the training portion but if you've got a machine learning model that you're trying to get into production you know that's only half the story there's a whole other half which is well how do I take what I've learned and actually serve that to customers or to whoever the end user is right in tensorflow the way we do that is you are gonna have to serialize your model into a saved model this saved model becomes the serialized format of your model that then integrates with the rest of the tensorflow ecosystem that allows you to deploy that model into production so for example we've got a number of different libraries and utilities that can take this saved model for tensorflow serving we're gonna be able to take that model and do web based serving request this is what we use at Google for some of our largest scale systems tensorflow lite is for mobile development tensorflow j/s is a web native solution for serving your models I'm gonna go or I'm not gonna have time to go over all of these in the next few minutes but I will talk about tensorflow serving and tensorflow Lite a little bit more but first how do you actually get to a saved model again in tensorflow 2.0 this is gonna be easy and out-of-the-box where you're gonna take your Karras model you call dot save and this is gonna write out the tensorflow saved model format this is a serialized version of your model it includes the entire graph and all of the variables and weights and everything that you've learned and it writes that out to disk so that you can take it pass it to somebody else let's say you can load it back into Python you're gonna get all of that Python objects state back as you can see here and you could continue to train continue to use that you could fine-tune based on that or you could take that model and load it into TF serving so tensorflow serving response to G RPC or rest requests it acts as a front-end that takes the requests loads them into or sends them to your model for inference it's going to get the result back so if you're building a web app for our rock-paper-scissors game you could take a picture send it to your server the server is going to ask the model hey what this send back a send back the answer based on what the model found and in that way you get that full round-trip tensorflow serving is what we use internally for many of our largest machine learning models so it's been optimized to have low latency and high throughput you can check it out at 10 to Flo org there's an entire suite of production pipelining and processing components that we call tensorflow extended or tf-x you can learn more about those at tensorflow org using that handy-dandy QR code right there and maybe you've got a model and you've got your web app but really you want it on a phone right because you want to be a future as mobile you want to be able to take this anywhere so tensorflow Lite is the library that we provide for converting your saved model into a very tiny small footprint so that can fit on your mobile device it can fit on embedded devices raspberry pies edge TP use we now run these models across a number of different devices the way you do this is you take that same saved model from that same model code that you wrote originally you use the TF light converter which shrinks the footprint of that model and then it can be loaded directly onto device and this allows you to run on device without internet without a server in the background whatever your model is and you can take it take tensorflow wherever you want to be know we've run through really quickly from some machine learning fundamentals through building your first model all the way through some of the tools that tensor field provides for taking those and deploying those to production what do you do now well there's a lot more out there you can go to google dev you can go to tensorflow org or we've got a great number of tutorials you can go to github this is all open source you can see the different libraries there ask questions send PRS we love PRS and with that like to say thank you back out [Applause] [Music] [Applause] [Music] 