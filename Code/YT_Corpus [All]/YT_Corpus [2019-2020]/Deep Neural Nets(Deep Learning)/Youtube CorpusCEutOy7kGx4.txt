 hi i'm kobayashi from sony in this video i will explain the basics of designing a neural network first let's briefly review about neural network the neural network has been technology to simulate the learning functionalities of the brain with a computer artificial neurons are the computer simulations of the nerve cells in our brains the neural network is the result of combining multiple artificial neurons for example in the image classification literature if you input the image to this input layer and enable neural network computation you will get the classification result on the output layer as the calculation result of the neural network for example this is 0 in handwritten digit and you get a classification result of 0 and that is neural network any combination of these artificial neurons is called the neural network therefore these really are various forms of neural networks typical applications for image classification use this kind of fade forward that is neural network in which data flows from left to right and this deep learning is a multi-layered version of the neural network by using a neural network with a very large number of neurons and the very large number of layers it is possible to achieve high performance that exceeds any classification performance you can learn more about deep learning in the video titled what is deep learning if you haven't seen it yet please refer to the link in the description box now i would like to explain the design of the neural network here is an example of face forward neural network the simplest structure of neural networks and this neural network can be expressed in terms of a combination of functions the neural network shown here is a four layer deep neural network that performs handwritten digit classification the input is a monochrome 28 by 28 pixel handwritten digit and the number of neurons is 1000 in the first layer 300 in the next layer and then 110 in the last layer the reason why the number of neurons in the last layer is 10 is because we want to identify the handwritten digit of this input image from 0 to 9 so there are 10 output neurons corresponding to the digits 0 to 9. this four layer numeral number can be represented by a combination of eight functions as shown below from the top are a fine temperature fine temperature of line 10 h by n sub max the div neural network can be represented by a combination of these eight functions now let's take a look at each of the functions mentioned here the first one is a fine which is a function called fully connected layer since the input neuron is a monochrome image of 28 by 28 pixels there are 784 input neurons of 28 by 28 in total in this affine function and the output neurons in the first layer were 1000 it is called a fully connected layer because the input and output neurons are all connected in all combinations as explained in the previous video on artificial neurons the value of this output neuron is determined by multiplying and adding the values of all the input neurons by different weights w respectively multiply this first neuron by a weight w and s then multiplied by another weight w and f then multiplied by another and f then multiplied by another and that for all 784 of these neurons and you have the value of the output neuron here then to determine the value of the next output neuron we again multiply these 784 input neurons by different weight w than we did earlier and add this function called the find does all these processes for these 1000 output neurons the weight w exists for all combinations of inputs and outputs for example if the number of input neurons in our example is 784 and the number of output neurons is 1000 that means that there are 784 thousand weights w here which is the result of 784 by 1000 this functional fine may be called by a different name depending on the arrival of the neural network for example it is called linear because it is linearly connected [Music] or since all inputs and outputs are connected in this way it is sometimes called fully connected in addition since all of these inputs and outputs are densely connected it is called dense and many other variations exist and the next function that comes up is 10 h this 10 is the hyperbolic tangent itself but in the world of neural network 10 gauge converts the input value into a non-linear one and keeps it in the range of -1 to 1. it is used to serve as an activation function 10 base looks like this when written in the graph the horizontal axis is the input value and the vertical axis is the output value as you can see when a large positive value is input the output value is stuck to 1 and when a large negative value is input the output value is stuck to -1 and when the value in this area is input the output changes almost linearly and i would like to remind you of the formula for artificial neurons look at the electrical signal that came in for this one neuron in the artificial neuron the first process was to multiply these electrical signals x1 x2 and x3 by a weight w and add and then add the bias the parenthesis in the formula for this artificial neuron correspond to the affine function then there was an activation function that performed non-linear processing on the output result this time we use 10h as the activation function there are many other types of activation functions besides tang h here i will explain using this traditional activation function 10h in the deep neural network which i mentioned earlier this affiliate and 10-minute process is repeated three times this combination of affine and 10-h can be used once to function as a neural network for one layer by repeating this three times a three layer neural network is constructed finally we use a fine again to compute the values of the 10 output neurons corresponding to the digits 0 to 9. then using a function called softmax these 10 values are converted into probabilities that add up to 1. for example if you input a digit 9 and the probability of nine is zero point eight or eighty percent then the remaining probability of the digits zero to eight should be twenty percent or zero point two using the soft max function we can convert them to probabilities that will sum up to 1. next i will explain the structure of the neural network called the convolutional neural network the deep neural network described earlier is rarely used in the image classification literature these days instead it is known that this structure called the convolutional neural network can be used to achieve very high performance let's take a look at the functions used in the convolutional neural network at the bottom of this section we have convolution max pooling and tange here we have new functions called convolution and max pooling after that repeat this convolution max boolean and 10h and after this it's exactly the same as the deep neural network i mentioned earlier the combination of these functions of line tange of i and softmax make up this convolutional neural network so the difference from the previous one is that the functions convolution and max pooling are used this is the difference from the deep neural network i mentioned earlier now this is what this function called convolution does it looks like this in the animation this process performs a filter completion operation on the image and outputs the processed image if you're not familiar with this compilation operation think of an application that processes images on a smartphone for example there are a lot of applications that blur or sharpen or change the color of an image for now it suffices to know that conclusion is to process images like this normally when processing an image a single image is output for each input image however in this neural network world completion applies several different filters to the input image in this case 6 different filters and output 6 different images for example if the first filter is a blur filter the first image will look like the input image is blurred if the second filter is a sharpening filter the second image will appear as if the input image has been sharpened that's how you would get 6 different images in this example each corresponding to a different filter for a different image this process is convolution this is followed by a process called max pooling the max pooling function is in a nutshell the process of down sampling for example having the resolution in this example the result of the convolution process is 6 24 by 24 pixel images while the input image is 28 by 28 pixels the max pooling function will down assemble the image in half lengthwise and width wide and at this point we will have 6 12 by 12 pixel images in half lengthwise and width wide 10 is after this is exactly the same process as the previous one it is a process to fit the result of the process so far to a range between -1 and 1 and repeat the same process using convolution the filter is convolved into 6 of these 12 by 12 images to produce a single output image this filter also uses multiple filters in the same way as earlier in this case 16 filters are used in total to output 16 8x8 pixel images now we'll run max pooling again to have the resolution in other words to 16 4x4 pixel images with half the height and the width the 10 edge function is then used to fit the value between -1 and 1. after this we have the same structure as deep neural network with a fine terminates a fine and soft max at this point we have 4 by 4 by 16 pixels or in other words 256 neurons so we take 256 neurons as input and perform a full connection and 10 h with 120 neurons as output and from there we do a file with 10 neurons as output the softmax function then converts the 10 output values to probabilities that add up to 1. you can configure the convolution neural network in this way now let's talk about the convolution process comparing it to a fine in a fine the input and output neurons are fully connected that is each output neuron receives the values of all the input neurons on the other hand the difference in convolution is that in the 5 by 5 convolution for example the top left pixel of this output image is only connected to the neurons in the top left neighboring area of the 5x5 pixel specifically this 5x5 kernel which is equivalent to the weight w is multiplied by the values of the upper left 5 by 5 pixels of the input image to the 25 values of this 5x5 and the values of the neuron in the upper left corner of the output is determined if you want to calculate the value of a neuron in one neighboring pixel you can move this five by five area to the right by one pixel and then multiply and add the five by five image in the input image by this 5 by 5 kernel weight w this is how to determine the value in one neighboring pixel this process is repeated from the top left to the bottom right and so on and so forth the value of the lower right neuron is similarly determined by multiplying and adding a 5 by 5 pixel image near the lower right pixel by this 5 by 5 kernel as you can see there is a big difference between a file and convolution in the fact that it is only connected to the neurons near the output pixel as opposed to full connection one more thing is the weight w earlier in the case of a fine if one output neuron was different it was multiplied and added by a completely different weight w in the case of convolution we use the common weight w when calculating the top left pixel when calculating the one next to it and when calculating the bottom right pixel in the case of the fully connected layer if there were 784 input neurons and 1000 output neurons there were 784 000 weights w of 784 by 1000. in the case of convolution the 25 weights w which are 5x5 are commonly used throughout the image so there are only 25 weights w per image processing session another major difference between affine and convolution is that the parameters are shared by each output pixel in convolution and the number of parameters is significantly less than with a fine now that we have introduced the deep neural network and convolutional neural network structures we have seen some functions such as affine convolution max pooling the activation function and softmax basically however the neural network is composed by arranging the structure that composes this single layer many times over when using a fine which is a fully connected layer a file and the activation function are repeated when completion is used the conclusion and activation function are repeated however in the case of convolution this down sampling process called max pooling can also be added if necessary basically the neural network is constructed by alternating the functions that perform multiplication and addition shown in the blue area and the activation function that perform non-linear transformation shown in the red area next we will explain max pooling using the figure max pooling is a function that takes the maximum value of neighboring pixels and outputs it for example if you want to do max pooling of 2 by 2 pixels take the maximum value of the adjacent 2x2 pixels that is 4 pixels and output it like this this process helps the vertical and horizontal resolution there are many other types of boolean for example average pooling takes the average value of these 4 pixels and outputs it in the case of max pooling it takes this maximum value and that's why it's called max pooling and what is often asked is the total number of neural networks the number of neurons in each layer and in the case of the convolutional neural network the number of images and the type of activation function we have introduced 10h as an activation function earlier but there are many other types of activation functions i am often asked how to determine this there is no particular correct answer to this the optimal structure of the neural network depends on the type of problem to be solved and the amount of data therefore each time we build a new classifier we need to decide the optimal number of layers number of neurons types of activation functions and other neural network structures by trial and error basically a neural network that can achieve higher performance in the problem to be solved or one with performance that requires less computation is better therefore in the process of developing a classifier using the actual neural networks we try to change the total number of neural networks the number of neurons and the activation functions in this way we train them and measure their performance to find neural networks with higher performance for less computational complexity through trial and error this is a very time consuming process so in recent years there has been a lot of research into automating this design process the deep neural network and the convolutional neural network introduced so far can be used for both vector-based and image-based discriminations and domains however only the end of the neural network needs to be designed to fit the problem you want to solve in the classification problem and category classification introduced so far the softmax function is used at the end to convert the output of neural network into probabilities that add up to 1. then we add a loss function this loss function specifies a training indicator of what the neural network should be trained with in this classification problem we use categorical cross entropy to calculate how well the probability of each category which is the output of sub matches the correct answer given by the person using cross entropy in order to give an indication that the neural network should be trained so that the cross entropy is small we specify a loss function called categorical cross entropy afterwards in the case of binary classification problems we use a function called sigmoid instead of submax as the last activation function this function sigmoid is similar to the tangent function we introduced earlier while tange fits the input value between -1 and 1 this sigmoid fits the input value between 0 and 1. the value from 0 to 1 output by this sigma is treated as a 0 to 100 percent probability we then use binary cross entropy as a loss function to calculate how well this probability of sigma's output matches the 0 and 1 given in the training data this binary cross entropy is specified in the sense that it minimizes the cross entropy between the output of sigmoid and the training data given by the person and when we want to estimate a regression problem that is a continuous value we often don't use an activation function the output of a phi in the neural network is used as the estimate then we use this loss function called squared error at the end to minimize the squared error between the output value of the neural network and the correct answer given by the person as we start to solve various problems in the neural network the last functions or optimization metrics are often designed by the developers themselves to fit the problem for now as typical loss functions if you remember the binary classification the classification problem and the regression problem you can apply them to various tasks so let's wrap up this video the number of input and output neurons in the neural network is determined by the size of the input data depending on the problem you want to solve the number of neurons in the output is then determined according to the size of the answer you want to get as the output of the neural network in the example of handwritten digit classification the number of input neurons was 28 by 28 since the image was 28 by 28 pixels and the answer we want to get was a digit from 0 to 9 so the number of output neurons was 10 neurons corresponding to each digit from 0 to 9. we first determine the number of neurons in the input and output then select the last activation and loss functions according to the problem you want to solve from the input to the last activation function the combination of a fine conclusion for weighted addition and the activation function was used to form a single layer which was connected many times to form intermediate layers and at this stage there is no particular correct answer for the number of neurons in each layer the type of activation function and the number of layers when designing a neural network we search for the best structure by trial and error in terms of the number of layers number of neurons types of activation functions and so on in this way we can achieve higher performance require less memory and require less [Music] computation 