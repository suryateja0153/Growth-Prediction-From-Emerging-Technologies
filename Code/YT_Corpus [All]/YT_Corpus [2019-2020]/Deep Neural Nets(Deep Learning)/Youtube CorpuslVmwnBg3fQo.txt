 tommy poggio who will address what i think is one of the big mysteries in in deep convolutional neural networks why and how they work given the enormous amount of uh parameters that they that they have so without further ado tommy thank you gabriel so um i tried to speak today for an hour about theoretical issues in deep networks um it's not clear whether this has much to do with the brain um because it's really an open question whether deep learning of today is something that the brains really implement probably not but deep learning today has this uh theoretical puzzle in front of the orientation we still don't know why it works as well as it does and um and so in understanding why this happens we can expect much better future learning machines maybe not of the deep learning type but growing from our understanding of how learning works the analogy the metaphor the historical parallel i'd like to mention is a bit grand but is electricity alessandro volta discovered electricity basically how to produce it for more than microseconds we discovered the pillar the battery this was at the end of 1800 of 1700 and it was made account by napoleon for that and once scientists had way to study electricity over a longer time than just sparks and lightning there were a number of practical applications um even if they did not understand really what electricity was applications such as the telegraph electrical motor electrical generators electrical lights and lightning and but it was only until maxwell appeared with his theory of electromagnetism 60 or 70 years later that real big progress ground on electricity started was the radio was the television was in the radar was computers was the internet was deep learning of today so um theory can be powerful and that's why we are trying to solve this puzzle uh the theory of deep of deep networks so let me remind to to all of you what the basic computation is you have a series of layers as we mentioned they started from uber and visual suggestion of a hierarchy of simple and complex diaper complex cells and this processing of information and the deep player goes through these stages in which uh the weighted sum of activities uh from units in the previous layers it's passed through a rectified non-linearity which then um outputs one number which is zero if this weighted sum was less or equal to zero and is the weighted sum otherwise that's it and you repeat that for all units and for all layers you set the weights using back propagation gradient essentially gradient decentral stochastic gradient descent i'll come back to that later but this is the basic architecture so there are a number of questions that one can ask and i'll group them in three different topics um this is somewhat arbitrary but one question is about approximation theory forget about learning for now just ask a very specific question why are deep networks better than cello network with just one layers and one layer of trainable weights and then there are questions about optimization how do i train the networks with gradient descent and about generalization why can network which have more parameters than data points more parameters than the size of the training set can generalize to new examples so predict well okay so let me start with approximation theory where the situation is a bit more clear and since the 80s we know that shallow networks on the left and deep networks on the right can both approximate essentially with arbitrary accuracy any nonlinear functions between say the inputs the inputs and one output so there are in virtual approximators in principle but they also are subject to what is called the curse of dimensionality which is that the number of parameters you need in order to perform this approximation is in the order this is an upper bound in the order of epsilon to the minus d where this is the number of inputs if you want in this example here is eight and epsilon is the accuracy that you want and so for instance epsilon is ten percent and d is ten you have a number of parameters can be as high as 10 to the 10 and of course if the dimensionality of the input is not 10 but say 1000 then this number this upper bound becomes really really large however if we want to ask why are some deep networks well let me phrase it again are there some functions some classes of functions for which deep networks are better than shallow networks and one answer is that for functions that have the structure given in the example above their functions of functions of functions like g3 a function of g21 which is function of g11 and so on and the dimensionality of the constituent functions the g1 1 g1 2 and so on is small then deep networks can beat the curse of dimensionality where shallow networks don't this is essentially exploiting prior information that you have or may have about the structure of the function that is can be represented in this um hierarchical compositional way um so essentially for certain classes of input output mappings that can have this representation then deep networks with the same kind of graph structure can beat the curse of dimensionality shallow networks cannot and this um network that can do that are essentially convolutional networks or the the class defined by the theorem is a bit more general than convolutional network includes convolutional networks that do not have weight sharing because it turns out that weight sharing can decrease the complexity the number of parameters you need but not exponentially where the locality of the constituent function the low dimensionality of the constituted function does ex beat the exponential curves of dimensionality okay the proof is simple and essentially based on checking that if you have an approximation error in the first layer for the first layer of constituent function this will not grow in the second and other layers there are papers i pointed to one of them in one of the slides and we can check empirically that this is what happens both for the training and the test error and this is c410 um experiments done by chan lee liao one of my great students and um you can see that on c410 um shallow networks do the worse and deep dense network are next and then convolutional networks uh without weight sharing uh next and then close by are convolutional networks with weight sharing which are the best one in terms of both training and test error um one can ask why uh convolutional network play such a big role and then this results you know i recall the big success stories of of deep networks like imagenet and similar applications in text and speech are using convolutional networks so commercial networks are really um the the ones providing the big wins and and this is what this our theoretical results in approximation would predict if assuming that the mapping they have to approximate when you classify an image or a or recognize a piece of text are mappings that have this hierarchical local structure can have this hierarchical local structure and so why is that in so many interesting problems it's a kind of philosophical question i don't want to go into that but these are interesting open questions [Music] okay so let me go next to the question of optimization and i'm asking a specific question here which is where is complexity control hidden in training deep networks now let me recall what is done typically in training deep networks you are trying to optimize the weights you are trying to find weights that minimize the error on the training side and you do this using a variation of gradient descent um in which you use a subset chosen randomly of the training data at each iteration of gradient descent instead of using the old set of training data this is called stochastic gradient descent okay and the originally and so far the most successful deep networks have used have been used for classification multi-class classification in the case of imagenet and other databases and and so they're using what are called exponential type loss function um like the cross entropy typically now in the binary case a simplified version of this is the exponential loss um and so the exponential loss makes you pay a price which depends on the product of the label which is plus one or minus one y n and the output of your function of course if the output of your function is positive then in this loss function you play you pay a small price because you have the exponent of minus the product y and f f and and but if the sign is wrong then you pay a much larger price and you are doing gradient descent on this or to cast a gradient descent and so the weights say of the weight i j in the matrix of weights at layer k changes um the time derivative of it that is iteration gradient descent as the expression given here okay so this is going on and i want to remind you that the puzzle is that you have um these are empirical findings that you can get zero training error if you have enough weights enough parameters you have zero training error and you have a test error which is pretty good and does not get worse as you increase the number of parameters so um you can get overfitting you fit all the training set very low exponential loss very very low and you still predict well so this seems a puzzle from the point of view of classical machine learning if you think about what learning is supervised learning like this is try to learn a function from a set of input output data but you want to learn a function that not only can reproduce the data but can all also predict well future data that you have not seen in the training site and there has been a lot of work over the last three or four decades on which conditions makes are necessary for from learning functions that can predict learning function from data in a way that can predict um just to remind you of um basic framework that you'll see later on next week especially with lorenzo rosasco but the classical framework for supervised learning is to think about a an unknown probability distribution on x y and to assume that you have training data that are given to you and they are drawn randomly iid from this unknown distribution mu if you were to know mu the problem would be solved but you don't know mu you have only some random data from the underlying distribution so you're trying to find to have an algorithm that can learn a map such that from the data to a space of function so that the function that you get from training on your training set s is predictive what does that mean well we have to define active things one is the expected error this is the true error the one that you'd like to be able to compute but you don't because it requires the knowledge of the distribution mu if you know that you could compute the probability for a given input for a given x of y um and so it it is really um being able to compute the the probability of a future error um what you can compute is in fact empirical parallel which is an error over your training set now the natural idea is to find condition under which this error on the training set is a good proxy for the error you cannot compute the expected error and so this is what what is called the distribution independent generalization or more precisely you can speak of a generalization gap going to zero this is the requirement that you your algorithm is so that your empirical error is a good proxy for your expected error in the sense that it will converge to it for number of data training data going to infinity okay turns out that a necessary condition for this to happen is to constraint the space of function over which you're optimizing and the more most general condition is that the space of function over which you are looking for the minimizer of the empirical error has to have this property to be a space of function which is uniformly uniformly glivenko can tell you but basically you have you need a constraint like a constraint for instance of smoothness it's more than you need or compactness for the space of age now so one of the messages from the work on learning theory by vladimir public and others like steve smale is that you need to control the complexity of the predictors in an implicit or explicit way if you want to predict why otherwise fitting your training set will not give you a good predictor all right um just a note that just counting the parameters is not a good way to constrain your predictor so it turns out that the norm of the weights and related quantities like the rather macro complexity of the hypothesis space are the quantities that really matter and control of the norm of course can be done and is what is done via regularization so typically in a regularization algorithm for instance the ones used with shallow networks of the kernel machine type what you are doing you are minimizing the empirical error on the training side the first term on the left but with the constraint you are minimizing the same time there is a constraint on the complexity of f which is given by the norm in the reproducing carnivore space typically this is a constraint of smoothness on the functions that you have smoothness or normal in the of the weights in in that space so i want to point out um here um because i will come to that later that there are some equivalents under quotation mark between thicken off regularization which is what we see here you're minimizing a loss function and with a regularization term controlled by lambda and what is called even of regularization that is minimizing the loss so this will be the empirical laws subject to a constraint on um for the complexity of the w for instance the l2 norm of the w being within a ball constrained by delta and models of regularization in a sense the dua dual version of even off in which you are now minimizing the norm for instance subject to the constraints and um are the equivalences to be taken with care for instance in the even of thicken off case it means that for every solution given a training set and lambda of the thicken of problem if there is one then there is um a corresponding delta that gives you the same solution and the even of case for the same dataset okay but if we come back to our um problem here with deep network we see that we have a minimization of l but there is no obvious constraint here you know there is no obvious constraint on the norm of the parameters or on l it's nothing that is explicit here so that's the puzzle if there is a constraint where is it and if there is not how can this work um and produce functions that are predict predictive okay you know our question is are deep nets so different that we have to throw away all of the classical machine learning theory all right so what is the solution so in order to go there let me define a couple of things uh it's back to our diagram there now the function that the network outputs will depend on the the x input of course but also on the weight matrices at the different layers uh 1 2k over the last in which the last layer may be a matrix with one row only if the output is just one output as i said we are considering mainly here for simplicity binary classification okay the important thing is that if you have rlu's then there is the following important property which is uh homogeneity of a certain type in other words you can multiply the weights in one layer by a certain number scalar alpha and the output of the function will be multiplied by alpha this can be gives also this property that the function of the network with the weight w is the same as rho multiplied the output of a normalized networks where rho is the product of the frobenius norms of the k weight matrices and the normalized network uh f tilde is a network where all the weight matrices are normalized to have probinus known one now notice that from the point of view of binary classification what matters is really the sine of f tilde because this would be the same as the sine of f since f is low time f tilde in a lot of what i'm going to say my also making the hypothesis that at some point your ingredient is sent we have separability in other words there is reading the center found set of weights such that the sign of the network is correct you have classification correct classification for all the training point as i've shown you this is relatively easy to achieve in for data set like cipher and um but this by itself of course does not knows anything about testing error so when just assuming that optimization can do that and the reason we can assume that is to is because there are a lot of parameters so that we assume there is enough over parameterization that gradient descent can find a set of ways that can classify correctly the training set okay so that's what we're going to consider and i want to go through two different approaches the first one it's a new one um i don't think it's been really published yet although it's in one of our memos implicit it's short to give you just an outline this basically says forget about the dynamics for the moment and consider the following theorem which is um a theorem that is a variation of something proved a few years ago by tb shirani and ruse and this says that if we minimize the exponential loss then we are maximizing the margin and this means that we are maximizing the value of f of x n um so that x n is in in this is the the value the training set gives you the smallest separability so the smallest value of everything and you are maximizing that value the margin if we do um minimizing the exponential loss um with this expression in which we have rho and fv where fv is the function with the normalized weights so so essentially minimizing exponential loss maximizes the margin then we have another result that says that maximizing the margin subject to the condition of normalization on the weights is equivalent to minimizing the weights subject to margin greater than or equal to one so minimum norm solution equivalent to maximum margin solution and then um we can use a a lemma in constrained optimization that says that we have if you are minimizing the norm subject to um to some to the number of conditions and conditions then the um optimal set of v is given by a linear combination of the df dvk over and these are the on the ones that are the support vector they are on the margin where the vxi is equal one and so and then there are other arguments you can use to say that if the the weights only depends on a subset of the training points then you can bound um what is called stability or come back to that later and that bounce the test error so that's one way and that essentially says that if your data set is good enough then you can bound the test error despite the fact that your training error can be zero and so on and so forth and despite the fact you don't have a constraint because you're using exponential loss and the second approach which is more direct consider the dynamics and let me walk you through it in this case as i said the question is not whether the w converge in fact if you look at the equation they will diverge because if you assume separation has been achieved then um then everything is positive so that the gradient of w the in fact the time derivative of w is positive so w is growing it's growing to infinity the question is whether you have convergence of the normalized weight these are the ones that matter you can think of running gradient descent getting some w at some point you stop gradient descent the w will be very large you divide by the norm that's what matters you don't need to divide by the norm you can just look at the sign but say you can divide by the number and the question is whether this will converge and so if you change variables essentially in the set of differential equations that the gradient descent gives you for the w i can express w as w k is a matrix in terms of row k v k v k equal one and then if i do the kind of butter some linear algebra that is needed i get this equation equivalent to the one on w but now in these two variables the row and the v and um the row and the v and i can see that rho uh we go to indeed to infinity uh the derivative of rho goes is positive after separation and at some point to go to zero for time going to infinity um and the question whether you have a v that converges and in fact it turns out that there are critical points of v um they will maximize margin and in fact you can see from this expression here that um the v have the worst the lowest value of f of x n so that the sort the x sign for which fv we have the lowest value so um essential classifier less sure those are the ones that will have the large value as rho grows as time goes to infinity row grows and the exponential term increasingly go to zero so the terms that have large value of f of xn the xn terms will go to zero first these are the ones that are not support vectors they don't matter and in the limit of infinite time there is one group of surviving let's call it support vectors training points with the same margin and they satisfy this equation okay so um so that's interesting and you convert to maximum margin and that's good and there is also this observation which is even more direct which is that if i considered this loss but now i impose a constraint on the vk so this is really like regularization i do that with a lagrange multiplier so the condition is that the k is one at all time not only the convergence and then i get equations um that in row dot and v dot they they contain lambda essentially but they can replace with lambda with an explicit value because because um i have this condition on lambda and if i'm if i if you see there imposing v transpose f equal one over k suppose k equal one and this gives me an expression for lambda which i substitute in the equation and so so what happens is that i get equations that are very similar to the ones i got before without imposing this regularization there is a row k squared different in the equation for the v dot which which means dynamic is somewhat different from the ones without it in fact the dynamic obtained in this way is exactly the same that you get if you use an algorithm which was proposed as weight normalization which is used and it's a bit similar to [Music] to batch normalization which is used a lot um they have the an implicit normalization of the weights as in what we did here and more importantly the critical points i get are the same as the one i get without any normalization at all so the circuited point at infinity but they want to determine convergent to the maximum margin solution so interestingly the there is an implicit constraint in um doing gradient descent on deep networks using exponential laws using the exponential so that's important and and so there is the you get this same critical point that you will get with um an implicit vanishing regularization you get the maximum margin minimum norm solution and this essentially in my view explain why um why deep network generalize as well as they do um you have to add a few a few um comments to complete to make my statement hair tight and i'm not going to go through it um but i'll make a few comments um one is that you have you can characterize a number of properties of your dynamics which are quite interesting for instance the rate of change is the same for all layers it's independent of k this means if you start with weight uh layers weight matrices that have the same norm at initialization and then you just do do gradient descent then the norm should remain the same for every layer and the way you prove that it's very simple you just take the time derivative or okay it's two okay dot okay time derivative or okay and this we found it before was rho over row k for something else and so the rocket disappears and now we have that the time derivative of r square k depends on rho and so on but not under okay so he's independent of quite interesting the margin is not decreasing for large times only can increase we can find things about rate of convergence for linear networks we can show experimentally that this happens this is work um with fernanda a great student and andy burbaski bubarski and you can see how the margin evolves across perturbation at different times and you can see for instance that is quite interesting how the margin behaves for sifa natural labels and sifar labeled randomly there was this interesting paper one of the author was a student of mine who did they work as a summer intern at google showing that you can that gradient descent and the deep convolutional networks five layers can fit exactly the training points or separate them zero classification error even when they are labeled randomly so you expect in this case that the test error would be 90 because there were 10 classes and obviously to expect random predictions um and so from the point of view of the training error you don't see any difference between the training error on randomly labeled shifter data and the training error on correctly label c for data but if you look at the margin you can see that the margin for the randomly labeled train data is very small and is small for all data points essentially all of the 50 000 training data whereas in the case of naturally labeled data you have imagine that is pretty significantly much higher several order of magnitudes um for the worst point so to speak but then is also getting better and better for all other points let me focus on this result that you are maximizing the margin here using exponential laws even without explicit normalization regularization okay now there are a number of caveats here and maybe i should conclude this and have enough time for for questions um here but so let me make clear where the carrots are what i assumed in the so far is vanilla gradient descent so no momentum no wait normalization no batch normalization no regularization no data augmentation of course people use all of this and more in the most in practice and each one of them will give you regularization etc some similar effect so i'm taking the worst case um and by the way that's important because if you look at results like recently about square loss behaving as well as exponential loss for classification this work done by misha belkin and student phase they find pretty much the same result but they are using algorithms that have all of this bench normalization and rest net and possibly data augmentation so um so to be taken with a grain of salt this is what the next point makes says all i told that you assume is exponential loss open question is the same for the square loss in terms of maximizing margin i don't think so some other mechanism may be there to control to control complexity but it's not the exponential mechanism [Music] now the point is that what i told you is that uh um give the incentives to categorize incent converge towards a minimum norm solution but in the case of deep networks non-linear deep networks or multi-layered networks um this is in general not unique to there are many minimal norm solutions and some may have better testosterone than others that's uh in open questions um so you know it's unclear exactly what you can say about the best you can do from the point of view of generalization and there are some other criteria than minimum norm that tells you really what is the unique best you can do um and then the other question that i was asking because i wanted to introduce some other slides was whether we can generalize what i said in a way that includes the classical theory and the classical approaches for kernel machines and there is a way to do that i'll it's a kind of research program based on the idea of cross-validation stability as a general criterion that you can use for um for different situations the traditional classical ones you need in which you typically have for a number of data going to infinity you have an under parameterized situation but you can also apply in the modern regime in which you could consider over parameterization also for and going to infinity stability gives you gives you a criteria that you can use for a theory in both the situation whereas you can use the standard approach of uniform convergent that requires a fixed hypothesis space okay let me finish here saying that i think we are on uh we're making progress on understanding why deep learning works but there is of course much more fun to be had in future work on these puzzles thank you great thank you tommy we have a few questions for you uh here's one from savia if regularization is interpreted as the log prior when minimizing the negative log posterior could you please give some intuition for the equivalent terms in ivanov regularizations well the classical let me address just the classical um regularization that you have you can have a bayesian interpretation for that and this essentially is considering gaussian priors on the function space and um so your your priors is to you to maximize the probability uh of uh functioning in in in in that and if you you can look at and whereas the lost term is essentially taking into account it's a likelihood term taking into account additive gaussian noise this is kind of the visual interpretation of classical generalization it has basic theoretical issues in terms of making it rigorous because we are dealing with infinite dimensional spaces in principle and so um some of this probability and measure become quite touchy but there is there is this patient interpretation yes great thanks uh the next one is from sreya banerjee uh does hypothesis or does the hypothesis of separability signify we have reached the global minimum and she has a follow-up of how does the theory hold for l1 l2 losses which are commonly used for most reconstruction problems and by theory she means maximum margin minimum norm so the um separability does not correspond to the the global minimum um the in the case of the exponential laws you need more than separability and in fact this corresponds to the fact that you can get separability you can continue to do gradient descent the training error in terms of classification error does not change because it was already zero but you converge to better and better solutions in terms of test error so for the exponential um so so separability does not correspond to the absolute minimum now um the zero loss minimum for in the case of exponential type losses com happens only at infinity so you never you never have it for finite times um for the square loss the situation is of course different you can have separability also without having um zero square loss um you have zero square loss when you have uh exact interpolation in the binary classification case you have exact interpolation of the labels plus one and minus one but you can have you can have a separability you know it's much easier to have than exact interpolation great thank you the next one is from roman barbal so could you map in any way uh similar to universal universality classes in critically or something like that in criticality or something like that constrained and unconstrained learning sorry can you repeat the question sure so could you map in any way similar to universality classes in criticality or something like that uh constrained and unconstrained learning yeah i'm not sure what criticality classes here mean um yeah let's let's postpone this question to i'll try to do it um um you know better email if the person asks this question wants an answer sure thing roman if you can follow up with tommy that would be great so the next one we have is from giambattista parascandolo the brain has about a hundred trillion parameters but generalizes very well to lots of tasks we care about so the bounds from learning theory seemed to be irrelevant there why should we be surprised that dnns that have much fewer parameters generalize well well from the point of view of statistics it's uh already quite surprising that you can have um a system that has more parameters than data predict well when i was starting to learn physics the one of the key concepts that i was drilled it was drilled in my brain was if you if you have a model of some physical system and you have say 10 parameters in that model you better get at least 10 measurements but really 20 is the minimum and you know 50 would be much better if you want to fit 10 parameters otherwise you're playing games you know think about a set of linear equations and now you have more unknowns in the equations than equations there is of course an infinite infinite number of solutions that solve the equations exactly which one do you choose you want one that will predict why which one do you choose that's a big question um you know it's there are ways to do it regularization is one but deep networks on surface seem not to do any regularization or being able to to do well without regularization so what's happening so that's a puzzle great and we have uh time i think for one more another from jim batista that got voted up do you think it makes sense to think of dnns as massive ensembles of sub-networks in the regime of reasonable data each sub-network then has reasonably few parameters and the contradiction would disappear well the contradiction as i said does not disappear again i should want to stress again in reality it's not the number of parameters that that constraints of the complexity such as constraints of the norm um so but what um what you say is is you know the burden of the proof is so new is that by having this uh the composition that you suggest you get somehow some constraint on how these networks are combined together they are coefficient for instance um you know if you can do that that would be great that would be a nice result i don't see how this can can be done but please send me an email if you manage 