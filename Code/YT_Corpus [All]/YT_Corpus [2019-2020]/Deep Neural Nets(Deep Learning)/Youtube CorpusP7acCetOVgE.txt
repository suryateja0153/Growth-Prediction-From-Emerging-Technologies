 hey everyone my name is fabrice and today we're going to be looking into the question of whether neuron coverage is a meaningful measure for testing deep neural networks let's get started uh so most of you that have made your way to this video have likely heard of neuron coverage before you've read about it in a paper you've used it in your research perhaps even in an industrial setting you've used it to justify the launch of a dl system into production but just in case it's a new concept let's take a minute to review what it is and why people care about it the idea of neuron coverage was originally inspired by code coverage within traditional software engineering code that gets executed by a test has the potential to reveal buggy behavior more than code that is never run and coverage tells you the proportion of code your tests actually execute you can measure this in terms of lines of code paths of code etc but what neuron coverage is doing is it takes that basic premise and extends it to the machine learning setting where instead of counting code statements we count the neurons in a dnn that are covered or activated by a set of test inputs now since neuron coverage was proposed in 2017 it spawned a slew of related works uh expanding on the idea of neuron based coverage measurements in addition to the original papers deep explore tool we have deep test deep gauge deep road deep what have you all collectively garnering over a thousand citations a lot of high impact work as a quick illustration let's take a look at how neuron coverage is actually calculated so here you can see along the top inputs to a single layer neural network which has five neurons in the second row each neuron has unscaled activations on top of the scaled activations in parentheses neuron coverage will count a neuron as being covered if its scaled activation exceeds certain threshold for the sake of simplicity let's assume a threshold of zero meaning that anything that's activated at all is counted here we can see that this test input results in three out of five neurons being activated for a coverage of sixty percent it's a pretty straightforward idea slash calculation uh that no doubt contributes to its popularity so now for why people care this goes back to the age-old question of how we build trust in the things that we create safety and reliability concerns are holding back many applications of dl systems from self-driving cars to autopilot and drones and commercial airliners to even medical diagnoses all of these safety critical systems require even sometimes by law evidence of correct behaviors and neuron coverage is designed to guide your testing efforts towards a set of potential defects for eventual correction but is it actually successful in doing this is it truly a meaningful measure for testing deep neural networks so we returned to the titular question and in order to evaluate it we conducted an extensive empirical study focused on computer vision where we created over 2000 test suites reflecting a broad cross-section of potential projects we consider eight different uh types of dnns with a variety of architectural properties uh some of them having one-dimensional convolutions two-dimensional convolutions fully connected layers high and low capacity networks and so on i'd also highlight that we considered both classification and regression data sets mnist and c410 being for classification the udacity self-driving car challenge data set for regression where you predict steering angle from images of road conditions there are a number of other variables at play and i'll refer you to the paper for those now each test suite contains 100 inputs uniformly distributed between the classes we then perturb those inputs using adversarial attacks the cw attack and the pgd extended with a diversity promoting regularizer that increases neuron coverage while still inducing errors on realistic inputs we'll talk more about that in a moment then we calculate some pre key criteria related to each test suite the first being defect detection which is the number of defects the suite is able to induce naturalness which is how realistic the inputs are and finally output impartiality which measures whether the test transformations that we made to the suites resulted in any strong biases that favored particular test classes over others we argue that all three of these are important factors to consider when designing a meaningful test metric and we see if neuron coverage is positively and strongly correlated with them finally we conduct a follow-up experiment to see if it's possible to predict if any classes may be overrepresented in a test suite modified with neuron coverage maximization as a guiding objective but first let's talk about how we increased neuron coverage in a new way so we did this by creating an entry based diversity promoting regularizer that can easily be incorporated into just about any algorithm that relies on incremental optimization for our study we chose to extend the cw and pgd attacks as i mentioned before because there are two of the most popular adversarial methods in the literature they just they both jointly maximize uh loss minimize the norm distance from the original inputs and now because of the regularizer maximize neural diversity uh it does this by measuring the kale divergence between a uniform distribution and the actual distribution of neural activations for a targeted set of layers in the dna the greater the divergence the higher the penalty so let's take a quick look at its impact on the left you can see the unskilled activations for a subset of neurons in a particular model layer only about 10 of them are activated beyond the 0.2 threshold after regularized perturbation the same inputs now activate 52 of the neurons above the same threshold representing a significant increase in neural activity so that's how it works but why invent a new approach if others have proposed their own methods for increasing their own coverage the the answer is that we wanted to have faster batch transformations that consider more than one input at a time can make more free-form changes to the inputs whereas previous methods generally considered a limited set of transformations like shears and adjustments to brightness uh and we also wanted to remove the overhead of differential testing which was integral to the original deep explorer approach with models becoming more and more expensive to train because of their growing size techniques that can operate on a single model are increasingly important so now let's talk about how this technique enabled us to build test suites with a variety of defects and how they correlated with neuron coverage our first key criteria is that of defect detection which we reframe as the attack success rate of the augmented adversarial attack quite simply the asr is one minus the perturbation accuracy of the model on the test inputs the definition is inherently classification oriented so for our driving data set we bend the continuous steering angles to permit for a consistent analysis so what's the first hypothesis that we investigate uh it is that neuron coverage is both strongly and positively correlated with defect detection and what did the results show so what we see here is the distribution of results of the correlations between neuron coverage and defect detection following standard standard procedure the correlations are bended to strongly moderately and weakly negative and positive groups with n a representing cases where a correlation could not be drawn at all because there was no variation in the number of defects highlighted in white are those results that supported the hypothesis of which only about three percent did even more concerning is that in over fifty percent of the results we observe a negative correlation where increasing neuron coverage saw a decrease in the amount of defective behaviors found so taking a look at a sample of the results from the cw attack on all three data sets we see that the convolutional 1d net exemplifies the na outcome it had 100 attack success rate no matter what level neuron coverage was and the other models show their respectively mixed results we would refer interested viewers to the paper for more visualizations like this so the next key criteria that we use to evaluate the meaningfulness of neuron coverage is naturalness often an implicit goal in testing we think it is important to explicitly consider how natural your tests appear to be unnatural inputs can deviate so significantly from the target data distribution that they become impossible for even human oracles to handle properly so it shouldn't be something that we hold against an ai model either basically the value of a defect increases if it's realistic decreases if it's not since we're working on the image domain subjective spot checks wouldn't be suitable they're unreliable unscalable and two people can reasonably disagree on the tricky to define term of naturalness fortunately related research into generative adversarial networks or gans has yielded more or less objective metrics that are used to automatically quantify naturalness we make use of two of them in our analysis the inception score where higher scores indicate increasing realism and the frechet inception distance where lower scores indicate more realism because the distance from the original inputs are smaller now both metrics are tuned upon an inception dnn trained on imagenet data which corresponds to a wide array of natural object scenes making the metrics themselves useful for data coming from similar distributions unfortunately mnist is arguably not natural in the same way where we have highly pre-processed numerical digits centered against a blank background so for this part of the analysis we exclude mnist and focus exclusively on cfart10 and the driving data set so what's the hypothesis here as of before the hypothesis is that neuron coverage is both strongly and positively correlated with naturalness now for the results when it comes to naturalness our second hypothesis has even less support with only about 1.5 percent of the results we're being strongly and positively correlated while almost 70 percent more negatively correlated to some degree increasing neuron coverage seems to correspond to more unnatural test suites now moving on to the charts looking at a sample of the cw attack results on the c4 data specifically we see that the general relationship between increases in neuron coverage and the change in naturalness again increasing fid is not actually a good sign those are counted as hurting the naturalness in the correlation calculations on a similar note within the driving data set we see some more mixed results with the overall trend being that increases in neuron coverage correspond to decreases in naturalness so finally let's talk about the third key criteria output impartiality this newly proposed idea emphasizes the importance of what the model thinks it sees in the inputs and measures the degree of skew introduced in the test generation process ideally a model will be tested with a variety of inputs and this is pretty easily controlled by manually enforcing an equal distribution of test types or classes at the beginning each one of these classes has certain combinations of features that characterize it and the model learns to detect those features for prediction purposes however when an objective like neuron coverage maximization is being used to guide the creation of new test cases class-specific features can be blended together and thus affect the post-transformation distribution of model predictions we measure the shifts in that distribution via pylu's evenness a well-regarded measure of ecological biodiversity applied for the first time to ml output behaviors in this context we refer to it as output impartiality or oi for short so same as before we calculate this oi score for each test suite and determine its correlation with neuron coverage however we also conduct a follow-up experiment that seeks to predict exactly which classes are likely to be overrepresented in the output predictions of the newly generated test suites this is done based on the observation that each class has a different average neuron coverage and therefore neuron coverage maximization which is how it's normally used in praxis is more likely to favor the features of classes with higher average uh baseline neuron coverage but first let's look at the main hypothesis of the correlation study as before we hypothesize that neuron coverage is both strongly and positively correlated with output impartiality here we see that d that the correlations are once again unfavorable for neuron coverage less than 5 percent of the results show a strong positive correlation with output impartiality which is more than any other key criteria but it's still not a rigging endorsement so lastly let's take a look at the sample of results showing a mixture of relationships again we'll refer you to the paper for more visualizations like this as for the follow-up experiment i'll explain the experimental setup the first thing to recall is that neuron coverage is different based on the class of the input that means that different classes have a different baseline activation uh how much they're exciting the network for example a network may activate more strongly when shown images of dogs than it does for images of fish uh first we pick ten instances of each class in mnist so we have ten zeros ten ones and so on for each grouping we calculate neuron coverage at a particular threshold starting with zero whichever class has the highest neuron coverage is ranked first and whichever class has the lowest is ranked last all the others are somewhere in between to get a robust measure of neuron coverage's preference uh for certain classes this process is repeated with different thresholds 0.1 0.2 etc until 0.9 the ranks at each step are then average to find the classes that were consistently over and under stimulating the dnn and the highest ranked class would then be predicted to be the most common class in the output distribution of a random test suite so the results showed that class 8 consistently had the highest neuron coverage with an average rank of 2.1 across all thresholds when perturbing a test suite with neuron coverage maximization as an objective the resulting output distribution showed a significant bias towards the a-class likely because the features belonging to the a-class could be most easily incorporated to attain those increases in neuron coverage that satisfy the objective theoretically this would make it more likely that other class features are not as well tested like those belonging to classes 6 1 and 9 and ultimately compromise the effectiveness of the entire testing effort so that concludes the main experimental results of the paper let's quickly discuss a few key points related to the comparison with previous work and neuron behavior conceptually before wrapping up with key contribution summary so it's worth wondering whether the results we found are a consequence of our diversity promoting regularizer and that other techniques might have more favorable favorable results for defect detection naturalness and output impartiality in order to address this concern we repeated smaller scale correlation studies with both deep explorer and deep tests for deep explore we were unable to find a single correlation that supported any of the three hypotheses and for d-test only one correlation supported the hypothesis for output and partiality the triangulated agreement between these three techniques suggests that our findings are generally applicable to neuron coverage and are not really dependent on the technique used to increase neuron coverage in the test suite furthermore both tools had a tendency to produce severely unnatural tests that we would argue should not be considered valid for example the first example comes from dbx4 modifying an input from the driving data set until it is completely white and lacking useful information the quote-unquote correct steering angle is indicated in green and the incorrect model prediction in red the next two are from deep test and show similarly questionable utility so then there's the matter of inherent uh neural behavior uh firstly research into neural visualization has shown that neural features are not discrete and regularly blend concepts together as you can see along the top here their power lies rather in the complex patterns of activations that collaboratively work to map inputs to outputs and is therefore how any metric that considers the behavior of individual neurons could give meaningful information about the behavior of the dnn as a whole even if we assume independent discrete encoding of features in the neurons of a dnn is it still desirable to maximize their activations and that is maximize neuron coverage if we think of coverage in a binary sense of activated versus domains and each neuron detects some feature in the input then there are two to the end possible combinations of neuron activations in a model with n neurons keeping it simple let's assume that there are two neurons one that detects cars and the other that detects pedestrians the nc maximizing objective is fully satisfied by an image that contains both a car and a pedestrian but remains silent about the other three possibilities that might be crucial for testing overall it seems like neuron coverage maximization as an objective focuses on one direction at the expense of other valid directions so wrapping this up in this work we propose a novel regularizer that can augment your favorite adversarial attacks to increase neuron coverage as well we conducted an extensive study of the relationship between neuron coverage and some key test criteria them being defect detection naturalness and output diversity and most importantly these empirical results invoke skepticism that neuron coverage is actually a meaningful test metric lastly if you're interested in getting your hands on any of this code that we've done for the analysis we've all made it available on github and you can have the link in the paper as well so this is the team that produced this research myself ling xiao gulzar chen chen and mir young and we all thank you for taking interest in this work until next time take care for now 