 numpy is fast but how can we make it even faster stay tuned to find out welcome to AI adventures where we explore the art science and tools of machine learning my name is Yu Feng Guo and on this episode we're going to look at a new library from Google research called Jack's and see how it can speed up machine learning Jack's can automatically differentiate native Python and numpy functions it can differentiate through loops branches recursion and closures and it can take derivatives of derivatives of derivatives it supports reverse mode differentiation also known as back propagation using the grad function as well as forward mode differentiation and the two can be composed arbitrarily in any order you want it can seem kind of like every other library these days supports auto differentiation so what else can Jax do well it's also able to speed up your code sometimes really significantly by using a special compiler under the hood accelerated linear algebra or excel a is a domain-specific compiler for linear algebra it can perform optimizations like fusing operations together so intermediate results don't need to be written out to memory instead of this time consuming process this data it gets streamed right into the next operation and this enables faster and more efficient processing Jack's uses excel a to compile and run your numpy programs on GPUs and Tipu's compilation happens transparently with numpy library calls getting sped up but Jax takes it a step further than xla alone as it lets you just in time compile your very own Python functions into excel a optimized kernels using a single function API called JIT compilation and automatic differentiation can be composed arbitrarily so you can express sophisticated algorithms and get maximal performance all without leaving Python what else is there other than JIT there's also P map applying P map means that the function you write gets compiled by Excel a just like JIT and replicated and then executed in parallel across devices that's what the P in P map stands for this means you can do compilations on multiple GPUs or TPU cores all at once using P map and then differentiate through them all Jax boils down to an extensible system for composable function transformations the main ones today are grad legit P map and also V map V map is used for automatic vectorization allowing you to turn a function that can handle only one data point into a function that can handle a batch of these data points of any size with just a single wrapper function let's take a look at how this all comes together using a familiar example training a deep neural network on the M nest data set this notebook starts out by creating two utility functions to make a neural network with just randomly initialized parameters I've printed out the dimensions of each layer for convenience we can see here that it takes a 784 unit wide input and passes through two hidden layers whose size is 512 each and the outputs are the usual 10 classes since we're predicting what digit is supposed to be in that image next we have a function that takes care of running an image through our predict function our predict function only handles one image at a time and we can confirm this by passing in a single random image of the correct dimensions which gives us a vector of size 10 representing the 10 logit values coming out of the final layer of the network but when we try a whole batch of images say of length 10 also it fails since the array dimension is no longer line up but we're in luck because wrapping our predict function in a V map will allow us to take advantage of matrix multiplication and run all 10 images through the model in a single pass rather than doing them one by one the resultant function can handle a batch of cherry sighs and we don't have to modify our function one bit notice that the output is now ten by ten representing the ten logic values coming out of the final layer for each of the ten examples in that batch now let's see how we can use the grad and JIT functions to build out the remainder of our model as well as our training code we'll add in a function to one hotend code our data and a couple more functions to calculate accuracy and loss finally we'll put together our update function which will take the result of the loss function and run Grad on it which will take care of the back propagation for us and return the updated parameters of the model now we're almost ready to run our model we just need to add in some code using tensorflow datasets to bring in our M this dataset Jax purposely does not include dataset loading functionality as it's focused on program transformations and accelerator backed numpy so now we're ready to train our model our training loop is set for 10 epochs and we have a timer added in as well because we want to see how it performs so let's run this and we can see that across 10 a box we end up spending about 22 seconds per epoch now you might be thinking wait wait didn't you think mention something about using the JIT function did we ever add that good catch let's add in the at JIT decorator at the top of our update function and we'll rename it JIT update now we'll have a before-and-after comparison let's reinitialize our parameters using the same init network params function we used earlier and then run our new training loop and we'll see how the timing works out okay so this is looking like it's taking way less time only eight seconds per epoch and all we had to do was add four characters to the top of the update loop now that's legit before I close this out I want to remind viewers that as of this recording Jax is still just a research project and not an official Google product so it's likely you may encounter bugs and sharp edges the team has even made a list of gotchas and a gotchas notebook to help you out since this list will be constantly evolving be sure to see what the state of things are if you on using JIT for your project thanks for watching this episode of cloudy adventures and if you enjoyed it click that like button and be sure to subscribe to get all the latest updates right when they come out for now head on over to github and try out Jack's send over some bug reports and let the team know what you think 