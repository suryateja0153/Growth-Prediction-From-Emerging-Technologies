 [Music] hello welcome to the NPTEL online certification course on deep learning in our previous lecture we have talked about the various non-linearity functions in today's lecture what we are going to talk about is the neural network and when we talk about neural network we will initially see that how different logic functions the simple functions like and function or function or XOR function can be implemented using the neural network then we will talk about the feed-forward neural network or multi-layer perceptron and we will also talk about the learning of the training mechanism of the feed-forward neural network which is known as back propagation learning so before we go to the neural network let us quickly recapitulate that what are the different types of nonlinearities or nonlinear functions that we have discussed in our previous picture so we have talked about the very simple type of non-linearity which is the threshold non-periodic that is if Y is a function of X then Y will be equal to 1 if X is greater than or equal to 0 and Y will be equal to 0 if X is less than 0 so this is a simple threshold function or a nonlinear function where the threshold value is equal to 0 I can also have a threshold function where the threshold value can be nonzero may be say I take the threshold value to be equal to 5 so in that case value of y will be 1 if X is greater than or equal to 5 and it will be 0 if X is less than 5 so this is the simplest kind of non-linearity then I can have which which is a threshold function the other kind of non-linearity that we can have is what is known as sigmoid all function which is used in logistic regression so the sigmoidal function is actually given by 1 by 1 upon e to the power minus s which is a sigmoidal function of the argument s now in this case since we are talking about the classifications or machine learning techniques where we will be frequently talking about the dot product of two different vectors W and X W is the weight vector and X is the sample vector then our argument s becomes W transpose X so the sigmoid along linearity or logistic regression will be given by Sigma W transpose X is equal to 1 upon 1 plus e to the power minus W transpose X so as you find in the right hand side the sigmoidal function has been shown graphically and you see that at W transpose X equal to 0 the value of the sigmoidal function is 1/2 and as w transpose x goes on increasing the sigmoid function asymptotically reaches a value equal to 1 of course it will never reach the value equal to 1 but asymptotically you can say that it reaches the value of 1 and as W transpose X becomes negative as it increases from the negative side or in other terms w transpose x goes on reducing on the negative side then the sigmoidal function as in protocol e reaches a value equal to 0 so this logistic regression actually gives an output a limit on the output where the output is limited between 0 and 1 and between 0 and 1 we have a smooth transition where at the center that is at the value of W transpose X equal to 0 the sigmoidal function passes through 0.5 so this is another type of non-linearity which we will see that this this is widely used in implementation of neural network the other kind of non-linearity is what is known as rectified linear unit or raloo which is given by Y is equal to maximum of 0 or X so if X is greater than 0 then value of y is equal to X if X is 0 or less than 0 then value of y will be equal to 0 and the representation the graphical representation of this reloj function is also shown on the light right hand side in this figure so Ray Lu is also our non-linearity which is widely used in modern neural networks particularly when we talk about in deep neural networks or deep learning so we'll come across all these different types of nonlinearities as we proceed in our discussion so let us come to the neural network now the heart of the neural network is neuron so when we talk about neural network the concept of neural network is actually inspired from the way or we believe our brain works of course till now nobody has been able to say with certainty how the brain actually functions but this is what till now what we believe how our train actually functions so in our brain we have a network of neurons and if you look at every neuron as is shown in this figure on the right hand side the neuron consists of a cell body the cell body so this is the center of this which is the cell body or the nucleus under still body collects information it receives information through a number of sensors coming to the cell body which through a connectors which are known as dendrites cell body processes this information and the information is outputted to our connection which is named known as action and action finally it branches out and connects to other neurons through synaptic connections and it is believed that the information as it passes through actions and then finally branches out and then it is passed on to other neurons in the network through a synaptic connections in this process there is a multiplicative interaction what is that multiplicative interaction if the signal outputted by the cell body is the X then when it reaches reaches the other neurons through this multiplicative interaction coming through the synaptic connections the value which reaches the other neurons is W times X so this is the kind of multiplicative interaction which is given in the neural network or in the network in the brain so when you talk about neural network we'll also see that the neural networks are derived from this particular concept so what we have in neurons in neurons we have sales which receives signals through dendrites and it pass it passes this signals after processing to the other neurons in the network through synaptic connections so the processing is done in an unit in the cell which is known as soma and action is the connecting path which transmits the signal from one neuron to another neuron so this is what is the concept of a neuron in human brains so when we talk about a neuron in our neural network you find that here also every neuron consists of a functional unit which is the sale body given by this unit this collects information X or the vector X through a number of inputs which if we are equivalent to Linda its and when this inputs are coming to the cell body they passed to our weighting function given by W or weight values given by the weight vector W and the output of the neuron is of the form some function of W transpose X where W is the weight vectors X is the input vector and output Y of the neuron will be a function of W transpose X and when you talk about neural network this function if in most of the cases is a nonlinear function like the nonlinear functions that we have discussed before so we'll come to the use of those nonlinear functions in neural network in our discussions so given this model of the neuron a neural network is nothing but an interconnection of all those neurons so here you find that in this figure what we have shown is we have a number of neurons which collects information X that is our information vector or sample vector and in every level it is passed through or x and weight vector W so I'll have a set of weight vectors over here this processed information from every neuron is passed to the other neurons through the dendrites or synapses and while it passes through these dendrites or synapses while passing they are also multiplied by another set of weights or weight vectors W and it continues and finally when you get the output the output of every neuron or every unit in this neural network is given by this one w transpose X and usually a nonlinear function f of this W transpose X that is what is the output of every new row right so this is how is the architecture of a neural network looks like so given this let us now see that how these neural networks can be used to implement various functions so the first function which is a very simple function that we discussed is an ant function and the and operation that we are going to consider which is a logical operation on two units or two inputs the inputs are X 1 and X 2 and obviously these inputs are binary inputs so I have the input vector which is given by X 1 X 2 and my output function which is an and function is given by Y as shown in the table on the left hand side so as you all know that if the input vector is 0 0 then given the function to be and function the output is obviously 0 if the input is 0 1 the output is also 0 if the input is 1 0 output is 0 only when input is 1 1 that is both the inputs both the variables on the input which are binary variables are 1 then only the output of the and logic will be equal to 1 so if I consider this X 1 and X 2 which are inputs to this and gate to be the features or x1 x2 given together become a binary feature vector then I have a feature space or a 2 dimensional feature space so if I plot these outputs in this two dimensional feature space as given on the right hand side of this figure over here you see that when X 1 is 1 and X 2 is 0 the output is 0 which is shown here if x1 is 0 and x2 is 1 then also output is 0 if x1 is 0 X 2 is 0 output is 0 only when both x1 and x2 they are 1 the output is 1 so this is how the functional values will be distributed in the feature space given by the features x1 and x2 now I can consider this to be a classification problem that is when I am considering the input to be a binary feature vector then I can consider the output to belong to one of the two classes or the input vectors belonging to one of the two classes in one class which is class 1 and the other class which is class 0 so all the feature vectors 0 0 0 1 and 1 0 they will belong to one class when the output should be equal to 0 and only when the feature vector is 1 1 it should belong to another class and output will be equal to 1 and those distributions of the feature vectors are as shown in this plot on the right hand side now consider and now find that considering this to be a binary classification problem I have to find out a classifying boundary or a classifier which classifies these two classes and as you see that this is a linear problem as I can separate these two classes by using linear boundaries and over here though there are multiple boundaries possible that is I can have this as a linear line which separates these two classes this can also be a linear separator which is deeper which separates these two classes but one of the option is as shown over here and you find that equation of the straight line in this two-dimensional space is given by x1 plus x2 minus 1 point 5 equal to 0 and as I said that this is one of the many possible linear boundaries that I can have it these two classes so considering this now you find that I can consider this to be a feature vector where my feature vector is 1 X 1 X 2 and I have an weight vector which is given by minus 1.5 1 1 so equation of this straight line in that case becomes W transpose X or X transpose W whichever way I put it because the value of W transpose the expand X transpose W is same so the equation of this straight line is given by W transpose X equal to 0 or X transpose in W equal to 0 and the feature vectors 0 0 0 1 and 1 0 that will fall on one side of the straight line and the feature vector 1 1 will fall on the other side of the state line and incidentally if you analyze you find that this particular equation the classifier that I get this is nothing but a 2 class support vector machine or a binary support vector machine because it maximizes the virtual margin and as I said that though there are many possible straight lines that I can draw for them the margin will be less than the margin which is given by this so this is also a support vector machine now given this now let us see that how I can implement this using a neural network so as I said before all the feature vectors taken together I can put that in the form of our matrix and we are also putting this in unified form that is I am adding in each of the feature vectors one additional element which will be equal to 1 so my feature vectors are 1 0 0 so 1 is added which is an additional element as we have shown over here so this is one of the feature vector which is 1 0 0 1 0 1 is another feature week there 1 1 0 is another feature vector and 1 1 1 is the fourth feature vector and we also said so all these feature vectors are represented are put together in the form of a matrix ok and out of this we know that this first four feature vectors they belong to say class Omega 1 4 which output will be equal to 0 and for this it belongs to class Omega 2 4 which output will be equal to 1 and I also have this weight vector W which is minus 1 point 5 1 1 so given this representation representing all the feature vectors in the form of a matrix and that weight victor now how my classifier will work let us see this one so I can put it in the form of X transpose W where ya I just put it this way here instead of writing this as X I will write this as X transpose because whenever we talk about evicted we usually talk the vector as a column vector so this 1 0 0 which is go over here it is actually 1 0 0 that is a column vector so instead of writing this matrix as X let us put this as X transpose so that every row in this matrix is actually transpose of our feature vectors right so with this understanding you find that the way the classified will actually work is if I compute the W transpose X sorry X transpose W where X is this matrix X transpose is this matrix and W is a weight vector which is this then the output of this multiplication this matrix multiplication is minus 1.5 then minus 0.5 minus 0.5 and 0.5 now if I pass it through a non-linearity so if you remember we said that this nonlinearities are nonlinear functions are widely used in neural networks so this vector that I get if I pass it through a non-linearity which is a threshold function my output becomes zero zero zero one so the special function is when the input is less than zero the output should be equal to output should be zero if the input is greater than 0 then output will be one so in all these cases here it is minus 1.5 which is less than zero obviously so I will have an output zero here here it is minus 0.5 again I will have an output zero here it is minus 0.5 again I will have an output zero here it is plus 0.5 which is greater than zero so here I get an output equal to one so you find that this matrix multiplication followed by this threshold operations actually perform an and operation which is a logical operation so given this now how a neural network I can design a neuron to perform this particular task so in case of neuron as eight inputs the feature vectors and I say that the feature vectors are inputted through the dendrites one of the input I'll put it as one because the feature vector X 1 X 2 we are converting that to 1 X 1 and X 2 we are adding an additional component and making that equal to 1 which is you know a unified representation so I have 1 over here I have X 1 over here and I have X 2 over here which are my input vectors then the weight vector I put it as minus 1.5 here it is 1 here it is 1 so this function of the neuron I can put it in two forms to two parts the first part computes w transpose X and what is this W transpose X W transpose X is nothing but X 1 plus X 2 minus 1.5 so it becomes X 1 plus X 2 minus 1.5 and then the second part of the neuron that gives you the threshold function this threshold is and at the output what I get is function y is equal to f of W transpose X so I can put this either in the form of W transpose X or I can also write it as W I X I I varying from 0 to 2 and function of this and in this particular case this function with these weight vectors will be an ant function so this is one of the ways you'll find that I can implement an ant alergic which I can pose as a classifier problem as a binary classifier problem with binary inputs so we have two-dimensional binary inputs x1 and x2 and that classifier can easily be implemented by a single neuron so I don't need multiple number of neurons or in neural network for that purpose so using a simple single neuron neuron having a threshold non-linearity can implement an end logic in the same manner let us consider that whether I can have some other logical functions to be implemented in the same manner so I consider here the other logical function which is an or function again in case of or function I again consider the inputs to be 2-dimensional binary vectors having components x1 and x2 so my function will be a both 1 x1 x1 and x2 then output should be 0 in all other cases that is if the inputs are only when X 1 and X 2 both of them are 0 then output should be 0 that is it belongs to one class and in all other cases that when X 1 X 2 is 0 1 or X 1 X 2 is 0 1 0 or X 1 X 2 is 1 1 then output should be equal to 1 indicating that these feature vectors belong to the other class again as before if I plot these feature vectors in the two dimensional feature space as given over here you'll find that when X 1 X 2 both of them are 0 the output is 0 in all other cases the output is 1 here again it becomes a linearly separable problem again you can see that I can have multiple number of straight lines or infinite number of straight lines which separates these two different classes 1 of all these straight lines one of the straight lines is given by X 1 plus X 2 minus 0.5 which is equal to 0 and here you can easily verify that both if both X 1 and X 2 are zeros then output becomes minus 0.5 however if any of or both X 1 and X 2 they're equal to 1 then output it becomes output becomes plus 1.5 clearly indicating that when it is 0 0 the output is negative in all other cases the output is positive ok so when it when both of X 1 and X 2 are 1 1 the output is 1.5 if one of them is 1 and the other one is 0 the output is 0.5 however in all these three cases the output is positive so given this so this becomes a simple linear classifier and when I have this simple linear classifier you find that a single straight line in the speech space can separate these two different classes how can I put it in the form of a neuron can i how can i implement it in the form of in iran so here as we have shown before in the same manner i can do it as X transpose W for the matrix X is the matrix which is formed from all those two dimensional feature vectors W is the weight vector which indicates what is the separating plane between the two different classes if I perform W transpose X then this is the output vector that I get again you pass through this threshold non-linearity so output becomes 0 1 1 1 so when my input vector is 0 0 output is 0 when the input vector is 0 1 output is 1 1 0 again output is 1 1 1 the output is 1 so this simple operation implements the or logic and how do i implement it in neural network again a very simple I put the input vector to be 1 X 1 X 2 and the weight vectors will be minus 0.5 1 1 this neuron computes W transpose X and I have this threshold non-linearity which performs f of w transpose X where this F is nothing but the threshold non-linearity and at the output what I get is an odd function so again you find that using a single neuron I can implement a or function so it has been possible to implement these logical functions using a single neuron because the problem that have considered they are linearly separable problems both and and or functions they are linearly separable but if the problems becomes non separable what will be our situation and how we can solve those problems using neurons or neural networks that we will explain in our next lecture thank you 