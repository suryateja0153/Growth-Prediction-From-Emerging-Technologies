 the following content is provided under a Creative Commons license your support will help MIT OpenCourseWare continue to offer high quality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu I'm determined to tell you something about the convolution rule I just get close to it but I haven't quite got there and actually I'd like to say something also about why convolution is so important this I mentioned here a paper about images in deep learning by it has three authors and these are two of them maybe you recognize Hinton's name he's originally English he was in in San Diego for quite a few years and now he's in Canada so Toronto and Montreal are big centers now for deep learning son he's he's really one of the one of the leaders and and so is skipper so maybe you know that the sort of progress of deep learning can often be measured in these competitions that are held like about every year for you know how how well how well does something people design and execute a whole neural net and this was a competition about images so that is really demanding because as I said last time an image has so many samples so many pixels that the computational problem is enormous and that's when you would go to convolutional neural nets CNN because a convolutional net takes fewer weights because the same weight is appearing along diagonals it doesn't need a full matrix of weights just one top row of weights anyway so this is one of the historical papers in the in the in the history of deep learning I'll just read a couple of sentences we trained so this is the abstract we trained a deep a large deep convolutional neural network but I'll just enter say that they took it ran for five days on two GPUs so it was an enormous problem as we'll see so we trained a large deep knit network CNN to classify 1.2 million high-res images in image net so image net is a source of millions of images and on the test data they well we the last sentences may be a key we entered a variant of this model in the competition 2012 competition and we achieved a winning top 5 test error rate of 15% compared to 26% for for the second-place team so 15% error they got 26% was the best that the rest of the world did and so so that and and when he shows the network you realized what's what's gone into it it has convolution layers and it has some normal layers and it has max pooling layers to cut the dimension down a little bit and it's and and half the samples go on one GPU and half another and at certain points layers interconnect between the two GPUs and so to reduce overfitting you remember that's the key problem is to reduce overfitting in this in the fully connected layers those are the ordinary layers with full weight matrices we employed a recently developed regularization called drop out so drop out as a tool which if you're in this world you you I think Hinton proposed it again by seeing that it worked it's just a careful it's a careful drop out of of some of the data it reduces the amount of data and it doesn't harm the the problem so so the neural network has 60 million parameters 60 million with 650 thousand neurons five convolutional layers and three fully connected layers I just mentioned this and you could if you just google these two names you would on the web view that paper this paper would come up but it's so it's we're talking about the real thing here convolution is something everybody wants to understand and I I'd like to since I've started several days ago and I'd like to remember what convolution means let me so if I convolve two vectors and I look for the case component of the answer the the formula is I add up all the C's times DS where the index i + j adds to k why do you do such a thing because c might be represented by a polynomial say x + + c + X to the nth and D might be represented by another one D 1 X plus the M X to the M let's say and convolution arises when I multiply those polynomials because for a typical and then collect terms because a typical power of X say X to the K the coefficients are well how do we get X to the K in multiplying these I multiply C naught times a DK somewhere in here would be a DK X to the K so a C naught times a DK would give me an X to the K term and a c1 times everybody sees this coming now c1 has an X in it already so over there we would look at DK minus 1 with one less X so it would be c1 DK minus 1 this is just what you do when you multiply polynomials and the point is that the way we recognize those terms is that the exponents 0 and K the exponents 1 and K minus 1 always add to K so that's where this formula comes from we take a C times a D hiding behind or a C X to the I and a DJ X to the J and when a plus I plus J is K this is X to the K and that's the term we're capturing so this is the coefficient of that term and let me write it in a slightly different way where you actually see even more clearly convolution operating so J is K minus I right so it's the sum of C I D J but the J has to be K minus I so this is the this is the way to remember the formula for the coefficients in the content C star D in the convolution you look at C's times DS it's it's a form of multiplication it comes from ordinary multiplication of polynomials and when you collect terms you're collecting see the I C and the K minus I D and you're taking all possible eyes so it's just sum over all possible eyes there to give you the case answer well just to see if you got the idea what would be the convolution of two functions suppose I have a function f of X and I want to convey the function G of X okay and notice that I have not circled this symbol so I'm not doing periodic convolution I'm just doing straightforward convolution so what are we going to have in the case of two functions what would that mean a convolution of functions I'm I'm in parallel here with a convolution of two vectors so think of these now have become functions the case component has become really I should say F star G at X that's really that's really the parallel to this so let me so I'm I'm telling you the answer at X here I told you the answer at K case the case component looks like that what does the x value of the convolution look like for functions okay I'm just going to do this I'm gonna do the same as this instead of summing what will I do integrate instead of C sub I I'll have f of X eyes the index I is changing over to the continuous variable X and now G instead of DK minus I what do I have here so so it's uh it's K minus I component that will go to let me just write it down t minus X so so in this translation F is being translated to C or sorry F is corresponds to C G corresponds to D K corresponds to X and oh sorry I corresponds to X and J and K minus I corresponds to t minus X so so K corresponds to T that's this is what would be the convolution of two functions oh it's a function of T bad bad notation to the T is sort of this the amount of shift you see I've shifted G I've reversed it and flipped it and shifting it by different amounts T it's it's what you have in a filter it's just all of always present in in in signal processing so that would be a definition or I could if you like I if you want an X variable to come out let me make an X variable come out by exchanging T and X so so this would be X minus T DT I like that actually a little better and it's the integral over t minus infinity to infinity if our functions were on the whole line so that will there will be a convolution rule for the that this will connect to the Fourier transform of the two functions over here I'm connecting it to the discrete Fourier transform of the two functions and I've been making the convolution cyclic so what does can I add cyclic now this is ordinary convolution this is what you had in the first lab I think from from Raja Rao the first lab you remember you had to figure out what the how many components the convolution would have and you didn't make it cyclic so a cyclic convolution if if this has n components and this has n components then the convolution has n components because you keeping yeah and it's the key number there the length of the period and similarly over here if F is 2 pi periodic and G is 2 pi periodic then we might want to do a periodic convolution and bring it get a get an answer that also has 2 pi of period 2 pi yeah so you could compute the convolution of sine X with cos x for example ok let's stick with vectors so so what's the deal when I make when I make it cyclic when I make it cyclic then in this multiplication I really should use I've introduced W as that instead of exocyclic X becomes this this number W which is either 2 pi over n and has the property then that W to the N is 1 so that all vectors of greater than n can be folded back using this rule to a vector of length n so we get a cyclic job so how does how does that change the answer well I only want K going from 0 to n minus 1 in the cyclic case I don't want infinitely many components I've got to bring them back again and let me just say what the rule would be you just ask say I plus J you would look at that modulo n that's what a number theory person would call it we only look at the remainder when we divide by n so we so now the the sums go only from 0 to n minus 1 and I only get an answer from 0 to n minus 1 well I've done that pretty quickly that's if I wanted to do justice to so is the the difference between non periodic so non-periodic and periodic will be the difference between so I have some number T zero on the diagonals t 1 t 2 t minus 1 t minus 2 and so on constant diagnosis the key name there is triplets and if it's periodic then I have I'll say C CC and then the next one we see one C one coming around to C 1 and C 2 coming around so it's so as n by n period n so it's a it's a circulant matrix n by n ok that's just that's the big picture and I think in that first lab you were doing your rest to do the non circulant case because that's the one where you have to do a little patience what will be the length yeah what would be the length of a non-circular so not circulant now suppose C suppose the C vector has P components and the D vector has Q components how many components in there convolution so I write that question down cuz I that brings out the difference here yeah so so if I have P if C has C has P components he has cue components then the convolution of C and D has how many so I'm multiplying so it's really this corresponds to a polynomial of degree P minus 1 right polynomials of degree P minus 1 and this guy would be degree Q minus 1 degree Q minus 1 and what I multiply them what's the degree just ad and how many coefficients well one more I have to remember for that stupid zero order term so this would have P plus Q minus 1 component so that would have been the number that you somehow had to work out in in that first lab so that if this had end components and this had n this would have to n minus 1 it's just what you would have like you say 3 plus X times 1 plus 2x in this case P is 2 Q is 2 2 components 2 components and if I multiply those I get 3 X + 6 X is 7x + + 2 x squared and so I have 2 plus 2 minus 1 equals 3 components X constant X and x squared yeah clear-rite yep so that's not the that's what I would get if I multiply these matrices if I had a two diagonal matrix Triplets matrix times a two diagonal Triplets matrix that would give me a three diagonal answer but if I am doing it periodically I would only have to that that 2x squared would come back if if I come back as a 2 so I just have 5 plus 7x good right good good good ok so that's a reminder of what convolution is cyclic a non-sex like vectors and functions okay then I can values and eigen vectors or the next step and then the convolution rule is the last step so eigen vectors eigen vectors of the circle of course I can only do square matrices so I'm doing the periodic case so the eigen vectors are the columns of the of the eigenvector matrix and I'm gonna call it f for Fourier so f is the first eigen vector is all ones an X eigen vector is the fourth root of one then the square root of one i 6 i ate i fourth i sixth and finally one i cubed i 6i9 okay that's f those are the four eigen vectors of the permutation P and of any polynomial in P so my circulant is is some C naught I plus C 1 P plus C 2 P squared + c 3p q okay and finally this is like the step we've been almost ready to do but didn't quite do what are the eigen vectors what uh what eigenvectors are its eigenvectors so those are the eigenvectors of p and now we have just a combination of peas so I think the eigenvectors I just multiply I take that same combination of the of the eigen vectors yep yep is that that look right so sorry it's I'm sorry its eigenvectors they're the columns of F the question I meant to ask is what are its eigenvalues that's the key question what are the what are the eigenvalues and I think that if I just multiply F times C I get the eigenvalues of the matrix C that's the beauty that's the nice formula if my matrix is just P alone then this is 0 1 0 0 and I get 1 I I squared I cubed but if C is some other combination of the peas and I take the same combination of the eigen vectors to to see yeah do you see it so I'm claiming that I'll get for eigen values of C from this multiplication so of course if C know if there's only C not then I only get C naught C naught C naught C naught I get it's a repeat four times repeated but if it's a if it's this combination then that matrix multiplication takes the same combination of this is a combination of the eigenvectors and and that gives us give it gives us the right thing okay so that's now I just have one more step for this the convolution rule and then I'm happy really the convolution rule is stating what we it's stating a relation between multiplication which we saw here and the convolution which we solve for the coefficients so the convolution rule is a connection between multiplying and convolution and so let me say what that convolution rule is and let me write it correctly so so here I take cyclic convolution I'm dealing with square matrices everything is cyclic here and then I get if I multiply by F what do I have now what does that represent this was C and B and Ike involved them so I got another circulant matrix so so so up here the multiplication of of matrices is C times D I want to connect multiplying those matrices with convolving the C's I want to connect make that connection right so and that connection is the convolution rule okay so this would be the eigenvalues of CD let's just pause there why why am I looking at the eigenvalues of CD because if I do that multiplication I get another tipless matrix C times D and the pollen only their coefficients associated on the diagonals of C times D are the coefficients of the convolution so the its diagonals its diagonals come from convolving C with D 6 - okay now I want to find those same eigenvalues in a second way and match and the equation will be the convolution rule so how could I find the eigenvalues of C D well amazingly they are the eigenvalues of C times the eigenvalues of D I'm gonna test this rule on two-by-two so you'll see everything happening but so this is the main this is the fact that I want to use because yeah because C and D commute C and D commute they have the same eigen vectors and then the eigen values just multiply so I can multiply I can get that in a second way by taking the eigen values of C and multiplying those by the eigen values of D and I multiply component by component I multiply the eigen value for the for the all one's vector by the eigen value for the all one's vector so I do a do you know this MATLAB command component by component multiplication this is it's an important one there's a guy's name is also associated with that so so that's a vector that's a vector and what comes out of that operation if I have a vector with three components so n is three here and I do point star or dot star I'm not sure what people usually say component by component a three component vector times a three component vector I get a three component vector just like that yeah yeah so this is the convolution rule that's the convolution and the proof is the fact that eigen that when matrices commute the eigen values of the product or just these eigen values times these eigen values because they have the same eigen the eigen vectors are always the same here for all these circulant so there's the convolution rule that I can convey then transform or I can transform separately and then multiply so just maybe better write that the convolution let's call it the C rule convolve then transform by F or transform separately by F and then then multiply point wise you know element by element yeah component by some bonus okay yeah so that's the convolution rule and why is it sort of why is it so important because transforming by F multiplying by the Fourier matrix is extremely fast by the FFT so so it's useful because of the FFT the fast Fourier transform to multiply yep or to transform whichever multiplied by F transform so it's it's the it's the exist presence of the FFT that makes this it gives us a really two different ways to do it in fact which is the faster way so we can produce the same result this way or this way and if I don't count the cost of if the cost of multiplying by F is low because I have the FFT which would you do which would you do so let's let me just think aloud before we answer that question and then we're we're good so if I so my vectors have n components so one way I can do is to do convolution how many how many steps is that if I take a vector with n components and I convolve with a vector with n components how many little multiplications do I have to do and square right because each of the C's has to multiply each of the DS so that takes N squared and Fourier is cheap it's n log n log to base 2 so the left-hand side is effectively in cubed what about this one how many up how many to do these two guys to find the Fourier transform to multiply by the matrix F ok those are fast again that's just I've got two multiplications by F so that's 2 n log N and what's the cost of this I have a vector with n components dot star vector another vector with n components how many little multiplications do I have to do for a Hadamard product or a or a component by component product n only in + in yeah I should maybe I should have made that + yeah yeah I had I had to I know I had one n log n plus it took M squared to find that vector and then n log n so it's effectively N squared but this one where I do the N log n twice and then I only it only takes me in more so this is the fast way so if you wanted to multiply two really big long integers as you would want to do in in cryptography if you had two long integers say of length 125 126 128 128 components to multiply those you would be better off to separately take the cyclic transform of each of those hundred and twenty-eight guys and do it this way take this take the transforms do the component by component product and then transform back to get that yeah yep so it's the convolution rule is what makes that go okay so that's oh one more thought I guess about all this convolution stuff suppose we're in 2d we have to think what's what is a two-dimensional convolution what does this become in two dimensions suppose we have functions so now I'm to be functions of x and y periodic or not periodic but what will we what's a convolution what's the operation we have to do in two dimensions well it's a double integral of course T and u we would do F of T and u times G of X minus t y- u d TD u and that would produce a function so i'm convolving a function of x and y with another function of X and again I'm looking for the this this this is the key to watch for x- t y- u that that's the signal of a convolution integral yeah so that that's what we would have in 2d in generals do you so much may be mine now my final thought is to move to think about two dimensional matrices and and their products and so on and this is why you need them because you're you if you're if you have two dimensional signals then the components fit into a matrix and we just have to operate in both dimensions so so the the key their key operation in 2d is in MATLAB the MATLAB command that you need to know to get if you know what you're doing in 1d and you want to do it in 2d the MATLAB command is cron so so we imagine we have one dimensional matrices a and B and so those are in 1d and we want to produce the natural to them two-dimensional matrix so these will be n by M and we want the sort of natural product let me call it k for cron which will be N squared by N squared I want to create a 2d matrix with connected to an image that's n in each direction so it has it has N squared pixels this is these are 1d signals and K is a 2d one and this cave would be there this is the operation to know given given to one dimensional N by n matrices cron produces a to an N squared by N squared matrix it's sick it's a operation to know so I'll just write it in if you know what cron is and you know what you know it before I write it so what so I want to produce a big matrix n squared by N squared somehow appropriately multiplying these two guys and the appropriate way to do it is to take a 1 1 and multiplied by B so there what do I have what size have I got they're already just in that one corner n by n right it's a number times an N by n matrix then a 1/2 times B that's another n by n matrix up to a 1 n times B so I have now sorry Kappa so I have tap-in matrices in a row each of those matrices is n by n so my that row has length N squared and of course the next row is I've I've allowed myself to number from 1 to N but very often that numbering should be 0 to n minus 1 and finally going down here down to a and 1 B so so that's the N squared by N squared matrix that you would need to know for example if you wanted to do a two dimensional transfer you transform it would be yeah so what would a two dimensional Fourier transform per do what what matrix is this the matrix you would use for for a 2d I haven't sort of got started properly on 2d Fourier transform so would it be F times F F cronic so let me write down the full name of this guy chroniker so it's called the Kronecker product it's just it's just the right thing to know in moving from one dimension to two dimensions for example let me I got one board left yeah yeah so here's a standard matrix call it a twos and minus ones so that that's that chorus that corresponds to a second derivative or actually minus the second derivative now suppose I have a mate another the same matrix corresponding to second derivatives in the Y direction same and what I really want to do is both I want to have a matrix K that corresponds to minus the second in the X Direction minus the second and the Y so this is the Laplace laplacian which is all over the place in differential equations at a typical point I want to do - one of these two of these - one of those in the X direction and I want to add to that minus one now that too becomes a four and minus one in the Y direction so I'm looking for the two by two mate the the two dimensional matrix that takes that does that five-point scheme five weights for each point takes four of the on the diagonal and minus one on the four neighbors and the operation that would do that would be you would use cron it wouldn't be cron of a B that would just K of a B is not what I what I was a cron of a B is not what I want yeah that would do one and then the other one and then that would probably produce nine non-zeros I want something that adds here so I I want cron of a time's the identity that gives me the the two dimensional thing for this part and then I'll add on cron of I be for the vertical derivative the derivatives in the Y direction so that would be so that's called a chroniker some this is a this is a the other was a chroniker product so that would be a chronica product this would be another Kronecker product and that total is called the chroniker some okay I wanted just to get that those notations out because really you know Fourier transforming I said is such a central operation in in in all of Applied mess and especially in signal processing okay so I'm good for today let's see I've got one volunteer so far for to talk about a project can I encourage an email from anybody it doesn't you don't have to be a superstar it's just willing to to do it tell us something about what you've learned get comments from the audience and you know ten ten or fifteen minutes is all I'm thinking about it yeah okay I'll let you send me an email if you're if you'd like to tell us that and get some feedback ok good so I'll see you Wednesday Thanks 