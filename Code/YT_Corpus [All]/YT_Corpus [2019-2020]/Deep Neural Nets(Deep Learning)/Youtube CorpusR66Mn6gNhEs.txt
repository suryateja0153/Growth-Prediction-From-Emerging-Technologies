 artificial neural network is a computing system designed to replicate the way humans analyze and work it forms the base of all artificial intelligence concepts that is why we have come for this tutorial on neural networks now before we go ahead to the session i'd like to inform you guys that we have launched a completely free platform called great learning academy where you can have access to free courses such as ai cloud and digital marketing you can check out and how it inspired us to come up with the concept of artificial neural networks going ahead we'll understand the implementation of artificial neural networks pulling which will comprehensively understand what is back propagation and then finally we'll have a demo where we'll be implementing these artificial neural networks with the r language so let's start off in a session in this video we will see what is artificial intelligence what does it mean how it has evolved over a period of time and what does it mean to you and me and what does it hold for us in the future artificial intelligence has been gaining a huge momentum in the recent world and it's making a tremendous impact in our career outcomes but before we delve into that let's understand what is artificial intelligence so what is ai let me ask you a question look at the photo above and can you guess who that famous personality is i think most of you guys would have guessed that it is albert instant but it's not an easy problem think about how your brain processed that information and how did it recognize it it might have noticed the facial features it might have noticed the curly hairs it might have noticed the distance between the eyes it might have noticed another million features to come to the conclusion that it is albert instinct so can you make a mission to think the same way which you think that's ai the process of making a mission to think like how a human does is what is known as ai however humans ability to think and make a rational decision in a critical situation is unparalleled and even science is not able to crack that so what does missions have for them how is it superior and how is it doing good what do they have is computational power and this has gone massively in the recent years let me ask you what is the square root of 200 you will not be able to tell me in a second but whereas the computers can but if you were to ask a computer to take a bottle of water from your kitchen will it be able to well the computer needs to know what does a bottle mean what does water mean and how to move and where is the kitchen whereas the humans don't have any problem in that so making a computer to learn about its surroundings adapt and then grow yes also yeah the term artificial intelligence has been around since a long time it was first coined in 1956 by mr john mccarthy at the darpa conference though the conference was a hit and it garnished a lot of attention from a lot of players well creating these missions or creating the smart missions required a lot of computational power and hardware which was apparently not there at that point of time well the tipping point came in the late 90s where ibm's deep blue stunned the entire world by defeating the world champion at a chess tournament so not a fully functional ai we see fragments of ai implementation in our daily day-to-day life think of the next time when you type in google to search something well how does it come up with the fact what are going to type next well google has tons of data based upon what have you typed so far and its a algorithms are running on the back to find out what are you going to type next based upon whatever you write netflix predicts what movies you want to watch next based upon what movies you have watched in the past whereas facebook detects all your friends in the same photo which you've taken and uploaded based upon the information it has gathered in the past about your friends in your own profile ai is usually divided into two broader categories machine learning and deep learning machine learning is a use of statistical methods to understand and analyze information from a given data whereas deep learning a broader area of machine learning or a broader subset of machine learning mostly deals with neural networks with computational power growing exponentially year on year the scope of ei has become vast and is being used in almost all the domains in all the verticals and across all the industries beat it retail telecom bfsi travel e-commerce any of these industries ai and ml has a future in all the facets of these industries hope this video was helpful to you guys if you like this video like share and subscribe to our channel and stay connected with us for future updates so with neural networks uh we'll start off with seeing briefly what a biological neural network is um we won't go into much of the details we'll just do an overview because it helps to understand what an artificial neural network does um then we'll see the overview of artificial neural network and a small example uh a pretty small example on uh what happens and how a neural network learns uh we'll see how we implement a n and and different steps that are a part of implementing artificial neural network uh one of them being back propagation then um the weight subtraction and then the error function then we'll also see the um summary of what the overall learning algorithm that the neural network follows and then we'll see how we implement neural network in r using the same um using that same iris data set that we used to do so that we only know what is the essence of using neural uh network in r and we're able to compare the outputs of what we did with previous algorithms and what the neural network gives us okay so with biological neural networks you would have seen already how um there are the basic building blocks of the um certain or sorry the central nervous system so these are the diagrams which may be repetitive for you but then they help us understand what a biological neural network does so not going very deep into biology um so something that you see in yellow is a neuron okay which is similar to a neuron in the artificial neural network uh wherein each contains some data about the um input data variables okay so like the neurons are interconnected at the point called synapses so in the diagram above um you see a red circle where there is a synapse so neurons are interconnected at the point called synapses and this is similar to the artificial neural network because um the different input variables that you take um that information is passed to the hidden layer and how that information is passed through the hidden layers is through a point which is similar to the synapse in the biological neural network um so there are three parts that are typically present in a biological neural network uh one is the dendrite which is the yellow um you can say like terminals of a neural network which are responsible to receive signals from the surrounding neurons then there are actions which are nothing like a path so uh comparing it to the artificial neural network like there is a path when you go from the input layer to the hidden layer um there is a path in between uh which carries the information so the action is similar to that then the synopsis uh synapses are responsible for receiving the signals and learning from the past activities how we correlated to the artificial neural network is um because if you if you know of the concept of back propagation even the artificial neural network learns from its past activities hence you're able to kind of relate the biological neural network with the artificial neural network so each of the neurons here acts as a neuron in the artificial neural network um the axons act as a path where the information is traversed from one layer of neural network to the another layer of neural network and the dendrite and synapses are used to send and receive information between one neuron to the other and then they also enable the neural network to learn from the past activities and that's how the algorithm improves itself okay so ah this is essential to understand only because the artificial neural network gets its name from the biological neural network okay okay um so this is again um a small diagram to just show you how the information is passed uh with the biological neural network and a similar kind of thing happens with the um artificial neural network as well okay so what happens is whenever a certain information is um coming to a neuron when a certain information is present in a neuron um so there's this thing called threshold which were also seen with the artificial neural network so once that threshold has reached one that threshold has been achieved the neural network transmits a signal through the axon and the similar kind of thing happens with the artificial neural network as well whenever the threshold is reached uh activation function triggers and then an information is passed from the input um layer to the output layer so we'll see how each of these steps in the biological neural network helps us also understand what is present in the artificial neural network okay so this is uh how a basic of how an artificial neural network works um so the inputs that you see here are nothing but the input data variables uh with the data set they are working with um then there are certain weights which are applied to each of the input variables um so the weights that are applied are nothing but they help the neural network to tell what is the importance of each of the variables so this is also what we saw with the previous algorithms we learned so each variable that is present in our input data set does not have equal importance okay each input variable does not have equal importance there are variables which are more important their variables which are lesser important how that is decided is through these weights so whichever variable is given a higher weight is comparatively more important than a variable which has a lesser weight so this is how the weights are used in the neural network so the set of inputs which are nothing but the variables from the data set are thought of as neurons this is how we correlate an artificial neural network with the biological neural network ah so then they are weighed according to the weights that are applied what i just told and finally this is uh we do kind of a sum product of the input variables with the weights sum product means to say um x1 into w1 plus x2 into w2 plus x3 into w3 and so on okay so the function um that you see here the summation function of the sigma function here this can also be called as a transfer function so this actually performs the sum product so it will sum up the weights and the input that it receives from each of the neurons the next step is uh to go to the activation function which will decide whether or not a neuron will fire so this is similar to the biological neural network wherein with the threshold we decide whether or not um the information should be sent to the next layer okay so if if the threshold uh so whatever sum you get using the transfer function uh you compare it to the threshold if it exceeds the threshold the activation function decides to fire when it fires the information is passed from the input layer to the output layer like similar to what happens in the biological neural network when the activation function fires information is passed from one layer to the other okay so like i told earlier also the weights can be used to amplify or de-amplify the original input signal meaning to say they are they are used to determine how each variable will be used uh what is the importance that will be given to each of the variables okay so consider a simple example so this example has got nothing to do with the neural networks actually um consider that you have to say two input variables x1 and x2 um so these are two input variables in a data set one of them having a way a value of 0.6 and one of them having a value of one um and the weights say that are applied to both of these input variables are 0.5 and 0.8 okay and you've also been given a threshold of 1. so what happens um in simple terms in the neural network is uh the first step of a transfer function what we saw in the previous slide uh so nothing but the sum product will happen here so 0.6 into 0.5 plus 1 into 0.8 so that gives you a 1.1 so this is similar to the uh summation function that we had seen here um next we compare this uh value that we've got with the threshold so say our value uh that we've got is 1.1 but the threshold is 1.0 so we see that our output is more than the threshold that was allowed so since the output is greater than the threshold so the neuron is activated and it fires okay so this is a very simple example this is not what actually happens in neural network because there are a derivative of the error function and all that is calculated so error function is nothing but the actual output uh the difference between the actual output and the desired output so there are no derivative of that is calculated and passed back to the um neural network but i i put this example only to show you how the threshold works um so when whenever the output exceeds the threshold the neural network will know that it needs to fire um a signal what is meant by firing a signal is nothing but a back propagation meaning to say that whenever an output increase the threshold the algorithm will uh sorry whenever the output increases threshold a back propagation will occur and a reiteration will occur a new iteration within the neural network will occur and the output the algorithm will try to improve the output okay now we'll see how we uh implement the artificial neural network um so this is a simple diagram to show how a neural network works so there are for example three input variables and two output variables here um so they can be any number of hidden layers that are present um in the neural network and the hidden leds actually perform whatever um the algorithm needs to be applied is done by the hidden layers only uh there are different ways on how we decide uh how many hidden layers are to be present and we'll see that when we work with r um so what happens here is that the input layer will take the input from the input variables so say you have three variables here so each input each of the neuron that you see in the input layer will have data from one variable each the input layer will take input from the input variables the in hidden layer will help the input layer to move to the output layer okay so the input layer will pass the information to the hidden layer with the weights present and then the hidden layer will move that data to the output layer but the hidden layer is essentially a big black box layer like i told you earlier you will never be able to understand what is happening with the hidden layer and the output layer will throw you the final output so this is another way to um a more detailed uh diagram of the above diagram so what i'm trying to show here is that there could be any number of nodes that one can be present at every layer and then there are weights that are applied to the inputs excuse me okay so then there are weights that are applied to the input that take them to the hidden layer so if you see in the diagram below there are three inputs one two three and then there are some weights which are applied to the each of the inputs when it is being taken to the hidden layer so each input variable with some weight applied will go to each neuron in the hidden layer so if there are three input variables and there are three hidden layers so each of the inputs like the input i1 will go to each of the neuron in the hidden layer and a different weight will be applied when it goes through each um of the neuron in the hidden layer and there is another category of weight so one is the weights that are applied to the input variables there is another category of fate which we called as the bias values um so bias values are again used to train the neural network um so this this can be compared to if you remember with um the regression also we used to have a similar thing where now we'll have some weights that are assigned to each of the variables so that were the coefficients back in regression when ah you used to get an equation something like y equals to um a plus b 1 x 1 plus b 2 x 2 plus b 3 x 3 and so on so when there were coefficients uh in regression are the weights which are in the neural network but there was also an intercept which was the unexplained variance or unexplained output that the intercept used to capture so similar happens here with the hidden bias so if there is some information that the input layer input variables along with the weights cannot capture that is captured by the hidden bias so with each of um the layer that you have apart from the input layer there'll be a bias added like if you see in the diagram here with the second layer that is the hidden layer present in the between there's a hidden bias that is added and with each hidden layer that you have like here we have just hidden layer but with each hidden layer that is present there will be a hidden bias for each of the layers and finally there all there will also be an output bias which is present for the output layer nothing but the intercept what we used to have with the regression equation okay that was the basic of what an artificial neural network does uh we'll go to black propagation now to see how the neural network loads okay so the basis of a neural network is a powerful learning mechanism um although it is a powerful learning mechanism but you will never be able to understand what that learning mechanism is but that being said the neural network has a powerful learning mechanism and it can learn any function given it has enough hidden units so each of the hidden units that are present enable it to learn to the uh input data set and how it learns us by the mechanism that we call as back propagation okay so like i told you earlier also when an input is fed to the neural network that is when the input variables are fed into the neural network um the input will some weights will be applied to the input that is uh coming some bias will be applied to the in addition to the input that is coming and finally that data will go to the hidden layer same process will be repeated and the data will move to the output layer when you get the data at the output layer the difference between what the actual output or the desired output was and the output that you are getting some error will be calculated based on the difference whatever is the error will be fed back to the first layer and the whole process repeats again okay so these errors are fed back to the neural network and then these weights to the question that you were asking the weights are then changed in order to try to reduce the errors and give the correct output so the first iteration random weights will be assigned but with each iteration that happens the weights are changed um using the errors that we have got in the previous layer so this is the diagram that explains back propagation so there are some input variables then there is a hidden layer so say there are three input variables here then there are two hidden um neurons with the hidden layer so each of the input variables is being um fed to the hidden layer and then there's also a bias which is being applied and then from the hidden layer we're going to the output layer and the output layer the error calculation happens which is nothing but the difference between the actual value and the calculated value whatever is the error calculation is back propagated and then the weights are modified this happens each time the algorithm runs and that's how the weights are improved and that's how the algorithm loads okay okay um so then once uh you're done with back propagation so all you did with back propagation was to uh get the error and pass it back to the input layer so that the weights are modified now how the weights are modified is after you've done the back propagation you need to update the weights now if you remember with the previous slide that we have seen here there are weights applied to the hidden layer and then there are weights applied to the output layer as well so both of these weights are different okay so um don't get too confused with what uh the equation here is but this is just meant to show that there are two different weights that are applied uh to both of the layers so w i j that you see here is the weight that is um passed to the variables from the input layer and w j k is the weights that are applied to the neurons from the hidden layer okay so both of them will learn from the errors that they are doing and both of the weights will be updated now don't go very deep into what the mathematical terms here are but this is just meant to tell you that both of the weights will be modified based on the error that we are seeing from the previous iteration of the algorithm there's another thing which you would be see uh which is seeing here which is um the alpha value um so alpha value is nothing but the learning rate learning rate means to say at what rate do you want the neural network to learn so this is nothing but the learning speed okay if you make the learning speed too quickly um or you make the alpha value you don't really specify these values this is what happens in the background but if the learning speed is uh very quickly the uh sorry the learning speed is very fast and alpha value is closer to one the algorithm would not be able to learn properly this is nothing like nothing but um trying to say that you're trying to learn something very quickly so you just run over it you don't understand the essence of what was there in the data and if if you uh make the learning rate very small you don't allow the algorithm to grow you don't allow the algorithm to learn only so an optimal learning rate is chosen but you don't choose choose it the algorithm chooses itself but an optimal uh learning rate is chosen for each iteration that happens with the neural network so when back propagation will occur some errors would have occurred and then a back propagation will occur that error will be passed back to the um algorithm the weights that are being passed that are being applied to the input data variables will be modified the weights that are being applied to the hidden layer will also be modified and finally a new output within produce and this process repeats across multiple iterations and each time the weights will be updated okay um then we come to the error function error function is nothing but again the difference between the actual value and the desired value here um so also this is a repetition but the input variable will be passed through a system so adaptive system here is nothing but the hidden layer and then you get a output y you compare that output to the desired output whatever is the error is passed back to the adaptive system nothing but the hidden layer then the weights are modified um so one thing to keep in mind is that the error should satisfy some particular properties uh this was this was present in the videos as well that we saw uh the first thing being that the error should be um derivable meaning to say that it should be possible to do a derivative derivative is nothing but d by d x what we used to do back in 11 12 mathematics um so it should be possible to do a derivative y um to answer to one of the questions earlier uh the threshold that we calculate the threshold that we specify is a derivative of the error okay so if if it is uh not derivable so the threshold thing will not work the threshold that we are specifying that we are telling the algorithm is nothing but derivative of the error that we are getting in each layer okay um as so this is very obvious although but as the output moves towards the desired value the error should obviously move to zero error should be negative again for the reason that we should be able to do a derivative of it um and then uh we need to choose um the error the error value sorry we're not choosing the error value uh actually but then uh when there is a too low value for the error or there are two high values for the error that actually uh impact how the neural network works if the error value is too high the accuracy of the neural network will be bad and then it will take a lot of iterations to be able to learn and if the error value is too small sorry that i have put it uh opposite so if the error value is high then um the neural network will take a long time to learn i'll just change it after this session okay so we'll finally see an overview of what happens with the learning algorithms um the learning algorithm overall so the bad propagation actually allows the neural network to get trained similar to how we train all the machine learning or the data mining models similar way the back propagation allows the neural network to learn um this happens in multiple iterations what i've been calling so far uh these iterations in terms of neural network are also called as epochs um so it goes through several epochs before the network has sufficiently learned to handle all the data that is provided uh this is again a different diagram shown in a different way to show how back propagation works but that will happen this is um that will happen for each of the iterational eats of the epochs uh the input data will be fed the output data will be compared against the desired value and then it will be whatever the error will be will be back propagated um using the error to update the weights um now when will um this stop meaning to say that each time when um a particular error is calculated based on uh what was the output versus what was the desired output and then a back propagation occurs so this process keeps on repeating so like we used to have a pruning step or stopping criteria for the decision trees that we saw last week similarly there should be a stopping criteria here as well um so multiple stopping criterias can be used for neural networks one of them being a desired mean square error and one of them being elapsed epochs desired mean square error tells what is the error that we can allow the data to have what is the flexibility that we're giving the data to have uh and elapsed epochs is nothing but what is the number of iterations that we want so a neural network cannot run indefinitely through 10 or one lakh iterations uh this varies with the data that you have but it cannot run indefinitely so we need to specify the number of iterations for which a neural network should run so both of these um are used as stopping criterias for the neural network okay um so again i'm using the iris data set that we had um seen with the previous um algorithms as well um to give you a quick revision of what the iris data set was so this data set has 150 um entries and which there are four input variables which are sample length width and petal length and width and finally there are three species 50 um of each specie and the aim of this algorithm is to uh try to predict what the partic what is the species of a particular flower given its attributes of sepal and petal length so with the last session we try to predict the species using cart um using random forest we also saw the same example with um clustering as well when we were trying to use the sample length width and better length and width to uh try to cluster these flowers together so we'll use that same data set um i'm repeating the steps what we did earlier of uh generating a random number shuffling the data so this again is a um optional step but i normally do it so that there's no biasedness which is present in the data one important step which you can note down here is the line number 12 um that i have with the r code so if you remember with neural network uh this was being done in the example that we did and this will also be useful in the example that you do with your mini project so if there is any variable which is a categorical variable something like um the species that you have here when it where is this where it is dosa uh versicolor and virginica when you have three species you cannot have the data in this format when you work with neural networks so you need to have three columns which are in forms of one and zero so um either you can do that with multiple commands either you can create um three new input variables and then uh pass one or and zero here but a simple step a simple command is the class dot uh ind command which you can use in one go to create all the three variables let me show that to you how it happens so if you see here with the single command i'm able to segregate the initial species that i had if you see the species i had here was um in the form of setosa or jennica and versi color and then i've split that data in form of zero and one with one single step okay so um what was done earlier was we used to create we used to do that uh for each of the variables one by one but using this command you can now very simply split the data in the required format okay and as a shortcut what i've also done is i've done the scaling in the same command here so what essentially did was from the initial iris on data set that i had i scaled the first four variables so the first four variables of sepal length width better length and width was scaled and then this width of the output variable which was one was split in form of zero one one to be able to fit the neural network so then i am um simply dividing my data set into training and testing data set uh so roughly a 70 30 but not exactly uh so first 100 i'm passing to my first 100 rows and passing to my training data set and next 50 to my testing data set and finally i am creating a neural network so we'll see here what is the parameters that we pass to the neural network um so the first thing is the formula that we need to give here so the formula here tells me that these three which now become my um output variables um c setosa versicolor and virginia which now become my output variables are a function of four independent or the input variables so this is the formula that i've specified data is the training data set hidden specifies the number of hidden layers that you want um so a thumb rule to choose the hidden layers is uh under root of the number of uh input variables that you have so if you have i had four input variables here so i chose two okay and um since you saw you can have multiple hidden layers here um so say if you have um maybe like here i had check input variables and hidden layers so since i had four input variables i did a square root of um this to choose that i'll have two hidden hidden equals to two here give me a second okay and um sorry uh so two here means i'm rounding it off to get a two so that will mean that i have one hidden layer with two neurons say you have maybe 20 input variables of in place of 4 so you do a square root of 20. which will be 4.47 now you round it off to 5 then you do a square root of 5 and that will be 2.23 so you can round it off to probably two or three so what i'm trying to tell you is that when there are four input variables i'll have just one hidden layer which will have two nodes when you have say 20 input variables you will have two hidden layers the first of which will have five hidden neurons the second of which will have um two neurons okay making sense um like if i have hundred input variables i'll have a first hidden layer which will have ten neurons then i'll have second hidden layer which will have three neurons then uh yeah so that will be here um say i have 500 input data variables so i have first hidden layer which will have um [Music] say 22 hidden sorry i'll have first hidden layer which will have 22 neurons then i'll have a second hidden layer which will have 4.7 or you can round it off to five neurons then again i'll have third hidden layer which will have two neurons so this is how you decide the number of hidden layers that you have with the data and number of neurons that will be present in each hidden layer let me let me kind of erase that and do this again okay so input variables and then the hidden layers so when i had four input variables i do a square root of 4 that will be 2 so i have one input layer which has so this will be the first hidden layer with two neurons okay so you kind of keep doing the square root multiple times so i am doing a square root here the first time second time when i do a square root of 2 i come to approximately 1 so then i stop ok meaning to say when i have 4 input variables i get a 2 so that will be first hidden layer with 2 neurons say if i have 10 input variables i'll do a square root which is approximately equals to 3 so i'll have a first hidden layer with three neurons and then i'll do a square root of 3 which is 1.7 and you can round it off to two so i'll have a second hidden layer with two neurons okay now now if you do a square root you keep on doing the square root iteratively ah so now if you do a square root you will get something like one point one point four or something so there you stop so with ten input variables you will have two hidden layers so with the hidden layer parameter that you are specifying here i did a 2 here because i had just one hit in there so you need to give hidden equals to 2 here with the 10 input variables you need to give hidden equals to c of 3 comma two three means first with three neurons second with two neurons okay like a bigger example if we take i have 500 input variables so i do a square root of 500 that will be say 22.36 and i'm rounding it off to 22 so i'll have first hidden layer with 22 neurons then i am doing a square root of 22 that gives me 4.69 and if i round it off to 5 so i'll have a second hidden layer with 5 neurons then i do a square root of 5 square root of 5 which gives me 2.23 so i round it off to 2 for example so then i'll get a third hidden layer with um two neurons and then when i'm doing this with r i'll give hidden equals to 22 comma five comma two so this means three layers first having 22 neurons second having five neurons third having two neurons okay um so next is uh the threshold so threshold is nothing but a derivative of the error that will be calculated at each step so we can also see that from the help in r see the threshold so threshold is numerical value specifying the threshold for the partial derivative of the error function as a stopping criteria so at each step some error will be calculated as a difference of the actual value and the desired value and then you calculate a derivative of it so say the error in the first step is 2 for example um so you calculate a derivative of it then back propagation will occur then a reiteration will occur and this process keeps repeating so this threshold equals to 0.01 says that keep repeating until you receive a derivative of the error function which is 0.01 okay so in the first the first iteration may be uh derivative of derivative of the error function will be say for example 1 then you back propagate reiteration occurs then the derivative of the error function may be comes down to 0.9 then you repeat it back propagation occurs then uh maybe the derivative of the error function comes down to be 0.6 then 0.2 0.1 and till it comes down to 0.1 you need to keep repeating the algorithm okay um the next step is so this is one stopping criteria threshold is one stopping criteria step max tells you what is the number of iterations you want it is possible that um even with 10 000 iterations or 10 000 cycles the threshold doesn't reach 0.01 doesn't mean that you will keep running this algorithm in finite times step max is another stopping criteria which tells us what is the number of iterations that we can allow our neural network to have so step max equals to 22 2000 tells us that we need to um we can run this algorithm a maximum of 2000 times okay so linear output equals to false is nothing but um telling whether or not you want the activation function to work so uh to revisit activation function was nothing but allowing a neural network to fire a trigger firing a trigger means allowing the neural network to backfire so sorry back propagate so when when you specify linear output equals to false that means that you're allowing the activation function okay so with with the lectures there were different activation functions that were also given but um there is not much difference practically speaking with uh the different activation functions and even if you don't specify it explicitly with your commands you are you're not missing out anything just specify that linear output is equals to false this is enough to tell r that you want the activation function to work okay um then life sign equals to full and life sign dot step equals to 10 are again optional steps um these are only to tell are that say i am telling here that i want 2000 iterations uh so these two steps are only to tell are that i want to see the output after every 10th iteration if you if you step if you skip this step even then the algorithm will run but it will not show you what is happening with each step so when i'm telling r that i want the life sign step to be 10 so after each 10 iterations like after 10 iterations after 20 after 30 after 40 so until 2000 r will show me what is happening with the neural network if you skip this it will directly give you an output okay then uh error dot fct is nothing but telling you how do you calculate the error function okay um so error is error function is nothing but difference of the um actual value and the desired value but how do you calculate it sse sum of squares error and this is the best you can do with neural networks because with neural networks you want your error to be positive so that you're able to do a derivative of it um so the best thing to use is an sse which is sum of squares error so supplying all these parameters when i run this command sorry okay so i ran this command see there are two hidden neurons that was what i had specified uh with each iteration so i told that i can do a maximum of 2000 steps but my algorithms stopped with 125 steps uh so when there were 10 steps when 10 iterations had occurred my error was 2.86 after 20 it reduced to 2.795 after 30 reduced to 1.487 and finally after 125 iterations my error reduced to 0.035 um so the stopping criteria which worked here was this threshold so a derivative of 0.0351 would have been lesser than 0.01 and hence it stopped here okay um so once the error of 0.03541 was achieved this was achieved with 125 steps so i did not have to do 2000 steps i got the output with each 10 iterations you see you can you could have done a life sign step of one to see what happened with iteration but then becomes too detailed so with each penetration you see the error was reducing and finally it came down to 0.0351 and this occurred in 17 seconds only okay now if i kind of try to plot the neural network this is what i get so see there are four input variables apple length width petal length and width uh i have just one um hidden layer that has two neurons uh these are the weights the ones that you see 1.83 on the top second yeah so this 1.3 that you see 1.83 that you see here is the weight these are all the weights that you're seeing um then the 0 the 6.6 that you see here the minus 12 that you see here sorry these are the biases so 1.83 0.76 the one that you see on the black lines are the weights that are being applied to the input variables the ones that you see in blue 6.6 and 1.2 these are the biases which are being applied so there's just one hidden layer with two neurons the biases are being applied at both the layers the base uh the hidden the weights are being applied to both the legs and finally an output is being thrown out okay uh one thing to note here is um that if i run the same command again the weights will be different okay so if you run the same command at the same time will be even then this will be different uh so so far what is happening is um iris underscore n is a neural network model which is being created it's just showing you that these were the four input variables ah these were the three output variables and something is happening and this neural network is getting trained okay um one additional step which can be done here is not with the small data set that i'm using but if you have larger data sets uh one another parameter that you can pass with the neural network command is um this step uh this parameter called start weights okay so what this is doing is that even though uh with back propagation the errors are being passed um and then the weights are being modified but additionally you can also pass the weights of the first step to the second step meaning to say uh whatever were the weights that were being applied in my first step i can also pass them as a parameter when i'm back propagating uh to the second step so that the same weights are not being used so this becomes an additional learning for the neural network uh so this is not applicable in the small data set that i'm using it it won't do any benefit but for the larger data sets that are present this steps become helpful then i'll show you how you do that um so you create another variable old weight or you can name it anything and in the iris underscore n was the name of the model that i've created that has some weights so i'm passing those weights to a new variable and then i can create another neural network which has the same parameters additionally i'm adding a parameter called as start weights in which i am passing the old weights that is the weight that i received in the first iteration so this is an additional step you can do to improve the performance of your uh neural network okay so so far with iris underscore nf just created a model now we'll see how does our model work okay so what i'll do with uh what i'll do here is uh with the data set here i had used the training data set to create a model next i'll be using the test data set to see how my model performs so um because i had scaled my training data set so i'll also be scaling my testing data set and try to predict the outputs of the testing data set so i am using a compute function to calculate predict the values of the output sorry the testing data set iris underscore n is the model that i had created neural network model that i had created and the data that i am passing is the test data set first four columns which are nothing but the sample length width better than that width okay so finally when i predict when i try to predict the values of my test data set this is what happens um now since this is a scaled data set so the numbers are achieved in this way but you can again uh use these numbers as well to also do a prediction let's see how do we do that um so you see we had three species which were possible um and in the results that i got i get uh these are 50 entries which was in the test data set and i get some scores for each of the species like for this particular flower i get three values for each of the species um so the score the probability that it is acetosa is 0.001 the probability that it is versus color is 0.01 whereas the probability that it is virginica is 99 0.99 which is 99 so this flower which is in the row number 131 will be a virginica similarly the flower which is in this row 10th will be a set dosa the flower which is in row 95 will be a horsy color the flower which is in 142 will be a can of virginica this one will be a percy color so what i'm essentially doing here is i'm trying to use the neural network model to predict what will be the class of my flowers in my testing data set okay so past the testing data set i pass the neural network model which contains the learnings that it has from the training data set and i'm use this to do doing this to predict so this this is the main step do not get confused with the other steps because i'm just doing kind of formatting to give you a nice looking output there but with this step i'm trying to predict the values in the testing data set using the learnings i have from the neural network one okay so with the results that i got here i'll kind of try to compare it so these are the predictions that my um neural network model is making these are the these are the predictions that i'm trying to make for my um testing data set using the neural network but i should also be able to compare it uh to the actual values to see how good my neural network is doing um so okay uh what i did with this step was nothing but uh i tried to like i was telling you theoretically here on how this would be a virginica and this would be a set of and this again will be a versi color so i kind of tried to uh put that in words so that we don't have to read through the numbers so these three commands that i'm doing uh here are doing nothing but producing this result from this result so the first one should be a virginica the second should be acetosa the third should be a versic color this is what i'm doing here telling r that whichever is maximum out of the three columns i want that column name to come in my result so this is the result that i get as a result of prediction after my neural network for the 50 entries that i had in my test data set these are the predicted species of the flowers using my neural network model now i'll be using these 50 predicted values against the initial uh in against the actual values to see how good my neural network is [Music] okay so these are the original values which were present and these are the values that i've got and we'll finally compare them in the table so you see we have out of 50 um so this this matrix if you don't know it already we call it as a confusion matrix to see how good how good our models do so we see there is an error of 3 here out of 50 there is an error of 3 so 47 values are being calculated fine and hence the accuracy of this model is 94 to tell you uh quickly again what i did here i used my neural network model to predict what species each of my flower belongs to in the testing data set then i compared it with um the actual species which were present and finally i get this table which tells me that i have an accuracy of 94 okay so weights will change according to the error uh whatever the error is the end aim is to get a threshold of 0.01 passing whatever weights possible so it is entirely possible that when you run this algorithm the second time um if you remember here we had done this with 125 steps okay when you run this the second time it may take lesser steps it may take more steps but towards the end your result will be the same okay because the algorithm will stop when it achieves a threshold of 0.01 uh just to tell you i'm running this algorithm fourth time since yesterday uh once it has run with 300 steps once it runs with some 95 96 steps and once it is running with 125 steps now but your end accuracy will be the same the same um this thing the same confusion matrix you will get no matter how many times you run the same algorithm because it will use the threshold to stop and the weights can be anything okay so uh for today's session i'll just be doing an um exploratory data analysis that will help you just see which variables should be used and which should not be used what's the next session we'll try to um actually solve it to see which uh what is the output that each of the algorithm throws okay i'm importing the data set okay so when i'm viewing this data set it contains approximately 3 000 or like 2940 rows and it has 35 columns um structure is just an additional step to see what the structure is so we have a total of two nine four zero rows and then we'll now go to each of the variables individually so when i start off with the address and variable one thing to note here is that these variables have been arranged in an alphabetical order um so you need to um if for better understanding um you might want to um rearrange these columns so that similar columns are together i haven't done that mu but maybe you can do that um arranging the similar columns next to each other so that it becomes easier for understanding so we see here when we start off with attrition um so there are total of 2940 employees out of which um 474 had a naturation okay so that becomes 474 upon 295 2940 so approximately um 16 percent of people had an attrition and we'll see what impact the addition of these 16 percent of the employees okay so we'll start off with each of the variables one by one and um see whether or not that variable should be included here so i am starting off with the 8 which is the first variable here um so you see the minimum age is 18 the maximum is 60 and the median excuse me the median is 36 which is somewhere in the mid um so the data looks like not being very skewed um we do a box plot of it and we see that it looks like a normal curve the age so there are people from the ages 18 to 60 um and the data is normally distributed now i'm trying to see uh so this is another way i did a box plot and this is another way to d do a clearer histogram we could have done it using the haste function as well but you can also use tg plot to do a histogram which gives you a more precise histogram to see the distribution of the age um and finally i'm again using gg plot to see the impact of age so what i did here was for the continuous variables that were there with the data set and try to see whether what is the impact that those variables have so when i do a gg plot of the age i get an output something like this so what this tells me is uh the pink color shows the employees that have an attrition and that do not have an attrition and the blue color shows the employees that have an attraction so what this tells me is that um people who have an attrition or who uh who had nitrogen are belong belong to the age somewhere from probably 28 to 39 and the ones that who do not have an attrition uh have an eight somewhere from 31 to say 43 um what this tells us is that when you're seeing just the age variable alone there is a difference uh in the ages when the people have an attrition or not so people in the smaller in the younger age groups are more likely to have a nitrogen compared to people in the um larger age groups so this helps us understand that when we see the impact of aids alone with attrition so age is an important variable when we are trying to study addition because there is a significant difference with this graph so the age should be included as a variable as an input variable when we are trying to do the modeling okay next we'll go to the second variable which is business traveler so it is a categorical variable that tells us um whether or not a person travels for business and how often he travels so out of the 2940 employees that we have so we see that most of the employees travel rarely and then we'll try to understand whether or not traveling has an impact on attrition so since this is a categorical variable i just created a simple table to see if there is an impact of business travel and attrition so we see um the ratio that you see between the no and yes here so for non travel the ratio of 276 to 24 and 416 to 138 and 1774 is two three one two this is almost similar there is no much difference uh let me do an actual numbers 276 by 24 one six divided by one thirty eight one seven seven four divided by three one two so there's not much difference um in the people who travel um to see whether or not they have an attrition so you can choose to ignore this variable because it doesn't look like um it is having an impact on whether somebody or not has an attrition from the organization okay it we saw has an impact but the business travel does not look to have a significant impact with um the business travel does not look to have a significant impact the iteration so you can choose to ignore it okay similarly we go to next um the daily rate so daily rate is again a continuous variable uh which shows the rate at which an employee is being paid on a daily basis uh so when we see a summary of it it is again a continuous variable the minimum being one zero two to a maximum being of fifteen hundred uh when i do a box plot it looks like a normally distributed data and you can also see that from an histogram as well but a good thing to see here is if i do the impact of daily rate on uh addressing probably the daily rate does not have a very significant impact let me uh go back to the histogram again to show you so see almost all of the the daily raters uh uniformly distributing meaning to say that across all the uh 2 900 employees that we have the daily rate was evenly distributed and when i did a comparison of people who it's right or not from the organization based on the daily rate there is no significant difference there is a very small difference here if i zoom it out um so this is 400 to 600 so this is somewhere around 450 or so so this is not a very significant difference compared to the range of daily rate that i have there is not significant difference in the daily rates of people who have an attrition or not so even daily rate you can choose to ignore when you're doing the modeling okay with aids we had a significant difference with daily rate we do not have a significant difference and also we saw from the histogram that the daily rate is not very varied it is almost constant the frequency of the daily rate within the employees is almost constant so it doesn't look like having a very significant impact so you can choose to ignore the daily date variable as well next we um go to the department which is a categorical variable so we'll just to see uh we'll just see a summary of it okay so see this is significant here uh with the human resources 1656 divided by 266 and so this is evident from the data as well but i'm trying to show it more clearly okay so with the department as well um the ratio between know and yes is almost similar okay the people who are trying versus the people who do not at right is again similar so department again you can choose to ignore because there is no significant difference with respect to there is um nothing like people of a particular department have more attrition compared to people of other departments so you can choose to ignore this variable as well okay so then or maybe okay 284 is also okay so then you see that there is a different significant difference with the other departments so if you see that for a particular department um the ratio between know and yes is different for a particular department compared to other departments so then you say that people of uh those that particular department have a more attrition rate compared to people of the other departments so that will mean that that particular department has a significant importance when you are sorry yeah when you are creating a model that department variable has a significant importance and hence it should be included but with the current data set that we have there is no significant difference with each of the in uh departments so you're okay to ignore it okay and um this is what happens when um uh in in industries when we are actually working so this this is again a data set with 35 variables but this is again not huge we had actually worked with one of the data set that had some around 170 variables or something like that so we need to uh you know most of the time most of the time gets spent in doing the exploratory data analysis only because um the models that you're creating like the neural network i created here is a two-liner command that you can execute within seconds if the data is huge it that command will take a minute or maybe one and a half minute to run but then it is again a two-liner command that can be executed quickly most of the time that goes in the organizations is doing the exploratory data analysis and that needs to be done for each of the variables here so here with the data set i haven't uh done so i had i had seen an overview of what the summary was so this this doesn't look like a data which contains outliers so in actual data uh in the actual data that you'll see there'll be outliers as well so how will you understand whether or not the data has outliers is by studying the individual variables okay you study the individual variables whichever variable you see looks like having an outlier that needs to be treated if there is a missing value with within within a data set that needs to be treated and finally using the summaries of each of the variables you need to decide whether or not that variable needs to be included in the data set so i've done that for each of the variables um so since this is clear now i'll quickly run through all the variables so when you do this exercise for the mini project that you're doing repeat uh this these steps in a similar manner for each of the variables then you also have this distance from home ah so that tells the distance of the employees home from the office which is a number from 1 to 29 and if you zoom this out this is a very uh you know intuitive as well that people who have so distance from home to your office is significant uh factor and that is also evident from the data so you see people who have more distances from home like uh the yes is the yes is in the blue color so people who have more distance from home uh are more likely to have an attrition compared to people who have lesser distances from their homes so distance from home is a variable which should be used as a factor for impacting hatred next is the educational level and the education field which are categorical variables so we will just see a table of them here so with this table i am not doing the actual uh ratios here but do the ratios of these variables to see if a particular education level if a particular education level you notice has more attrition compared to other education levels then you need to include education in the as an independent variable but if all the education levels behave in the same way then you can choose to ignore it similar is the case with education field as well uh if there is a significant uh difference between the attrition of yes and no for any particular education field then it should be used otherwise it should be it not um now one thing to see here when you do different models like for example if you're doing a random forest or a neural network so even if you do not do all these steps the model will run um so especially when you're doing a random forest and you do not do these steps the model will run and um the random forest and for that matter cart also both of these algorithms uh choose the best variables by themselves okay because they use the like card uses the ginny gain concept so it it sees all the variables itself as well and um chooses which variable is good which variable is not good for splitting so with those algorithms even if you don't do these steps is okay but doing them is a better thing but if you don't do them is okay but with neural network if you simply pass all of um the variables um at once if you pass all of the variables the neural network model uh the model will run but the accuracy will not be good so this step becomes essential when you're doing a neural network to achieve accuracy okay because with neural network there is no particular uh you know algorithm to decide which variable is important which variable not important whatever the variables that you are passing through the neural network it will learn from all of the variables the back propagation the error rates the gradient everything will be calculated on each of the input variables and it has no method to decide which variable is good and which variable is not good so then the model accuracy is not very nicely so very nice especially when you're doing a neural network do this as a mandatory step okay and and until unless you do this you won't be able to uh also see so there could be some variables which are correlated there could be some variables which are intuitive there could be some variables which can be calculated from each other so you won't be able to understand that until you do an explanatory analysis so see like for example i show you one of the variables here for example this employee count so employee count is doing nothing but it is uh like do uh telling whether or not this employee is distinct here just to check that there are no duplicate entries so you know that this variable doesn't make sense when you're passing it to a model because uh just by doing a quick check once on this particular variable you can decide whether or not there are unique employees or not random forest doesn't know that so random forest will try to consider in one or more of the trees that it is creating random forest will try to consider it as a variable wherein you yourself know that it is not important so i just pass it at all okay similar is for one more variable which is i think over 18 here so you have an age variable which is enough to calculate the impact of 8 and the over 18 anyway contains all y values so you know that it is not significant the random forest doesn't know that it is not significant or it doesn't make sense so i just pass it at all okay because in case it it sees that it is significant but you know that it is not significant so it is advisable that you do not pass such variables at all um there's another thing i'm showing you for an example with which you should avoid when you are um doing the modeling in addition to the data set say you have a salary of like you had monthly eight years say you have salary of january salary of february salary of march and so on till salary of december for the employees and then you also have a total salary in your data set um assume that um the salaries for each of these months are different they are not the same say this is thousand this is 1200 this is 1150 this is 1300 and so on um so these are not same and then you have a total salary which is sum of all the salaries and you have all the variables here now these variables are may not necessarily be correlated it is possible that for uh one employee probably uh the salary of february was more for the second employee maybe the salary of february was lesser than january the salary of mart shooted for for any reason and then um the total salary will be accordingly so if you see salary of january salary of mars february san data set it will not be correlated but the important thing to note here is that total salary contains the essence of all these variables so when you have a data set something like this ignore all of these variables include just the total salary okay so this is an additional step apart from correlation the salary of dan and feb may or may not be correlated here the salary of done and total salary may or may not be correlated here but then the total salary captures the essence of all of these variables so if you have a data set something like this ignore all of these variables include just the total salary so i i i did this for all the variables here like we have the employee count like i showed so employee count is just to see whether or not there are duplicates in the data and uh there are two nine four zero employees and uh the count is always one here so unless you do an analysis of this step alone uh you won't be able to determine this so this variable has no importance at all and you should ignore this okay so i kind of did that for all the variables for environment satisfaction this again is a categorical variable and you can decide whether or not to include it same was for gender it was a categorical variable same was for the early rate here so you see um the hourly rate it is very much similar whether or not there is an address on the early rate is very much similar there's no difference so don't include this variable when you're doing the modeling one more thing i wanted to show you was what we had seen earlier as well so i'm doing this for all the variables and you can decide whether or not to include them okay for the monthly income so you see um the summary of monthly income it ranges from one zero zero nine to say twenty thousand approximately uh when i do a histogram of it okay so we have seen this when i showed you the uh air pollution regression example uh we had seen this but see the histogram of monthly income is highly skewed okay so this again should be considered uh before you do the modeling that there should not be any skewness in the data because if it's not normally distributed the data will be biased to learn more from the monthly income like here probably the monthly more people have monthly income in the ranges of probably 2500 to 7500 contains most of the people so the model will learn more from the employees that have salary ranges between 2500 to 7500 and lesser from the employees that have salary changes in the um other that have salaries in the other ranges so the data should also not be skewed when you are doing the modeling so when i do a log of it the skewness gets removed and hence when you are passing this variable you should not pass the monthly income you should pass log of monthly income as the independent variable before doing the modeling okay so couple of things to check is one is the missing values one is the outliers then see um whether or not they are correlated uh the variables are correlated then also see uh what i told you in the example if one variable can be computed from the other variables then use just one of them then also see whether or not the data is has outliers or not if it has outlier sorry whether or not the data is normally distributed or not if it is not normally distributed um do a transformation and then pass uh the transformation as a variable rather than the individual variable okay so monthly ratio monthly income you see has a significant impact on the attrition the people who have a smaller monthly income and the people who have a higher monthly okay i've done that for all the variables this is again uh over 18 variable which contains all wise that all of the people are number 18 so that that is a totally uh non-significant variable uh we see one overtime variable again see this tells you that over time out of 578 and 254 and then this is approximately 1900 and 220 so people who do over time have a more likelihood of having an attrition rather than people who do not do over time so over time is a significant variable um the same thing what we saw for monthly income also applies to percent salary hike um so see this again is a heavily skewed data set so you need to uh do a transformation of it uh before you actually pass it so i use this uh box cox test which we had seen earlier when we did um regression so i used the random um sorry um the lambda value here and when i plotted ah transformed value of the person's salary high the skewness looks like having been removed so then i instead of passing the person's salary hike i should be passing a one upon percent salary hike as an independent variable okay so you need to do that for all one more variable here was was for standard r which is 80 for every one so you can ignore it do that for all the variables okay one easy way to check for the missing values is a command that if put here uh either you can do it for all the variables one by one but this is one simple command to check uh the missing values all at once so when i run this command i'm just checking whether or not there are any n is in any of the values and i'm creating a matrix based on that so it shows you for each of the um variables whether or not there are any missing values okay so it shows zero zero zeros in all of the values means that there are no missing values in any of the variables thank you okay thank you 