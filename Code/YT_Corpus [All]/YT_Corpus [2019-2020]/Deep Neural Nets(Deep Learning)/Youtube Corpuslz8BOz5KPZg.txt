 [Music] hello my name is david von dolan and i'm here today to talk about lair wise Learning for quantum neural networks I'm an area lead and technical manager for Volkswagen group of America working on a variety of topics including quantum computing and machine learning out of our advanced technologies group based here in San Francisco so to start I'd like to give you a little bit of an intuition about quantum neural networks a quantum neural network is a type of circuit where we have a register of qubits with which we load some data whether it's classical or quantum for our project we looked at classical data specifically the emne Stata set and then you apply a series of unitary gates now these can be random rotation gates namely rotation Y X or Z and a series of control Z gates finally we apply a readout on one qubit and with this we calculate a gradient for our parameters for our unit ares now no problem for quantum neural networks is what's called the Barron plateau problem and essentially what it identifies is that as the depth of a quantum neural network grows the variance of the gradients in randomly initialized quantum neural networks decay exponentially as a function of the number of qubits and so given this problem we developed this technique layer wise learning and so when we look at our technique we we address the vanishing gradient problem but we also looked at using this new library tensor flow quantum to train and experimentally verify our algorithm the great thing about tensorflow quantum is that it handles all of our train overhead and we can focus on research rather than coding and getting deep into the internals so looking at this vanishing gradient problem we may utilize larger gradients in shallow shallow quantum neural networks we can avoid configurations and random initializations which may lead to a barren Plateau problem when we apply layer wise learning and we can successively grow our quantum neural network layer by layer by training freezing applying another layer freezing and then also training and freezing batches of layers so when we look at this layer wise learning we can think about this phase of sweeping over the network where we look at the first layer we train parameters we freeze we train our second parameter and we freeze and so on and so on in our second phase we sweep through and we freeze batches of layers and when we do this we find a speed-up in regards to training times and we also see a performance gain in our test error so when we looked at doing binary classification for the digits 6 & 9 from memnos we noticed a an advantage when using 10 epochs per layer in in doing layer wise learning over what we call complete depth learning where we trained all the layers at once so to talk a little bit about tensorflow quantum we can generate our quantum neural network layers really easily by using simple and Cirque to construct our circuit and then we can inject that using a tfq parameterize quantum circuit layer into tensorflow chaos and we can use the Terrance of flow chaos loss functions and optimize to train the gradients for our parameters for our quantum neural network and if you're interested in more detail we have an upcoming white paper this is has been a really great collaboration between Volkswagen and Google and if you have any questions please feel free to reach out to us thank you very much [Music] 