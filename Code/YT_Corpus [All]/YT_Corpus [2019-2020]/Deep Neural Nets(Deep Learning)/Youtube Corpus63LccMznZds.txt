 [Music] so in this lecture we will discuss about the update rules for the parameters in convolution neural networks so this lecture we will discuss about the update rules for the parameters in convolution neural network which I call it as CNN also we have seen in the course that Professor Cheyenne has discussed the ideas of multi-layer perceptron in detail starting from the ideas till the derivation of update rules for the parameters in multi-layer perceptron and we see that that multi-layer perceptron which I call it as MLP forms the backbone Network for convolution neural network however there are important differences between multi-layer perceptron and CNM so what we will do is we will briefly visit the multi-layer perceptron and then we will get into the details of the derivation of update rule for the parameters of CNN okay so multi-layer perceptron so in multi-layer perceptron we have input layer we have hidden layer and output layer so let's assume that we have input from a d-dimensional space and we have capital M number of hidden neurons and we have K output output neurons and this network multi-layer perceptron is a fully connected network in the sense every neuron is connected with every other neuron so so this algorithm or multi-layer perceptron network has two steps first step is the forward pass so in this step what we do is for every input we compute the output using the existing parameters and those parameters are nothing but the weights connecting I and the jth neuron in the layer l that is w IJ of n so using these parameters we compute the output in the forward pass the actual learning happens in the second step which is the backward pass so now we will look at the backward pass so what the actual learning happens in this step and this requires the computation of local gradients at each layer so this is the I the this is the idea behind multi-layer perceptrons so since we have said this forms the backbone of CN n we look at the similarities between the MLP and CNM so let us look at the architecture of CN n first so now if you look at the architecture of CNN we have at the input side let's say an image now size 3 cross 3 in general it could be an image of size n by n so here as an example I have considered 3 by 3 image let the pixel value of this image be X 1 1 X 1 2 X 1 3 and so on now this is the input we have a filter of size say 2x2 and in general it can be of size K by K which is less than n by n and let the parameter parameters of this filter be W 1 1 W 1 2 W 2 1 W 2 2 now what happens in the in the convolution neural network is that this filter will be first rotated by 180 degree so by that what I mean is the following so you just consider this filter and you exchange these two elements and similarly these two elements so we get W 2 2 W 1 1 W 2 1 W 1 2 now this this is rotation by 180 degree and we we slide this mask or filter over the image starting from top left corner to bottom right corner so we slide this mask or kernel or we call that as filter over the input image starting from top left corner to the bottom right that is we have the input image like this x11 I am repeating the same here I will place the mask at the top right left corner and the parameter values are the this is the rotated mask so I have W 2 2 W 2 1 W 1 2 W 1 1 so the operation that I perform is as follows I define a term called H 1 1 which is given by X 1 1 W 2 2 plus X 1 2 W 2 1 X 2 1 W 1 2 and X 2 2 W 1 1 similarly the filter will be moved to the next location now I define another term called H 1 2 which is given by X 1 2 W 2 2 X 1 3 W 2 1 X 2 2 W 1 2 X 2 3 W 1 1 and similarly when the filter is placed here I get h21 as x2 1 W 2 2 X 2 2 W 2 1 X 3 1 W 1 2 X 3 2 W 1 1 and finally I get H 2 2 as X 2 2 W 2 2 X 2 3 w 2 1 X 3 2 W 1 2 X 3 3 W 1 1 so because of these operation I get these four different values I'm calling the mass H 1 to H 1 1 H 1 to H 2 1 H 2 2 now the next step in the convolution neural network architecture would be to apply activation functions feel on these values typically the activation function is reloj function and then one can use polling now this form one block of one block in the convolution neural network and this block will be repeated many times depending on the application or depending on the computational resources available therefore what we are doing here is we will consider only one block and we'll derive the update rules for the filter or the kernel or the mask now let us look at the difference between MLP and CNN so first we will look at MLP so as I discussed before we have set of inputs set of hidden neurons and output neurons now this network is a fully connected Network however when we look at the convolution neural network architecture we will see that there are important differences and that is in CNN the network is partially connected and there are another important difference is that the concept of weight sharing now in CNN we will see the differences when compared to the MLP architecture now we will understand these differences with an example so for that let us go back to the set of equations that I wrote here we have four different terms H 1 1 up to H 2 2 using these equations we will understand the differences so for that so we will see the CNN here so I have input X 1 1 X 1 2 X 1 3 X 2 1 X 2 2 X 2 3 X 3 1 3 2 X 3 3 no I defined for different parameters which are H 1 1 up to H 2 2 so I have H 1 1 H 1 to H 2 1 H 2 2 now let us draw the connections by looking at this set of equations so in order to get H 1 1 we need X 1 1 paid by the parameter W 2 - ok similarly for in order to compute H 1 1 we need X 1 to wait by the parameter W 2 1 and need x21 paid by the parameter w12 and finally we need W x2 to weight by W 1 1 so so these are the connections that we can write from the input layer to the next layer weighed by different parameters now let us look at how one can obtain H 1 2 so if you go back and see the equation for H 1 2 we need X 1 2 X 1 3 X 2 2 X - 3 so we need X 1 2 that is here weighed by the weight W 2 2 therefore we have connection from X 1 to 2 H 1 2 wait by the parameter W 2 2 so if you observe the connection between X 1 1 and H 1 1 and the connection from X 1 to 2 H 1 2 we see that the same parameter is being used so this is the idea of rate sharing so similarly let us proceed with the other connections so we need x13 wait by W 2 1 we need X 1 3 may by W 2 1 and we need X 2 2 wait by W 1 2 and finally we need X 2 3 made by W 1 1 so similarly I can write other connections so X 2 1 X 2 1 wait by W 2 2 X 2 2 up to 2 1 X 3 1 raid by w12 and x32 with w11 and finally to get h22 we need x2 to with w - 2 X - 3 w 2 1 X 3 2 and we have one more journey it is x-33 so this completes the connection between input time the next layer CNN so now one can apply activation functions for these values another important observation that one can make here is that it is not a fully connected Network so this is not a fully connected Network and so now so during forward pass so we have an input we compute these hich H I J's and we apply activation functions and then we perform pooling and this this is continue so now let us understand in the backward pass how can we update these rate parameters so for that we need if you go back and see the multi-layer perceptron we need to compute the local gradients at each layer so now let us assume that we have local gradients in the in this layer now let Delta 1 1 be the local gradient at this node Delta 1 to Delta 2 1 and Delta 2 2 now we need to compute the local gradients at these per at these nodes so let's see how it is computed so at this point at the node x11 the local gradient would be computed as follows so we have to take local gradient from the previous layer weighed it by weighed by the parameter connecting this node X 1 1 node and H 1 1 node so therefore we write Delta 1 1 made by W 2 2 similarly if I want to calculate the local gradient for the node X 1 2 we have to look at all those connections from the previous layer to the node X 1 2 so we have connection from h1 1 and H 1 2 so therefore we write W 2 1 Delta 1 1 plus w2 2 Delta 1 2 similarly for the node X 1 3 we can write Delta 1 2 W 2 1 so like this one can compute the local gradients at this layer so this can also be computed as follows okay we have we have the filter which is rotated by 180 degree like this and we have set of local gradients Delta 1 1 Delta 1 to Delta 2 1 and Delta 2 2 now what we do is we perform convolution between this and this so what happens is the following during convolution so this mask will be rotated by 180 degree and that will give us W 1 1 W 2 2 3 1 2 W 2 1 and then this mask will be will be moved over this set of local gradients starting from top left to the bottom right we will see that in a minute so initially the filter will be placed like this so this will give us Delta 1 1 W 2 2 next the filter will be moved to the right so we have W 2 1 W 2 2 W 1 1 W 1 2 now this gives us this gives us W 2 1 delta 1 1 plus W 2 2 Delta 1 2 then so similarly one can move the filter here and can get W 1 2 sorry Delta 1 2 W 2 1 so now finally one can get all the values at each node which is given by under one one so in this way one can get all the local gradients and the input layer so here we we saw that we are looking at the rotation of this the the filter so this was intuitive intuitive way of getting all the local gradients at the input layer involving the rotation operator on this kernel or the mask now we will see through mathematics how this is carried out so in the standard MLP the local the local gradient of neuron J in the L layer is given by Delta J of L as dou a by dou V J of L where E is the error and VJ of L is summation over all possible k w j k l p VJ L minus 1 VK at the L minus 1 layer plus the bias at the end layer we see that in ml p we have used the dot product between the weight the weights and the activation function however in CNN this is replaced by convolution operator now with this we define the local gradient as Delta X comma Y at layer L where X comma Y look indicates the location of the mask on the image as ro e by 2 V X comma Y in the end layer where the X comma 1 L is given by summation over all possible a comma B comma B L U of l minus 1 X minus a y minus B plus B X comma y at the end layer so for the simplification yields Delta X comma Y L given by two summations over all possible X prime over Y and y prime doey by 2 e X prime Y prime at the L plus 1 layer so what this equation really means is that we have to look at all the local gradients from the previous layer connected to the node X comma Y okay and that's the reason we are using the chain rule and summing over all possible x-prime y-prime from the previous layer so let us simplify this further so this is nothing but Delta X prime Y Prime at the L plus 1 layer therefore so let's substitute for V of X prime Y prime at L plus 1 layer now let us define X my X prime minus a as x and y prime minus B as Y so if you look at this term the derivative is non zero the derivative of free with respect to V X comma Y at the El player is nonzero only when X prime minus a is equal to x and y prime minus B equal to 1 and all the other terms will behave as a constant and hence the derivative will be 0 with respect to V and also you can see here this the bias term is is independent of X comma Y and hence this term will go to 0 so with these conditions we can write X prime Y prime Delta X prime Y prime at L plus 1 layer as W a comma B at L plus 1 layer and V prime X comma Y at the nth layer also we know that X prime minus a is equal to X implies a is equal to X time minus X and similarly Y prime minus t is equal to Y and we get B as my prime minus y now if you substitute for a and B here we get this is nothing but the local gradient at the edge layer for the node comma for the location X comma Y and this can be compactly written as the convolution between the local gradient at L plus 1 layer with W - x - y L plus 1 layer where W - x - y at L plus one layer is obtained by applying that 180-degree rotation operator on W X comma Y at L plus 1 layer so now what we have seen is the computation of local gradient at the else layer but what we really want is the gradient with respect to the parameters so we will see that now so we want doey by dou a comma B at L there so this is given by summation over all possible X comma Y which is basically looking at all the nodes connected to the to the node a comma B from the previous layer so this term is again Delta X comma Y a tilt layer and let us substitute for V of X comma Y at L layer as before again if you look at this term the derivative term is nonzero only when a prime is equal to a and B prime is equal to B that is for this term however if you look at the bias term which is independent of a comma B and hence this term goes to zero therefore we can write so this happens only when a prime is equal to a and B prime is equal to B this can be compactly written as convolution between the local gradient at the health layer and the activation function at the L minus one layer where C of V minus a comma minus B at L minus one layer is given by C of rotate 180 degree operator operating on a of a comma B L minus one the L minus one layer so now we have the gradient of error with respect to the weight parameter and one can use this information to update the rate parameters now let us summarize the update rule procedure for the parameters of CNF so the first step is for every input we compute the output of each layer as your X comma Y at L layer is convolution of the weight parameters with the activation functions from the previous layer plus the bias term and the N layer second step is to compute the error e at output third step is the actual backward pass where the actual learning happens so during the backward pass we compute the local gradient Delta X Y at the nth layer has convolution of local gradients from the L plus one layer with 180 degree rotated eight parameters or the mask from the L plus one layer along with the activation derivative of the activation function of the layer and finally we can use the gradient of error with respect to the rate parameters which is given by convolution between the feet off rotate 180 degree operator operating on me of a comma B at L minus one layer so these four steps will provide the procedure required to update the weight parameters in the convolution neural network thank you 