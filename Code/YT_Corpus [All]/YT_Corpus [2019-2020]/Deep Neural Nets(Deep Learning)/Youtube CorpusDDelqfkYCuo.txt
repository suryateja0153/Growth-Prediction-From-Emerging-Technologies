 hello i am kobayashi from sony in this video we will discuss quantization one of the basic techniques used to increase the influence speed of trained neural networks links are provided in the description to other videos that cover deep learning in more detail for those interested as has been discussed in previous videos neural network computation requires a massive number of multiplication and addition operations some neural networks require billions and billions of multiplication and addition operations to perform a single calculation if using trained models to make an inference without ensuring that calculations are performed efficiently the time to complete the calculation could be quite significant because of this doing so will significantly increase costs whether due to operating costs from using cloud services or installing faster processors into edge devices also some neural networks will have connection weights w consisting of millions or tens of millions of neurons storing all of this data requires a massive amount of memory in this video i will introduce the technique of quantization this technique is the most basic and easy way to use trained models to make inferences quickly reducing operation costs reduce calculation loads and reduce memory consumption quantization is a process to represent the continuous value expressed as a real number as an integer multiple which is a quantum of the smallest unit as an example of quantization in computer signal processing and mathematical operations we could take a continuous value expressed as a floating point value and represent this as an integer value using as few bits as possible this means that calculations are performed using integer calculations instead of floating point calculations neural network training calculations are normally performed using 16-bit or 32-bit floating point values with quantization we can perform these calculations using integers which are represented with a smaller number of bits floating bond calculations normally require larger logical circuits in comparison with integer calculations in other words integer operations need to be smaller circuits in order to perform the same number of operations on floating point operations and if the circuit size is the same more operations can be performed in fact many processors that support both integer and floating point calculations can do more multiplication and addition calculations per unit time when performing integer calculations if the connection weight w of neurons represented by a 32-bit floating point number can be replaced with an 8-bit integer for example the amount of memory required to store the weight at runtime is reduced by 75 as such i would like to use this video to discuss quantization as a way to reduce calculation memory and make calculations more efficient when making inferences using trained models a simplest form of quantization would be post quantization of models trained using floating point values in practical terms first we need to determine the number of quantization bits for each layer you will need to select a bit size supported by the arithmetic operation unit in the processor such as 8-bit or 16-bit for example then we input the values of actual data validation data and test data the minimum quantization unit is set so that the data and weight are within the maximum absolute value the value is expressed as an integer multiple of the minimum quantization unit if you want to quantize a plus or minus 1.0 floating point value into 8-bit integer for example you would choose a value of 1 128 of the maximum value as the unit of quantization resulting in the floating point value being represented by an 8-bit signed integer in doing this actual calculations will be performed using the quantized values as mentioned at the beginning this enables all calculations to be performed by the arithmetic logic unit this also results in needing less memory to store weights performing calculations via fpgas or a6 for example enables the reduction of logic circuit scale approximating values with integers like this can affect actual computation results and result in the degradation in performance however we have observed experimentally that using 16-bit quantization for most models does not result in any noticeable degradation in performance we have also observed that even using 8-bit quantization for data handled near the output layer of neural networks does not result in any noticeable degradation in performance as of 2019 many production environments support this kind of post quantization the benefit of this technique can often be better utilized by changing the way libraries are used to run inference using simple post quantization has definite limits on the quantization range more reductions in bitcoins result in more quantization errors or significant differences between the quantized value and original floating point value this can result in significant error in computation results as such you should probably come up with another approach instead of trying to use bitcoins lower than 8. by performing the training process in consideration of quantization error instead of using post quantization we can more significantly reduce the number of bits without losing accuracy as compared to post quantization specifically we will perform the training process that quantizes convolutional affine weight and data for each iteration of four calculations this results in the use of only quantized values of weights activation functions in the data path will include quantization processes so instead of quantizing and generating errors we use these stepwise activation functions of course the use of such an activation function results in the loss of information so extreme quantization will reduce accuracy of the system yet this method eliminates quantization mirrors that would have been a problem with using post quantization or it at least allows us to perform the training process with quantization errors being accounted for one issue to be aware of when using quantization is that back propagation as discussed in the mechanism of neural network training video cannot be used as is stepwise functions like this cannot be differentiated the gradient for functions performing quantization like this will practically always be zero this means that it will no longer be possible to update parameters using the back propagation method discussed previously in the mechanism of neural network training video the solution to this issue is the technique of using a straight through estimator to replace differentiation of the quantization function with differentiation of approximate function if the approximation of the quantization function is an identity mapping that doesn't really do anything this differentiation will be one as such the gradient returned will be provided to the previous layer as that is the point of quantization this enables us to perform the training process using back propagation on neural networks utilizing quantization in other words parameter values will be stored as floating point values during the training process then parameters will be updated while quantization layers are used to dynamically quantize weights and data afterwards the final quantize parameters will be used when running inference so that calculations are performed using integers resulting in high performance and the ability to run inference using quantization using quantization during the training process in this way enables us to obtain nearly the same performance as without using quantization in the case of an image categorization model for example we can achieve this performance while using a low 8-bit quantization if we quantize weights and data paths in 4-bit we have seen very little loss of performance experimentally you can perform training with quantization in neural network console i plan to release a future tutorial video describing the specific process to do so binary neural networks in which weights and data were quantized down to one bit values were actively researched around 2016. in addition to quantization pruning unnecessary neuron operations is another approach to making neural networks more compact and faster other techniques such as reforming the architecture of convolutional neural networks are also being researched i would also like to release future videos on other techniques to make neural networks more compact you 