 miguel eckstein who was my advisor in graduate school so um you know uh so so megan and i go go back a long time as as i'd say and i remember peculiar joke that you know when miguel has influenced most of my work on on faviation i would i was very excited for like my first paper coming out and told miguel you know the faviation revolution one day might be happening and and and i hope it does and mega will tell you well you know i've been working the vavian revolution has been happening for quite quite some time martino 25 30 years before you were born so without further ado uh miguel ekstein will be talking about visual search differences between your brain and deep neural networks take it away zero thanks everybody who uh having me inviting me participate in this workshop i've uh i've enjoyed the talks this morning and this afternoon they have been great um so uh some of the work i'll be talking about fits in quite well with with uh some of the previous talks um i will be uh comparing human uh performance and uh some deep neural networks um it will be for this talk it will be quite simple it would be at the behavioral level it would be at the output level of the dns rather than looking at the uh some of the layers and the representations i sort of put together a talk that has three examples about some three exact three examples of differences in which uh but in which uh by which uh humans and actually dnns uh search so uh so so there are three examples from visual search the first two examples um sort of are intellectually more interesting they illustrate different solutions um to these problems possibly reflecting sort of human constraints or different goals uh that humans might have when they're doing these go uh these tasks relative to the more narrow goals that the dnns have and the third example is sort of a little bit more obvious um but it has sort of uh interesting implications sort of in the practical world it's uh it really sort of reveals some of the more obvious limitations of of humans when they actually search uh through uh images uh but it sort of has important implications so i will start with the two examples we'll see how we do i am gonna start with a demo which is a few years old now you might you might have seen it uh it might also uh be you know it might work not so well over over over zoom but we'll try it anyways so what i'm gonna ask you to do is to just uh look for a toothbrush and then i you know you can't raise your hand or whatever you can clap or whatever put your little icon there and then we'll see where we go so i'm going to put an image up and you're going to look for the you're going to localize a toothbrush okay so and by now i think most of you um probably found the toothbrush and if it worked most of you probably found this toothbrush right here and but maybe some of you have missed this one right here that larger than that there and again it depends on visual angle sometimes but uh this is what typically happens uh now what why does it happen why you know arguably you could actually say that this is actually what the the large toothbrush is actually way more salient than this small toothbrush but what happens is when you when i tell you to look for a toothbrush your brain rapidly processes the scene and it's actually sort of um attends to sort of spatial skills when you're looking for the toothbrush they're sort of consistent with the rest of the objects and the rest of the scene and this toothbrush is consistent with that spatial scale for the target while this one is actually um not consistent uh it's a miss scale and it would be very unusual to actually have a toothbrush of that size so it's actually sort of a rational it's an error but it's sort of a rational strategy uh because these toothbrushs are just quite uncommon in the real world um so so this is sort of the main phenomena this idea that uh that that uh the humans actually miss these uh giant objects uh but the idea is that that is actually a useful strategy because if i actually um have other large objects like you know a big broom or something that might look like a like a toothbrush you might be able to discount it just because it's at the wrong scale so your brain rapidly looks at sort of scales that are consistent with the likely size of the object that you're looking for so we did this due to you know obviously um i wanted to show we wanted to uh aside from a demo have some data so and and of course it you know we wanted to do all the giant toothbrush we wanted many objects and it's actually quite hard to actually have a buy some of these uh giant so what we did is we actually had an experiment that actually used these are simpler um sort of computer-generated uh scenes where we actually changed the scale of the object so it was consistent with the scene or inconsistency and typically larger like i just showed you and we had 40 different scenes we had different types of targets and people would actually look at a fixation then you would actually have an image that tells you okay look for for for an object a toothbrush or cork or whatever you're looking for so just one object per trial and then the stimulus will come up for a second so you have time to make eye movements and then you actually could tell us whether the object was present or absent so half the time the object was not present and half the times the object was present and of those times that when it was present sometimes it was at the right scale sometimes it was at the wrong scale and every single participant just saw that scene with that object once okay so you wouldn't have any sort of learning and so on um and this is sort of the okay so this just this is just an example of of you know this the toothbrush at the right scale right here and that's sort of the the you know sort of a miss scale so it's actually larger than what you'd actually expect and um here is what these here are the results so let's look at what i just showed you for um human performance so this is actually humans at the objects at the normal scale so this is hit rate so how many times they actually correctly uh i you know um found that target and said that so it's a target present when it was actually at the normal scale and this is when it's actually mis-scaled as you would imagine yes if you do this many many many times people will pick up that you're tricking them and they can adjust their strategy it is not the case that you know you remain you will miss all giant objects forever eventually you pick up that somebody's tricking you and you can adjust your strategy but this is you know our our our experiments were about 42 trials so they were they were quite short and and we got a very strong effect and here's sort of uh now we see in the for the these are three different sort of uh cnns and this is actually from 2000 these were the state of the art in 2016. this is a few years back now and this is the target object probability for the objects that were sort of normally scaled and the miss scale now what you can see is this dissociation which is that the cnns do not have any of this dependence of the relative size of the object relating you know the the size of the object relative to the rest of the scenes of or the objects around them so this is the first sort of interesting thing that these up state-of-the-art object detectors are are searching for these objects in a in a drastically different way than humans you humans you know you know are really sort of guided by the relative positions and sizes of objects to other other to the rest of the scene while these object detectors are actually you know sort of a little bit more indifferent to that and of course this this might be changing with time but this is actually uh in 2016. i'll show you a slide of uh another version of this uh algorithm yellow which is i think now is up to version three or 3.0 so on um so uh just as as uh the next slide i'll show you just a little snippet of of we you know we've been interested in this and we've been interested in trying to find out where actually this happens in the brain so what we've done is we've put humans in an mri while we actually change the relatives the relative size of an object relative to the rest of the scene and try to look at the bold activity as a function of that relationship and we've looked at maybe seven or eight areas uh that were sort of uh predetermined or pre segmented we're using functional localizers and i'm just using perhaps i'm just showing you one of these areas uh tos which is an area that actually cares quite a bit about the actual physical size of is is sort of responsive physical size of objects whether the objects are large or small and here what we see is that this area is actually also caring about the relative size of an object uh with the rest of the scene so what you see here in blue is when the object is in its normal size relative to the rest of the scene and then here as you actually it's become too small we're either doing two manipulations we're either shrinking basically the renault the size of the object itself so it's too small relative to the rest of the scene or we're actually we are also uh changing the actual field of view and keeping the rental size of the object and those two give you sort of a similar effect and this perhaps you know here the the object and sometimes the object is getting smaller so this is less surprising but this is a more surprising thing when you actually make it larger um when you make the object larger in relative size to the rest of the scene the activity in tos sort of uh decreases and that is not the case for other areas if i show you instead of ffa or v1 this is not happening um so this is sort of uh identifying where actually this happens in the brain and we we do similar things these are now this is a years later so we have yolo version three we do get that when the object gets smaller the we have a significant drop but we don't get this decreasing activity when the object gets you know too large relative to the rest of the scene so again sort of highlighting this dissociation in which uh with which uh humans and sort of these dnns are actually processing uh these scenes okay um so so now you might be wondering what is what is it that's behind this uh dissociation and we're we're sort of also we don't have a full answer but we're wondering about that too and um there are sort of uh there are two set of possible answers there might be more um so one of is is that there's really the current object detectors uh have sort of you know processing bottlenecks as you know many of the object detectors actually have sort of a a bounding box so they're not they're sort of partitioning and and sort of partially uh analyzing uh the scenes so they're not really analyzing the entire scene just mostly having to do with data processing bottlenecks that might uh that in the next few years uh we might get over one exception is yolo the yolo uh uh algorithm actually uses uh a sort of a version of the entire image although it's a little bit low resolution so it's actually surprising that that one doesn't pick up any sort of spatial relationships um so it might be that as we actually as these models and these models are mostly feed forward so it might be that is it it might be that as actually um as we build more things from other domains and i'm thinking of natural language processing where contextual information is actually fundamental there uh maybe we might start seeing these object detectors show some of the uh properties that we actually see instead of human behavior and and instead of the second one is perhaps related to the these things from natural product uh natural language processing this is just you know maybe the object detectors are missing something fundamental that you know and cannot learn these contextual relationships and this might tie into some of the things that we've seen in other talks having to do with feedback and so on that might actually allow to a faster and better learning of these contextual relationships so that is um example one so uh now i'm going to move to a second example where we see a another dissociate interesting dissociation between uh how dnn search and um and humans do okay here is the task this is a person search in the wild um what you see are two uh targets so uh there's ian and amanda uh uh and just one of them is going to appear in the scene or neither of them so it's sort of uh people are sort of answering a three class uh sort of a three class task ian they say either say ian they say amanda or neither and the way this works is they're fixating here we're going to eye track them our subjects and then the stimuli and these stimuli are actually videos so they're going to actually be moving and these videos were filmed over a period over a month and a half or two months on campus at uc santa barbara and ian and amanda were instructed to actually dress differently each time and the people around them and the environments actually also changed um so on okay so um and this is what we're gonna do here what we are interested in is we're interested in understanding which parts of the body the faces their uh heads are contributing to performance for humans and for the cnns which features are important so we're going to do that two ways one way is we're going to look at where people are actually looking when they're doing this task looking at their fixations and the second way is we're gonna manipulate the presence of features in these videos and i'll show you how that uh how how we did that okay so here is the intact so these each of these dots is actually a different subject and then of course i'm just it's the same video seen across different trials because they're different people you know we we're not eye tracking them all simultaneously it just collapsed the data from everybody and what you can see and this is the in what we call the intact condition i'm gonna play it again and what you see is that people spend a whole lot of time looking at um you know they look around but they look a lot a lot of times to the head now this is one condition in a different condition we actually um eliminate it this is what we call the um there we go the headless condition we've eliminated the heads so their stats remains the same you still have to identify whether ian um amanda or neither is are present in the videos but you've seen lots of different videos different environments ian amanda with different clothing but no heads are present and we see is the interesting thing is that people sort of still are looking towards the top part of the body which we were sort of surprised they're still looking for that head then we have a bodyless condition where the body got eliminated and this is fairly obvious they tend to look right there at the at the head and then we had a fourth condition which is this was hard to do because we had to segment the face and keep the head it's called the faceless this is the best we could do but you can see it's not perfect so we've eliminated we try to eliminate the face but keep you see the hair is there so part of the hair is there and the head is there and you can see people are looking at that little empty spot quite a bit so uh what was what we try to do is really combine from all these videos by registering that you know you know scaling the bodies of of these people across videos because they're subtending different angles and they have different sizes and registering all the fixations into one silhouette and this is what we got when we did this sort of a heat map of what were people looking at it's not not you know unsurprisingly when it's an intact they're actually looking at right there at the faces but and when it's face up are we going to body less well when there is no body they're still looking there there's not really much to look at than there and then for faceless even there's no face there they're still looking there and now even for headless it does move somewhere because there is nothing here but it remains you know quite high so you know quite high in the body they don't really look to a center of mass or anything they really started looking just towards like the neck uh where you know where the you know where the body finishes and there that would be the head but the head is not there uh so this sort of gives you this idea that people are actually you know focusing in on faces and heads and there's this is a over practice and and uh and pretty uh common strategy okay when we looked at performance across these four conditions what we saw is the following um this is performance in the intact chance is 33 percent now when you eliminated the face you had this big drop in performance and then when you eliminate the head that was another you know that's sort of similar you know drop it performance as a face now if you actually eliminate the body you can see that human performance did not vary much and there has been literature that people can use body to identify um observers can use uh bodies to identify people that's been documented a lot of those studies are actually people sort of walking by themselves and not less than these more complex cluttered scenes that i actually had sort of in a search scenario so this is sort of a little bit surprising now when we ran the um cnn in this case this is a resnet 18 where we actually trained the sort of the last layers to actually to you know be able to identify amanda and and ian and what we found is that we found a sort of different pattern of results uh in particular we found that you know the the eliminating the face had very little you know the you know impact on performance for the cnn and the body was actually quite important um there you can see that detriment and performance and the head itself maybe the hair or something was also important so here's a second dissociation in which uh humans and uh and cnns are actually doing this sort of search task humans are very very face centered in the way they do this and uh and they lose they lose they use the body to lessen less of an extent while the cnns are actually using the body quite a bit and you know sort of care less about the face um now why why would this be why would we have this dissociation in in in sort of strategies well this is sort of what we were thinking we think you know here cnn is trying to optimize this single task the search for these people or amanda but in general humans in everyday life are really need to simultaneously optimize a number of different tasks they're important identification but also sort of you know the emotional state sort of inferring what's the emotional state of the person the gaze orientation and so on so what we think is that this partly this might reflect this idea that the human strategy is really optimizing a multi you know sort of a multiple task while the cn is sort of optimizing one and the second one is is perhaps you know there are sort of obviously constraints on on on human cognitive resources and in particular in memory so i as much as we could we tried to have a ian and amanda wear different different clothes i do not you know there's like i said it was over 45 days the filming i you know i am sure they repeated the the you know the the the shoes one day or another so the the it could be that the cnns pick up on all these things while uh well while humans it would be sort of an incredible memory load to pick up all these uh uh individual features even though they're you know appearing once in a while and they're predictive of somebody's identity their humans obviously are have memory constraints and faces are incredibly are invariant and across at least short periods of time um and and so they have a high priority stability so that's another reason why uh you know humans might actually focus more on faces to do these tasks um so that is the second example that i had that i brought uh instead of highlighting how a set of humans and cnn search differently okay so this is just i had this summaries like just in case i ran out of time uh but i did not so we're gonna go to the third example i still have a few minutes so this one this one i i'll have you uh you will have to pay a little bit of attention because this is a task you might not be so familiar with um so i'm gonna explain what's going on here what you see to the left this is actually from medical imaging this is actually from breast screening where doctors radiologists are trying to detect cancer what you see on the left is the traditional way that breast screening has uh transferred for you know decades so these are x-ray 2d x-ray mammograms and and doctors are interested in little features like masses and microcalcifications masses are a little bit bigger microclassifications are a little tiny i'll show you i'll try to show you a simulated one in a sec to the right is sort of a technology that's sort of um become prevalent in the last uh maybe five years or so five six years uh maybe now uh this so what you see here is it's called digital breast hemosynthesis it's sort of a it's also x-ray based but it actually ends up you end up with something like a 3d volume and what this does the the goal of this is it it allows radiologists to better segment the targets they're interested from the normal anatomy and typically it actually improves performance so radiologists do better diagnosing cancer with these three 3d breast synthesis images than the 2d mammograms in general and they're now i'd say they're actually probably last time i checked there were about 40 of the clinics in u.s that had this this is probably now by 70. i need to really check the number this is a few years ago but it's sort of like what's what's now in most clinics coming or it's there already in the big cities um so we were interested again and how you know the relationship between how radiologists uh look for targets in these two dimen two mammogram 2d mammograms and these 3d images and the relationship between performance in these two conditions and how it compared to a cnn and we particularly focused on on one task that we thought was that is clinically important which instead of a the detection of these very small targets that are you know very salient when you look at them but they're sort of hard to see in the periphery these are this is just it's called a micro classification typically they actually appear in clusters not in not sink by themselves so this is a little bit of a simplification so we stuck we sort of inserted these this is a collaboration with the university of pennsylvania and the santa barbara's women's imaging center we stuck this at random locations in these phantoms these are actually these are not actual uh dbt's these are actually phantoms created by pen so complicated phantoms they look similar they're not exactly what real dbts but they look similar to those and we stuck they're about 100 slices i'm just showing four in the 3d and we stuck this at a random location and we also put it at a random location in the 2d and then the humans and the radiologists and the cnn had to search for that little target and when you actually do that and you measure cnn the the this is actually a cnn is actually a specific unit that's specific for for for medical images that sort of it was 2016 was sort of a state of the art in image segmentation for medical images and we trained the last layers to do our task and you can see that it does better in the 3d than the 2d which is actually consistent with this notion that the 3d because it has volumetric information carries more information than the 2d image but when we actually looked at our radiologists look what happened we had this incredible dissociation where radialis did much poorly in the for this particular target that i just showed you and uh in the 3d than in the 2d and so this is actually an interesting result uh because there's you know a big dissociation but it's if you're if you're not a vision scientist if somebody in medical imaging or radiology you might think this is actually sort of oh what does this happen but if i those those of you in the audience you're a vision scientist so you probably have figured out what's going on what happens is this target is extremely salient in the when you focus it and the 2d image you can actually wrap you know rapidly but with moderate time you can actually scan the image and actually find it but in this huge volume of 100 slices if you count the typical clinical times about about two to four minutes people don't radiologists do not have the time to scrutinize every slice and every spot of every slice so they often miss it so here's an example i actually show you so this is so what you're seeing here just before i'll pause it for a sec in green is are the fixations of the radiologist and at some point you're gonna see a little white ring that's gonna appear for a few slices and that's where the target is gonna be you can see how you're gonna see how they miss it so they're scanning around they're fixating here there there that's it right there i know everybody saw it it was right there i'll put it back there we go so right you're gonna see here right there the maybe i missed it okay there there you go that's where the target is it's hard to see but if you're looking away it's gonna be it's very very easy to miss um so clearly the the the what explains this association is really that the humans have this foe individual system and they're underexploring this 3d volume which leads to this dissociation with cnns which are non-phobiated and actually thoroughly exploring the entire 3d volume okay so to summarize i gave you sort of three examples very different examples of how um humans and cnns search in different ways the first examples um sort of emphasizes how the human brain heavily relies on contextual relations for object search while current cnns and like i said i haven't tested you know we need you know we this could have changed maybe i'm if you have a point towards one cnn we need to try to see if it has these properties that'd be that'd be awesome but the ones we had tested till recently do not seem to incorporate such relationships um the second one uh was really visual search of a person in the wild and sort of illustrate how the human brain adopts strategies that optimize performance across a battery of tasks rather than you know cns are trained to do these narrow tasks and that might explain sort of the difference in in uh strat and strategy and then the third one was uh instead of identified uh sort of in a you know in a practical domain instances that reveal sort of visual cognitive bottlenecks in in humans that um you know result in these uh large search errors for these small signals and it sort of um sort of suggests that that those might be good circumstances where ai might greatly help mitigate um cure human errors and that was presented sort of in sort of cancer screening by radiologists so with that thank you very much 