 [Music] hi and welcome to this series on zero to hero for natural language processing using tensorflow if you're not an expert on AI rml don't worry we're taking the concepts of NLP and teaching them from first principles in this first lesson we'll talk about how to represent words in a way that a computer can process them with a view to later training a neural network that can understand their meaning this process is called tokenization so let's take a look consider the word listen as you can see here it's made up of a sequence of letters these letters can be represented by numbers using an encoding scheme a popular one called ASCII has these letters represented by these numbers this bunch of numbers can then represent the word listen but the word silent has the same letters and thus the same numbers just in a different order so it makes it hard for us to understand sentiment of a word just by the letters in it so it might be easier instead of encoding letters to encode words consider the sentence I love my dog so what would happen if we start encoding the words in this sentence instead of the letters in each words so for example the words I could be one and in the sentence I love my dog could be 1 2 3 4 now if I take another sentence for example I love my cat how would we encode it now we see I love my has already been given 1 2 3 so all I need to do is encode a cat I'll give that the number 5 and now if we look at the two sentences they are 1 2 3 4 and 1 2 3 5 which already show some form of similarity between them and it's a similarity you would expect because they're both about loving a pet given this method of encoding sentences into numbers now let's take a look at some code to achieve this for us this process as I mentioned before is called tokenization and there's an API for that we'll look at how to use it with Python so here's your first look at some code to tokenize these sentences let's go through it line by line first of all we'll need the tokenizer api's and we can get these for intensive flow Cara's like this we can represent our sentences as a Python array of strings like this it's simply the I love my dog and I love my cat that we saw earlier now the fun begins I can create an instance of a tokenizer object the num words parameter is the maximum number of words to keep so instead of for example just these two sentences imagine if we had hundreds of books to tokenize but we just want the most frequent 100 words in all of that this would automatically do that for us when we do the next step and that's to tell the tokenizer to go through all the text and then fit itself to them like this the full list of words is available as the tokenizer x' word index property so we can take a look at it like this and then simply print it out the result will be this dictionary showing the key being the word and the value being the token for that word so for example my has a value of 3 the tokenizer is also smart enough to catch some exceptions so for example if we updated our sentences to this by adding a third sentence noting that dog here is followed by an exclamation mark the nice thing is that the tokenizer is smart enough to spot this and not create a new token it's just dog and you can see the results here there's no token for dog exclamation but there is one for dog and there's also a new token for the word you if you want to try this out for yourself I've put the code in a collab here take it first spin an experiment you've now seen how words can be tokenized and the tools intensive flow that handled that tokenization for you now that your words are represented by numbers like this you'll next need to represent your sentences by sequences of numbers in the correct order you'll then have data ready for processing by a neural network to understand or maybe even generate new text you'll see the tools that you can use to manage this sequence in the next episode so don't forget to hit that subscribe button [Music] 