 mr. presentation hello I am now accumulating University of Tokyo I am presenting on our research title site sotto voce an ultrasound imaging based siren speech interaction using deep neural networks so we will begin with a demonstration video which provides an overview of the study here is a station you recently listened to orchestral music from Amazon music okay I've seen in the in the video when I move mouths without actually emitting voice images from the oral cavity are obtained through the sound a neural network then converts this image to speech the generated voice operated in Alexa in this demonstration the generated sound was amplified through the speaker however if you input the voice directory you can apply gel Excel developed antibodies here is a station you only listen to orchestral music from Amazon music so our research is for science speech interaction which applies a voice user interface without our voice so we start with the significance of a caesarian speech interaction a wheeze advancement in speech processing technology we can actually ask question to device and receive answer from them but nobody here right now with asked her what is silent speech moreover it is also difficult to use voice in user interface in public place using information such as a house addresses / passwords because they may lead to your legal privacy so and furthermore and most importantly those who cannot speak due to the physical reasons such as those who did not have vocal cords I cannot use in voice interface thus the more widespread the Bose interface the more of a struggle we appear in fact both these problems are caused by essential condition our voice interface requiring speech sorry so therefore we need to develop our voice interaction without the voice okay I explained our basic idea for your eyes in time speech the book relation processes drawers are divided into two speaker system the local codes created buzzer sound and tone English case pronunciation is created by the shape and remover pencil or a cavities finally the sound LS discharged outside of speech therefore our pronunciation should be estimated from the shape and movement this video was captured by MRI but Mr Isaac not a very vocal about the usage so we use the sound when I put the sound probe attached in the direction from the jus to the mouth we can capture these images Alexa play music Alexa play music so we want to estimate speech speech audio from the imaging and neural network but to use network I have to only have to correct the data set for training so using microphone for recording voices and by speaking while holding a rooster sound probe we can correct the data set with ultrasound imaging boys for example we could obtain a sound and shape of the mouth produce when unexplained music was pronounced okay we then trained the neural network to estimate stage using the ultrasound images only after the training we live reduce the same mouth shape and as that in the data set data the data used for training but with no sound okay your network estimates sound to be uttered from images as a result you can make speech without waking sound outside this is our approach okay various approaches are used to the are assigned speech for our so far for example outer ago uses electromyography to estimate internal voice however this can be termed non detection instead of speech and there's reworded there's a gesture recognition years in masses along the not starting for study I party area like I uses ingress and speech and allows the spatial recognition with a silent voice however it requires the user to hold the device in front of their mouth and it also requires special skills with interrupts trucks the users lives with the front camera of the mobile device and guesses the command through was a conversion on your network all the methods using the camera or RGB camera are very simple but you have to somehow locate the camera at the distance from the front of your face so within trucks relies on the user's hand to create in this distance as ultrasound probes are mainly used in medical applications so they are often sold as a device like this however in fact they can be cropped like this thus the even existing products can be scaled down to 10 centimeter by 3 centimeters if it design specifically it could be even smaller and the design of ultrasound probe is very flexible so the sound is compact and flexible so Optima tree wearable devices like this can be designed using these features it has a miniaturized with us on the probe and often a year one this is just a prototype but completely feasible this will allow you to always ask your smart assistant without being noticed by others although it is not the leaves out of this study but in our ongoing one the network can be driven by guru mo y TPU research only pleading a synthesized in the audio from images from images of face or lip lips using deep inner are deep learning are similar to ours there are a few but some combined there are few but some combined with the sound image and deploying however that combination of with the sound image and convolutional neural network is the first as far as we know and in addition to evaluate in addition to variation on data set with voice it is unique that actual silent speech was performed this is our system that has made audio using the images the neural network takes us time series of water sonic images and outputs sound representation factors in our case males case spectrum and finally then output vectors are converted to the actual sound waves spider Griffin learned about this conversion process involves two neural network net two and let it ring at one in mid to call we call the goal of net one is image two audio feature translation it consists of four congressional metal layers and two three connected layers each to the completion network was followed by a dropout and patch normalization to prevent overfitting then each Y years of each layer was activated to using wake relative finally it generates a 64 dimensional feature vector which restored to 20 millisecond Audio Matawan was trying using a dataset of operas approximately fifty hundred utterance of Alex comments which were collected while participant the speech of the data set was encoded into the spectrum and used as a target data our system is used a dependent data set should be corrected and network trained for each users we estimated each acquire peace as the winds shift each don't like each generalized output of 64 dimensional feature vector these were aligned to create a sequence of audio feature vectors when then use equation Lee method to recover the speech this graph shows the learning process when at one this is a grunt race I like so what's the weather like and as learning from progress is the generated result approach to the ground like an Excel what - like I like so what's the weather like line and net to improve the generality speech on perk month basis the input is order feature vector output from net one and the training data is audio feature vectors of ground truth this is the difference between the net one out with an entity output although the difference is smooth to the human year but probably probability alcoholics of reaction increases so this table shows how often are actually acted to the generated sound the test data years here were obtained during the audible speech so it's not foreign speech as we heard in the previous slide the generated speech is understandable to humans however the generated speech is different from humans of all voice so Alex Avila clinician late is not very high the liquation light will be improved by two unit electrons using generated speech since the speech generation news and test data was successful the actual end-to-end silent speech was examined in this case I'll mouth the speech command without an actuary image in the voice at first I could not speak well at all the sound is corrupted and by the beating actions of our time is recognizer certain tips and began to be generated create Priya voice is that Alex pledges and the finally we confirm that smart device could be operated using homicidal speech here is a station you recently listened to our campus in 752 PA I we enjoy some of these tips here the first is exhale little when speaking completely without exhaling air what we usually imagine as a lip sync it is difficult to reproduce the same mouth movements as that training data which allows omitting a voice so we had to exhale little air to get close instead of this our voice is in PI this is a speech with voices emphasized more than in the RT by post-processing for presenting for this presentation that noise released bikes in this be supposed to express very suppressed e-40 digital or less furthermore it is necessary to exaggerate some movement when uttering consonant this is a it's a for example when saying Alexa what's the weather like we have to emphasize the movements of K alike in the end in the end of the sentence no worry it does not require a much tongue movement but when general lighting silent speech it is important this time we could call a source re can hear so I exaggerated that K in the light okay this time we could compensate the a is failure to some extent using these tips and in other words our human training however the fact that system did not produce good results even if it's succeeded with the test data leads to important invocation so that is the difference between our evil speech and siren speech machine learning requires the test data and train des that will be in the same distribution so far it has been assumed that there is no difference in mouth movement between audible speech and science page so meaning that distribution match therefore research are feisty audio or Liberty audio or sound image - audio was applied to silent speech problem but that may not be the case at least in our experiment we needed to humans before to get clear voice Alexa play music this is a horrible speech mariksa play music so although we have not yet evaluated strictly what what our observations show is that word for image T audio of a science speech might not actually go to the legalization of science which is luxury okay these are our future work we have to do first there is an instruction speak above vocabulary now only five phrases 14 words could be used industry even in our ongoing research we have been able to use a blossom prothonotary 15 phrases and the forty was and the secondary in our case the gap between speech and solid speech was filled with human training however this gap could be could also be filled by developing new methods of creating a data set or by normalization with image processing finally this research is not intended to exclude other modalities combining information from extra-mile graffitis or axial methods and Nam microphone may improve the quality I didn't find the most useful combination of these modalities can be the subject of visualization in our ongoing study we have stopped using the second neural network net to since the Nets we generated the speech on Berkman basis so real-time generation was not possible that it is now possible to operate almost in real time like this and ultimately we aim to create a system that enables the user to speak with their mouth closed actually only moving the mouth without making any sound looks a bit strange so although I cannot speak trigger was yet [Music] but I actually be able to sleep like this oh okay thank you for listening it's a bit hard to hear because hard to hear a question because of the reaction because of the echo and I'm not good at this until the English so I'm happy PA speaks Rory question or have talked with me after session so any questions thank you hi we have time for one quick question so if you could state your name and affiliation as the student volunteer well hand you a microphone for whoever wants to ask a question hi I'm Paige mom from University of Manchester and I was just wondering that is the ultimate goal to sort of control your devices with with silent speaking so that there would be no image to audio you would just speak and somehow this would be converted to messages that your Alexa or whatever device which is understand what you're saying without actually any audio being there sorry I didn't understand question sorry I could we talk directly after session yeah sure thank you we have time for one quick question I think John look I'm Emily University of Sussex thanks for your talk so my question is did you use gel on top of the transducer and if you didn't did you consider using Doppler imaging sorry did you use what ultrasonic gel on top of the transducer okay okay great thank you very much okay thank you for 