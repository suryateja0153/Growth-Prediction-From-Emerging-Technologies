 hi everyone uh welcome to lecture number seven um so up to now i believe can you hear me in the back is it easy okay so in the last set of modules that you've seen you've learned about convolutional neural networks and how they can be applied to imaging notably you've played with different types of layers including pooling max pooling average pooling and convolutional layers you've also seen some classification with the most classic algorithms all the way up to inception and resnets and then you jumped into advanced application like object detection with yolo and the fast rcn and faster rcnn series with an optional video and finally face recognition and neural style transfer that we talked a little bit about in the past lectures so today we're going to build on top of everything you've seen in this set of modules to try to delve into the neural networks and interpret them because you noticed after seeing the set of modules up to now that a lot of improvements of the neural networks are based on trial and error so we try something we do hyper parameter search sometimes the model improves sometimes it doesn't we use a validation set to find the right set of methods that would make our model improve it's not satisfactory from a scientific standpoint so people are also searching how can we find uh an effective way to improve our neural networks not only with trial and error but with theory that goes into the network and visualizations so today we will focus on that we first will see three methods saliency maps occlusion sensitivity and class activation maps which are used to kind of understand what was the decision process of the network given this output how can we map back the output decision on the input space to see which part of the inputs were discriminative for this output and later on we will delve even more in details into the network by looking at intermediate layers what happens at an activation level at a layer level and at the network level with another set of methods gradient ascent class model visualization data set search and deconvolution we will spend some times on the convolution because it's a it's a cool it's a cool type of mathematical operation to know and it will give you more intuition on how the convolution works from a mathematical perspective if we have time we go over a fun application called deep dream which is super cool visuals for some of you who know it okay let's go mentee code is on the board if you guys need to to sign up so uh as usual we'll go over some contextual information and small case studies so don't hesitate to participate so you've built an animal classifier for a pet shop and you gave it to them it's it's super good it's been trained uh on imagenet plus some other data and what what is a little worrying is that the pet shop is a little reluctant to use your network because they don't understand the decision process of the model so how can you quickly show that the model is actually looking at a specific animal let's say a cat if i give it an input that is a cat we've seen that together one time everybody remembers so i'll go quickly you have a network here's a dog given as an input to a cnn the cnn assuming the constraint is that there's one animal per image was trained with a softmax output layer and we get a probability distribution over all animals iguana dog car cat ants and crab and what we want is to take the derivative of the score of dog and back propagate it to the input to know which parts of the inputs were discriminative for this score of dock does that make sense everybody remembers this and so the interesting part is that this value is the same shape as x so it's the size of the input it's a matrix of numbers if the numbers are large in absolute value it means the pixels corresponding to these locations had an impact on the score of doug okay what do you think the score of dog is is it the output probability or no what what do i mean by s of dog it's a score of the dog yeah but is it uh 85 that's what i mean no the actual formula is used to compute the 485 going through the soft max factor the z plus w x plus b and so on yes so it's the it's the score that is pretty soft max it's the score that comes before the soft max so as a reminder here's a a soft max layer and this is how it could be presented so you get a vector that is a set of scores that are not necessarily probabilities they're just scores between minus infinity and plus infinity you give them to the soft max and the softmax what it's going to do is that it's going to output a vector where the sum of all the probabilities in this vector are going to sum up to one okay and so the issue is if instead of using the derivative of what we called y hat last time we use the score of dog we will get a better representation here the reason is in order to maximize this number score of dog divided by the sum of the score of all animals or like maybe i i should write exponential of score of dog divided by sum of exponential of the score of all animals one way is to minimize the so the scores of all the other animals rather than maximizing the score of dog so you see so maybe moving a certain pixel will minimize the score of fish and so this pixel will have a high influence on y hats the general output of the network but it actually doesn't have an influence on the score of dog one layer before does it make sense so that's why we would use the scores pre-soft max instead of using the scores plus softmax that are the probabilities okay and what's fun is here you cannot see the slides are online if you want to if you want to look at it on your computers but you have some of the pixels that are roughly the same positions as the dog is on the input image that are stronger so we see some white pixels here and this can be used to segment the dog probably so you could use a simple thresholding to find where the dog was based on this pixel uh pixel derivative the pixel score map doesn't work too too well in practice so we have better methods to do segmentation but this can be done as well so this is what is called saliency maps and it's a common technique to quickly visualize what the network is looking at in practice we will use other methods so here is another contextual story now you've built the animal classifier they're still a little scared but you want to prove that the model is actually looking at the input image at the right position you don't need to be quick but you have to be very precise yeah so going back on the last slide is the saliency map that edge detection white pixel border no the saliency map is literally this thing here is the values of the derivative okay so it's like a gradient so you you take the score of dog you back propagate the gradient all the way to the input it gives you a matrix that's exactly the same size as the x and you you use like a specific color scheme to see which pixels are the strongest thank you yeah okay so here we have our cnn the dog is for propagated and you get a score of a probability score for the dog now you want a method that is more precise than the previous one but not necessarily too fast and this one we've talked about it a little bit it's occlusion sensitivity so the idea here is to put a gray square on the dog here and we propagate this image with the gray square at this position through the cnn what we get is another probability distribution that is probably similar to the one we had before because the gray square doesn't seem to impact too much damage at at least from a human perspective we still see a dog right so the score of dog might be high 83 percent probably what we can say is that we can build a probability map corresponding to the class dog and ha and we will write down on this map how confident is the network if the gray square is at a specific location so for our first location it seems that the network is very confident so let's put a red square here now i'm going to move the gray square a little bit i'm shifting it just as we do for convolution and i'm going to send again this new image in the network it's going to give me a new probability distribution output and the score of dog might change so looking at this score of dog i'm going to say okay the network is still very confident that there is a dog here and i continue i shift it again here same network's still very confident that there is a dog now i shift the the square vertically down and i see that partial the the face of the dog is partially occluded probability of dog will probably go down because the network cannot see one eye of the dog he's not confident that there's a dog anymore so probably the the confidence of the network went down i'm going to put a square that is tending to be blue and i continue i shift it again and here we don't see the dark face anymore so probably the network might might classify this as a chair right because the chair is more obvious than the dog now and so the probability score of dog might go down so i'm going to put a blue square here and i'm going to continue here we don't see the tail of the dog it's still fine the network is pretty confident and so on and what i will look at now is this probability map which tells me roughly where the dog is so here we used a pretty big filter compared to the size of the image the smaller the sorry the pretty big gray square the smaller the gray square the more precise this probability map is going to be does that make sense so this is if you have time if you can you can take your time with the pet shop to explain them what's happening you would do that yeah would you ever in an occlusion type of situation have an increase in the probability not just to do this say you removed the noise from the image and we'll see that in the next slide that's correct so let's see more examples here we have three classes and these these these images have been have been generated by matthew taylor and rob fergus this paper visualizing and understanding convolutional networks is uh one of the seminal paper that has led the research in visualizing and interpreting neural networks so i'd advise you to take a look at it and we will refer to it a lot of time in this lecture so now we have three examples one is the pomeranian which is this type of cute dog a car wheel which is the true class of the second image and an afghan hound which is this type of dog here on the last image so if you do the same thing as we did before that's what you would see so just to clarify here we see a blue color it means when the gray square was positioned here or centered at this location the network was less confident that the true class was pomeranian and in fact if you look at the paper they explained that when the gray square was here the confidence of pomeranian went down because the conference because the confidence of tennis ball went up and in fact the pomeranian dog has a tennis ball in the mouth another interesting thing to notice is on the last picture here you see that there is a a red color on the top left of the image and this is you exactly at what as what you mentioned adam is that when the square was on the face of the human the network was much more confident than the true class that the true class was the dog because you removed a lot of meaningful information for the network which was the face of the human and similarly if you put the square on the dog the true class that the network was outputting was human problem does that make sense okay so this is called occlusion sensitivity and it's the second method that uh you now have seen for interpreting where the network looks at on an input so let's move to class activation maps so i know if you remember but two weeks ago pranav when he discussed the techniques that he has used in healthcare he explained that you get a he get a chest x-ray and he manages to to tell the doctor where the network is looking at when predicting a certain disease based on this chex x-ray right you remember that so this was done through class activation maps and that's what we're going to see now so one important thing to notice is that we discussed that classification networks seem to have a very good localization ability and we can see it with the two methods that we previously discussed same thing for those of you who have read the yolo paper that you've studied in this set of modules the yolo v2 algorithm has first been trained on classification because classification has a lot of data a lot more than object detection has been trained on classification built a very good localization ability and then has been fine-tuned and retrained on object detection data sets okay and so the core idea of class activation map is to show that cnns have a very good localization ability even if they were trained only on image level labels so we have this network there is a very classic network used for classification we give it a kid and a dog uh this class activation map is coming from mit the mit lab with bolejo at all in and you four propagate this image of a kid with a dog through the network which has some conver max pool classic series of layers several of them and at the end you usually flatten the last output volume of the conf and run it through several fully connected layers which are going to play the role of a classifier and send it to a soft max and get the probability output now what we're going to do is that we're going to prove that this cnn is generalizing to localization so we're going to to convert this same network in another network and the part which is going to change is only the last part the downside of using flatten plus fully connected is that you lose all spatial information right you have a volume that has spatial information although it's been gone through some max pooling so it's been down sampled and you lost some part of the special localization flattening kills it you flatten it you run it through a fully connected layer and then it's over you it's it's super hard to find out where the activation was corresponds to on the input space so instead of using flattened plus fully connected we're going to use global average pooling we're going to explain what it is a fully connected softmax layer and get the probability output and we're going to show that now this network can be trained very quickly because we just need to train one layer the fully connected here and can show where the network looks at the same as the previous network so let's let's talk about it more in detail assume this was the last conf layer of our network and it outputs a volume a volume that is size to simplify four by four by six so six filters were used in the last comp and so we have six feature maps now that make sense i'm going to convert this using a global average pooling to just a vector of six values what is global average pooling is just taking these feature maps each of them averaging them into one number so now instead of having a four by four by six volume i have a one by one by six volume but we can call it a vector does that make sense so what's interesting is that this number actually holds the information of the whole feature map that came before in one number being average over it i'm going to put these in a vector and i'm going to call them activations as usual a1 a2 a3 a4 a5 a6 as i said i'm going to train a fully connected layer here with the softmax activation and the outputs are going to be the probabilities so what is interesting about that is that the feature maps here as you know will contain some visual patterns so if i look at the first feature map i can plot it here so these are the values and of course this one is much more granular than four by four it's not a four by four it's much more numbers but this you can say that this is the feature map and it seems that the activations have found something here there was a visual pattern in the input that activated the the feature map and the filters which generated this feature map here in this location same for the second one there's probably two objects or two patterns that activated the filters that generated this feature map and so on so we have six of those and after i've trained my fully connected layers here my fully connected layer i look at the score of dog score of dog is 91 percent what i can do is to know this 91 percent how much did it come from these feature maps and how can i know it is because now i have a direct mapping using the weights i know that the weight number one here this edge you see it is how much this score was dependent on the orange feature map does that make sense the second weight if you look at the green edge is the weight that has multiplied this feature map to give birth to the output of a dog so this weight is telling me how much this feature map the green one has influence on the output does that make sense so now what i can do is to sum all of this awaited some of all these feature maps and if i just do this weighted sum i will get another feature map something like that and you notice that this one seems to be highly influenced by the green one the green feature map yeah it means probably the weight here was higher it probably means that the second filter of the last conv was the one that was looking at the dog that makes sense okay and then once i get this feature map this feature map is not the size of the input image right it's the size of the height and width of the output of the last comp so the only thing i'm going to do is i'm going to up sample it back simply so that it fits the size of the input image and i'm going to overlay it on the input image to get my class activation map the reason it's called class activation map is because this feature map is dependent on the class you're talking about if i was using let's say i was using car here if i was using car the weights would have been different right look at the edges that connect the first activation to the activation of the previous layer these weights are different so if i sum all of these feature maps i'm going to get something else that make sense so this is class activation maps and in fact there is a dog here and there is a human there and what you can notice is probably if i look at the class of human the weight number one might be very high because it seems that this visual pattern that activated the first feature map was the face of the kit okay so what is super cool is that you can get your network and just change the last few layers into global average pooling plus a softmax fully connected layer and you can do that and visualize very well it requires a small fine tuning yeah so were these like saliency maps but for the activation uh no so it's a different vocabulary i would use failure seam apps for the back propagation up to the pixels and class activation maps related to one class it's not about propagation at all it's just an up sampling through the to the input space based on the feature maps of the last comp layer so it's mostly just examining the weights and sort of doing like a max operation on yes any other questions on class activation maps just taking place yeah that's a good question so taking the average does it kill the spatial information so let me let me write down a formula here this is the score that we're interested in let's say dog class c what you could say is that this score is a sum of k equals 1 to 6. of wk which is the the weight that that connects the output activation to the previous layer times what times a of the previous layer um let's say we we use a notation that is like k is the case feature map and i j is the location and i sum that over the locations can you see in the back roughly so what i'm saying is that here i have my global average pooling that happened here and i can divide it by the certain number so divided by 16 four by four okay i can switch the two sums so i can say that this thing is a sum over i j the locations times sum over k equals 1 to 6 of what w k times a k so the activations of the k fitter map in position aj ij and times the normalization 116. does this make sense does this make sense so i i still have the the location i still moved i still moved the thumb around and what i could do is to say that this thing is the score in location i j of the class activation map it's a class score for this location i j and i'm summing it over all locations so just by flipping what the average pooling was doing over the locations i can say that by weighting using my weights all the activation in a specific location for all the feature maps i can get the score of this position in regards to the final output does that make sense so we're not losing the the spatial information the reason we're not losing it is because we know we know what the feature maps are right we know what they are and we know that they've been averaged exactly so we exactly can map it back to each amount right we're not able to get one weight yeah because we assume that each filter that generated these feature maps detects one one specific thing so like if if this is the feature map it means assuming the filter was detecting dog that we're going to see just just something here meaning that there is a dog here and if there was a dog on the lower part of the image we would also have strong activations in these parts i i'd say if you want to see more of the math behind it check the paper but this is the intuition behind it you can flip the summations using the global average pooling and show that you keep the the spatial information the thing is you do the global average pooling but you don't lose the feature maps because you know where they were from the output of the comp right so you're not you're not deleting this information that makes sense yeah so the summation of uh the activations k divided by 16 is the step of taking the average right for that class yeah okay let's move on and watch a cool video on how a class activation map work this video was from kyle mcdonald and it's it's live so it's very quick so you can see that the network is looking at the speedboat okay so now the three methods we've seen are methods that are roughly mapping back the output to the input space and helping us visualize which part of the inputs were the most discriminative to lead to this output and the decision of the network now we're going to try to delve more into details in the in the intermediate layers of the network and try to interpret how does the network see our world not necessarily related to a specific input but in general okay so the pet shop now trusts your model because you've used occlusion sensitivity saliency map and class activation maps to show that the model is looking at the right place but they got a little scared when you did that and they ask you to explain what the model thinks a dog is so you have this trained convolutional your network and you have an output probability oh yep let me take one in the back yeah um so what are some good ways to visualize like non-image data non-image data that's a that's a good question it's actually so the reason we're seeing image is what most of the research has been focusing on images if you look at let's say time series data so either speech or natural language the main way to visualize those is with the attention method are you familiar with that so in the next set of modules that you're going to start this week and you're going to study in the next two weeks you will see a visualization method called attention models which will tell you which part of a sentence was important let's say to output a number like assuming you're doing machine translation you know some languages they don't have a direct one-to-one mapping it means i might say i love cats but in another language maybe this same sentence would be cats i love or something like that it's flipped and you want an attention model to see to show you that the cat was referring to the second i think it's it's it's okay okay sorry guys so going back to the presentation now we're going to delve into inside the network and so the new thing is the pet shop is a little scared and asks you to explain what the network think a dog is what's a representation of dog for the network so here we're going to use a method that we've already seen together called gradient ascent which is defining an objective that is technically the score of the dog minus a regularization term what the regularization term is doing is it's saying that x should look natural it's not necessarily l2 regularization it can be something else and we will discuss it in the next slide but don't think about it right now what we will do is we will compute the back propagation of this objective function all the way back to the input and perform gradient ascent to find the image that maximizes the score of the dog so it's an iterative process takes longer than the class activation map and we repeat the process forward propagate x compute the objective back propagates and update the pixels and so on you guys are familiar with that so let's see what what what we can visualize doing that so actually if you take an imagenet classification network and you perform this on the classes of goose or ostrich or kid fox husky dalmatians you can see what the network is looking at or what the network think of dalmacian is so for the dalmacian you can see some some black dots on a white background somehow but these are are still quite hard to interpret it's not super easy to see and even worse here on the screen better on your computers but you can see a fox some here you can see orange color for the fox it means that pushing the pixels to an orange color would actually lead to a higher score of the kit fox in the output if you use a better regularization than l2 you might get better pictures so this is for flamingo this is for pelican and this is for heartbeat so a few things that are interesting to see is that in order to maximize the score of flamingo what the network visualized is many flamingos it means that 10 flamingo leads to a higher score of the class flamingo than one flamingo for the network talking about regularization what does l2 regularization say it says that for visualizing we don't want to have extreme values of pixel it doesn't help much to have one pixel with an extreme value one pixel with a low value and so on so we're going to regularize all the pixels so that all the values are around each other and then we can rescale it between 0 and 255 if you want one thing to notice is that the gradient ascent process doesn't constrain the inputs to be between 0 and 255. you can go to plus infinity potentially while an image is stored with numbers between 0 and 255 so you might want to clip that as well this is another type of regularization one thing that led to beautiful pictures was what jason josinski and his team did is they four propagated an image computed the score computed the objective function back propagated updated the pixels and blurred them blurred the picture because what what is not useful for visualizing is if you have high frequency variation between pixels it doesn't help to visualize if you have many pixels close to each other that have many different values instead you want to have a smooth transition among pixels and this is another type of regularization called gaussian blurring okay so this method actually makes a lot of sense in in in scientific terms you're you're maximizing an objective function that gives you what the network sees as flamingo which would maximize the score of flamingo so we call it also class model visualization yes does a more realistic class visualization correspond to a more accurate model um a more realistic class model visualization correspond to more accurate so it's hard to map the accuracy of the model based on this visualization but it's a good way to validate that the network is looking at the right thing yeah we're going to to see more of this later i think the most interesting part is actually on this slide is we did it for the class score but we could have done it with any activation so let's say i stop in the middle of the network and i define my objective function to be this activation i'm going to backpropagate and find the input that will maximize this activation it will tell me what is this activation what does this activation fire for so that's even more interesting i think than looking at the input and then yep does that make sense that we could do it on any activation yep any questions on that okay so now we're going to do another trick which is data set search it's actually one of the most useful i think uh not fast but very useful so the petshop loved the previous technique and asked if there are other alternatives to to show what what an activation in the middle of a network is thinking you take an image for propagated through the network get your output now what you're going to do is select a feature map let's say this one we're at this layer and the feature map is of size five by five by 256 it means that the conv layer here had 256 filters right you're going to look at these feature maps and select probably uh yeah what you're going to do select one of the feature maps okay we select one out of 256 feature map and we're going to run a lot of data for propagated to the network and look which data points have had the maximum activation of this feature map so let's say we do it with the first feature map we notice that these are the top five images that really fired this feature map like high activations on the feature map what it tells us is that probably this feature map is detecting shirts could do the same thing let's say we take the second feature map and we look which data points have maximized the activations of this feature map out of a lot of data and we see that this is what we got the top five images probably means that the other feature map seems to be activated when seeing edges so the second one is much more likely to appear earlier in the network obviously than later on so one thing that you may ask is these images seem cropped like i don't think that this was an image in the data set it's probably a subpart of the image what do you think this crop corresponds to any idea how we cropped the image and why these are cropped like why didn't i show you the full images how was i able to show you the cropped so the the filter you look at this region and then select anything outside it's not important that's correct so let's say we pick an activation an activation in the network this activation for a convolution on your network oftentimes doesn't see the entire input image right it doesn't see it what it sees is a subspace of the input image does that make sense so let's look at another slide here we have a picture of eunice 64 by 64 by three it's our input we run it through a five layer convent and now we get an encoding volume that is much smaller in height and width but bigger in depth if i tell you what this activation is seeing if you map it back you look at the stride and the filter size you've used you could say that this is the part that this filter is seeing this this uh this activation is seeing it means the pixel that was up there had no influence on this activation and it makes sense when you think of it you're the easiest way to think about it is looking at the the top picks the the top entry on the encoding volume top left entry you have the input image you put a filter here this filter gives you one number right this number this activation only depends on this part of the image but then if you add a convolution after it it will take more filters and so the deeper you go the more part of the image the activation will see so if you look at an activation in layer 10 it will see much a much larger part of the input than an activation in layer one that makes sense so that's why that's why probably the pictures that i showed here these ones are very small part cropped small crops of the image which means the activation i was talking about here is probably earlier in the network it sees a much smaller part of the input yeah so what you look at is which activation was maximum you look at this one and then you map this one back to crop that makes sense [Music] okay so here's your nest again up and same this one would correspond more in the center of the image this intuition makes sense okay cool so let's talk about deconvolution now it's going to be the hardest part of the lecture but probably helping with with more intuition on deconvolution you remember that that was the generative adversarial networks scheme and we said that giving a code to the generator the generator is able to output an image so there's something happening here that we didn't talk about is how can we start with a 100 dimensional vector and output a 64 by 64 by 3 image that seems weird we could use you might say a fully connected layer with a lot of neurons right to upsample in practice this is one method another one is to use a deconvolution network so convolutions will encode the information in a smaller volume in height and width deeper in in depth while the deconvolution will do the reverse it will up sample the height and width of an image so that would be useful in this case another case where it would be useful is segmentation you remember our case studies for segmentation life cell microscopic images of cells give it to a convolution network it's going to encode it so it's going to lower the height and width the interesting thing about this encoding in the middle is that it holds a lot of meaningful information but what we want ultimately is to get a segmentation mask and the segmentation mask in height and width has to be the same size as the pixel image so we need a deconvolution network to up sample it so deconvolution are used in these cases today the case we're going to talk about is visualization remember the gradient ascent method we talked about we define an objective function by choosing an activation in the middle of the network and we want the objective to be equal to this activation to find the input image that maximizes this activation through an iterative process now we don't want to use an iterative process we want to use a reconstruction of this activation directly in the input space by one backward path so let's say i select this feature map out of the max pool 255 sorry 5x5 by 256 what i'm going to do is i'm going to identify the max activation of this feature map here it is it's this one third column second row i'm going to set all the others to zero just this one i keep it because it seems that this one has detected something don't want to talk about the others i'm going to try to reconstruct in the input space what this activation has fired for so i'm going to compute the reverse mathematical operation of pulling relu and convolution i will unpool i will unreal let's say doesn't it this word doesn't exist so don't use it but unrelu and the conf and i will do it several times because this activation went through several of them so i would do it again and again until i see oh this specific activation that i selected in the feature map fired because it saw the ears of the dog and as you see this image is cropped again it's not the entire image it's just the part that the activation has seen and if you look at where the activation is located on the feature map it makes sense that this is the part that corresponds to it so now the higher level intuition is this we're going to delve into it and see what do we mean by unpool what do we mean by unrelu and what do we mean by decomp okay yes if we hadn't zeroed out the rest we checked them all at whatever values they were at would we have just gotten a reconstruction of the whole image so the difference is you mean if we don't zero out all the activations it's just that this reconstruction would be messier it would be more messy yeah doesn't doesn't necessarily mean you will not get the full image because probably the other activations probably didn't even fire it means they didn't detected anything else it's just that it's gonna it's gonna add some noise to this reconstruction okay so let's talk about the convolution a little bit on the board so to start with the convolution and you guys can take notes if you want we're going to spend about 20 minutes on the board now to discuss the convolution okay to understand the deconvolution we first need to understand the convolution we've seen it from a computer science perspective but actually what we're going to do here is we're going to frame the convolution as a simple matrix vector mathematical operation you're going to see that it's actually possible so let's start with the 1d comp for the 1d convolution i will take an input x which is of size 12. x1 x2 x3 x4 x5 x6 x7 x8 so 8 plus 2 padding which gives me the 12 that i mentioned so the input is a one-dimensional vector which has padding of two on both sides i will give it to a layer that will be a 1d column and this layer would have only one filter and the filter size will be four we will also use a stride equal to two so my first question is what's the size of the output can you guys compute it on your on your notepads and and tell me what's the size of the output input size 12 filter of size four stride of two padding of two five yeah i heard it yeah so remember you use n x sorry n y equals nx minus f plus 2p divided by stride and you will get 5. so what i'm going to get is y1 y2 y3 y4 y5 so i'm going to focus on this specific convolution for now and i'm going to show now that we can define it as as a mathematical operation between a matrix and a vector so the way to do it is i guess the easiest way is to write the system of equation that is underlying here what is y1 y1 is the filter applied to the four first values here does that make sense so if i define my filter as being y w1 w2 w3 and w4 what i'm going to get is that y 1 equals w 1 times 0 plus w 2 times 0 plus w3 times x1 plus w4 times x2 this makes sense just the convolution elementwise operation and then sum all of it y2 is going to be same thing but with the stride of two going two down so it's going to give me w1 times x1 plus w2 times x2 plus w3 times x3 plus w4 times x4 correct everybody's following no same thing we will do it for all the y's until y5 and we know that y5 is element wise operation between the filter and the four last number here summing them so it will give me w1 times x7 plus w2 times x8 plus 0 plus w 3 times 0 plus w4 times 0. okay now what we're going to do is to try to write down y as a matrix vector operation between w and x we need to find what this w matrix is and looking at this system of equation it seems that it's not impossible so let's try to do it i will write my y vector here y one y two y three y four y five and i will write my matrix here and my vector x here so first question is what do you think will be the shape of this w matrix correct we know that this is five by one this is twelve by one so of course w is going to be five by twelve right so now let's try to fill it in zero zero x one x two x three blah blah blah x eight zero zero can you guys see in the back or no yeah okay cool so i'm going to fill in this matrix regarding this system of equation i know that the y1 would be w1 times 0 w2 times 0 w3 times x1 w4 times x2 so this vector is going to multiply the first row here so i just have to place my w's here w1 we come here multiply 0 w2 will come here w3 would come here and w4 would come here and all the rest would be filled in with zeros right i don't want any more multiplications how about the second row of this matrix i know that y2 has to be equal to this dot product with this row and i know that it's going to give me w1 x1 plus w2 x2 plus w3 x3 x1 is the third input on this vector third third entry so i would need to shift what i had in the previous row with the stride of 2. it will give me that does this make sense so if i use the dot product of this row with that i should get the second equation up there and so on and you understand what happens right this pattern would just shift with the stride of 2 on the side so i would get 0s here and i would get my w1 w2 w3 w4 and then zeros and all the way down here and all the way down here what i will get is w4 w3 w2 w1 and zeros so the only thing i want to mention here is that the convolution operation as you see can be framed as a simple matrix times a vector of the on the yes side for the top row shouldn't it be the left because that's going to multiply okay for the top row why the zeros are on the right side yes because i don't want y y one to be dependent on x3 to x8 so i want these to be zero multiplication pliers got it thank you okay so why is this important for the intuition behind the deconvolution and the existence of the deconvolution is because if we manage to write down y equal wx we probably can write down x equals w minus 1 y if w is an invertible matrix and this is going to to be our deconvolution and in fact what's the the what's the shape of this new matrix yes twelve by five we have twelve by one on one side five by one on the other it has to be twelve by five so it's flipped compared to w so one thing we're going to do here is we're going to make an assumption first assumption is that w is an invertible matrix and on top of that we're going to make a stronger assumption which is that w is an orthogonal matrix and without going into the details here same as when we proved uh xavier initialization in sections we made some assumptions that are not always true this assumption is not going to be always true one one intuition that you can have is if i'm using a filter that is um assume the filter is an edge detector so like uh plus one zero zero minus one in this case the matrix would be orthogonal why a matrix that is orthogonal means that if i take two of the columns here i dot product them together it should give me zero same with the rows you can see it so what's interesting is that uh if the stride was four there will be no overlap between these two rows it would give me an orthogonal matrix here let's try this two but if i replace this w one by minus one zero zero plus one sorry plus one zero zero minus one and minus plus one zero zero minus one you can see that the dot product would be zero the zeros will multiply the ones and the ones were multiplied to zeros give me a zero dot product so this is a case where it works practices doesn't always work the reason we're making this assumption is because we want to make a reconstruction right so we want to be able to have this w minus one this this this invert and the reconstruction is not going to be exact but at at a first order approximation we can assume that the reconstruction will still be useful to us even if this assumption is not always true in the case where w is orthogonal i know that the inverse of w is w transpose or another way to write it is that for orthogonal matrices w transpose time w is the identity matrix so what it tells me is that x is going to be w transpose time y times y so let's see what we get from that let me write down the menti code so let's say now we have our x and we want to regenerate our we will have our y and we want to generate our x using this method so i would what i would write is to understand the 1dd conf we can use the following illustrations where we have x here which is 0 0 x1 x2 x3 all the way down to x8 okay and i will have my w matrix here w transpose and my y vector y 1 y 2 y 3 y 4 and y 5 here and so i know that this matrix will be the transpose of the one i have here right so i can just write down the transpose the transpose will be w1 w2 w3 w4 okay i will shift it down with a stride of two and so on and this whole thing will be w transpose so the the small issue here is that this in practice is not is going to be very similar to a convolution but because uh but it's going to be a tiny little different in terms of implementation another question i might ask is how can we do the same thing with the same pattern as we have here it means the stride is going from left to right instead of going from up to down i'm going to introduce that with a technique called sub pixel convolution and for those of you who read papers in segmentation in visualization oftentimes this is the type of convolution that is used for reconstructing so let's see how it works i just want to do the same operation but instead of doing it with a strike going from up to down i want to do it from a strike going from left to right one one thing you wanna you to notice here is that the two lines that i wrote here are cropped and the reason is because we're using a padded input here we will just crop the two top lines and same for the two last lines they will be cropped look at that w1 we'll multiply y1 and this one will multiply y2 and so on so this dot product will give me w1 times y1 but i don't want that to happen because i want to get a padded zero here so i will just crop that in this matrix is actually going to be smaller than it seems and it's going to generate my x1 through xy 8 and then i will pad the top values and the bottom values okay just a hack so let's look at the subpixel convolution i have my input and i would do something quite fun i would perform a sub pixel operation on y what does it mean i will insert zeros almost everywhere i would insert them and i will get zero zero y one zero y two zero y three zero y four zero y five and zero zero even more one more zero here so this vector is just the vector y with some zeros inserted around it and also in the middle between the elements of y now why is that interesting it's interesting because i can now write down my convolution by flipping my weight so let me explain a little bit what happened here what we wanted is in order to be able to efficiently compute the deconvolution the same way as we've learned to compute the convolution we wanted to have the weights scattered from left to right with the stride moving from left to right what we did is that we use a subpixel version of y by inserting zeros in the middle and we divided the stride by two so instead of having a stride of two as we had in our convolution we have a stride of one in our deconvolution so notice that i shift my weights from one at every step when i move from one row to another second thing is i flipped my weights i flipped my weights so instead of having w1w2w3w4 now i have w4w3w2w1 and what you could see is looking at that first look at this row the first row that is not cropped the result of a dot product of this row with this vector is going to be y1 times w3 plus y2 times w1 yeah now let's look what happened here i look at my first row here the dot product of this first row with my y here is going to be uh sorry sorry we these two are cropped as well and same here so looking at my first non-cropped row here as a dot product with this vector what i get is w3 times y1 plus w2 oh sorry plus w1 times y2 so exactly the same thing as i got there so these two operations are exactly the same operations they're the same thing you get the same results two different way of doing it one is using a weird operation with strides going from top to bottom and the second one is exactly a convolution this is a convolution convolution plus flipped weights insertion of zeros for the subpixel version of y and on top of that padding here and there so this was the hardest part okay does it give more intuition on the convolution here you know now how convolution can be framed as a mathematical operation between a matrix and a vector and you know also that under these assumptions the way we will deconvolve is just by flipping our weights dividing the stride by two and inserting zeros if we just do that we're deconvolving forward propagating a convolution the following way you wanna deconvolve just flip all the weights insert zeros sub pixel and finally divide the stride and that's the deconvolution it's a super complex thing to understand but this is the intuition behind it now let's try to have an intuition of how it would work in two dimension let me write it down why do we use that because in terms of implementation this is the same as what we've been using here is very similar well this one is another implementation so you could do both the same it's the same operation but in practice this one is easier to understand because it's exactly the same operation of the convolution with flipped weights insertion of zeros and divided stride that's why i wanted to show that yeah what happens when uh the assumption when the assumption doesn't hold yeah so oftentimes the assumption doesn't hold but what we want is to be able to see a reconstruction and if we use this method we will still see a reconstruction practice if we had really w minus one the reconstruction would be much better but we don't so uh let me go over the 2d uh the 2d example we're going to go a little over time because we have two hours technically for one hour and 15 minutes and and let me go over the 2d example and then we will answer this question on why we need to make this assumption so here is the interpretation of a 2d convolution let me write it down here the intuition behind the 2d comp is i get my input which is 5 by 5 and this i call it x i forward propagate it using a filter of size two by two in a conv layer and a stride of two this is my convolution what i get so if you do five minus two plus the padding which is zero divided by two plus one oh i forgot the plus one actually here plus one and you floor it so so five minus two divided by two gives you uh three divided by two plus one um no actually it will give you three by three yeah three by three a y of three by three that's what you get and now this you call it y what you're going to do here is you're going to deconvolve y in order to deconvolve y in order to deconvolve it you're going to use a stride of one and what we said is that we need to divide the stride by two right so we need a stride of one and the filter will be the same two by two and you remember that what we've seen is that the filter is the same it's just that it's going to be flipped so you will use a filter of two by two but flipped [Applause] and now what do we get we hope to get a five by five input which is going to be our reconstructed x five by five input and the way we're going to do it is this is the intuition behind it yeah um five by five here that's what we hope to reconstruct the way we will do it is we will take the filter f is two by two we will put it here and we will multiply all the weights of this filter by y11 all the weights will be multiplied by y1y so i will get four values here which are going to be w4 y1 1 w 3 y 1 1 and so on now i will shift this with the stride of 1 and i will put my filter again here and i will multiply all the entries by y 1 2 and so on and you see that this entry has an overlap so it will it will it will be updated at every step of the convolution it's not like what happened in the forward pass so this is the intuition behind the 2d convolution 3d same thing you have a volume here so your filter is going to be a volume what you're going to do is you're going to put the volume here multiply by 1 1 1 and so on and then if you have a second filter you would put it again on top of it and multiply by 1 1 1 all the weights of the filter and so on it's a little complicated but this is the intuition behind the convolution okay let's get back to the lecture i'm going to take one question here if you guys need clarification don't worry if you don't understand the convolution fully the important part is that you get the intuition here and you understand how we use it so let me make a comment uh why do we need to make this assumption and do we need to make it when we want to reconstruct like we're doing here in the visualization we need to make this assumption because we don't want to retrain weights for the deconvolutional network what we know is that the activation we selected here on the feature map is has gone through the entire pipeline of the covenant so to reconstruct we need to use the weights that we already have in the covenant we need to pass them to the deconvolution and reconstruct if we're doing the segmentation like we talked about for the life cell we don't need to do this assumption we're just saying that this is a procedure that is a deconvolution and we will train the weights of the deconvolution so there's no need to make this assumption it's just we have a technique that is dividing the stride by one and inserting zeros and then beam we retrain the weights and we get an output that is an up-sampled version of the input that was given to it so there's two use case one where you use the weights and one where you don't in this case we don't want to retrain we want to use the weights so let's see let's see a version more visual of the up sampling so we do the subpixel image this is my image four by four i insert zeros and i pad it i get a nine by nine image i have my filter like that and this filter will convolve i will it would convolve over the input so i would place it on my input and at every step i would perform a convolution up i will get a value here the value is blue because as you can see the weights that affected the output were only the blue weights i would use a stride of one beam now the weights that affect my input are the green ones and so on and i would just convolve as i do usually and so on and now one step down i see that the weights that are impacting my input are the purple ones so i would put a purple square here and so on so i just do the convolution like that and so on so one thing that is interesting here is that the values that are blue in my out 6x6 output were generated only using the blue values of the filter the blue weights in the filter the ones that are green were only used you were only generated using the green values of my filter so actually this subsample subpixel convolution or deconvolution could have been done with four convolutions with the blue weights green weights purple weights and yellow weights and then just just replaced such that the adjustment would be the output just put the output of each of these comp and mix them to give out a six by six output only thing you need to know we have an input four by four and we get an output six by six that's what we wanted we wanted to upsample the image we can retrain the weights or use the transposed version of them so let's see what happens now we understood what uh what decom was doing so we're able to decomp what we need to do is also to unpool and to unreal fortunately it's easier than the decom so we're not going to do board work anymore so let's see how unpool works if i give you this input to the pool link to a max pooling layer the output is obviously going to be this one 42 is the maximum of these four numbers assuming we're using a two by two filter with ride of two vertically and horizontally 12 is the maximum of the green numbers 6 is the maximum of the red numbers and 7 the orange ones now question i give you back the output and i tell you give me the input can you give me the input or no no why you need you need you only keep the maximum so you you lost all the other numbers i don't know anymore the zero one and minus one that's where the red number is here because they didn't pass through the maximum so max pool is not invertible from mathematical perspective what we can do is approximate its inverse how can we do that spread it out that's a good point we could spread out the the six among the four values that would be an approximation a better way if we manage to cache some values is to cache something we call the switches we cache the values of the maximum using a matrix that is very easy to store of zeros and ones and we pass it to the unpooling and now we can approximate the invert because we know where 6 was we know where 12 was we know where 42 was and 7 was but it's still not invertible because we lost all the other numbers think about max pullback propagation it's exactly the same thing these numbers 0 1 minus 1 they had no impact in the loss function at the end because they didn't pass through the forward propagation so actually with the switches you can have the exact back propagation we know that the other values are going to be zeros because they didn't affect the loss during the forward propagation that that makes sense okay so this is max pooling on pool link on max pooling and we can use it with the switches you can approximate it why not just cache the whole original matrix yeah why don't we just cache the whole order geometry we could could cache the entire thing but in terms of back for back propagation in terms of efficiency we would just use the switches because it's enough no for i'm pulling them yeah for unpooling you're right we could catch everything but then it's cheating like you you kept it so just give it back you know okay so now we know how unpooling works let's look at the relu so what we need to do in fact is to pass the switches and the filters back to the unpool and decomp in order to reconstruct switches are the matrix of zeros and ones indicating where the maximums were and filters are the filters that i will transpose under this assumption on the board okay and so on and so on and i get my reconstruction i just need to explain the relu now i give you this input to relu and i forward propagate it what do we get all the negative numbers are going to be equalized to zero and the others are going to be kept now let's say i'm doing a back propagation through relu what do i get if i give you that this is the gradients that are coming back and i'm asking you what are the gradients after the relu during the back propagation how does the relu behave in backdrop zeros which ones are zeros the negatives are zeros do you agree the negatives in this yellow matrix are going to be zeros during the backdrop i guess sure think always about what was the influence of the input on the loss function and you will find out what was the back propagation look at this number this number here minus 2. did this number have the fact that it was minus 2 did it have any influence on the loss function no it could have been minus 10 it could have been minus 20. it's not going to impact the loss function so what do you think should be the number here zero even if the number that is coming back the gradient is ten so what do you think should be the relu backward output same idea as max peeling what we need to do is to remember the switches remember which of these values had an impact on the loss we passed the switches all these values here that are kind of a y you know this is a y all these ones had no impact on the loss function so when you back propagate their gradient should be set to zero it doesn't matter to update them it's not going to make the loss go down so these are all zeros and the rest they just pass why do they pass with the same value because relu for positive numbers was one so this number one here that passed the rally during the forward propagation it was not modified its gradient is going to be one that makes sense so this is really backward now in this reconstruction method we are not going to use relu backward we're going to use something we call relu decomponent let's say the reason we're not the intuition between why we're not using relu backward is because what we're interested in is to know which pixels of the input positively affected the the activation that we're talking up so what we're going to do is that we're just going to do a relu we're just going to do a relu backward another reason is when we reconstruct we want to have the minimum influence from the forward propagation because we don't really want our reconstruction to depend on the forward propagation we would like our reconstruction to be unbiased and just look at this activation reconstruct what happened so that's what you're going to use again this is a hack that has been found through trial and error and it's not going to be scientifically viable all the time okay so now we can do everything and we can reconstruct and find out what was this activation corresponds to it took time to understand it but it's super fast to do now it's just one path not iterative we could do it with every layer so let's say we do it with the first block of conver max pool i go here i choose an activation i i find the maximum activation i set all the others to zero i unpull relu decomp and i find out the reconstruction this specific activation was looking at edges like that so let's delve into the phone and see how we can visualize inside what's happening inside the network so all the visualization we're going to see now can be found in much new zealanders and rob fergus's paper visualizing understanding convolutional networks i'm going to explain what they correspond to but check check out their papers if you want to understand more into details so what happens here is that on on the top left you have nine pictures these are the cropped pictures of the data set that activated the first filter of the first layer maximum so we have a first filter on the first layer and we run all the data sets and we recorded what are the main pictures that activate this filter this was the main ones and we did the same thing for all the filters of the first layer and there are nine times nine of them there are a lot of them i think in the bottom here you have the filters which are the weights that were plotted just take the filter plot the weights this is do it this is important only for the first layer when you go deeper in the network the filter itself cannot be interpreted it's super hard to understand it here because the weights are directly multiplying the pixels the first layer weights can be interpretable and in fact you see that the let's look at the third one the third filter here on the first row the third filter has weights that are kind of diagonal like one of the diagonals and in fact if you look at the datas that maximized this filter's activation the feature map corresponding to this filter they're all like cropped images that correspond to diagonals that's what happens now the the deeper we go the more fun we have so let's go results on a validation set of 50 000 images what happened here is they took 50 000 images therefore propagated to the network they recorded which image is the maximum the one that maximized the activation of the feature map corresponding to the first filter of layer two second filter and so on for all the filters let's look at one of them we can see that okay we have a circle on this one it means that this the filter gener which generated the feature map corresponding uh to this has been activated through probably a wheel or something like that so the image of the wheel was the one that maximized the activation of this one and then we used the deconf method to reconstruct it any questions on that yeah the function is not removed but literally below or good question yeah what if the activation function is not really in practice you would just use a backward to reconstruct if it's 10h you would use the same the same type of method and you would try to approximate the reconstruction okay let's go a little deeper so now same layer 2 4 propagate all the images of the data set find the nine images that are the maximum activate that lead to the maximum activation of the first filter these are plotted on top here what you can see is like for this filter that is the sixth row first filter features are more invariant to small changes so this filter actually was activated to many different types of circles spirals wheels and so it's it's still activated although the circles were different sized can go even deeper up third layer what's interesting is that the deeper you go the more complexity you see so at the beginning we're seeing only edges now we see much more complex figures you can see a face here in this in this entry it means that this filter activated for when it see this when it has seen a data point that had this face then we reconstructed it cropped it on the face the face is kind of red it means that the more red it was the more activation it led to and same top nine for layer three so these are the nine images that actually led to the face these are the nine images that maximize the the the activation of the feature map corresponding to that filter and so on so here is a very fun video because can you put this sound up the network normalization layers we can switch back and forth between showing the actual activations and showing images synthesized to produce high activation he's giving his own image to the network right now by the time we get to the fifth convolutional layer the features being computed represent abstract concepts so these are the gradient for example this neuron seems to respond to faces we can further investigate this neuron by showing a few different types of information first we can artificially create optimized images using new regularization techniques that are described in our paper the one we talked about synthetic images are on fires in response to a face and shoulder this one is that they also cause the images from the training set that activate this neuron the most as well as pixels from those images most responsible for the high activations computed via the deconvolution that deconvolutionary feature responds to multiple faces in different locations and by looking at the dcom we can see that it would respond more strongly if we had even darker eyes and rose your lips we can also confirm that it cares about the head and shoulders that ignores the arms and torso we can even see that it fires to some extent for cat faces using backprop or decom we can see that this unit depends most strongly on a couple units in the previous layer con f4 and on about a dozen or so in conjunction they're trying to track back words let's look at another neurons so what's this unit doing from the top nine images we might conclude that it fires for different types of clothing but examining the synthetic images shows that it may be detecting not clothing per se but wrinkles in the live plot we can see that it's activated by my shirt and smoothing out half of my shirt causes that hack of the activations to decrease finally here's another interesting neuron this one has learned to look for printed text in a variety of sizes colors and fonts this is pretty cool because we never asked the network to look for wrinkles or text or faces the only labels we provided were at the very last layer so the only reason the network learned features like text and faces in the middle was to support final decisions at that last layer for example the text detector may provide good evidence that a rectangle is in fact a book seen on edge and detecting many books next to each other might be a good way of detecting a bookcase which was one of the categories we trained the net to recognize in this video we've shown some of the features of the deepviz toolbox and a few of the things we've learned by using it you can download it yeah so they have a toolbox which is exactly what you visualize here and you could test the toolbox on your model takes time to to get get it to run but but if you want to visualize all the neurons it's very helpful okay so let's go quickly we'll spend about three minutes on the optional deep dream one because it's fun and uh yeah feel free feel free to jump in and ask questions so the deep dream one is is implemented by google and uh the the the blog post is by alexander uh morde vincent the idea here is to generate art using this knowledge of visualization and how they do that is quite interesting they would take an input forward propagated through the network and at the specific layer that we call the dream layer they will take the activation and set the gradient to be equal to this activation the gradient at this layer and they will back propagate the gradients to the input so earlier what we did is that we defined a new objective function that was equal to an activation and we tried to maximize this objective function here they doing it even stronger they take the activations and they set the gradients to be equal to the activations and so the stronger the activation the stronger it's gonna become later on and so on and so on and so on so they're trying to see what the network is activating for and increase even this activation so for propagated image set the gradient of the dreaming layer to be equal to its activation but that propagates all the way back to the input and update the pixel of the image do that several times and every time the activations will change so you have to set again the new activations to be the the gradients of the dream layer and by propagating and ultimately you would see things happening so it's hard to see here on the screen but you would have a pig appearing here you would have like a tree somewhere there and some animals and a lot of animals are going to start appearing in this cloud it's interesting because it means let's say you see this cloud here if the network thought that this cloud looked a little bit like a dog so one of the the the feature maps was which would be generated by the filter that the text dog would activate itself a little bit because we set the gradient to be equal to the activation is going to increase the appearance of the dog in the image and so on and then you would see a dog appearing after a few iterations so it's quite fun and if you zoom you see that type of thing so you see a peak snail it's kind of a pig with a snail uh carapace camelberg dog dogfish i advise you to like look at this on the slides rather than on the screen but it's quite fun and uh same like if you give that type of image you would see that because the network thought there was like a tower a little bit you will increase the network's confidence in the fact that there is a tower by changing the image and the tower would come out and so on it's quite cool uh yeah and if you dream in lower layers obviously you will see edges happening or patterns coming because the the lower layers seem to detect an edge and then you will increase its confidence in this edge so it will it will create an edge on the image this is a fun um deep dream on a video [Music] so everything that the network thinks is something he knows [Music] and what's fun is that there's so many [Music] animals [Music] [Applause] it's too trippy i'm gonna stop it so one one inside that is fun about it is um if the network and this is not only for deep dream it's also it's mostly for gradient assets let's say we have an output score of a dumbbell and we define our objective function to be the dumbbell score and we try to find the image that maximizes the dumbbell we will see something like that it's interesting is that the network thinks that the dumbbell is a hand with a dumbbell not only the and you can see it here you see the hands and the reason is it has never seen a dumbbell alone so probably in imagenet there is no picture of a dumbbell alone in a corner and labeled as dumbbell but instead it's usually a human trying to push hard okay so just to summarize what we've learned today we are now able to answer all the following questions what part of the input is responsible for the output beam occlusion sensitivity class activation map seems to be the best way to go what is the role of a given neuron filter layer deconvolve reconstruct search in the data set what are the top images and do gradient ascent check can we check what the network focuses on occlusion sensitivity saliency map class activation maps how does the network see our world i would say gradient descent maybe deep drain the cool stuff and then what are the the implications and use cases of these visualizations you can use sales in cmap to segment it's not very useful given the new methods we have but the deconvolution that we've seen together is widely used for segmentation and reconstruction also for generative virtual networks to generate images and art sometimes these visualizations are also helpful to detect if some of the neurons in your network are dead so let's say you have a network and you use the toolbox and you see that whatever the input image you give some feature maps are always dark it means that the filter that generated this feature map by convolving over the input probably never detected anything so it's not being even trained that's the type of insight you can get okay thanks guys sorry we went over time 