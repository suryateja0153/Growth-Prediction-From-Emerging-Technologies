 I'm David I work upstairs on the brain team doing machine learning stuff today I'll be talking about some work that I have done along with a number of other people in collaboration on broadly speaking just machine learning on proteins and relating protein sequences directly to their function so before we go any further I just wanna tell you a little bit about me so I've been doing machine learning since 2009 I worked for a couple years at this place BBN technologies doing speech recognition research then I went to grad school at UMass Amherst working with Andrew McCollum doing NLP and knowledge bases and graphical models all sorts of fun ml stuff and then I came here and I decided to pivot a little bit away from NLP because I got really excited about life sciences applications in part because here in Kendall Square there's just so much excitement around that in general and also because there's just a lot of low-hanging fruit in applications of ml to the life sciences that I thought it'd be fun to engage with okay so it's just I just want to double emphasize that this work has been done by a lot of people and I'm which is one of them a lot of the credit is not due to me okay so another thing that's worth mentioning is that a lot of this work has been done in collaboration with the Google accelerated science team which some of you may not have heard of it's worth mentioning because it's really cool and so gasps seeks to just use Google technology to accelerate science and really what to focus on improving humanity not on you know product stuff and there's a lot of things that Google has unique expertise in that can help achieve these goals and so one of them is machine learning expertise the other is just scale so there's a lot of applications and biology that involve just a tremendous amount of data we're good at that we have a lot of computers here and that helps so I have something running right now that's using like 40,000 computers that's nothing like my manager tells me to use more so just a very very high-level explanation of what proteins are for the sake of this talk so proteins are makeup they're like the box of life they're encoded they're coded for in your DNA and then they're transcribed into RNA which is transcribed into C which is amino acids and amino acid you can think of I mean it's just a sequence of amino acids it looks like an NLP data and what happens is that this is a molecule that folds into a three-dimensional structure in a three-dimensional structure dictates its function and what I'm interested in is applications where we can actually measure this function in the lab and there's this talk is gonna have two parts so the first part of the talk is essentially considering this forward direction so going from sequence to function a lot of you may have heard of these classic protein structure prediction tasks so there's like this this cast prediction thing over your deep mind got a lot of press recently for doing this and that's different in that they're explicitly trying to predict 3d structure that's not something that we're trying to do in our team because we don't think it's a good way to solve the problem so we're actually trying to go directly from sequence to protein function without modeling 3d 3d structure in between mostly because we have a lot more data if we skip the middle part and then the second part of the talk I'm actually gonna flip the arrows around and I'm gonna talk about protein design work so there the idea is that you have some function that you want to optimize and you're gonna search in the space of sequences of amino acids in order to achieve that goal okay so in terms of classifying proteins I'm gonna talk about some very a very specific topic and this is a paper that I worked on recently and so we're working at one very specific data set but the modeling technique is quite general and so there's this database called PKM which is a an ontology of proteins and it's actually ontology at protein domains which are like sort of modular subsections of proteins and the idea here is that there's this underlying assumption that similarity in sequence space is associated with similarity in the space of like function of proteins and that's true in part because all these proteins came from evolution and evolution is this local search process and so there is actually a lot of sort of analysis you can do to deduce sort of function and the thing that the community has done over the years is that they've done a combination of human and machine curation so there's this data set called ppm seeds which are human verified and their you know sir carefully curated there's a small number of them and then the idea is that people have fit models on these statistical models on them and use that to expand the set of proteins by searching large databases to find things that are likely to be similar to the seeds and so you know the sort of data is available online like this you know has nothing to do with our work it's been around for a while people use it all the time and there's a lot of these colleges of protein it's not just pecan and so what we're doing actually I may skip this slide so the the classic model for this this sort of data is to use what's called a profile hidden Markov model so the idea there is that you have a big list of sequences that you think are related and oftentimes they're quite similar to each other because they came from evolution they're they're like from their siblings in the phylogenetic tree and so what we do is we align them using some multiple sequence alignment algorithm and then we stack up the aligned positions and we get these little distributions per position and we get a little transition distribution between adjacent positions and in Markov model and so what you can do is you can given a new sequence you can say well is this a likely member of this family and if so add it to the family so there's this iterative curation of these protein families by using the statistical models it's like bootstrapping of ontology of proteins and so the question is can we use neural networks to do this differently and so you know hmm have been replaced by deep neural networks in speech recognition which is something I experienced firsthand on my earlier life and perhaps we could do that for for pet genes as well and this highlighted sentence here from an article last year is is very very important so being able to curate these proteins is hugely important because Nature has given us a lot of proteins with interesting properties especially in bacterial genomes and the cost of sequencing has gone down so much that we just have so much data the problem is that we can't annotate that we don't have statistical models that are able to generalize sufficiently far from the data we've seen so far to actually create this sort of categorization and this is like a sort of a fundamental Google style problem right like our goal is to organize the world's information and there's a lot of information in the world's proteins right there's hundreds of millions of them but we don't know any like what they're doing and so this guy max on the team was like you know what this is basically just a computer vision problem right it's like you want to do some image classification or some image segmentation and image image captioning and like the nice thing about deep learning is that it's it's like extremely application agnostic right it's like you have some inputs you have some outputs you have some hundred men tional vectors in between and what we're you know this is overall seem in machine learning these days so basically like skipping hard-coded features and just learning everything end to end and this sort of flies in the face of a lot of wisdom in the protein world where there's a lot of emphasis on the secondary structure so these like small sort of sub pieces of three-dimensional structure that inform the confirmation of the overall structure and we're basically completely avoiding that and so we use basically here this is just to say that we have a classification task with a very large number of output classes like 17,000 and it's extremely heavy tailed the distribution over sizes so it's just like a classic cardigan classification problem we're just gonna throw at this like a very generic neural network it's a comp net with residual connections and dilated convolutions and that's sort of a standard machine where workflow we get a plot of results like this this is gonna I'm gonna unpack this this takes a little bit of time and and you know some things will come out so the basic idea here is that we're contrasting our models so there's this Prout CNN pro tienen don't worry about the difference between them they're both neural networks and then there's this blast P and P Homer which our nearest neighbor classifiers and so there's been a lot of work on very application specific similarity functions between proteins and so you can do nearest neighbor searches and these are really slow because you're often matching on to a database of like 100 million proteins or something but they work well in practice and you know there's websites like Pulaski org or whatever that people use another natural baseline is this topic hmm thing which is a profile hidden Markov model which is like a modeling technique that the community is you know deeply entrenched in and so when we first put out results on the bio archive with this paper there was a lot of pushback from people that had built their careers around using profile hidden Markov models and so we've had to do extremely careful evaluation in order to demonstrate to them that these neural networks have a place at the table and so we're doing here is something like an evaluation procedure that I want to talk about in detail because suppose that you are never gonna think about proteins again but you are interested in machine learning I think the way that we evaluated this model is like a general takeaway that you might want to use in your own personal work which is that rather than just taking you know we have five models and we have some accuracy score and we could have a table with five numbers and we could show that our model is best we actually don't do that we stratify our analysis by bucketing each test example like by bucketing the test examples in terms of how far they are from the training set so on the x axis here we have the similarity of a test example from the train set so the idea is that more to the right means you're more similar to the train set and not surprisingly the error rate goes down as you are more similar to the training set but this is really important because the name of the game with machine learning is is extrapolation right it's it's how far away can you get from the training data right and understanding the shape of this curve can have huge impact on where you trust your model right and models might have different trade-offs where they're actually extremely good close to the training data but don't do well far from the train data and stuff like that or vice versa like they might have different shapes and so this is really important for proteins because the sort of gold standard task is this thing called remote homology detection which is you know what like classifying things that are close to what we already know is pretty boring but finding remote homologs which you're basically cousins are really far in the phylogenetic tree but are similar functionally is super important for like this sort of information retrieval workflow where a practitioner wants to engineer some new protein they want to find something that's similar to something we've seen before but like the sequences are actually quite different but some model tells them that they're similar and so the left hand side of this plot is what everybody cared about and this is how we really got buy-in from other professors and stuff like that that you know the stuff is worth taking seriously so this is sort of stratified analysis of your test set I think it's super important clearly you could do this with natural English data for example and so then this is just zooming in on the weapons out of that plot this is like the sort of standard table that I'm saying you shouldn't do and you should do the more sophisticated analysis but it's worth noting that we're the best and then we had this experience where the the people in the community that were skeptical of our results were like no you need to make the data set even harder what you need to do is have this very structured relationship between train set and test set so you can really probe it for this remote homologue question so we did there is another thing that I think is a general technique that you should consider which is we clustered our data and we put certain clusters in the train set and certain clusters in the test set so this is a technique that occurs a lot and these complications and the reason is that because your data came from evolution if you just naively split it in the train and test data there will be test examples that are one mutation away from a train example and so you have to be really careful and so basically what this gave us is more statistical power at the left inside of that of that plot because we just have more examples that are far in the test side and we still do better so that's cool okay another thing that we did is we took our model we trained it we chopped off the top and we just use it as a mapping from protein sequences to thousand dimensional vectors just like an embedding function and the idea is that the this vector space like sort of Euclidean structure in this vector space should capture some semantics of neighborhood structure in protein sequence space and so here I'm actually looking at models that were trained on a different set of data these enzyme commission numbers and we show that which are hierarchical and we show that our models embedding space when projected down to two dimensions has some consistencies with the overall hierarchy that has been annotated which is exciting mister thing is a great way to get buy-in from scientists and then another thing you can do is actually if you have this mapping from sequences to vectors you can actually use it for what we call few shot learning which is that you can consider classes that test time that you never actually saw during training time and you can do some setting nearest neighbor classification which would be helpful right because you might have this like sort of growing set of labels you want to predict this is true and all sorts of applications okay so another thing that we found was really important to get buy-in from biologists is to show that our model is making predictions for the right reasons and this is really hard in general and we can do something that's like you know a step in the right direction which is the sir that our model has behavior that's consistent with some biological phenomenon that the community has accepted to be true so on the right we have this thing called the bottom sixty-two matrix which is a matrix of pairwise substitution affinities it's basically like if I were to change certain letters a certain a certain amino acids a certain other amino acids it might not be that damaging a mutation because they're sort of like structurally or chemically similar and this just comes from observational data from evolution on the right on the Left we take the intermediate activations of our model and we look at cosine similarity of these things in embedding space and what we find is that we actually recover extremely similar neighborhood structure so this is cool because there's this like sort of biological invariant that we also maintain so one question is why are these models useful and obvious answer is well we want to cattle class ax Phi things like I have a new protein sequence so I you know have some bacterium I found in a pond and I sequence it and I want to know what the function that bacteria is another thing we can do is we can actually ask we can use the model as a surrogate for experiments to perform intervention so we can say like if I were to have this mutation what would happen what would be the functional effect of this mutation and this is like you know the really important question that will help drive things like drug development right and so on the only thing you can do is there's what's typically called saturation mutagenesis which is you just change you just consider every single possible single site mutation of a protein sequence and you do this in vitro you like actually run an experiment what we're doing is we're doing this in silico which means that we just feed multiple sequences into our model with little mutations and what we're showing here and this is not particularly compelling yet is that the models sensitivity to certain mutations is consistent with certain known patterns of secondary structure in the protein enabling these alpha helixes but this sort of using the model as a surrogate like as a surrogate for the experiment in order to prune away gospel hypotheses is helpful and certainly like you know this sort of thing gets and gets that practice okay we've also done some work on interpretability I'm gonna skip it in the interest of time we've also done some work on actually segmenting proteins into subsections that are like sort of like functionally modular subsections which is really important and then what I want to do is spend more time talking about this which is actually protein design which is going from a functional measurement to a sequence I want to design sequences that maximize some function and I I just realized I missed an author Lucy Coe well who's a professor at University of Cambridge was instrumental in this work as well okay so there's this really cool thing called an AAV capsid so AV which I don't think I wrote stands for oh my god I'm so embarrassed I adeno-associated virus I always forget this it's not a particularly interesting name and the idea is that it's this big molecule that's actually 60 wittle proteins that stick together in this very specific conformation and it's like this big capsid thing that floats around in it in it can deliver cargo like it's a virus and so what's neat about though is that it actually the immune system like just doesn't really care about it it's like invisible to the immune system and so it's used as a vector for delivering all sorts of gene therapy the problem is that we want to make it really specific so we want to say I want to deliver gene therapy to lung cells for example and so the goal is to design new versions of AAV that are stable in a sense that they have the same conformation or similar confirmations but have a specificity to only bind to certain cells so just very quickly so at the bottom here we have this like you know three-dimensional structure it's actually 60 copies of this little protein stuck together the genome at the top of this virus is like on the left and blue is just like some general lifecycle machinery for then on the right is this thing that we can actually edit so this is a sequence of amino acids that we're gonna start messing with and the idea is that this is just the building blocks that are gonna fold into this little protein and we're gonna start changing it hopefully such that it becomes more specific in terms of what it binds to but the immune system still ignores it so there's a lot of details in terms of how to actually make edits to this protein and this is why we are collaborating with wet lab people that know how to do these things it's quite sophisticated it's not like you know I just have a CSV file and then magically I get a number back right and so basically what you're doing is you're you're printing these like these chips with these tiny little mutations and then there's a sort of like you're using some transcriptional machinery to pulling these changes on to existing protein and stuff like that it's cool and what we're measuring here is we're not actually measuring specificity which is you know binding specific cells all we're doing is we're trying to get a diverse set of Av capsids they're all structurally stable so the idea is that there's like two rounds of protein design the first is to get a set of candidates that we think are cool using a cheap experimental procedure and then once we have a small set of candidates and then we're gonna do an expensive thing that involves like actual specific cell lines and stuff like that and so the name in the game here is not just to find you know it's not we're not actually maximizing a function it's more like this find a diverse set of sequences of amino acids that all satisfy some constraint and the way that you do this is that you just do some basic like sequencing of DNA or proteins and so you you're just measuring basically like the amount in over the amount out and by amount out I mean like how many of these things actually like you can screen things they're actually folded versus we're just like junk that was flying around floating around and and that's the experimental procedure and so the way it works is that we first do some single site insertions and substitutions to these sequences in order to sort of probe initially like what what matters and then it's very simple statistical model is that these things are additive so like the sort of effect of multiple changes to a sequence is just the sum of its parts which is a bad statistical model but it's a good place to start and then what we do is we use this to reason about the quality of new sequences and then we make them in the lab and we measure them and here what we're plotting is the relationship between how far the sequences from like the known AAV that occurs in the wild from whether it folds and it drops off really really fast and this is problematic because the whole name of the game is finding a diverse set of things that that fold and so what we want this curve is to you know go out much wider and so the question is can we use machine learning to do this and so we have a variety of different ways to collect data the problem is that oftentimes the data is biased because it was collected by some smart grad student that had like some ideas in their head about how to design proteins but it's not sort of like a statistically it's a nice thing you can sort of like reason about statistically train a bunch of neural network models just like bread and butter machine learning we then do some optimization of these models and so the idea is that we fit the model to the data we now pretend that this is a surrogate for the actual underlying function and then we just optimize these models with respect to their input this gives us a bunch of sequences and then we get as we get these curves which show that we can actually ignore the top row for now the bottom row shows that we can actually get way farther away from the starting sequence while maintaining the property that the protein folds if we use the machine learning model to guide us to high quality sequences and so this is really exciting also because the sequences too aren't just far from the starting sequence they're actually diverse from each other so this is the original curve and this is what the curve looks like now there's a lot of details in terms of how you collect your data and this matters a lot I'm in general for your machine learning applications this is something you should worry about a lot there's all sorts of biases that can be in your data when it wasn't sampled you know randomly then you have to correct for this but this is really exciting and the next up of course is to detect which of these have you know specific binding to certain cells great so I format left I'd love to take questions about you know various relationships between the life sciences and she learning [Music] you 