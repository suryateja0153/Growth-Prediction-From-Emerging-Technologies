 In order to train the parameters of logistic regression we need to define a cost function. So just a brief recap of what we saw in the previous video. We calculate the Weighted sum of this input and we add to it bias, and we get a value which we will call Z and This Z in particular Z(i). So here this Superscript (i) denotes i-th training example is this term W-transpose X(i) which denotes i-th Training example + bias So here it will denote this and this will be the prediction for i-th example and here Z(i) is given as 1/ (1 + e^-z) and we are given a training set of of m training examples from 1 to m and we want That the prediction for i-th Training set is very close to the actual label value of y. We will now define the loss function or error function. This loss or error function is a measure of how well we are doing on a single training example and Here you see single training example and not the entire training set for that. We will separately see cost function So first, let's understand loss function. So what can be a good loss function? one loss function you can think is that You find the squared difference So this is for one given example, so if you're talking about let's say i-th Value then this will be (i). Everything you can replace by superscript (i). so for i-th training example we see what was the Actual labelled data and what was the predicted value we find the difference of that we square it and we do half of that But this is not used in logistic regression. Although it's a good loss function So it's not used in logistic regression because when you come to learn the parameters you find that The optimization problem which we will talk about later becomes non-convex So we end up with the optimization problem with multiple optima And gradient descent may settle on one of the local minima for example, let's say here and It may not discover the global optimum. So that's why we will not be using this loss function in logistic regression. So in logistic regression what was function we will use we will use And here we are talking about single training example, so I will omit the superscript Negative of Y the actual label log Y-predicted + (1- y) Log(1 - y-pred) And we always want the loss function to be as small as possible So this entire quantity should be as small as possible. So let's say the actual label is y Actual label should be y. so what we will want. We will want that Predicted values would also be close to 1 and this Y is Calculated using this sigmoid function and we have seen that the sigmoid function Always takes values between 0 & 1 so it can never cross 1 and it can never be less than 0 In the middle, it's 0.5 so it can never Cross one. In fact, it's very difficult also to get exactly one. It's not possible It tends towards one, but it's always slightly less so Our aim will be that in such case For example y equal to 1 denotes the images of cats then in that case. We will want that this predicted value Should be close to 1 So, let's see how this loss function works here. So if Y is 1 what will happen This becomes 1 - 1 which is 0. So this term goes away and we will be left with Loss equal to -y log y-pred So when y becomes 1 this is the scenario, Let's see the graph of this so we know that log Y will be this value at 1 log of 1 is 0 log of Less than 1 quantity is negative and at close to 0 its close to negative infinity. So this is the plot for Log Y We are taking minus of that so reverse it so it will become like this And this is 1 So you see here it's close to infinity this loss function loss function is on y axis which which is minus log y This red line is the actual loss function and close to 0 it's very high very very high close to infinity and it decreases decreases and At 1 it's exactly 0 It cannot be more than 1 due to this sigmoid function. So it will try to reach as close to 1 as possible This predicted value of y in order to minimize the loss So see this works. Well, so if actual level is one this loss function forces it to Make Y-predicted also close to 1 in order to minimize the loss now. Let's see the other scenario when y = 0. Then the first term goes away Zero multiplied by anything is zero. And this is zero. So this is 1 - 0 so we are left with So it becomes - log of 1 - y-predicted. and here also this y is not required. It's 1 so this was log Y which was the loss function Now let's plot for this so Let's first plot 1 - log y so when why he had becomes 0 this will be 0 So at 0 its 0 at 1 it's close to negative infinity So it's something like that This log 1- y but we are taking negative of this so this will become like this And it will continue in this direction We will not consider that because we are limiting to 0 & 1 This is the loss function L then Actual level is 0 this was the loss when L was 1 actual level was 1 So you see close to 1 it's very large value close to infinity At zero it's exactly zero so this predicted value of y will Try its best to reach to version 0 in order to minimize the loss function So we see this loss function works for both the scenario in both cases this loss function enforces the model to the logistic regression to Make its predicted value close to the actual label value now, let's look at the cost function so cost function is how will an estimate of how well we are doing on the entire training set and not the single training example, which was the case with loss function and it Loss function applies to a single training example, but cost function is a cost of your parameters and our parameters are W and b So we will denote the loss function by this capital letter J and It will be a function of our parameters So the job of cost function is to or train these parameters in such a way that the cost function is minimum So after one Epoch, you can think them all the parameters are updated. And again, the cost function is calculated and Again, one pass through the training example. And again, these are updated until the cost function is minimized. So We will define it as sum of all the training examples So what we will do we will just add the loss function And i is from 1 to m. m denotes the number of training examples so for each training example, we will compare this and we will Add and take the average. This will be our cost function So what was this loss function we can expand it It was y(i) log Y-predicted(i) + (1 - y) Log(1 - y-predicted) So this will be our cost function and Our job will be to find parameters W and b which will minimize the cost function J So we saw how to define loss function and cost function for logistic regression. In a way This logistic regression can be thought of as a mini neural network And not a deep neural network, so we will see more about neural networks in future lessons. 