 To support the production of more high quality content consider supporting us on Patreon or YouTube membership. Additionally, consider visiting our parent company, Earth One, For Sustainable Living Made Simple. In videos past with this deep learning series, we have gone from learning about the origins of the field of deep learning, to how the structure of the neural network was conceived. Along with working through an intuitive example, covering the fundamentals of deep learning. The focus of this video then will be to tie up many of the loose ends from those videos and really delve into some of the complexities of deep learning. When we first started working on our image pattern detecting neural network, we set up an idealized system. In other words, all the weights in that network were perfectly set so the network could detect patterns right away. Now, as we stated previously, this was done for the sake of explanation. So we can initially see how a deep learning system builds layers of representation. In the following video, we reset the weights, all 181 of them for our example, to see how this representation was built. As an actuality, the process of getting the right combination of weights is what the learning and deep learning means. By this meaning, the combination of the problem representation space, coupled with the evaluation of a cost function from that space, and then optimizing the values through gradient descent and propagating them back through the network until we reach a desired point where our system is accurately identifying patterns in the input image. However, even with the level of detail we have gone into in these past videos on deep learning, there are still many concepts we didn't cover and made assumptions and generalizations about. The first of these that I want to discuss that we didn't cover in the past is the bias parameter in neural networks. This is not to be confused with the bias results from a neural network, which is a whole entire topic of its own for a future video. Coming back on topic, in these past videos, we've stated that the output to a node is all the connections into it multiplied by their respective weights, and then summed up. In actuality it is still that, but with a term added to the end, the bias. For simplicity of explanation sake, we didn't mention this before, but just as with the weights, the bias is another parameter that must be tweaked in order to learn representation. Our neural net without the bias parameter had 181 Wade parameters. However, what the bias, we would add another 25 parameters, one for each node in every layer except the input. Leading to a total of 206 weight and bias parameter combinations. So then, now understanding that the bias is another parameter, what purpose does it serve? As stated in earlier videos, nodes activate strongly when a certain threshold is met. The bias parameter acts as a jumpstart so to speak, to reach that threshold value. In more mathematical terms, the bias is a Y intercept to a linear equation what the weight being the slope. With a larger positive bias our node is closer to activating and needs less weighted values to do so. And with a negative bias, it needs more weighted values to activate. This is why this parameter is called a bias. It sets certain nodes to be biased to activate more or less easily. Viewing this graphically with the same simple network as a last video, but now with the bias terms, you can see that the bias sliders translate our output decision boundaries up and down. In other words, the intercept of our decision boundary function, whereas the weights affect the slope. To visually understand more about the effects of these parameters, I highly recommend this video by Sebastian Lague. Now, speaking of parameters, this leads us to our next talking point, parameters versus hyperparameters. A neural network parameter is a variable that is internal to the model and required for building representation for a problem. These variables are required by the model for its predictive capabilities and are estimated or learned from data. Some examples of model parameters includes the weights and biases and neural networks, the support vectors and support vector machines, the coefficients and regression formulas and so on. A model hyperparameter on the other hand is a configuration or variable that is external to the model and whose value cannot be estimated from the data. These variables, however, are very important and ensuring the model can correctly predict and optimize. There is no best way to find these hyperparameters for a given problem. And to do so one must use rules of thumb, copy of values from other similar problems, search for the best values and configurations by trial and error, et cetera. Hyperparameter tuning and optimization is a whole sub field in and of itself and different techniques are needed for different parameters. In our example, I just assume these parameters for the sake of explanation. From the many types of hyperparameters, the ones I assume for our pattern recognition example was a number of nodes in each hidden layer, the number of hidden layers and the activation functions for those layers. These, while sounding simple, can have huge impacts on an actual network. For our case, we just assumed they would work but in actuality, there are so many things to take into consideration. For instance, certain activation functions are harder to train for and can also lead to issues like the vanishing and exploding gradient problem, which have huge impacts on the output. These issues are beyond the scope of this video, but mentioned to satisfy curiosity. Other hyperparameters we will mention but not divulge into include the number of clusters in K-nearest neighbors, the number of latent factors in matrix factorization, the kernel size in convolutional neural networks, the code size in auto encoders, the list can go on and on. Now one hyperparameter I do want to spend a bit of time discussing is a step size, also called the learning rate. As I hope you can recall from previous videos in this series, this is a rate at which our optimization algorithm approaches a minimum. The value of this learning rate has huge implications for the representation a neural net will build. Let's view this on a plot of error versus epoch, with an epoch being the number of gradient descent iterations. Ideally with a well-selected learning rate, the error for each iteration will initially drop off fast, then hit an inflection point referred to as the plot elbow and begin decreasing slowly to an optimal solution. To find this idea of learning rate takes a lot of work, but is needed in order for our neural network to function properly. For a simple 2D one-way case, as shown here, one such solution to get the step size might be to make it proportional to the slope. So with a steep slope, we have a larger rate and as a slope flattens the learning rate decreases until stopping at the minimum. To further discuss the implications of the step size, if this learning rate is too low, then it will take a very long time to reach a minimum. In real-world terms this is very costly as a waste of a lot of computational power. On the other hand, if the rate is too high, then while initially the air will decrease quickly, it will plateau at a non-minimum value. Additionally, with a low learning rate, we endure the risk of overfitting to data. And with a high rate, underfitting. Underfitting is when our model doesn't describe the data at all or very inaccurately. In other words, no correlations or patterns are found, the model didn't land in a minimum. With a high learning rate this makes sense as initially the rate will help to get to a lower error, but then due to the large rate, it prevents it from settling into some minimum. This can lead to oscillations around a minimum value, but never landing on one. On the other hand, overfitting is when our model becomes too rigid. In other words, it can account for new data, once it lands on some optimal solution, it believes its truth, it won't deviate from it. This makes sense when considering a low learning rate, as it moves very slowly towards some solution and would either take forever to respond to new data or not be able to at all. To view an example of overfitting, consider us feeding our randomness pattern into our system. We would expect either no nodes to light up or all of them to light up evenly, signaling ambiguity by the system. An overfit system would describe a single output note to that random input, even though it was wrong. Over and underfitting are both in part issues caused by step size, but a much larger problem in feature engineering, our next talking point. Feature engineering is a subfield of machine and deep learning in which one must pick the input features to a network that accurately described the problem at hand and would lead to meaningful results. Essentially feature engineers must analyze a signal to noise ratio of a problem and the potential features that would describe it. In other words, what features actually contribute to strengthening the signal and what others are just noise and mean little to nothing to the problem? Just as with hyperparameters, there is no 100% correct way to do this. And one must rely on a variety of techniques beyond the scope of this video. For example, if building a network that could price a house, things like proximity to schools, how size, number of bedrooms, et cetera, would be good features. We've just listed a few, but in actuality, there could be tens to hundreds of features. There would also be many upon many features that have no impact at all, or varying amounts of impact. Feature engineering requires immense knowledge of how a network behaves and what your desired output is for your specific problem. And as such, is very difficult. There are also a variety of problems associated with picking too few features, which are going to lead to underfitting where the model is too general. And too many features, which leads to overfitting where the model is too specialized and too brittle to respond to new data. Adding too many features also contributes to the curse of dimensionality, which as stated in our video on unsupervised learning as due to the correlation between features and the dimensions needed to represent them. In general, for every feature you have you acquire the same number of dimensions to represent them. So then, the curse of dimensionality is when with many features, the data becomes too spread out over the decision space and it becomes hard to impossible to find any meaningful patterns in the data. Now, in that unsupervised learning video, we discussed some ways to overcome this curse, such as the manifold hypothesis. Beyond manifold theory, other complex fields of mathematics are also involved. Such as game theory, knot theory for entangling data, so to speak and many more. And then obviously one of the biggest tools for these many feature-having problems has been deep learning. Rather than manual, done-by-hand feature selection, as we've seen throughout the videos in this series thus far, a deep learning model's internal representation is able to extract features from large data sets, which otherwise may have succumb to the curse of dimensionality. This automated feature extraction can then be supplemented with more precise feature selection for machine learning engineers and data scientists, allowing the ability to accurately solve and predict a wider scope of problems. Now, while able to solve a much wider berth of problems, deep learning can't solve everything. And there are still issues with the cost function optimizing for the wrong patterns and features. We will discuss more about the types of problems deep learning will excel at and the issues that has in our upcoming videos in this AI series. For example, one such issue is how most real world problems are non-parametric. Non-parametric problems can have a fluctuating number of features. This leads to vast amounts of futures and an increasingly difficult time landing on a meaningful solution. As I hope you can see now, on a high level it sounds simple to talk about neural networks and deep learning, but in actuality, there's a lot of engineering that goes into these systems. And we didn't even talk about topics, like gradient boosting and so on. The media and overhyped with videos and blogs would have you believe that the things these deep learning systems achieve is magic and will soon evolve into some form of general or superintelligence. While truly remarkable, it is no feat of magic and takes a tremendous amount of engineering and design to be able to derive meaning from data. Yes, once a model is properly tuned, it can find amazing patterns and derive conclusions from data with little to no human intervention. But to get to that point requires a great deal of intervention from many talented individuals and groups. Coming back to our example, as stated multiple times by now, we made some broad generalizations about how artificial neural networks should work versus how they wouldn't actuality. By this, I mean in theory, we'd hope our network would arrange itself in perfect layers of abstraction, first finding lines and then combinations of lines to form images. But in actuality, it is a lot more random. 3Blue1Brown's video delved much deeper into this and is a great watch to see the mathematics of why this is. One big reason why this is, is due to one of the most important hyperparameters to choose, the type of network or networks to use for a problem. The network we have used in our example is a feed forward neural network. Essentially the same type of network from the eighties and chosen because it is simple to understand when first learning about deep learning. In actuality, many types of neural networks have since come to fruition that would be much better suited to solve this problem among other problems. We will be discussing these networks in the upcoming videos in this deep learning series. If you want to learn more about deep learning and I mean really learn about the field from how these artificial learning algorithms were inspired from the brain to their foundational building blocks, the Perceptron, scaling up to multi-layer networks, different types of networks, such as convolutional networks and recurrent networks, and much more, than brilliant.org is a place for you to go. Now, what we love about how the topics in these courses are presented is that first an intuitive explanation is given and then you're taken through related problems. If you get a problem wrong, you can see an explanation for where you went wrong and how to rectify that flaw. In a world where automation through algorithms will increasingly replace more jobs, it is up to us as individuals to keep our brains sharp and think of creative solutions to multidisciplinary problems. To support Futurology and learn more about Brilliant, go to brilliant.org/futurology and sign up for free. Additionally, the first 200 people that go to that link will get 20% off their annual premium subscription. At this point, the video has concluded. We'd like to thank you for taking the time to watch it. If you enjoyed it, consider supporting us on Patreon or YouTube membership to keep this brand growing. And if you have any topics suggestions, please leave them in the comments below. Consider subscribing for more content and check out our website and parent company, Earth One, for more information. This has been Ankur. You've been watching Futurology and we'll see you again soon. (upbeat music) 