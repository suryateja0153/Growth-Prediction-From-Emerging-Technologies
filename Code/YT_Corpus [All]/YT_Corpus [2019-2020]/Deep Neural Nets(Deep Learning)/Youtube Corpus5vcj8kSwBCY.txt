 Okay. So I'm delighted to introduce, um, our first lot of invited speakers. And so we're gonna have two invited speakers, um, today. So starting off, um, we go and have Ashish Vaswani who's gonna be talking about self attention for generative models and in particular, um, we'll introduce some of the work on transformers that he is well-known for along with his colleagues. Um and then as a sort of, um, a special edition then we're also going to have Anna Huang talking about some applications of this work. There are actually at least a couple of people in the class who are actually interested in music applications. So this will be your one chance in the course to see music applications of deep learning. Okay, um, so I'll hand it over to Ashish. Thanks, Chris and, uh, thanks, Evie. Uh, Anna is actually here to make the class less dull. So [LAUGHTER] she's the highlight on this one. So uh, so, uh, hi everyone. Um, um, uh excited to be here. This is a very large class. Uh, first invited speaker, no pressure, so hopefully this will all go well. Uh, so yes, so the talk is going to be about, uh, self attention. Um, and so the purpose is, is not going to be just to talk about a particular model, but, as, as, as, as empiricists and, and, like, well, I'm an empiricist and I consume machine learning to apply it to various tasks. And, and, and, well, starting point always is to ask this question, you know, what are the- what's the structure in my dataset or what are the symmetries in my dataset, and is there a model that exists that that's a very good- that, that has the inductive biases to model these properties that exist in my dataset. So hopefully, over the course of this, uh, this, this lecture Anna and I will convince you that, uh, self attention indeed does have some- has the ability models and inductive biases that potentially could be useful for the problems that you care about. Um, so, um, this talk is going to be our learning representations primarily of, uh, variable length data where we have images but, uh, most of it is going to be variable length data. And, uh, and, and, and all of us care about this problem because we- in deep learning, and deep learning is all about representation learning. And if- and building the right tools for learning representations as, as, as, as sort of- is an important factor in, in achieving empirical success. Um, now, uh, the models of choice, the primary workhorse for perhaps even now and or up to this point had been recurrent neural networks. Um, um, how, how many people here are familiar with RNNs? [LAUGHTER] Okay. So definitely up to this point, the primary workhorse have been recurrent neural networks, and some of the more, uh, some, uh, some gated variants that explicitly add multiplicative interactions like LSTMs, they also, they also have mechanisms that allow for better gradient transfer. And some recent variants like gated, uh, recurrent units that are simplification, they're kind of the- they're- they dominate this, this recurrent landscape. Um, and typically how did recurrent neural networks, uh, learn or, um, produce representations? They consume a string or a sentence, um, even an image, imagine, you know, in a particular- in sequentially and, uh, at each, at each, uh, position, at each timestep they produce, they produce a, a continuous representation that's summarization of, of everything that they've actually crunched through. Um, now, so in, in, in the, in the realm of large data, uh, par- having parallel models is, is quite, is quite beneficial. In fact, I was actually reading Oliver Selfridge. Uh, he was a, he was a professor at MIT and, uh, he had this, uh, sorry, he wrote the precursor to deep nets its it's called Pandemoniums. I would recommend everybody to read it. And he has this fascinating note that, you know, if you give me more parallel computation, I'll just add more data and make it slower. So you can consume more data. Um, and, and recurrence, uh, recurrence sort of just by construction, um, limits parallelization because you have to, you have to wait until- your wait un- for a particular time point to produce a representation. Um, but if there's any questions, please raise your hands, I'll hopefully look around and, and, uh, be able to attend to your question. Um, and again, and, and now because we're actually producing these representations, we're sort of summarizing, you know, if you want to pass information, if you want to pass co-reference information, then we kind of have to shove all of this inside this fixed size vector, so it could potentially be difficult to model. And, uh, while they have been successful in language, uh, explicit they don't have- the architecture doesn't have a very clear explicit way to model hierarchy which is, which is something that's very important in language. Um, now, um, so they have been devin- it has been excellent work of, a precursor to self attention that actually surmounted some of these difficulties. And what were these difficulties basically is a convolutional sequence models where you have these limited receptive field convolutions that, again, consumed the sentence now not, not sequentially but in depth. And they produce representations for every- they produce representations of your variable length sequences. Um, and, uh, they're trivial to parallelize because you can apply these convolutions simultaneously at every position. Each layer is trivial to parallelize. Uh, the, the, the serial dependencies are only in the number of layers. Um, you can get, uh, you can- you can get these local dependencies efficiently because that a single application of a convolution can consume all the information inside its local receptive field. Um, now if you want to have these really long distance interactions while you don't have to pass through a linear number of steps, you still because these, because these receptive fields are local you might need something like linear and depth or logarithmic if you're doing something like dilated convolutions. So there's still need- the number of layers that are needed are still a function of the length of the of, of your string. Uh, but they're a great development and they actually pushed a lot of research like WaveRNN, for example, is a classic sort of success story of convolutio- convolutional sequence models even by net. Um, now, so far attention has been like one of the most important components, the sort of content-based, you know, memory retrieval mechanism. And it's content-based because you have your decoder that attends to all this content, that's your encoder and then just sort of decides what to wha- what, what information to absorb based on how similar this content is to every position in the memory. So this has been a very critical mechanism in, uh, in neural machine translation. So now the question that we asked was, like, why, why not just use attention for representations and, uh, now here's what sort of a rough framework of this, this representation mechanism would look like, uh, the way- just sort of repeating what attention is essentially. Now imagine you have- you want to represent the word, re-represent the word representing, you want to construct its new representation. And then first, uh, you, you attend or you, you compare yourself, you compare your content, and in the beginning it could just be a word embedding. Your compare content with all your words, and with all, with all the embeddings and based on these, based on these compatibilities or these comparisons, you produce, uh, you produce a weighted combination of your entire neighborhood, and based on that weighted combination you, you summarize all that information. So it's, like, you're re-expressing yourself in certain terms of a weighted combination of your entire neighborhood. That's what attention does, and you can add feed-forward layers to basically sort of compute new features for you. Um, now, um so the first part is going to be about how, like, some of the properties of self attention actually help us in text generation, like, what inductive biases are actually useful, and we empirically showed that indeed they, they move the needle in text generation. And this is going to be about machine translation, but there were other work also that we'll talk about later. So [NOISE] now with this, uh, with this sort of, uh, with this attention mechanism you get this- we get a constant path length. So all pairs or a word can in- position can interact with any position, every position simultaneously. Um, hopefully if the number of positions is not too many. Uh, attention just by virtue of, like, it's a construction, you have a softmax, you have these gating and multiplicative interactions. And again, I'm not gonna be able to explain why, but it's, it's interesting, like, you've seen these models, like, even, even the, uh, even Pixel, PixelCNN, uh, or, um, when it was actually modeling images, they explicitly had to add these multiplicative interactions inside the model to, to basically beat RNNs, and attention just by construction gets this because you're, you're multiplying the attention probabilities with your, with your activations. It's trivial to parallelize, why? Because you can just do attention with matmuls, especially the variant that we use in our paper, uh, in our work. And, uh, so now the question is convolutional sequence to- convolutional sequence models have been very successful in, in, in, in ge- generative tasks for text. Can we actually do the same or achieved the same with, uh, with, uh, attention as our primary workhorse for representation learning. Um, so just to sort of add some context and there's been some, there's been some- up to- up to the transformer there have been a lot of great work on using self attention primarily for classification within. There was, there was work on self attention within the confines of, like, recurrent neural networks. Um, perhaps the closest to us is the, is the memory networks, uh, by Weston, Sukhbaatar, where they actually had a version of recurrent attention, but they didn't have, uh, but they didn't actually- empirically, they didn't show it to work on sort of conditional modeling, like, uh, translation and their mechanism was, uh, like, they were using sort of a fixed- they were using a fixed query at every step. So there's- it, it leaves something to be desired. They still had this question, is it actually going to work, um, on, on, on large scale machine translation systems or large-scale text generation systems. So this is sort of the, the culmination of, um, of the, the self attention, our self attention work. This is the tran- the- and we put it together in the transformer model. And, uh, so how does this look like? So we're going to use attention pri- we're going to use attention primarily for computing representations so- of your input. Imagine you're doing English to German translation. So you have your words, and notice that, uh, attention is, uh, permutation invariant. So you just change the order of your positions. You change the order of your words and, and, uh, it's not going to affect the actual output. So in ord- in order to maintain order we add, we add position representations. And, uh, there's two kinds that we tried in the paper, these, these fantastic sinusoids with no entropy invented. And we also use learned representations which are very plain vanilla both of them work equally well. Um, and, uh, so, so first we have- so the encoder looks as follows, right? So we have a self attention layer that just recomputes the representation, uh, for every position simultaneously using attention, then we have a feed-forward layer. And we also have residual, residual connections and I'll, I'll sort of give you a glimpse of what these residual connections might be bringing that is between every, every layer, and the input we have a skip connection that just adds the activations. Uh, and then this tuple of, uh, self attention and feed-forward layer just essentially repeats. Now, on the decoder side, uh, we've- we, we have a sort of standard encoder decoder architecture. On the decoder side, we mimic a language model using self attention, and the way to mimic a language model using self attention is to impose causality by just masking out the positions that you can look at. So basically, uh, the first position it's- it can't look forward, it's illegal to look forward. It can look at itself because we actually shift the input. Um, so it's not copying, uh. It's kind of surprising that parti- with these models, it's very easy to copy at one point, when early on it was even harder to ge- you know, do copying with recurrent models. But now, at least, you can copy really well, which is a positive sign, I think overall. Um, but, uh, so now on the decoder side, uh, we have, uh, we have this causal self attention layer followed by encoder-decoder attention, where we actually attend to the, uh, last layer of the encoder and a feed-forward layer, and this tripled, repeats a mul- a few times, and at the end we have the standard cross-entropy loss. Um, and, um, so, um, sort of, staring at the- at, at our parti- at the particular variant of the self- of the attention mechanis- mechanism that we use, we went for both- we went for simplicity and speed. So, um, so how do you actually compute attention? So imagine you want to re-represent the position e2. And, uh, we're going to first linearly, linearly transform it into, uh, a query, and then we're gonna linearly transform every position in your neighborhood or let's say every position at the input because this is the, uh, uh, the encoder side, to, uh, a key. And these linear transformations can actually be thought as features, and I'll talk more about it later on. So it's like- it's, it's basically a bilinear form. You're projecting these vectors into a space where dot product is a good- where just a dot product is a good proxy for similarity. Okay? So now, you have your logit, so you just do a so- softmax computer convex combination. And now based on this convex combination, you're going to then re-express e2 or in terms of this convex combination of all the vectors of all these positions. And before doing- before doing the convex combination, we again do a linear transformation to produce values. And then we do a second linear transformation just to mix this information and pass it through a- pass it through a feedforward layer. And this is- um, and all of this can be expressed basically in two- in two- in two-matrix multiplications, and the square root factor is just to make sure that these, these dot products don't blow up. It's just a scaling factor. And, uh, and, and, wha- why is this particular- why is this mechanism attractive? Well, it's just really fast. You can do this very quickly on a GPU, and simul- you can do it simultaneously for all positions with just two matmuls and a softmax. Um, on the decoder side it's, it's exactly the same, except we impose causality by just adding 10 e- minus 10 e9 to the logits. So it basi- it's just- you just get zero probabilities on those positions. So we just impose causality by, by adding these, uh, highly negative values on the attention- on the attention logits. Um, is, is everything- [LAUGHTER] I thought that was a question. So, um, [LAUGHTER] okay so attention is really, uh, attention is cheap. So because it's- because this variant of attention just involve two- involves two matrix multiplications, it's quadratic in the length of your sequence. And now what's the computational profile of RNNs or convolutions? They're quadratic in the dimension. Because, basically, you can just think of a convolution just flattening your input or just applying a linear transformation on top of it, right? So- and when does this actually become very attractive? This becomes very, very attractive when your dimension is, uh, much larger than your length. Which is the case for machine translation. Now, we will talk about cases when there's- when the- when this is not true, and we have to- we have to do a- we have to make other model developments. Um, but, uh, but for short sequences or sequences where your length does- where your dimension dominates length, attention is a very- has a very favorable computation profile. And as you can see, it's about four times faster than an RNN. Um, um, and, and faster than a convolutional model where the- you have a kernel of- like filter with, uh, three. So, so there's still one problem. Now, here's something- so in language, typically, we want to know, like, who did what to whom, right? So now, imagine you applied a convolutional filter. Because you actually have different linear transformations based on let- relative distances, like this, this, this, this, linear transformation on the word who, uh, o- o- on the concept, we can have- can learn this concept of who and, and, and, pick out different information from this embedding of the word I. And this linear transformation, the lre- the red linear transformation can pick out different information from kicked and the blue linear transformation can pick out different, different information from ball. Now, when you have a single attention layer, this is difficult. Because all- because they're just a convex combination where you have the same linear transformation everywhere. All that's available to you is just a- is just mixing proportions. So you can't pick out different pieces of information from different places. Well, what if we had one attention layer for who? So you can think of an attention layer as something like a feature detector almost, like, because a particular- it, it might try to- it might- because it carries with it a linear transformation, so it's projecting them in a space that- which starts caring maybe about syntax, or it's projecting in this space which starts caring about who or what. Uh, then we can have another attention layer for or attention head for what, did what, and other- another attention head for, for, for whom- to whom. And all of this can actually be done in parallel, and that's actually- and that's exactly what we do. And for efficiency, instead of actually having these dimensions operating in a large space, we just- we just reduce the dimensionality of all these heads and we operate these attention layers in parallel, sort of bridging the gap. Now, here's a, uh, perhaps, well, here's a little quiz. I mean, can you actually- is there a combination of heads or is there a configuration in which you can, actually, exactly simulate a convolution probably with more parameters? I think there should be a simple way to show that if you had mo- more heads or heads are a function of positions, you could probably just simulate a convolution, but- although with a lot of parameters. Uh, so it can- in, in, in the limit, it can actually simulate a convolution. Uh, and it also- we can al- we can continue to enjoy the benefits of parallelism, but we did increase the number of softmaxes because each head then carries with it a softmax. But the amount of FLOPS didn't change because we- instead of actually having these heads operating in very large dimensions, they're operating in very small dimensions. Um, so, uh, when we applied this on, on, on machine translation, um, we were able to drama- uh, dramatically outperform, uh, previous results on English-German and English-French translation. So we had a pretty standard setup: 32,000-word vocabularies, WordPiece encodings, WMT14-, uh, WMT 2014, uh, was our test set, 2013 did the dev set. And, uh, and some of these results were much stronger than even our previous ensemble models. And, um, and on English-French also, we had some- we had some very favorabl- favorable results. Uh, and we- and we are, we, we, we achieved state of the art. Now, ste- stepping back a bit, uh, I- I'm not claiming that we, we arrived at an architecture that has better expressivity than an LSTM. I mean, there's, there's, there's, there's theorems that are- that say that LSTMs can model any function. Um, perhaps, all we did was just build an architecture that was good for SGD. Because stochastic gradient descent could just train this architecture really well, because the gradient dynamics and attention are very simple attentions, just a linear combination. And, uh, um, I think that's- I, I think that's actually favorable. But hopefully, uh, as we- as we go on, but the- well, I'd, I'd also like to point out that, you know, we do explicit mo- we do explicitly model all, all path connection, all, all, all pairwise connections and it has its adva- advantage of a very clear modeling, very clear relationships directly between, between any two words. Um, and, like, hopefully we'll be able to also show that there are other inductive biases. That it's not just like building more architectures that, that are good for- that are good inductive biases for SGD. So frameworks, a lot of our work was initially pushed out in tensor2tensor. Maybe that might change in the future with the arrival of JAX. There's ano- there's a framework also from Amazon called Sockeye. There's also Fairseq, uh, the se- the convolutional sequence-to-sequence toolkit from Facebook that the, they prob- I'm actually not sure if it has a transformer implementation, but they have some really good sequence-to-sequence models as well. Um, okay. So the importance of residuals. So, uh, we have these resil- residual connections, uh, between, um, so we have these residual connections that go from here to- here to here, here to here, like between every pair of layers, and it's interesting. So we, um, we- so what we do is we just add the position informations at the input to the model. And, uh, we don't infuse- we don't infuse or we don't inject position information at every layer. So when, uh, we severed these residual connections and we loo- stared at these, uh, stared at these attention distributions, this is the center or, sort of, the middle map is this attention distribution. You actually- basically, it- it's been unable to pick this diagonal. It should have a very strong diagonal focus. And so what has happened was these residuals were carrying this position information to every layer. And because these subsequent layers had no notion of position, they were fi- finding it hard to actually attend. This is the encoder-decoder attention which typically ends up being diagonal. Now, so then we, uh, we said okay. So then we actually continued with- continued to sever the residuals, but we added position information back in at every layer. We injected position information back in. And we didn't recover the accuracy, but we did get some of this, sort of, diagonal focus back in. So the residuals are doing more, but they're certainly, definitely moving this position information to the model there. They're pumping this position information through the model. Um, okay. So, so that was- that was- so, so now we saw that, you know, being able to, sort of, model both long- and short-, short-term relationships, uh, sh- uh, long and, long- and short-distance relationships with, with attention is beneficial for, for text generation. Um, what kind of inductive, inductive biases lay- actually, uh, appear, or what, what kind of phenomena appear in images and something that we constantly see- constantly see in images and music is this notion of repeating structure that's very similar to each other? You have these motifs that repeat in, in different scales. So, for example, there's a b- it's another artificial but beautiful example of self-similarity where you have this Van Gogh painting where this texture or these, these little objects just repeat. These images are- these different pieces of the image are very sa- similar to each other, but they might have different scales. Uh, again in music, here's a motif that repeats, uh, that could have- it could have, like, di- various, like, spans of time between in, in, between it. So, um, so, so this, so we, we, we, we attempted after this to see, well, to ask this question: can self-attention help us in modeling other objects like images? So the, the path we took was, sort of, standard auto-regressive image modeling the- or probabilistic image modeling, not GANs. Because it was- well, one, it was very easy. We had a language model almost. So this is just like language modeling on images. Uh, and also training at maximum, likely, it allows you to, sort of, measure, measure how well you're doing on, uh, on, on your held-out set. Uh, and it also gives you diversity, so you hopefully are covering all possible, uh, different kinds of images you- So, um, and to this point there's al- we had an advantage that's also been- there are- there've been good work on using recurrent models like PixelRNN and PixelCNN, that, that we're actually getting some very good compression rates. Um- And, um, again here, originally the argument was that, well, you know, in images because there- because you want symmetry, because you want like if you have a face, you want, you want one ear to sort of match with the other. If you had a large receptive field, which you could potentially get with attention at a lower computational cost, then it should benefit- then it should be quite beneficial for, for images, for images and you wouldn't need many layers like you do in convolutions to actually get dependencies between these far away pixels. So it seem like self-attention would have been a- what, what, what was already a good computational mechanism, right? But this sort of- but it was actually interesting to see how it even modeled- naturally modeled self-similarity, and people have used self-similarity in image generation like, you know, uh, there's this really cool work by Efros where they actually see, okay, in the training set, what are those patches that are really, that are really similar to me? And based on the patches that are really similar to me, I'm going to fill up the information. So it's like actually doing image generation. Uh, there is this really classic work called non-local means where they do image denoising, where they want to denoise this sort of, this patch P. And they say, I'm going to- based on my similarity between all other patches in my image, I'm going to compute some function of content-based similarity, and based on the similarity I'm going to pull information. So as- and exploiting this fact that images are very self-similar. And, uh, uh, this has also been sort of, uh, applied in some recent work. Now if you just took this encoder self-attention mechanism and just replace these word embeddings with patches, and that's kind of exactly what it's doing. It's, it's computing this notion of content-based similarity between these elements and then based on this content-based similarity, it constructs a convex combination that essentially brings these things together. So it's, it's a very ni- it was, it was quite- it was very pleasant to see that, oh, this is a differentiable way of doing non-local means. And, uh, and we took the transformer architecture and replaced words with pixels. Uh, there was some- there were some architecture adjustments to do. And, uh, so this was but- this was basically the kind of- it was very similar to the original work, and here the position representations instead of being, you know, one-dimensional, they were- because we are not dealing with sequences, we have two-dimensional position representations. Um, okay. So I pointed out before, attention is a very com- very favorable computational profile if your length- if your dimension dominates length, which if- which is absolutely untrue for, absolutely untrue for images. Uh, because even for like 32 by- even for 32 by 32 images, when you flatten them and you- and you flatten them, you have 30- you get 30, 72 positions, uh, so it's your standard CFIR image. Um, so simple solution, uh, because like convolutions of- I mean, you get- convolutions are basically looked at local windows and you get translational equivariance. We said, "Okay. Let's adopt the same strategy." And also there's a lot of spatial locality and images. Uh, but now, we will still have a better computational profile. If your- if your receptive field is still smaller than your dimension, you can afford- you can actually still do much more long distance computation than a standard convolution because you're, uh, because you're quadratic in length. So as long as we didn't increase our length beyond the dimension, we still had a favorable computational profile. And so the way we did it was, uh, we essentially had, uh, two kinds of rasterizations. So we had a one-dimensional rasterization where you had a sort of single query block, uh, which was, uh, which was then attending or to the- into a larger memory block, uh, in this rasterized fashion along the- along, along the rows. Um, then we tried another form of rasterization, falling standard two-dimensional locality, where you had- where we actually produced the image in, uh, in blocks and within each block we had a rasterization scheme. Um, again, these- the image transformer layer was very similar. We had two-dimensional position representations along with query- with the same- with a very similar attention mechanism. Um, and we tried both super-resolution and unconditional and conditional image generation. Uh, this is- this is Ne- Niki Parmar, I and a co- and a few other authors from Brain, um, and we presented it at ICML. And, uh, we were able to achieve better perplexity than existing models. So PixelSNAIL is actually another model that used- mixed both convolutions and self-attention and they- they outperformed us on, on, on, on, on, bits per dimension. So we were measuring perplexity because these are probabilistic- these are probabilistic models. It's like basically a language model of images and, and it just- and your- and the factorization of your language model just depends on how you rasterize. In the- in this- in the one-D rasterization, we went first rows and then columns. In the two-D rasterization, we went blockwise and inside each block we rasterized. On ImageNet, we achieved better perplexities, and, uh, so yeah, I mean we're at a GAN level, right? I mean this weird- this is- I think probabilist auto-regressive Image generation, uh, by this point had not reached GANs. At ICLR 2019, there's a paper by Nal that actually uses self-attention and gets very, very good quality images. But what we, what we observed was, we were getting structured objects fairly well. Like can people recognize what the second row is? Cars. [OVERLAPPING] I heard- I said- most- almost everyone said cars. I'm not going to ask who said something else, but yes, they're cars. yeah. And, uh, so the- and the last row is another vehicles like, uh, so essentially when structured jo- structured objects were easy to capture. Um, like frogs and sort of, you know, objects that were camouflaged just turned into this mush. Um, and- but on super resolution, now super-resolution is interesting because there's a lot of conditioning information, right? And, uh, when you have a lot of conditioning information, the, the sort of possible- you break- you, you actually lock quite a few of the modes. So there's only a few options you can have at the output. And super- our super resolution results are much better. We were able to get better facial orientation and structure than previous work. And these are samples at different temperatures and, uh, and, uh, and we wou- when we quantify this with actual human evaluators, we- like we flash an image and said, is this real, is this false? And we were able to, uh, we were able to fool humans like four times better than previous results in super resolution. Again, these are not- these results like I, I guess the, the latest GAN result from Nvidia makes us look like a joke. But, I mean this is, I mean, we're starting later than GAN. So hopefully we'll catch up. But, but the point here is that this is an interesting inductive bias for images, so very natural inductive bias for images. Um, and, uh, and, and there is hope to apply it- for applying in classification and other such tasks also. Um, so one interesting thing, just to sort of both out of curiosity and asking how good is maximum or like does maximum likelihood. Well, one, does the model actually capture some interesting structure in the role? Second, do you get diversity? Well, maximum likelihood should get diversity, by, by virtue, by virtue of what it does. Uh, so then we just- we did image completion. And why is- why image completion because as soon as you lock down half the image to the goal truth, you're actually shaving off a lot of the possible modes. So you have a much easier time sampling. So, uh, so the first is, uh, first is what we supply to the model. The, the, the right row- the right most column is, is gold, and we were able to generate different samples. But what was really interesting is the third row. Uh, so the rightmost column is- the rightmost column is gold. Uh, now if you look at the third row, this horse. So actually there's this sort of glimpse or a suggestion of a pull, but the model hallucinated a human in some of these, in some of these images, which is interesting like in- it does capture at least the data teaches it to capture some structure about the world. Um, the dog is just cute and I guess it also shows that, you know, there was this entire object, this chair, that the model just completely refused to imagine. So there's a lot of difficulty. And I guess Anna is gonna talk about [NOISE] the another way to exploit self- self-similarity. Thank you. [APPLAUSE] So thank you Ashish for the introduction. Uh, so there's a lot of self-similarity in images. There's also a lot of self-similarity in, in music. So we can imagine, transformer being a, a good model for it. Uh, we- we're going to show how, uh, we can add more to, to the self attention, to think more about kind of relational information and how that could help, uh, music generation. [NOISE] So, uh, first I want to clarify what is the raw representation that we're working with right now. So analogous to language, you can think about there's text and somebody is reading out a text, so they add their kind of own intonations to it, and then you have sound waves coming out of that speech. So for music there's a va- very similar kind of, uh, line of a generation where you say the composer has an idea, uh, writes down the score and then, a performer performs it and then you get sound. So what we're going to focus on today is mostly, uh, you can think of the score but it's actually, er, a performance, um, in that it's a symbolic representation where MIDI pianos were used and, uh, um, professional amateur, uh, musicians were performing on the pianos. So we have the recorded, uh, information of their playing. So in particular, um, at each time se- step modeling music as this sequential, uh, process, what is being output are, okay, turn this note on, ah, advance the clock by this much, and then turn this note off. And also there is, uh, dynamics information, so when you turn the note on, you first say like, how loud it's going to be. Uh, so traditionally, uh, modeling, uh, music as kind of a language, we've been using, uh, recurrent neural networks. And, um, because as Ashish introduced and, and talked about, there is a lot of compression that needs to happen, like a long sequence has to be embedded into like a fixed length vector. And that becomes hard when, uh, in music you have- you have repetition coming, um, at a distance. So, uh, I'm first going to show you, um, samples from, from the RNNs, from a transformer and then from a music transformer that has the relative attention and kind of let you hear the differences and then I'll go into how we, uh, what are, what are the, uh, modifications we needed to do on top of the, uh, transformer model. Uh, so here, uh, this task is kind of the image completion task. So we give it an initial motif and then we ask the model to do continuations. So this is the motif that we fed. [MUSIC] How many people recognize that? Awesome. Okay. [LAUGHTER] Yeah, so this is a, uh, kind of a fragment from a Chopin Etude piece. And we're going to ask, uh, the RNN to do a continuation. [NOISE] [MUSIC] So in here, like in the beginning, it was trying to repeat it. But very fast, it, er, wandered off into, its other different ideas. So that's one challenge because it's, uh, not able to directly look back to what happened in the past, uh, and, and can just look at kind of a blu- blurry version, and that blurry version becomes more and more blurry. Uh, so this is what the transformer does. Uh, so so, uh, a detail is, uh, these models are trained on half the length that you're hearing. So we're kinda asking the model to generalize beyond the length that it's trained on. And you can see for this transformer, it, it deteriorates beyond that. But it can hold the motif pretty consistent. [MUSIC] Okay. You, you, you ge- you get the idea. [LAUGHTER] So initially, it was able to do this repetition really well. Uh, so it was able to copy it very well. But beyond the length that was trained on, it kinda didn't know how to cope with, like longer contexts. And, uh, what you see, uh, the, the last one is from the music transformer. I think so that kind of [NOISE] the relational information. And you can just see visually how it's very consistent and kinda repeating these [NOISE] these larger, uh, arcs. [MUSIC] Yeah. So that was, uh, music transformer. And so in music, the, the self similarity that we talked about, uh, so we see, uh, the motif here, and so, so there we primed the model with a motif, and this is actually a sample, unconditioned sample from the model. So nothing, er, there was no priming that the, uh, model kinda had to create its own motif and then, uh, do, uh, continuations from there. And here, uh, if we kinda look at it and analyze it a bit, you see, uh, a lot of repetition, uh, with gaps in between. And if you look at the self attention structure, we actually do see the model, uh, looking at the relevant parts. Even if, if it was not immediately, uh, preceding it. So, so here, uh, what I colored shaded out is where the motif, um, occurs. Uh, and you can, uh, see the different colors, there's a different attention heads and they're kinda focusing, uh, among those, uh, grayed out sections. [NOISE] So I'll play the sample and we also have a visualization that kind of shows you as the music is pa- uh, is being played or what notes it was attending to as it was predicting that note. And, uh, this was generated from scratch. And, uh, so the self attention is, um, from, from kind of note to note level or event to event level. So it's, it's quite low level. Uh, so when you look at it, it's, it's ki- a little bit overwhelming. It has like multiple heads and, er, a lot of things moving. Uh, but there's kind of these structural moments where you would kind of see more of this, uh, clean, uh, kind of, uh, sections where it's attending to. [MUSIC] VOkay. So, um, how, how did we do that? And so starting from kind of the the regular attention mechanism, we know it's, uh, a weighted average of the past history. Uh, and the nice thing is, uh, however far it is, we have direct access to it. So if we know, uh, there are kind of motifs that occurred, uh, in in early on in the piece, we're still able to based on, uh, the fact that things that are similar, uh, to be able to retrieve those. Um, but, uh, it also becomes, all the past becomes kind of a bag of words, like there is no structure of which came, uh, before or after. So there's the positional sinusoids that Ashish talked about. That, uh, basically in this, uh, indices indexes into a sinusoids that are moving at different speeds. And so close-by positions would have, uh, a very similar kind of, uh, cross section into those multiple sinusoids. Uh, in contrast for, er, for convolutions, you kinda have this, uh, fixed filter that's moving around that captures the relative distance. Like 1B4, 2B4. And these are kind of, uh, in some ways like a rigid structure that allows you to be, uh, a kind of, uh, bring in the, the distance information very explicitly. Um, you can imagine relative attention, um, with the multiple heads, uh, at play, uh, to be some combination of these. So, uh, on one hand, you can access, uh, the the history very directly. On the other hand, you also know, er, how you rel- relate to this history. Uh, capturing for example, like translational invariance and, er, and we, uh, and for example, we think one of the reasons why in the beginning, uh, priming samples that you heard that the, uh, music transformer was able to generate beyond the length that it was trained on at a very coherent way, is that it's able to kind of rely on this translational invariance to to carry, uh, the relational information forward. So, if we take a closer look at how how how the, how this works is, uh, the regular transformer you have, you compare all the queries and keys, so you get kind of this, uh, square matrix. You can think of it as like a self similarity, uh, matrix, so it's, uh, a square. Uh, what relative attention does is, to add an additional term that thinks, uh, that thinks about whenever you're comparing two things, how far are you apart? And also based on the content, do I, do I care about things that are two steps away or three steps away or I maybe care about things that are recurring, at kind of a periodical distance. And, uh, with that information gathered, that influences, uh, the the similarity between positions. And in particular, uh, this extra term is based on, um, the distance. So you wanna, uh, gather the embeddings, uh, that's irrelevant to the, uh, the query key distances, uh, on the [NOISE] on the logits. So, in translation, this, uh, has shown, uh, a lot of improvement in, um, for example English to to German translation. Uh, but in translation, the sequences are usually quite short. It's only a sentence to sentence. Uh, a translation for example, maybe 50 words or 100 words. But the music, er, samples that you've heard are in the range of 2,000 time-steps. So it's like 2,000 tokens need to be able to fit in memory. So this was a problem, uh, because the original formulation relied on building this 3D tensor that's, uh, that's very large in memory. Um, and and why this is the case? It's because for every pair, uh, you look up what the, what the re- so you can compute what the relative distance is, and then you look up an embedding that corresponds to that distance. So, um, for like this there's a length by length, like L by L, uh, matrix. You need like, uh, to collect embeddings for each of the positions and that's, uh, depth D. So that gives us the 3D. What we realized is, you can actually just directly multiply the queries and the embedding distances. [NOISE] And they, uh, come out kind of in a different order, because now you have the queries ordered by a relative distance, but you need the queries ordered by keys, uh, which is kind of a absolute by absolute, uh, configuration. So what we could do is just, uh, do a series of skewing, uh, to to put it into the right, uh, configuration. And this is, uh, yeah. Just a, just a quick contrast to, to show, um, the difference in memory requirements. So, er, a lot of the times the challenge is in, uh, being able to scale, uh, you know, being able to be more memory efficient so that [NOISE] you can model longer sequences. So with that, uh, this is, um, I can play you one more example if we have time. But if we don't have time, we can, go ahead. We'll see more of that. Okay. [LAUGHTER] So this is, this is, uh, maybe a one, uh, about a one-minute sample and I- I hope you like it. Thanks. [MUSIC] Thank you for listening. [APPLAUSE]. [LAUGHTER] Thanks, Anna. Um, um, great. Um, so to sort to, um, so relative attention has been a powerful mechanism for, um, a very powerful mechanism for music. It's also helped in machine translation. Um, one really interesting, uh, consequences of, uh, of, um, one really interesting consequence of relative attention in, uh, images, is that, um, like convolutions achieve, uh, convolutions achieve translational equivariance. So if you have, let's say, you wa- uh, you have this, this red dot or this feature that you're computing at this red dot, it doesn't depend on where the image of the dog is in the image, is in the the larger image. It just doesn't depend on its absolute location. It's going to, it's going to produce the same activation. So you have- convolutions have this nice, uh, translation equivariance. Now, with, with relative, uh, positions or relative attention, you get exactly the same effect because you don't have any- once you just remove this notion of absolute position that you are injecting [NOISE] into the model, uh, once you've, once you've removed that, then your attention computation, because it actually includes I mean, we've, we've- Niki and I couple of others have actually, and Anna were actually working on images and seems- and it seems to actually show, uh, better results. Um, this actio- this now satisfies this, uh, uh, the- I mean, it, it can achieve translation equivariance which is a great property for images. So there's a lot of- it seems like this might be an interesting direction to pursue if you want to push, uh, Self-Attention in images for a self-supervised learning. Um, I guess on, on self-supervised learning so the geni- generative modeling work that, that I talked about before in, in itself just having probabilistic models of images is, I mean, I guess the best model of an image is I, I go to Google search and I pick up an image and I just give it to you, but I guess generative models of images are useful because, if you want to do something like semis-, uh, uh, self supervised learning where you just pre-train a model on a lot of- on a lot of unlabeled data then you transfer it. So hopefully, this is gonna help and this is gonna be a part of that machinery. Um, another interesting, uh, another indus-interesting structure that relative attention allows you to model, is, uh, is, is kind of a graph. So imagine you have this, uh, you have this similarity graph where these red edges are, are this notion of companies, and the blue edge is a notion of a fruit, uh, and um, an apple takes these two forms. And, uh, and you could just imagine relative attention just modeling this- just being able to model, or being able to- you, you, yourself being able to impose these different notions of similarity uh, between, uh, between, uh, different elements. Uh, so if you have like, if you have graph problems, um, then relative self-attention might be a good fit for you. Um, there's also, there's also a simi- quite a position paper by Battaglia et al from Deep Mind that talks about relative attention and how it can be used, um, within graphs. So while we're on graphs, I just wanted to- perhaps might be interesting to connect, um, uh, of- some, uh, excellent work that was done on, uh, on graphs called Message Passing Neural Networks. And it's quite funny, so if you look at, if you look at the message passing function, um, what it's saying is you're actually just passing messages between pairs of nodes. So you can just think of self attention as imposing a fully connect- it's like a bipe- a full, a complete bipartite graph, and, uh, you're, you're passing messages between, you're passing messages between nodes. Now message passing, message passing neural networks did exactly that. They were passing messages between nodes as well. And how are they different? Well, the only way that when- well, mathematically, they were only different in that message passing was, was, uh, forcing the messages to be between pairs of nodes, but just because of the Softmax function where you get interaction between all the nodes, self attention is like a message passing mechanism, where the interactions are between all, all nodes. So, uh, they're, they're like, they're not too far mathematically, and also the me- the Message Passing Paper introduces an interesting concept called Multiple Towers that are similar to multi-head attention, uh, that, that Norman invented. And, uh, it's like you run k copies of these message passing neural networks in parallel. So there's a lot of similarity between existing, you know, this connects to work that existed before but these connections sort of came in later. Um, we have a graph library where we kind of connected these both, both these strands message passing and, uh, we, uh, we put it out in tensor2tensor. Um, so to sort of summarize, um, the properties that Self-Attention has been able to help us model is this constant path length between any two, any two positions, and it's been, it's been shown to be quite useful in, in, in, uh, in sequence modeling. This advantage of having unbounded memory not having to pack information in finite, in, in sort of a finite amount of- in a, in a fixed amount of space, uh, where in, in our case our memory essentially grows with the sequences is, is helps you computationally, uh, it's trivial to parallelize. You can, you can crunch a lot of data, it's uh, which is useful if you wanna have your large data sets. We found that it can model Self-Similarity. Uh, It seems to be a very natural thing, uh, a very, a very natural phenomenon if you're dealing with images or music. Also, relative attention allows you to sort of, gives you this added dimension of being able to model expressive timing and music, well, this translational equivariance, uh, it extends naturally to graphs. Um, so this part or everything that I talked so far was about sort of parallel training. Um, so there's a very active area of research now using the Self-Attention models for, for, for less auto-regressive generation. So notice a- at generation time, notice that the decoder mask was causal, we couldn't look into the future. So when we're, when we're generating we're still generating sequentially left to right on the target side. Um, so, um, and, and, and, and why, why is generation hard? Well, because your outputs are multi-modal. I f you had- if you want to translate English to German, there's multiple ways and, and, and your, your second word that you're translating will depend on the first word. For example, if you, if you first- the first word that you predict was danke, then that's going to change the second word that you predict. And if you just predicted them independently, then you can imagine you can just have all sorts of permutations of these which will be incorrect. Uh, and the way we actually break modes is just- or we make decisions is just sequential generation. Once we commit to a word that makes a decision, and then that nails down what's the next word that you're going to predict. So there's been some, there's been some work on, it's an active research area, uh, and you can kind of categorize some of these papers like the non-autogressive transformer of the fast- the third paper, fast decoding. Um, the fourth paper towards a better understanding of all Vector Quantized Auto-encoders into this group, where they're actually make- doing the decision making in a latent space, that's being, uh, it's e- either being learned using word alignments, uh, fertilities, or that's being learned using Auto-encoders. So you make- you do the decision making in latent space, and then you- once you've made the decisions in latent space, you assume that all your outputs, are actually conditionally independent, given that you've made these decisions. So that's how they actually speed up. There's also- there's ano- there's another paper. The second one is a paper that does Iterative Refinement. There is also a Blockwise Parallel Decoding paper by Mitchell Stern, uh, Noam Shazeer, and Jakob Uszkoreit, uh, where they essentially just run multiple models like, uh, and rescore using a more- a decode using a faster model and score, using the more expensive model. So that's how it sort of it speeds it up. Um, [NOISE] transfer learning has had the- Self-Attention has been beneficial in transfer learning, GPT from OpenAI and BERT are two classic examples. There's been some work on actually, scaling this up, like add a factor as, uh, efficient optimizer. Um, there's a, there's a recent paper by Rohan Anil and Yoram Singer. Um, there's also Mesh-Tensorflow, which actually they've been able to train models of just several orders of magnitude larger than the original models have been trained. So there's, I mean, when you're working this large data regime you would probably want to memorize a lot of- you want to memorize a lot of things inside your parameters used to train a larger model. Uh, Mesh-Tensorflow can uh, can let you do that. Um, there has been a lot of interesting work, universal transformers, sort of recurrent neural networks can actually count very nicely. There's these cute papers by Schmidhuber where he actually shows that recurring neural, the count- the cell mechanism just learns a nice counter, like if you're- you can learn kind of a to the n, b to the n, uh, with LSTM. So then, uh, universals transformers brings back recurrence in depth inside the transformer. Uh, there is a really cool Wikipedia paper, um, simultaneously with the image transformer paper that also uses local attention. Transformer-XL paper that sort of combines recurrence with Self-Attention, so they do Self-Attention in chunks, but they sort of summarize history by using recurrence, it's kinda cute. It's been used in speech but I don't know if there's been some fairly big success stories of Self-Attention in speech. Uh, again, similar issues where you have very large, uh, um as positions to, uh, to do Self-Attention over. So yeah, um, self supervision is a- if it works it would be, it would be, it would be very beneficial. We wouldn't need large label datasets, understanding transfer, transfers is becoming very succe- becoming- is becoming a reality in NLP with BERT and some of these other models. So understanding how these, what's actually happening is a- is an interesting area of ongoing research for me and a couple. And a few of my collaborators and uh, multitask learning and surmounting this, this quadratic problem with Self-Attention is an interesting area of research that I- that I'd like to pursue. Thank you. 