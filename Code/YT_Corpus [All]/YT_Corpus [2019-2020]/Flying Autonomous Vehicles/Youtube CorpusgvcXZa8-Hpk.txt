 [music] [music] - Welcome to "NASA Edge." - An inside and outside look at all things NASA. - I tell you what, Blair, we have a great and exciting show today. We're talking about Navigation Doppler Lidar. - And joining us later on the show is our good friend, Franklin, who's been busy shooting footage of the tests being done with NDL. - First up, we have my former boss, so we have to make sure we're-we're in good standings. - Yes. - We have Walt Engelund, who is now the Deputy Associate Administrator for Programs in the Space Technology Mission Directorate. How're you doing, Walt? - I'm doing great, Chris. Good to be back. - Now in case-- for some of the people who don't know, we have this really cool new program at NASA called Artemis. Can you kinda share what that-- what that's all about? - Yeah, we're really excited about Artemis. Artemis is the program that's going to take us back to the moon, the first woman and the next American man to set foot on the moon in 2024. We've been investing in capabilities, the SLS rocket, the Orion spacecraft, standing up the Gateway, which will truly be the gateway to the moon and-and points beyond. And the next step is to develop the human landing system, which-which will take astronauts down to the surface of the moon. And part of that landing system requires technologies to include precision landing and Navigation Doppler Lidar. - So being in the Space Technology Mission Directorate, what are some of the suites of technologies that you're working on? - Well, we're developing in a whole host of technologies, both robotic technologies for robotic landings and for the human landers that will-will follow. Solar-solar electric power, uh, nuclear power systems, cryogenic fluid management, precision landing and hazard avoidance technologies, which will enable the human landing systems to get to very precise locations where we're aggregating assets both robotic and for human exploration with. - H-how challenging is it to develop not only the new technologies for-- or develop the landers themselves, but concurrently test all these new technologies that have to work in a complementary fashion with Artemis? - Uh, that's a great question, and w-we're doing that with a very deliberate, uh, plan. W-we're using both robotic systems, small robotic landers, medium-class robotic landers, and eventually the big, full-scale human landers. And we need to do those in parallel so that by 2024 all the technologies, all the capabilities will be available and ready to fly astronauts to the surface of the moon. - EDL, entry, descent, and landing, has always been an integral part-- I mean, a-an important part of-of, uh, discovering another world or-or learning more about other worlds. Is that just as important as landing on the lunar surface? 'Cause it really doesn't have an atmosphere compared with-- to the Martian surface? - Well, that-- the Martian surface is actually harder to get to, because it does have an atmosphere. - Right. - But we are using the moon as a proving ground, essentially, in th-the Doppler Lidar and the precision landing technologies that we need to get astronauts to the surface of Mars, which is the endgame. W-we want to demonstrate on the moon, and then understand how they perform and what we need to do to extend those to the-- to the Mars surface. - And so that means, for example, when this technology matures as it's actually deployed, it'll probably fly on missions prior to that, and we'll see test data and-and verify all that, uh, functionality, even ahead of the astronauts arriving.  A-absolutely. There are things we can do with Doppler Lidar o-on the ground here on Earth to-to develop it and-and prove its capabilities. And we are doing that as we speak. We can do it on the ground or with aircraft or suborbital launch vehicles. We're gonna fly the Doppler Lidar on a Blue Origin New Shepard next year. And then we're going to take the Doppler Lidar on two small robotic landers and demonstrate it at the moon in a fully autonomous sense before we put it on the system that's gonna carry the astronauts down to the surface. - I understand that there's a project called Splice, and that's where you have the hazard avoidance, the-the Navigation Doppler Lidar and all the sensors that are part of this-this program? - Right, Splice has a number of different precision landing technologies, including terrain relative navigation, which we actually use on Earth here. But you use cameras to define waypoints and understand visually where you are. Uh, we don't have a GPS network at the moon, so we need other ways of navigating down to the surface. We've got a hazard detection capability, so when you get close to the surface, you want to be able to look and see where the rocks and slopes and even craters are and avoid those. You may remember Neil Armstrong in Apollo 11. He was literally within about 30 seconds of running out of fuel, because they flew over a big crater, and he didn't want to set the-the lander down in the crater. So we want to do that-- we want to be able to do that autonomously, uh, with-with the hazard avoidance technologies and then obviously the Doppler Lidar. And the Doppler Lidar is really about precision, g-guidance, velocimetry, and position. So with radar systems, we get position, and you have to derive the velocity and rates of the spacecraft. But the-the Doppler Lidar gives us a much more precise, laser-based capability to pinpoint the landing site and understand your velocity and rates and your position as you get near the surface. And I'll tell you the beauty of the NDL technology is it has tremendous potential for Earth applications as well. So we're developing it for this very specific spacecraft, Human Landing System, but it has applications here on Earth potentially to drive autonomous cars, with aircraft, uh, marine applications. It has a really broad base spinoff capability that people are looking at very seriously. - Right. - Now I know that doesn't drive what we do, but that's gotta be really, uh, a good feeling, especially on the engineering and science standpoint to make these breakthroughs meet your objectives, but then at the same time actually open up opportunity for a whole host of different applications. - And that's actually always been part of NASA's mission, not necessarily explicitly, but if you look at all the spinoff technologies-- - Right. - That we have-- we have developed that we use here in our daily lives at Earth. It's a tremendous investment. And this is probably one more in that long list of things that you'll see in the-- you know, in our daily-- daily lives. - Well, speaking of NDL, you had a chance to-to learn more about it. - Absolutely. I had a chance to sit down with Farzin, the principal investigator for, uh, Navigation Doppler Lidar and talk to him about the specifics of how it works. Farzin, Navigation Doppler Lidar-- when I hear the word "Doppler," I think of, like, weather radar. How is that different for Navigation Doppler Lidar? - Well, it actually uses the same principles that are used in weather radars. They call them Doppler, right, because it uses the Doppler effect. And the Doppler effect is essentially when an object is moving, it causes a shift in the frequency or change in the frequency of the radiation. In our case we have a laser radiation. Uh, on the vehicle when we're traveling, we are shining the laser beam to the ground. And as we are moving, it causes a Doppler shift. And from that we can get the velocity of the vehicle. This sensor also uses the same laser beam to get the position or the distance to the ground, and by transmitting multiple beams-- in our case we do three beams-- we can actually figure out what the direction the vehicle is traveling. - But of course in this case you're using lidar, so how does the NDL actually work? - Okay, the NDL in general provides position, speed, and direction of motion. These are the parameters you need for navigating. You use that when you drive your car, right? You use your GPS and your speedometer in the car. So these are the same type of things. Now of course when we go to other planets, or we go to the moon, there are no GPS, so we have to have an onboard sensor to give us position and velocity information, and that's what this system does. - I understand that most of the time we do this using radar, but this offers some advantages. - That's true. You mentioned weather radars. Radars are also used on vehicles, space vehicles in particular, to measure the same type of parameters, you know, position, velocity, your speed and direction you are traveling. However, laser provides significant advantage over radar. Being a much higher frequency means that it's more precise. The radar diverges, and the laser beams are pencil beams. That mitigates a lot of issues that inherent with radar system. So it's both precision and data quality that this provides. - And this makes a lot of sense with spacecraft, 'cause as you said, we're gonna be going to other moons, planets, types of surfaces-- - Yes. - That we're not already very familiar with. Was that part of the thinking when you started developing this technology? - Well, when we started, actually, the reason was that when we were talking to the engineers who are building these vehicles to go to Mars and other places, we hear a lot of complaints about the performance of the radars. - Hmm. - So that triggers for us to start looking at alternatives. And using laser was the obvious choice. So that's how we got it started. And so throughout the development, of course, we are working with those engineers in order to come up with the best optimum design that we could. And that's kind of the process that it went through. - When it comes to the moon and perhaps Mars, do you have to do anything different, uh, with the system to handle different atmospheres or anything, or is this a one-size-fits-all kind of system? - Well, when we-- as we are developing these for space, uh, we try to envelope all the applications, so we don't have to make design changes from going from moon to Mars. So we tried to design it such that the same sensor system that can be used for both applications or other places in the solar system. - So as we prepare with Artemis to go to the moon, where are you guys in the development timeline? I mean, do you have a working model? Where's-- what's the status? - Yes, we are working on a working model right now. It's almost done. So the first engineering units are gonna be demonstrated on two commercial landing missions in 2021. So throughout this, we are gonna be-- keep improving it, making it more robust, so the Artemis will be a whole lot more reliable to ensure that the mission is successful. - And that's really interesting, too, because by flying on these commercial missions, you're actually getting, uh, flight performance and actual-- real data to help iterate the-the system. - Absolutely. This is quite a luxury for us, because normally, uh, when NASA develops systems, they go through a bunch of tests, you know, in the laboratories, you know, doing vibrations. They put them in a vacuum. They run them over different temperature. They radiate them. You go through all of those, and you still have a little bit of discomfort when you go to space. Having this demonstration is extremely helpful to us as a developer, and it's also helpful to the engineers that they will have more confidence in these sensors. They will have all this data. They know exactly how it's gonna perform. So it's a quite valuable experiment. - And joining us back now in the studio is Franklin Fitzgerald. And, Franklin, you know, uh, you actually had a chance to spend time with Farzin and the team over the past several months shooting B-roll a-and covering a lot of the tests. Tell us about that experience. - Yeah, Chris, several months ago we got a call, and they started talking about, hey, we're gonna be, uh, putting this together. And we actually went over to the laboratory where they were assembling the NDL-- the box that we keep talking about, the small suitcase box. - That's the engineering unit that they're, uh, building right now. - Absolutely. And after it was assembled, uh, they said, hey, we're gonna be putting together several more boxes. Uh, but the one that they completed, they started to do testing on it, like vacuum testing and thermal testing. And I documented that at some of the laboratories here at Langley. But the third test that they called me for was the test that they did at the Langley Air Force Base, where they used the lasers that we have here-- models on our table-- to shoot a target down the runway in the back of an open cargo van. Yeah, yeah, I know. - They improvised. - They improvised. But th-th-the van was so far away, I couldn't even see the cargo van with my naked eye. But when I looked through my telephoto lens of my camera, you could see it. And you could see what they call atmospheric turbulence or what looks like heat coming off the surface. - Like a mirage. - Exactly. And-and Farzin told me that, as the lander approaches the surface, you have atmospheric turbulence, uh, but the lasers can work through that to get an accurate location and land like it's supposed to. - And with all these tests, Franklin, I mean, you guys would call them up, and they seemed to be working constantly on this project. - It was-- it felt like it was around the clock. And in one test in particular, the vacuum test, I called Farzin. I said, "Farzin, are you gonna be available at 10 o'clock this morning?" - He said, "I'll be available at 10 o'clock this morning, 10 o'clock tonight. You can come at 3 o'clock in the morning. We're gonna be doing around-the-clock tests for the next couple of days, uh, in the vacuum chamber." So there were technicians working around the clock. - We know it's important, 'cause this NDL unit is gonna be going on two CLPS landers, you know, in a year or so. So they have to be ready. So they have to be working around the clock to get it ready. - Around the clock. - And I think that's interesting, too, 'cause a lot of times we hear about the technology, we see it demonstrated, and you forget about the actual hours that it takes by not just one and two people, but a whole team to get this testing done so it's ready to fly. - They haven't haven't been working on NDL just for a couple years. This has been a long process. I mean, they've been going back pre-2008 working on this. And I had a chance to sit down with Glenn Hines, who is the chief engineer for NDL, and he talked about that history. Hey, Glenn, thank you for being on the show today to talk about NDL. We've been trying to get you on the show for the past 13 plus years. Glad to have you. - Well, thank you so much for having me. - You know, Farzin did a great job talking about, uh, NDL. What I want to do is kinda look at the timeline. Let's-- take us back to the beginning. And to do that, I got a little cheat sheet here. Of course, I have to have my little board here. - Mm-hmm. - Uh, starting back, uh, you know, in 2008, uh, when you started this project, tell us-- - Yeah. - How did it get started? - So actually, the project started a little bit before 2008, um, Farzin's ideas and vision of this particular instrument. But our first helicopter flight of it was in 2008, and that was out in the desert. We worked that project with JPL. It was a huge system at the time. It was, like say, the size of a refrigerator. - Okay. - And then we eventually flew it on the Erickson Skycrane in 2010. - Th-that's huge. That's a big box. - Yep, and it was a very difficult environment to fly in. There's a lot of vibration on the Erickson Skycrane. And it was still big. It was now 60 kilograms at the time, the size of about half a refrigerator at the time. - Okay. - But then we started miniaturizing it, and we built it into what we called our Gen 2 system and dropped the weight down from 60 kilograms down to about 28 kilograms and made it the size of a rack-mount unit. - Okay. - We flew that on some helicopter tests and did some successful flights. And then eventually we flew that same box on the Morpheus vehicle in 2014. - Okay. - And we flew that at the Kennedy Space Flight Center. And it was working in close loop at the time with the other systems onboard. What that means is the vehicle took off by itself, autonomously, picked out a safe spot to land, and found that, and then autonomously flew down to a hazard field that we had built down at the end of the runway. And the vehicle landed there all by itself. - So this was the first test of the system-- - Yes. - In a sort of simulated lunar landing environment. - Exactly. This was a progression of moving the instrument from being huge-- - Okay. - To being small enough to be able to fly, to support these landing systems. - Okay. - And these tests proved out not only its capability, but it proved that we could also do the engineering to make it small enough to fit on vehicles. And we made it one more incremental step after the Morpheus flight. - Okay. - We made it into the size of a toaster. In fact, that's what we called our Gen 3 unit. And we flew that on the Masten vehicle back in 2017. And that's leading us up to what we're building now, what we call the engineering test unit. We're going to make it robust and resilient enough to work in space. - So what would you say-- you know, take us back to 2008 and-and beyond. What was probably the biggest engineering challenge that you had to face during that time? - The biggest engineering challenge was the size of the box, getting it small. And the force and function for that was the computing system. There's a lot of high-speed computing that goes inside the box to make this thing work. You've got lasers. You've got optics. You've got all the components in there. But you need to get all the data processed in real time, to get your answers out in real time, so that the field can react to it, can use the instrument and data to fly. That normally takes a lot of computing power, and we were able to put that onto a board, a board we built here at Langley, to fit inside the box and enable it to shrink. That is pretty much defining the footprint of the box now. So from an engineering perspective, that's probably been our biggest challenge. There are other subsystems in there that we don't put a lot of time and effort into making them better, but that's the biggest one from a science perspective. - NASA a-as an agency, we develop a lot of cool technology that we use in space. But then a lot of that technology can be used back here on Earth. How could NDL be possibly used in terrestrial applications? - Oh, that's-that's an excellent question. There are a lot of different ways that it has usage right back here. One of them is autonomous cars. You know, you see that as an up and coming, emerging technology. A small version of NDL can be used to track the speed of vehicles very precisely. In theory you can also track objects around the vehicle also, so you can use it to some degree for hazard avoidance for the vehicle there, too. But giving it a very precise vehicle speed is of utmost importance to any autonomous vehicle. It needs to know precisely how fast it's going. - Let's kind of fast forward to next year. Where are you gonna be when you-your NDL system is-is sent on those two landers? - Mm-hmm. - They're gonna have to land on the lunar surface, and NDL starts working, and it's working beautifully. - The state we'll be in is a state of happiness. [laughing] That's the state we'll be in. We will be extremely excited, extremely elated that this has, uh, worked successfully. - Well, Glenn, thank you so much for taking time out today t-to kind of give us the brief history of NDL and the technology. I-I'm hoping-- you know, I know you're gonna-- you're gonna do a spot-landing on the moon. - Yep. - And I know you're gonna do on Mars one day. - Yep. - But what I want to see is, I want to be able to drive in a car, autonomously, without driving, knowing that Glenn, Farzin, and team have an instrument onboard that's getting me from A to B. - Excellent. I hope to see that day soon. - Thank you, sir. - Thank you so much, Chris. - Thank you. - I'm very excited about the NDL technology for sure, but this autonomous driving stuff is awesome. And if these guys don't deliver, I'm gonna call them personally, see if they'll set up a car pool with me, because I'm ready to avoid the traffic situation, uh, get some productivity on the road. - You know, one of the bigger challenges that they're dealing with is getting it down from that suitcase-size box to even smaller to work inside of vehicles. The technology has already been transferred, and they're working on it, actually, close to here at Langley where we were. - Don't forget, Walt said at the beginning of the show, he talked about Splice. Because remember, NDL is part of a bigger suite of instruments. So imagine not only having NDL, but then you have the terrain relative navigation built in, and then you have hazard avoidance and detection. One day you're gonna be driving, and you get some-some type of obstacle in front of you, it's just gonna drive around it, and you don't even have to worry about touching the steering wheel. - Perfect. - Let it go on its own. - You know, it's interesting, because that size reduction, you're talking about getting the-the basic box down to the size of a can of soda. So just think of-- there's the lander for Artemis and things like that, but how many other landers that are gonna be involved in space exploration that can use that same technology? - Now we do have some of that same technology in our cars today. We have the side view, uh, lights that let you know if people are next to you. - Blind spot, mm-hmm. - But that's using cameras. But this is more precise. This is better technology, and hopefully that will be incorporated in other things besides cars and jets and things like that. - Absolutely, absolutely. Now, Chris, what do you think about the long-term prospects? We've taken a-- seen a technology like this, and it's being developed and deployed. What's the future look like using this technology? - Yeah, I-I can't wait till we have those two CLPS missions where the-- where the NDL will be flying on the landers. Hopefully we'll get successful data. And eventually we'll send humans in 2024, they'll be on the-- on the human landers. And then eventually as we get on to Mars, this will be a staple for all the technology and all the landers that will be sent out. - So right now we're doing a show on technology that'll be part of that first return to the moon that we-we'll have in 2024, and then eventually landing on Mars. - Where are you guys gonna be in 2024? - You know what? I'll probably be at the Kennedy Space Center, doing some programming, doing some live shows, uh, with you and you, probably. - Hopefully. - Hopefully, yeah. - Yeah. - And, um, and I think we're just gonna get those goosebumps that we had for, uh, the many milestones that we've witnessed over the years here at NASA. You know, I-I was alive when man stepped on moon-- on the moon, but I was very, very young. So I didn't remember it like I remember the shuttle missions. So this is gonna be very interesting, and I'm looking forward to it. - You know what the difference is gonna be? - What's that? - For 2024, he'll have his AARP card. - Ah! You know what, that is absolutely right. - You know, look, it won't matter. What's really interesting is all this technology that we see in development now, is going to be riding on the lander, on Ar-- part of Artemis. - That's right. - Taking the first major step in this process, and we're all gonna get to witness it. That is one of the most exciting things that I can think of in terms of NASA in a long, long time. And I'm excited about it, so I don't really care where I'll be. - And knowing this t-technology right here is-is a part of Artemis, so. - Yeah, yeah. Partemis. - Pretty cool. - Partemis of Artemis. - Partemis of Artemis. - You're watching "NASA Edge." - An inside-outside look. - At all things NASA. 