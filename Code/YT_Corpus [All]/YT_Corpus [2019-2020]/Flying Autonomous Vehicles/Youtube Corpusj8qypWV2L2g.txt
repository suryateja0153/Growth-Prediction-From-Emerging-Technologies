 it's an honor to introduce Raquel soon chief scientist at over 80 G and associate professor at the University of Toronto she's she's been all over the place you've been in many places right so originally from Spain PhD in 2006 the PFL and then you've gone through even a couple years here at MIT Berkeley Zurich in Chicago and finally in 2014 got a position at the University of Toronto which was kind of in the midst of that exciting ecosystem in Toronto and in Canada in general for AI where Raquel has been sort of like a key player in many of the things that have happened the vector Institute she has a Canadian chair for machine learning and computer vision and sort of especially Bob and maybe most relevant to this talk in 2000 like about three years ago she was instrumental to crafting this kind of unique relationship between a university and an ax company which is over 80 G right next to Toronto University now the University of Toronto where she's chief scientist so very interesting relationship between those two worlds maybe you're you probably know Raquel already for many things but maybe one of the things that you know her the most is sort of this kitty data set of benchmark which was a CBPR paper in 2012 I think and then as you are in 2013 and basically sort of product the question as to whether sort of the performance that the traditional algorithms in computer vision we're having were really a thing or we're just a byproduct of the biases in the in those data sets so she provided data set in a much more unstructured environment so it was captured an actual car driving around and demonstrated that sort of the performance was actually dropping quite considerably sort of bringing up they need to work on more realistic environment so I don't know how much of that sort of drove you to start working with over but I'll leave it just with one question which is actually not my question but it was your question which is at the title of the kitty paper data set which was are we ready for autonomous driving I was rereading today the paper and I don't think you answer the question in the paper I don't think you give a straight answer so are we here I think we have made big strides and hopefully today I will show you some of the great work that my students have been doing pushing the frontiers of what cell driving was towards what's the driving should be hopefully what we need to go in the future so so thanks for the kind introduction and it's really a pleasure to be here last time I was here was 14 years ago almost it's been a long time that reminds me how old I am but at the same time yeah I sister was the first place in North America that I was my home and it was very very interesting cool so today I'm going to be talking about as I mentioned what we have done in the past maybe 25 years in terms of self-driving in my research lab which is part of over ETD which is a big program in cell driving with many many many many people so and I'm gonna stay quite high level today because I want to give you more of the intuitions of what we did the things that we did and whether the open problems but more than happy to go into more details if you have questions so the first thing to to sort of think about is if you want to do cell driving at the scale and here we're talking about uber a scale right it's a very very large scale there is really like three components that you need to think about on one side you need the physical the body right so you need to be able to build this vehicle at a scale and this vehicles are not yet regular vehicles at the same time you need to be able to develop the brain of the cell driving car that is able to operate in very complex scenarios and environments right that is able to react to many different things that can happen in the real world now what is important to notice that it's not years about building cool technology and being able to also fabricate these these vehicles but also we need to deploy them and create a product that people are going to use now we so driving what's happening is that we're going to see that the ownership of course is going to go away fully and it's really fleets and you're gonna have a service and this is going to allow us to share the resources of the environment which is you know of our planning which is actually a good thing cool so a doober we know about how to build these networks to dispatch things right and we are partnering with car manufacturers in order to build these robots we have a deep collaboration with Volvo UBC some of the prototypes here and we develop a lot of things in-house we develop the brain of the car as well as computer cetera cool so then the robots that we have this is my only car Boris slide so sorry for the hardware people here so we have this this amazing robots that basically have an over complete set of sensors and the reason why we have another complete set is that we want to don't want to limit the space of possibilities right the solution to surviving on the scale does not exist yet right and we need to be able to get there are in order to get there faster right so we want to limit what those sensors are so we have laser sensors on top of the vehicle as well as high-definition cameras that both see 360 degrees around the vehicle we have ultrasonics we have radar etc right so that have very very complimentary information we have customized computers it's on the trunk with some really cool cooling such that you can do a lot of processing everything is nearly it based right so there is a lot of stuff that goes into a computation as well now what this this robot actually see so here I'm illustrating to the main sensors on outside you can see for you you can see what the cameras see on the right hand side is labor point cloud' see from the top so you might anywhere in the sky looking down right and the first thing that you can see right is that this sensors are extremely complimentary right the lidar is fantastic I'll give you a 3d positioning right and the images are not so good at that right but they're very good at giving you symmetry information about the environment right so this is one of the reasons right also why you want to fuse and use all these different sensors now radar is fantastic I give you velocities for example right so that it's also very complimentary to any of this information as well all right so so now you know as an AI personal right the first question to ask is well sure I can solve this test with AI and now the question is in practice how do I do that so there is like two different philosophies of how you can do this one that you see a lot in academia as well as some of the startups is to say that well I can capture data right and I can train on you remember that takes the Sun the sensor output as the input to the neural network and then this neighbor can basically spit out my control commands right whether it's acceleration a steering command etc right the advantage of doing something like this right is that it's very easy right it's a few lines of code or your favorite you know that's our Python whatever is your favorite language right so that's great right but at the same time the problem is that is going to be actually really difficult to bring the little prototype into something that is safe a can scale and work everywhere in the world right and some of the issues here is that you don't have compositionality it's very hard to understand when you make a mistake how to improve this appropriate say maybe has to change the architecture right I said you don't have so much insight into how should I do this problem better and it's very very important that for us that we have explained ability because imagine something goes wrong right we need to be able to explain into what was the thing that made a system actually do something that is potentially catastrophic so this is you know some of the reasons that yes is very easy to get a predator but it's very hard to actually get some to where that is the real thing and we want to have you know many many many vehicles all over the place in the world right so the alternative that we see is what you see in industry and almost every player which is more of a traditional service stock and the way that this works is the following you have the output of the sensors I'm illustrating this here with the later okay but do you have the other sensors as well and the first thing we all do is we basically localize the vehicle okay where it is in the world and the reason that we localize the vehicle is that we want to use our prior knowledge about the environment before we even see anything from the sensors right so here am illustrating the map with some of the components for example where the lanes are traffic lights crosswalks all the stuff right so before you even see you have all this knowledge about the environment that can allow you to actually plan further away right beyond what the sensors see as well as you know be more careful into you know maybe I should pay more attention here maybe the potentially this modern city of pedestrian somewhere so once the vehicle is localized then typically what you do is you do perception right and perception basically you want to estimate the categories of interest as well as potentially other stuff will be happening in the scene right so here when the boxes are vehicles and pedestrians are these circles and you need to do this in three-dimensional space okay this is different than what you see also in many academic papers which is an image domain and Katie was very biased though was image domain as well then but this understanding the present is not sufficient for cell driving we need to also understand the past because we won't understand motion how this thing is moving the environment right so this is done within you know tracking or some of these frameworks and but this yet is not sufficient right if you want to plan a safe maneuver you need to somehow understand how the future my hand roll right and that's typically called the prediction problem in so driving and the idea is that here I'm highlighting this with weapons and this is basically the most likely trajectory of where these actors might go in the near future now typically you do this with multimodal distributions but for illustration purposes where we me this is a single trajectory reactor okay so basically with the rezoning of the pass reason and about the present and reasoning about the future now once we respond this way right we are ready to say well there is a region of interest around us that we can actually reach in the next few seconds now we want to plan a trajectory that they save given all these things that are happening in the environment okay and we do this replanting at every cycle basically okay now motion planning is not the end of the search and the reason why is that you might plan a manoeuvre but maybe the car is not able to execute exactly this maneuver maybe didn't have the right vehicle dynamics maybe there is friction maybe something else were this an error right so what happens right is that you use control in order to bring what the car does to what you intended to do in the first place okay so so what are the advantages of such a system is that now we have compositionality and we have modernized what we need to do so we can actually divide and conquer I understated what you see in industry is that you have teams of you know hundreds of people in every one of these models developing the stuff and we can incorporate our prior knowledge because this is very reminiscent to how we think about the environment right so that's a positive now what is the negative in doing something like this so there is a few a few problems one is that typically since you have these gigantic teams what happens is that they agree on a API into what is the information that is going to be passed from one model to the next and this information typically is extremely simplistic and small typically for example if you detect the vehicles or the actors in the environment you pass vehicle state maybe the past projectory maybe some notion when you have prediction of where you might think that is going into the future but you have lost so much information already so what happens if you actually made a wrong prediction if you make the brown detection right it's impossible for the system to correct for your mistakes now the other thing that happens is that the reaction time of the vehicle has to be extremely fast right and there is a lot of computer needs to be done and there is a lot of modules here so you need to make a lot of compromises into whether those models are in order to reach the computer that is say a few milliseconds right so you know when I use the latest greatest blah blah blah right because it's too slow instead you're gonna do something that is actually more simplistic okay now the other thing that is important as well is that these teams are developing typically this these different models right and they are basically just thinking about the pro- relation of everything else okay so if you look for example and this was also my fault one activity is that we say okay we have a detection challenge and we're going to measure average precision right and that's the way that we're going to rank on a score everybody now what happens in so driving is that well if you look at the scene every actor is not equally important right so this aggregate metric that basically tells you that you should detect everybody equally well it's actually not that hastily me to solve imagine for example that you need to pass between two vehicles and you have to squeeze by you need to really understand their shape very closely or you're gonna maybe bump into them right however if there is opposite traffic that you know you have already passed you're never going to interact with our vehicles it doesn't really matter whether you detect or not that vehicle well you might want to detect it because there is interactions with other things that might be closer to you right by the end of the day what happens is that this way that we train the systems is another way to really get to the performance of the food system that you need to and what also happens is that as the engineers are you know as I mentioned there is a lot of people working on this there is a lot of changes that go on the stack right and this is going to affect everybody else's so you end up you know engineers into detail of other engineers and people saying hey I make this change maybe it affects your motion planning right and this is one of the reasons why the progress has been relatively slow in the past few years make sense all right so so what are we trying to do it at uber that is different from this to traditional or these two approaches so we are trying to bring something closer to something in between right so what we like to have is a single a is system that is n2n trainable that is able to do the full thing but at the same time is doing this through reasoning pretty traditional stuck typically motion planner that's written unless a good thing we don't want to lose at that bit at the same time when I generate intermediate interpretable representations so that we can meet our safety requirements and we can prove safety as well we can have the compositionality testing for example and as I mentioned we want to learn this with a push of a button because if we don't want to rely on engineers talking to engineers hey there's a change in my effect your thing he said the system should self correct for the angle try and train all the different parameters such that you are optimal for the task at hand which is to drive safely and as comfortable as possible given safety now what is also important is that by doing something like this you can use the sensor data for any one of these subtasks right you haven't compromised and you can share a lot of the computations through the different tasks and otherwise you to potentially have much more complex algorithms in the same timeframe or even faster that the traditional pipeline you can incorporate prior knowledge because you have this intermediate representations and a priori this is a much simpler system that traditional traditional stars have hundreds of thousands of lines of code millions of lines of code which is extremely complicated in this expert system to understand how all these things relate to each other alright so so let me tell you a little bit on this is the vision of a trying to build into the different generations of things towards achieving a vision so we started by building perception algorithms that first started with later only because it's a you know the main sensor that is the strongest sensor despite being a computer vision person to start with to my despair and then we build more and more generations that were able to actually blend information between the multiple sensors as they go and it's important is neither early fusion nor Lee fusion right instead the never can learn to collect and grab this information from the different sensors as it goes through different levels of processing right because depending of the level of semantics or the representation right you want to use this information in different ways so that was important and here what I'm showing is one of our latest greatest obviously detector so is 3d era uses all this so it uses camera as well as images and you see here as multi-class it runs in a few milliseconds on the vehicle okay so you can you know achieve very very high levels of performance right well exploiting all these different sensors now what is important to note is that by doing all the detection you are not going to solve this task and the reason why I say this is that it's not safe to only drive with detection so detection is something where decide on a threshold and you put bounding boxes around around objects but there may be evidence that is something there F is below your threshold and I mean you're gonna run into something right so we build systems that instead of just doing detection they not only do that but they also explain all the different pieces of evidence so for every later point you wanna explain what it is in this case I'm showcasing this by showing some of the categories that current we can press on about which is the road background and the different actors of interest you can do the same thing for images right and then basically explain every single pixel right and for images is important that you do something that in the academia is called Panoptix segmentation right where you not only estimate the classes of is this a car versus not you also delineate this is a different car that this car next to me okay and this is a way that again you try to explain this is what we can do this is real time on a single with a single image this is independent per frame yes for you to show that now there is a lot of work that you've seen literature right about how to do detection of certain categories of interests however if you work in so driving in the world there is so much stuff that actually happens on the road or potentially can interact with the vehicle right like for example here is a deer in front of our vehicle right and the question is should i bill should I build a deer classifier what about the moose people what about a matter is the first of the Carla is driving in front of me right so it's impossible to be able to build all the possible categories in the world right so we need to have a safe fallback system that is able to not only understand the categories of interest but also that there is is able to say that they said in there maybe I don't know Excel what it is but maybe I know the physical properties I know how it's moving I can estimate how it's going to move in the future and I can plan a safe manoeuvre based on this thing okay now in the literature you see also is that people build different systems one for detection or one for this category and another one for rare objects and the prominent then is that you need to have a fuse in a fuse operation that is able to to decide into through a trace this system or should I try the other system and if you do this then you're making this very complex decision with almost no evidence right so you're ready listen instead when we do in the lab is we try to build single algorithms that are able to do all this reasoning together so that you don't need to have a second system trying to make a decision for you and as you train the whole system this is actually safer so in terms of this so you can train object detectors at the same time train embeddings for every one of your in this case representation with little points such that independent of whether what they are you can actually merge it merge them together or Clutton that together if they belong to the same instance and that's the way that you can do semantics right at the same time as grouping things that do you don't necessarily know what they are okay within a single Network so as I mentioned right this is the computer set of open set segmentation where you wanna understand known things you wanna understand unknown things and at the same time and non stuff for example the road right it is countable things and countable things and things that can happen that you don't know what they are and so you can leave this representation such as you can cluster them and at the same time they maintain the categorization for those things that you know what they are cool so this is just a little video of we can do rule they can do semantic classes and you see popping up things that basically are different instances that although we don't know what they are they can be segmented automatically we can do dogs this way we can do our deer etc this is a very interesting direction this is really as a first prototype there is on this direction is very important all right so so this is all about perception right well I talked about so far but I mentioned that we want to have a single layer system that is able to do the whole stack so let me build that little by little for you so the first thing that we look on that domain was instead of trying to do everything right and you you're lacking in the inside we take smaller programs and we build into bigger problems and bigger problems right as you build your intuitions so we build systems that the beginner could do perception and prediction could understand the past the present and the future okay you can do this within a single network in a few milliseconds I'm just going to show you the results here so what you see here is so as as the driver you're detecting all the different vehicles in this case where we place these random boxes in three-dimensional space the color indicates that they are track over time so that is the same under standard motion right and the little dots emanating for every one of this vehicle is basically our predictions into the future okay this is the first version which we call fast and furious which was able to estimate short-term predictions but you can see here also the difficulty of the Wenzel driver right is so much stuff so much occlusion right it's extremely difficult to get resource of this cadet at this level now we can go one step forward right in terms of the prediction and in terms of motion planning we would like to not only understand how things are going to move potential in the future we will also want to understand their intention right so that we can use that in terms of better planning so in this case it's a single number again that's detection prediction as well as intention okay and I want you to focus there is a lot of stuff happening in this video so sorry for that but we need to focus on the bottom okay and what I want you to see is that the different colors of the different detections are basically probability of the different intentions the more relatives the higher the probability of arriving to a stop the more purple it is the higher the probability of being Park as you see how much of car Parker's here and the more yellow it is a high the probability of continued driving right so the difference between park and stop is quite business but it's very very important for cell ramming because what you will do as a planner would be very different in these two scenarios now with the aerosol so emanating from the from the different vehicles what you see is some other types of interactions whether we're gonna keep lane where they're going to make a turn whether it's left or right or whether potentially we're going to change lanes okay and you have this multimodal distribution over all these intentions and are conditioned on the intention you can predict your most likely trajectory and you have a multiple distribution this way okay and what I'm showing you here is the system actually running on vehicle okay so this is what is very interesting is that you can take the research that you know is you can break you know interesting paper or someone not well actually this stuff runs on production systems and that's just a very interesting thing about the set up with with over and Hyrum showcase in a particularly difficult scenario where with happen spoil otherwise during the night and our cities right there is so much stuff happening right there's so many vehicles here there's so many pedestrians right and here we are able to predict nicely where they are what they're going to be doing in multimodal distributions including you know some bigger maneuvers by some of the vehicles that were you turn in in this in this case so this comes you know this this shows you know how difficult driving is but it is just once in Rye of the medicines that you need to handle properly so this was about doing perception on prediction let me tell you a little bit about what we did in motion planning and let me illustrate motion planning first by giving you an example so this is our vehicle driving around and it arrives through this this this case were basically there is a parked vehicle right and in order to continue throughout the cell ramming car has to go in the incoming line which is a very dangerous maneuver right and basically just go around this vehicle so you want to be very careful into when you do this maneuver right and in this case there was an oncoming traffic rights and iteration about we don't have enough time right we need to let them pass and then we can go around into incoming traffic now this to Yuma is similar like an easy thing disable original is not so easy with AI right so the question is how can we do that with modern techniques so the way that our planners or motion planners typically work is as follows so given a situation in this case I'm showing you two different scenarios right there is typically a behaviour planner that decides what is the type of action I should be doing for example shall i acc behind this car or should i pass right and condition on this decision there is typically a trajectory planner that is going to choose the best trajectory to do for the self-driving car in order to drive safely and arrive to the goal etcetera okay now the problem of such a thing why is that you have a behavior planner and a trajectory planner and both things are not necessarily talking to each other correctly meaning that the behavioral planner is optimized or is a very different algorithm from the trajectory planner right and as a consequence they might they might do inconsistent decisions because they don't have to Mai's the same objective right for example this guy equals say oh I'm going to change lanes but then suddenly the trajectory planner is trying to really say okay now that I know that I want to change means what is the trajectory maybe there is no physical trajectory right and then what do you do this is not safe right so one of the things that we look at is well instead of doing this these separate models for a behavioral planning and trajectory planning instead why can we build a system basically has the same objective and it's just a matter of how you do inference on the other objective right because at the end of the day they recently the behavioral planner is because it's too costly to do very fine-grained trajectories all over the place right but the objective itself you're trained to the motion planning should be the same right and that way at least it will be consistent right and you have this notion of whether you can execute that behavior or not and the other thing is that you can actually learn motion planning there is no reason why you have to hunt you in this waste which is typically what is done in okay so so we can do an enter a normal motion planner right that actually does behavioral plan and that's what a trajectory planner and the idea is that you can think of behavioral planner as I wanna I want to basically explore the space right so I have coverage and it's reactor poly planner is going to refine the possible solution or maybe you can maintain multiple modes and refine those modes so that no matter what you always have a safe manoeuvre and you can think of this as sampling versus optimization within the same objective function and that's just a particular inference algorithm for this task so this is the output of such a thing right and what I'm showing you here is basically what the human did so we learn this thing by imitating what humans do while not having collision and driving on where we're supposed to drive on the lenses and satisfying the routes of traffic so traffic lights etc okay and this is fully and to entrain there is no human eye saying that all we should do this type of behavior versus not which is typically in industry what happens is that even your capability for the planner you go multiple months you have a full team developing only the capability here you can use directly internet and I'm integrating this with some fairly complex maneuvers and emotion planner has to do learns all these complex things automatically as I mention all right so now we know how to do motion planning potentially in a learn fashion right and we know how to do a joint perception and prediction we can ask the next question which is well can we actually do everything jointly right now we have the pieces right we have been the intuitions about these different tasks right so we are ready for the next generation of this thing so I'm going to show you the first generation of near emotion planners that we built we are now in our third generation I guess which I will be able to talk about it quite soon but not yet so I will show you this one so the idea here is that the motion planner basically is going to do detection prediction into the future as well as motion planning and when I'm going to illustrate in the video is the detection sorry has one in boxes the predictions are log metric horizon here I believe is 5 seconds they're going to be these waypoints okay and the motion planner so they share the same trunk of the of the neural network and then basically we learn to output a full cost volume for the planner so the basically planning is nothing but your sample realizable trajectories that are physically valid and then you evaluate your cost function okay and you can do this extremely fast because your cost volume is just your nearer on it activations and it's just indexing on those neural nets of activations for those trajectories and then planning is nothing but I evaluate all the trajectories and I someday what I index and I basically take them in those are operations that you can do within your nets so this is just a neural net okay so you can train again and to anything and the way I'm going to be slow is the cost volume is three-dimensional so it's very complicated when I'm gonna show you here areas of low cost and different colors at different times in the future okay so I'm predicting this three-dimensional massive thing into two dimensions and only showing a thresholded version of this cost volume and this will give you an understanding of what the car is thinking into whether possible behaviors this is again and train in turn called detection prediction as well as as well as motion planning and training twin by imitating humans and satisfying rules of traffic and not having collisions okay and you see here in this case this planner didn't have a goal so that's and it's on purpose so you see the multi modality of weather all the options that I have for example when I arrived on intersection and once you pass a little bit right there is only one option the turnin is not any more an option and it learns this automatically you see here also that it likes to change the lanes - so it's not think it's here but there is a whole bunch of examples where he likes to change lanes and ACC and it's already planning for that right you can see that with the cost value so it learns this very complex behaviour all all automatic without any any tuning for example not raya or a super much here it's a very difficult maneuver again it's just learning and there is not really many cases if any of this thing buddy can generalize alright so so I show you a full end-to-end system right and one of the things that it was underlined is the fact that this autonomous system using uses high-definition maps one of the other things that we do in the lab is how can we create an analogy that will allow us to create this high definition Maps in a somatic fashion as possible such that we can scale again to the whole world and this is one of the mapping is really one of the elephants in our skeletons in the closet that people don't talk about but it's really preventing companies from a scale into it's very far away - it's very very big Odie's so we built acknowledged e that can create city like 3d reconstructions for example by emerging in this case I'm showing merging cameras as well as later so that you can increase the field of view so as I mention before we also do a lot of the automation so this maps have for example things like where the lanes traffic lights all the stuff right and in this case I'm showing your neuron numbers that we build at from only a few hundred examples learn to create for highways like full route apologies this is the fully automatic case but they are built in a way that are human-in-the-loop friendly many other human cool correct if they make any mistake and it will recollect yourself and that's the way that you get to this you know from a very very expensive human process of clicking this is how these high-definition maps are done in industry to something where you have high level of automation without giving you to be fully automatic which is extremely difficult it's just a Holy Grail right and in this case so I'm showing here where strain in this case our USANA highways they see them Arizona and then we are gonna go into San Francisco hopefully in a second San Francisco highways are very different no training example no domain adaptation out-of-the-box you can actually handle much more complex topologies and this from a machine and perspectives a very interesting model it's a deep structural model you can think of it of a marriage between graphical models and neural networks but in this case we have to solve for the graph we don't even have the graph and it's the link topology the graph and the nose basically told you what are the attributes like is this submerged versus not as well as where exactly does not should be in in space okay so it's very very interesting from mathematical perspective some of the other things that we done in the lab also this was offline we also build high definition maps online this was one of the first things that that we actually did a doober were basically contrary to what you see in our competitors we are able to so this is we can drive with no high-definition map this is high-definition map on the fly okay so given the cameras as well as the later we estimate all the lanes in the environment and that's something that is different typically you will see Eagle in maybe your lane next to us here is like the full map in 3d no yes 2d okay and we actually used to drive with this technology on highways with no Maps no priority maps for hundreds of kilometers with no intervention okay so this is actually something very very robust one of my favorite topics is that well life is more complicated than just this right there were changes all the time so your maps can be you know can go out of date quite quickly and construction is something that happens a lot these are capture during my commute I work to work and Toronto is either winter season or construction season right and so there is a lot of very interesting configurations of construction and construction is is quite exciting and so basically we build also the online systems that are able to esteem i estimate on the fly where all the construction elements are in terms of in three-dimensional space so we've used later as well as images right to get this very precise description of what these construction elements are and for the cones we can detect for the first time on average at least a hundred fifty meters from the vehicle that there is a little cone and it's alabaster for example a slowdown or start up and over that will allow us to change lanes with sufficient room so that we can do this in a safe manner and this runs in real time on the vehicle as well right all right so so the last thing i want to talk to you to you about is simulation right so we saw autonomous systems with some technology into how to build maps i even talked today about the cali station with something that we also have a lot of efforts on but i wanted to talk to you of a simulation because this is something that is extremely extremely important it's very important for increasing the speed of autonomy development is extremely important for being able to do system engineering and testing verify your systems as well as to make the safety case okay and so ambient updating here once more the autonomy stack right so because i want to illustrate ideally we would like to test the whole system right so that we can actually verify and made a safety case now what happens in industry is the following is that simulating sensors is extremely difficult so what do people do they chop that right and they basically start with bonding boxes right i'm going to simulate the favors of the pending box level and then i'm going to input this to the rest of the stack right what happens with this is that grade you can test your motion planner but in order to fully test your motion planner you need to have a very precise description of the noise model of your perception system and that's very hard on top of this you're actually not verifying the food system because if you verify perception independent of motion planning right you might have a good perception system and with motion planet system but if their errors correlate in a bad way then you can have an extremely unsafe system right that's what you need to do something different you need to be able to simulate the sensors and then be able to test the full thing now as you probably all know right simulating sensors is extremely hard so what do you see typically in the literature and this is from car simulator and show that a lot of you use this right is that what you see in the literature why is that you have artists that create virtual worlds and given this beautiful world you have a model of rendering for the sensor right this is what your physics kicks in right and then you can render it either cameras on the left or the later sensor on the right now what happens with this is that it's extremely difficult to build realistic world right the movie industry or the gaming industry uses its uses like hundreds of millions of dollars for a small as scenes that are realistic right so this is not going to scale to the whole world and so what can we do right so the alternative is the following which is at uber we have driven three million miles so we have so much data how the world looks like so what we can do is build these virtual worlds from our observations of the world and we can also build the assets and everything from our observations and then we can compose the world to create these scenarios that we want to simulate and then we can have a simulator where physics as well as machine learning can actually give you a high level of fidelity of what the simulation should be which is very different than having artists doing something and then just physics rendering all right so so this is just showing some of the reconstruction so we can register the city scale right how our cities are and this is very important because we can actually simulate particular intersections that are actually our operation domain if we wanted to write we can actually also capture as we drive along all the different vehicles that we see this and actually reconstruct but yes driving around standard you know logs that are not for simulation there are yes I were driving and we have we have carried a 25,000 of these guys right that you can see here the diversity of their appearance it will be very expensive to have cut models of this level of fidelity right and not only that we also have intensity of our labor for example so we can simulate that we will need materials if we wanted to do so right so it will be even more expensive to generate this and the shape distribution of these assets are actually quite diverse and you see here like all sorts of interesting examples right we can do all sort of objects we can do cyclists pedestrians as well right and then basically we can reconstruct the world this way we have all these different assets and then a generation time you can decide and one internet scene in this particular place in the world right with this particular traffic configuration and then we're going to have reactive actors that are intelligent right and AV is going to be placed on that in that particular scenario right and then we can test particular scenarios that are of interest to us and it would be very very realistic and then the way that we do rendering is that we basically have a physics model that that's really custom and on top of this the AI system correct such as the noise is similar to the noise of the sensor right so here we're using physics as well as AI and the combination little things is actually very very powerful instead of us giving everything to AI and hopefully this thing will do it okay so this is our behavioral simulation hopefully you can sit on it here so this ant like things are vehicles that react to each other are able to understand rules of traffic etc right so you can create you know this very complex scenarios of how different behaviors and then basically what we can do is that we can then decide where place the assets do this behavior and then test our motion planner okay on the left hand side you see our simulation on the reference side you see the real world so here we are placing the configuration of the assets to mimic the real world okay and when you say is that there is very little difference between the two but what is more important is that on greenie has you see perception output from a system that is only trained on real data and is tested on the left hand simulation and on the right on the real world and there is almost no difference right so this allow us to test on simulation this allow us to create additional examples and train even further our systems now we can create safety critical things that we cannot capture rather than when I capture for safety but now we can test all the stuff now what is interesting I'm just showing this in in the case of semantic segmentation so if you train on bread you have a certain level of performance these are persons if you train using Carla your performance drops a lot right this is 30% down or 35% down and what happened is that then people are very excited about the main annotation right because oh there is a domain gap I need to adapt let's try to do something you know intelligent into how to bridge this gap however if you build a simulator that is very realistic the mini levitation is not necessary and that's why you see here if we train on simulation and we test on the real world the performance is basically the same within 1% right so so this is this was quite interesting because I have a particular interpretation that I have to just basically stop it because you don't need them in a lab session now so this is very good news right I mean bad news for the project pretty good news for us because now we can use this for safety verification and all that stuff right and we don't need to do anything especially and I mentioned before we can use this for training so here is some performance when you train iran real say 71% when you train on a lot more example some riyadh you can increase this if you train or a few real and a lot of simulation you can get to more or less the same level performance of training at the same amount unreal right and then you can further add more simulation right and get further boosting performance this is with very simplistic techniques right so now there is you know can do better and better and better right and basically get your systems to react to different scenarios now grease or so for the acid there's a lot of things that we do in order to make them more dense for example we mirror them and we complete them in a way that is you know we are able to get more complete ships right now they have some horizontal stuff and you will see a little bit more noise in our later in terms of the intensity here is difficult and also this was Valona in data where there is some Creasy modulation of the intensity based on a whole bunch of things including saturation and whatnot so the intensity is not great the geometry is pretty decent and the geometry is sufficient for our networks to don't make a difference in terms of performance so the network's are active yeah so it's not a level of realism right but in terms of the neighbors per se so I would never be something be be they suppress I view there are pretty robust they're not being precise some of the other neighbors that are more like projecting into image space they are really much more sensitive to this so that's also tied to which representation you have but for for our soup reception systems this there is as I say 1% difference there's almost no difference yeah so this is also like the members are actually quite robust to certain noise and noise things so you don't need to have perfect rendering or perfect simulation for you to be able to do this so what we can also do is in this case this is our reconstruction of the environment and we're going to add two two assets that were in there in the first place we are adding this large vehicle and we're basically creating safety difficult scenarios in this case this is one of the typical cases of you wanna see how you react to somebody coming just into your lane okay and this is just added on top of the traffic that was in this particular scene okay so we agree we can create this in our simulation we can use our assets and then we can render us how the the simulation will look like given the later right and then we can test our perception systems and you can basically do this at scale these are on different areas of in this case different cities we place exactly the same type of scenario now with variations of the parameters etc right so you can create all these different test examples for one particular say it scenario family or scenario type that you want to test okay so this allows you to do this and the scale very very easily and I guess we call this letter C so you can do similar things with images now images are much more difficult to be able to simulate in a way that the neighbors don't don't see a difference and start by asking your question one of these vehicles is fake do you know which one who things is the big vehicle on the right whenever they went in front what about this guy here the pickup on the back right so now with the projector is helping me as well right but basically this car here it's fake so the way that we do this is the following is we it's a similar idea right you can we have driven so many models right that you can take what the logs right where the car has seen and then you can in paint things and we are also doing a stud that removes things and this is the way that you can get very you know very good background right as well as you can test like different things now the women we do this is known as image centric we do it in a way that is consistent in three-dimensional space and then you get the right size of the vehicle right they get right orientation as well as we you can have image simulation consistent with level simulation and this is extremely important for us because we need to test systems that take multiple sensors are simple now this is this is still working progress there is a lot winner in this showing you another example here you can see a little bit one of the issues but basically you can crop the form piece correct that's basically the idea and you do it in a way that is three-dimensional aware okay and that allows you to give you more level of realism and I think that this direction is it's actually a very good direction in order to get the level of reality that will allow you to train networks better and better cool so yeah so to finish so self-driving is something that you know when these there are the security is going to have a lot of benefits there is 1.3 million deaths every year on the road so this technology is gonna save a lot of lives also it's going to provide mobility for many people that actually as the population gets older right they don't have a way to move from point A to point B ideally it's going to reduce the cost of transportation and it's going to allow us to also share the resources and have less congestion now one thing is important is that you know we can develop this technology I'm very excited about the technical challenges but if every one of us goes out and invites us a driving car basically we will not be able to move anywhere prices really on the combination of public transit car sharing and right share and I think where we can actually share all these things and be able to move in a way that we want and I'm just going to finish with a video for you [Music] in car is dropping you there [Music] [Music] rora maneuver on this sciences [Music] [Music] we might be congested [Music] it's my summer like science fiction to you [Music] you think about it this kind of thing now is much better [Music] [Applause] thank you for the talk you said before that there is a reasoning layer above all the intelligent mechanisms so let's consider a situation that an accent is inevitable and we're not even talking about whose fault is this let's consider that a person is thrown in front of the car and is there a sequence of priorities of what to do like something bad is gonna happen I either hit the person I hit a car I flip and kill myself so is there a sequence of priorities is there something to like minimize this is the situation yeah so let's see without going into the trolley problem which is why you're hinting I'm not hinting at the same time this is a week without going into how you said those priorities right you need to you know when you do motion planning and you know something inevitable is going to happen you need to think about potentially the consequences of the choices right and maybe those consequences is something that you can use in order to guide what is the best decision for your planner to make now this is also a way you know a firm rating this mathematically also can allow potentially regulators right to tell us what you're doing with situations that they want to have the trolley problem right but there are ways to incur this kind of thing right within the motion planners yes thank thank you for the talk and for the save a simulation camera's image simulation and I imagine you guys also do some stair matching to get the depth does the stereo matching performs the same on the fake car and as well as the real car so without going into how we actually do this so for us we have more than just a stereo we have a set of cameras we have later so we can actually have a fairly good estimation of 3d and you can exploit that to do better simulation there is a whole bunch of work in the lab now going into different representation so allow you to do so and also for the lighter simulation intensity also various light conditions and temperature conditions is also included in your simulation so so we did a lot of work into trying to model given some of those factors that you can measure like also incident angle and saturation of the whole thing and things like that but for sensors like valid Orion is so far was impossible to get better than noise level for better sensors you can do things like that now this is very difficult to your point this is something we're together like very precise intensity is pretty hard the question is how important is intensity for the networks as well right and can you mimic the statistics of the networks sufficiently close so that that's okay you guys use the intensity for line detection so I assume there will be somewhat important yeah so for I mean we did some experiments I guess we learn including in the paper where you can do Lane detection with this and what they results are you see here is just nearest neighbor and if your logs are not too crazy then that's sufficient but obviously you would like to do other things if you want to go to test different weather conditions and then you can build more complex models and those sort of like next generations of this tech it's a lot to be that here this is not nothing of it all right this is sort of the next generation and I mean just need to continue iterating into more and more things Thanks sorry yeah yeah thank you for the talk I was wondering Aston use of networks increases throughout the stack whether you see the need for maybe simpler fallback mechanisms to still be in place or whether you think these networks will become so robust that even those will not be needed anymore yeah so so a very good question I saw then they wait to get to you know where single algorithms are to the performance on your nail right one way to get there is to have to build a system that is robust and typically they whether you think about simpler systems is that I'm gonna have if this mechanism al then I have another one if this fails I have another one in this first I have another one now if you think about those decisions the question is are those decisions something that you hand design or is that a single system that you still train to be robust right and to me this is just the scope of your system right so it's not it's not different than any of this right but the decision into which system to trust is the one where the bottleneck is and one has to be very careful yeah fantastic okay can you say something or are you allowed to say something about the scale of these learning problem right like how big are the datasets that do you have to generate for to get to that 90% or how long does it take to train them I don't know is it something that is feasible to do it in a lab or you there's no way let's see so on the papers we showcase what in academia is large-scale data sets they are not the production data sets the production data sets are bigger I can't tell you the scale of the production data sets obviously and there is so many competitors Indians but we are so far all our systems we are almost one of our papers we also tested on the public benchmarks so that you can also see and what is very interesting is that we develop in our internal data sets we yesterday with really no changes on the public and allow us to show how he generalizes to changes of sensor configuration different traffic etc and so far it's been like a general is really really well we don't need to do much student at all yeah sorry for the hut hunter so let me ask you a naive question somebody that doesn't work in the space I always has been curious if it's a hard phenomena that it seems that nobody is used in communication with the infrastructure yeah great question so some of the one of my favorite topics these days is the to excommunication I didn't put anything on the table you will see very very soon from the live camera and some very interesting work on vehicle to vehicle communication of learning what information to transmit as well as how to aggregate the information to get much better performance and safety than single systems which is very exciting and the same kind of thing you can do you can use vehicle 2x for example I'm interested in if you could use things like cameras play some intersections stuff like that to for the blind spots of the vehicles etc I think this is an extremely very interesting question there is privacy concerns and other things so there is safety concerns as well in terms of every time there's be to something if somebody hacks so we are doing all sort of work in adversarial and robustness to adversary stuff so you will see a lot of that stuff as well so that's a good the next generation coming yeah it is it's a very very interesting hobby there is so much to be done but we have some very very interesting neural net designs for for compressing tiny messages that gives you a lot of stuff you mentioned that you are creating some maps on the fly hey how do you deal with the uncertainty in that case if you are going from from a point to a be point without a classical closing loop are you using some kind of local reference frame so how are you dealing with the elements yeah when I say without maps so yeah I mean I guess explain properties so there is two levels of maps there is the regular map that is the one that your phone whatever uses right that you typically use for the high level planner right and this is from A to B these are the streets or whatever what I want to drive and for that you need some sort of localization right otherwise how are you gonna get to where you intend to be doing over your now the type of Muslim talking about is for sale driving you use as an additional sensor the map of the environment has all this very useful information like lanes where the traffic lights are where the crust works are etc and the sort they would type of things that we can generate on the fly but you can if you need to go from A to B you need to know where a is and will be is obviously so you need some sort of something make sense but for sale driving when I say on the fly here I was referring to if we don't have the high-definition map which is the one who is very expensive the other one you can use in a commodity or Sam don't on Google Maps whatever is your favorite any other questions maybe in this side ladies one hour there is the other side example right thanks for the talk so right at the beginning you mentioned compositionality was a desirable attribute of this sort of factored system and you said we gained all these advantages by going back to the monolithic neural network have we lost compositionality is that the compromise they were making why do you think we missed compositionality because if the network is spitting out a volume a cost volume at the end you're not able to compose the individual predictions of the bounding boxes of the cars and the trajectories yeah okay so that's getting faster so in the neuro motion planner I guess in the when I talk here you have some Aleta's approach why were you detection see the predictions and then you have a header for the cost volume and they are separated in generations two and three that's not the case anymore so you can bring back that and be consistent and they compositional it is their great question yeah let's see so I guess give me one second and maybe I can show you that cool so localization so what you need for localization is three things low cost real time and high accuracy and they way that you can do this thing in a cheap manner is you can learn to do intensity intensity based localization is very very precise so the idea is that you have a map that's just an intensity intensity map of how from the top the world looks like right and you have the online leadership and then you basically learn too much but you do in a way that you can compress the map so that the whole North America fits in as more disk okay and the idea so they basically have a recursive process where you're matching on around certainty this is the way that it allows you to learn or require even calibration of the sensor which is typically what is required for these kind of approaches so you learn to be robust of calibration and things like that you learn to compress the way you can treat things and then you match with neural nets and there is a whole bunch of cool design into how you can do this in a few milliseconds with some specialized cameras so we build this kind of approach for localization for example we have also built it's a whole bunch of papers on the lab we have little things that when we use plain Americans as well as science that's actually quite useful if you do a TAS you can also do more intensive sorry more geometry based localization etc but this is one that works extremely well with an average is three centimeter away from the truth and at scale so hopefully the answer habit all right I think we can continue the conversation outside and there exception thank you very much okay [Applause] 