 hello I'm Michael small I'm developer advocate at local i/o and today I'm here to talk to you about even siding containers ninja infrastructure so what countries is where all of these consumers were producing and that is hundreds of thousands of them we have them in the wild on all thoughts of different infrastructure in different places and we have little to no idea what's going on normally with them you can get loads and you information but what do with this so you can have all these things going on the background and I have clear understanding of your entire architecture and infrastructure to know what's going on so possible solution is using open source tools to provide you some of the observability onto the inside the application now observability is a principal made up of three propellers that is event mobs systems metrics and tracing now I'll go through each one of these components in a moment now a lot of you may be thinking right now I don't need that's your infrastructures not that big or you don't have that many components out there in the wild or don't think that it's something you need to worry about you have a self kneeling system things and gracefully restart data is always written to disk on crash so knowing where and why the problems are happening isn't necessarily an immediate concern however as everybody user where problems are inevitable your infrastructure will crash VMs may die operating system updates may not take place and may conflict with code updates that do take place there's a million and one things and software that mean your system go down which means when you're thinking about all these hundreds thousands of containers are in the wild you don't think it's going to be easy if you take for example Netflix's architecture they have this huge graph of all the individual microservices not the individual containers just the services themselves interacting with each other I'd only know where the information is coming from going to where the crash happens where there's a bottleneck where anything there isn't completely optimal happening where where it's happening so we'll go through the three pillars now and see if we can build a better understanding with a look into what our containers are doing and durably what our infrastructure those containers are are doing as well first up we have the event Lots now what are they in reality they are immutable data points their data points made up of discrete event information so that could be something you've put out the other console or something that's gone out to STV log error etc and the timestamp associated with it so these could it be y'all load balancers so when their every request is producing a lot of items to represent that requesting usually indicative Vietnam the method the host the Plath etcetera are all in a single string with Adam understand two and five when I requested made and it usually comes in the form that maybe that plain text ring usually single line or multi-line data so a lot of it can be in JSON format being the most common Oh binary and think of these as protobuf or bin logs these are all sorts of things you can also stream the information so you can stream log data through to an aggregation tool so what are you they useful now this is something that every developer CISAC men or somebody who's messed around with a computer in production is aware of it's used for debugging and auditing and seeing what's going on so these logs can be used to work out where the problem is what the problem is and give you a little bit of indication of where in the application it is so that you can go and fix it now this is important when you know systems scale that you have all these aggregated into one place now what are the best open source tools for this well humble opinion the e lk stack blk stack if you haven't heard of it it is made of elastic search logstash Capanna elastic search is the storage engine where you feed the logs into log stash is the data processing pipeline the way you manipulate the data to make it understandable and the way you can build out the dashboards and information you want Gabanna is a visualization discover and discovery tool which allows you to work out where and what the problem is you can also use beets beets are the data shippers these send information into elasticsearch rather than after to put it through blogs - for you - named human news and for logs you're very much looking at five beat this one is a protocol that will take read the log she pried it and ship them over to for you why is this easy mode while to be brutally honest it's the easiest one to generate pretty much every application programming language appliance produces logs in some format all you need to do is connect to it something that can ship those logs over to somewhere else now in a lot of situations the platform's you run this on or the tools you're using will provide you ways of doing this for example if you are a like I am a JavaScript programmer you will find libraries like wood stone or Bunyan which have protocols designed for shipping over to HTTP or even streaming via UDP etc so this allows you to then take everything from your program and send it over to a single point to be aggregated so when you have 100 1000 10000 identical containers you can see what all the information is that's going on next up Bluetooth system metrics intermedium Oh now what our sister metrics they're immutable numeric representations of data so these are singular quantifiable things that you can do things like CPU disk i/o memory percentage usages they normally consist of their name or label or and/or label the specific data points so the integer the percentage and etc and their unique timestamp these can be defined values so a lot of systems have predefined ones that they all produce so pretty much every computer on the planet well if I allow you to find a via some means it's CPU memory usage disk i/o network traffic etc and you can gather those and send it over so there are a lot of predefined plug-ins for this all they can be custom metrics for example number of users on at one point that is a quantifiable metric information you can send over number of orders in your queue if you're running a ecommerce platform these are things that you can then put into your metrics tool and use them as additional flags and behaviors to see what the health of your system is like this time so what are they useful well reviewing as a time series date set see the overall health system all the components within your system by reviewing the is quantifiable pieces of information I've repaired a time you can see trends or spikes or problems now this is information that has to actively be reported it is also a problem if nothing has been reported so if your container is doesn't report anything for a couple of hours that is a metric you can track the go there's something wrong all those components down are they being fed the information from a component that interacts with it this allows you to see what your overall health of your application is one of the best tools for this of humble opinion graph honor and Prometheus I I'm gonna concentrate on a graph on or is it c1 I almost aware of it have you used the most but Prometheus and Griffin out give you the same give you similar functionality why is this into intermediate mode well there were plenty of means of capturing matrix with customer Tribunal's we're sent over metric beat collecting them off your devices but this also makes it hard because depending on your application and what your workloads are exit arow some of these metrics can be information overload or completely pointless all you just don't have access to them in the ecosystem you're running for example in functions as a service you don't necessarily have access to things like the disk i/o or the memory usage or the network so those basic ones that you would use if you are running the VM yourself you don't have access to so you can't provide those they give you alternatives like length of execution data objects we can send through the size of those objects so the this isn't a one shape fits all this is finely tailored to what your workloads and application is going to be doing next up is a third now my opinion this is part right a lot of people may disagree with me but to me tracing is car so what is tracing tracing is the end-to-end flow of your application so where data is being passed from A to B this usually consists of traces and spans a single trace will be the single pass the information takes if it goes from the single top layer application into another layer another layer another layer and another layer that will be a single trace in each of those components will then be made of spans so let a layer B layer C the way the information is processed through those will be individual spans and as with things like this tracing was designed for large distributed systems so there that's used in microservices what can be used our other assistance designs it doesn't really work brilliantly and monolithic applications you only because there is harder to break up the individual components on where they interact so what are you useful reviewing the overall flow of your system so you can see where information is going or even if it is going for example you may have components in the middle breaking down so as before when I said with metrics where they may not reporting anything that is a bad site with tracing you can see where in that flow the break is happening which is that means you can review your logs and see why that's happening be that the data that's coming in means when it comes to a fork of where it goes you can see that we just aren't receiving any information that is telling it to take this path well well you can see that there's a break in this one life service at there's one point stopping anything happening below it you can also check for both legs so being able to see where certain components in your system are just churning away and they can give seconds rather than thousands of microseconds and allowing you the data to completely pass I'm one of the open-source tools that work for this well my favorite yay gif which is a project built out of uber and is now growing and is becoming huge within the space this is it kit which wants a similar functionality as so why is this part like well it has to be baked into your application you actually have to include the agents on the devices collectors etc and then you have to actually program into your application the points of which it passes the information out for the spans so they can actually build these traces and give this information out now all of these actually use or can use well use and can use elastic search under the third as the data let's store information so the elk stack obviously uses elastic search as its data layer but grow fauna lung Jagr can use yet elasticsearch ones it's data laugh so that's where all this is the thing that glues all these together for me anyway so dammit I'm what's going to our logs I account it should be completely void of information right or which it is thank God and over here we have a little just a sample application doing absolute thing in the universe and it's producing no logs all metrics or anything so this is currently a black box this black box is currently sat on a UNIX environment somewhere and it's reporting we could convict a file beat into the docker container that is part of that doctor proposed setup then have that report the information or we can set file beat up on the host machine and have hat read off the logs from the doc tail command but there's a simple option we can just loading this and this is the docker collector that we are loved to have got for this like rabbit is simply a docking container ring far beat internally that a provided its pre-configured to run with Loctite so it will have our shipping token which is our way of identifying your account except for the listener that it needs to listen on because we have clusters in many geographies so we need to know which geography your cluster is end and it's just contain connecting the container information from the host to the container itself so it can be streamed out so if I run this build pull it right there we go it started we should now have the metrics conductor collector right if I just there are a few things Lansing there we go all that request being made from that it from page my throat as is all the HTTP requests see they're easily identifiable as pipe and whether container they're coming from the idea the image the idea itself all things that we can build more intelligent lookups on what about the performance data well Gallo's firms wealth so we have a document which grabs the performance information it's once like you file around that and execute that so that's gonna be pulling the image from well from docker hub and then building executing good check says it's alive now if we just for your executions and then reload you we have multi-line piece of information about performance which we can only use later on to tell rebuild badly performing containers no of our metrics well we could configure metric beat we could put this inside of the container itself once again or we could put it on the host and read off from that but once again we have the easy option which is the metrics collector built by us so we provide it on metrics that get now metrics inside the logs IO are under a separate sub account of your primary account just so that you can keep your logging information your metric information separate just to make life a little easier so grab and more than that over here too you you now we go to our looks count go to metrics which are metrics is built on great honor you can see that we are starting received some information now if I change which dashboard were running on and look at the [Music] dr. overview you'll start seeing that we are receiving a number of containers running so in this case there is one of each of these containers running could obviously step this up and increase the number of WordPress instances looking in the database number of containers so this is really useful in say a auto scaled environment where we all want to know how many containers a ring at any one time lovely pieces of a pair information for actually making intelligent decisions they also include things like memory allocation etc we can see where how much memory has been used at any one time between metrics and also see the CPU allocation for there you containers the IO now what about if we want know what's going on with the host itself so we have this one we should looking at the information for the containers or about the host well we have one for that as well this is a pin add that wall mount your host information to the Container and allow that to be shipped over just to be completely transparent that's not actually the advised route the advised route is to use metric beat and configure it on the device locally to do it however this comm we might as well use docker for doing everything build that there we go now if we go to hmm we should start seeing a lot more information so we're seeing the standard but if we go to the responding deverel we will then start seeing Lee in a moment here we have the additional of or system itself so this will be returning the Linux and level metrics that we can use so for example if we have a container which has got double running with a bunch of autoscale containers on top doing say background tasks but then they have to have a read-only application of MySQL so having it inside the container it's on the machine itself as a reader read-only replica we wouldn't be able to see that MySQL is causing any issues with and the docker metric selector so by allowing the system's metrics collector we are then able to intelligently see where things on the system may be affecting how the application inside of the containers is running you now you will maybe lasting where is the demo and the information on tracing and I'm going to be honest the best example I think possibly give you is going to the Yaeger examples and looking through the code for the other example as well as power interacts with the agent the collector and the AUI the reason for this is tracing is very much a application-specific thing so unlike we're tracing it sorry with metrics or logs where you can read them out from outside of your application and just consume those inside of Gabbana or graph Anna and have them posted inside of elastic search these require you to go into your codebase now Jaeger has got libraries and does follow the open tracing standard so you can implement it inside of your most of your languages of choice but there's also the open telemetry program which Jaeger is actively taking part in which means it provides a simple set of api's that allow for a common layer for logs metrics and traces from within your application to be sent out open telemetry has engaged with most languages to produce libraries so you just have to use the library and provide it with the details of where your Jager collector is and where your metrics and want are etcetera and it will consume it and go from there it's also been introduced into common frameworks so spring boot has got an open telemetry limitation plugin for it this means from a framework perspective you don't have to then put everything into your codebase you can include it in your overall framework and allow that to do the work for you and where do we fit into all this as logs ahead well we are an observable platform we are built on the best of breed of open source as you can see from my walkthrough we are built on the ALK snack so we are using elastic search and Cabana for our logs we are also using on top of elasticsearch for our metrics this means you have all of the brightest brains in open-source and their knowledge and we're just providing and operationalize access to it you can also we have blogs on this connect up again to our elastic search infrastructure so you can store your traces with us and then use Cabana for visualization or connect your a UI and write them from there as well so we do allow for all perspectives of observability within our platform we also because of the fact we are running all of these we have started to make small improvements to interconnect them so we have made it so that you can drill down into the information for example a climb span in your metrics can then be translated over to your logs so you can directly see what information your application is producing around that and then practice where necessary so if you have any questions I'm around you can also email me here at mic tiles more out logs I am or you can send me a tweet that you came out else very much [Music] 