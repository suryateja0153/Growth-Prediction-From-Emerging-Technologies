 hello everyone I'd like to welcome you to our webinar adopting AI in optical networks my name is Hebe i'm the communication specialist of Huawei transmission and access network product line, just a little housekeeping before we get started if you have any questions during the presentation please type them into the question box in your control panel I'll print them up during the presentation and we'll also have time for questions at the end now we are turn the time over to Dr. Christopher Janz our presenter today Dr. Janz is Technical VP of Huawei Canada's Optical Systems Competency Center, hello Chris, hello Hebe and everyone morning good afternoon good evening wherever you may be thank you for joining us the topic today as Hebe said is optical intelligence and applying that to maximizing network operations efficiency and perhaps a good place to start is what do we mean by intelligence in this context and thinking about that I think the right way to put it is the role of network intelligence is to deliver information that's needed to support operational decision making okay and there's really two uses or contexts for using such information the first is really about enabling the best operational decisions it's the idea that the more accurate the more timely the more case-specific the information intelligence we synthesize the better the quality of the operational decisions we can make and that leads to improved operational outcomes whether we're talking about improved resource efficiency service quality and integrity or even revenue and profits and we could call this idea of improving operational outcomes with better information you could call that optimization of course the second aspect is automation right so if we've used concepts like SDN etc - if you like software-ize the operations of the network we placed control of the network state much more substantially in the hands of software we can go from policy to action through software, provided we have this decision making information synthesized in software really to marry to policy, determine actions to be taken and allow an action loop to be closed and automation is really all about minimizing operational cycle times minimizing errors and costs that are associated with manually intensive processes it's an OPEX sort of proposition and the marriage of the two I like to call that autonomy where we get both outcomes minimize capex and OPEX maximize service quality and velocity of delivery and revenue and profits so as you see there's really two pillars the first is general software-ization of network control and the second is network intelligence that's synthesized in software. Now how does this work architecturally well there's really three components to this game the first in the physical network itself we're bringing intelligence there first of all that's through an enhancement of ASON we call that ASON 2.0 where we're bringing greater compute power able to support larger scale network operations where we're looking to improve ASON processes for faster provisioning self-healing and also we're bringing progressively new kinds of optical sensors to the physical network and this is very important as you'll see typically we've had really alarms that look for significant degradation of service but really what we're talking about here is bringing measurement capability that's directly relevant to the optical physical platform measuring powers OS and ours things like that will touch more on that later the second component is let's call it the brain in our product language that's network cloud engine this is the cloud-based platform where we measure management functions with control and analytics that can create that closed loop operation and where we have all of our scenario based applications that enable zero touch operations and associated with that it's something that sometimes we call digital twin this is really the cloud-based adjunct associated with NCE where we have really our optical intelligence synthesis functions going on so that's a series of algorithms and models it's Big Data so it is a data platform among other things it's the home for a collection of these new types of optical sensors in the network and plenty of computing power that's optimized for AI functions so a few examples if we look at use cases we can consider things like the following resource prediction so what new network elements and other resources will be required where and when to quickly support new capacity and service support on the network smart commissioning right heading towards zero touch even at the optical layer how do we know that when we press the button things will come up as expected and within parameters network health prediction so understanding closely the performance of optical services on the network how they may be evolving and why and the ability to extrapolate that so we can if you like preemptively act in the face of the approach toward seriously disturbing degradations and going along with root-cause analysis how can we automate that right so we can understand exactly what the sources and locations and explanations for various types of degradations are without a lot of manual effort and if we look at the kinds of algorithms we need to support various functions like this we see that well on the one hand there's fairly classical methods like time series analysis and prediction regression but there's also if you like new flavors of algorithms that are largely in the machine learning or AI camp things like neural networks aggregation and clustering I'll show some examples big data collection of course is part of this as I mentioned NCE and digital twin are a big data platform in the cloud and of course the idea is that we're moving from collection of relatively limited sets of alarms to more continuous collection of ongoing sensory data and various types across the network with faster sampling and going along with that is an increasing use and embedding of AI capable of computing power now that's both on physical network elements right for example so that we can pre-process some of the sensory data to minimize or reduce the streaming of that information into the cloud platform but also on the cloud-based platform itself where we're leveraging AI capable process and technology to get better results. Now as I mentioned autonomy is really a lifecycle automation idea right where we're moving in a continuous loop from planning, deployment and provisioning through ongoing monitoring assurance and analysis in a closed loop and there's different categories of let's call it use cases these are benefits that we target with this kind of functionality the first you could classify among different ways of bidding these things you can classify in a revenue attack category and these are things like OVPNs and premium private lines for enterprise where we are supporting automated service provisioning even handing over control of dynamic service provisioning directly to customers supporting plug-and-play with customer premise elements sophisticated SLA assurance and of course mass customization if you like of services along various dimensions connectivity endpoints and bandwidth but also latency controls latency targeting and restoration performance targeting a second category is all about low touch maintenance and this is directly an OPEX target it's things like clear health visualization health prediction automated troubleshooting RCA as we've discussed and smart commissioning and the third category I think you can call generally planning another main for at least part of that is network resource assurance this idea that we facilitate getting a clear and accurate understanding of these sources on the network how they're being used the trends on service and capacity utilization the ability to predict needed physical element accretions to the network and automated recommendation of planning for pre positioning deployment and service turn up on those resources now I think at this point Hebe wants to do a poll the question is "which areas would be your priority for adopting Optical Intelligence in your networks?" number one advanced private line service delivery number two service configuration automation, number three network health troubleshooting and fault preemption, number four capacity and resource planning could you please select, and many choices are available. Ok let's have a look at the results most of the people select network health troubleshooting and fault preemption and the second is service configuration automations do you have any comments for this one Chris? ah no, just that this is pretty consistent with I think you know what I hear from customers, some are particularly concerned about provisioning automation and the move toward zero touch in many customer cases premium private lines, VPNs as a needed to support growing demands it's pretty important and many others emphasize the lowest zero touch maintenance with the various health management features trending toward better accuracy and lower manual process so yeah I think pretty consistent with what i've seen in the field, i have also heard many customers say troubleshooting is very important for them if they can know what will happen before the trouble occurs it will be much better for them ok let's continue, ok so now let's look at a few of those examples in a bit more detail and talk about optical service commissioning really trying to take it from the status quo where of course wavelength coupling which means various optical services that pass through the same amplifiers the amplifiers couple if you like the behaviors of those co-propagating signals so that when we turn up a new service, extinguish or reroute an existing service there are impacts to other services propagating through the same amplifiers so that introduces a complexity and an element of risk, of course there's always the question of the performance from a noise and ultimately BER perspective on a service that we're about to turn up this tends to require a certain degree of skill expertise and caution which leads to low time efficiency or slowness of process and really what we're trying to get to is through the synthesis and application the relevant intelligence for first of all a De-skilled process where it's software that's telling us what we can do and what we can't that trends toward let's call it safeties there are interruption of services and high efficiency we can now move directly to much faster automated actuation of service commissioning and this depends on things like particularly because amplifiers are the source of the coupling of behaviors among channels having very accurate models of EDFA behaviors that we can use to accurately model the implications on power variations and on channels but also the evolution of noise and ultimately BER on new channels or adjacent channels and then using that information to generate very smart batching and sequences and ultimately for parallel actuation of state changes across the network and that's what gets us acceleration of actuation of condition and state change and of course as in the general picture here most of this intelligence and calculation this happens in the brain but we rely on the nervous system of the network and that's really the WSON control plane to drive the actuation once the trigger is given by the network brain. Health prediction and it's trend toward proactive O&M of course on the optical network yes we have some hard and rapid onset faults like fiber brakes that are difficult to predict and we've got long-standing mechanisms to simply react to those kinds of faults but of course many faults are in fact degradations that have gotten worse over time and that have various sources anything from laser wavelength or filter drift to degradation of fiber whether that's through the accumulation of losses over time from splicing or reconnect operations to physical degradation of the fiber so in general the troubleshooting of faults what exactly is going on where is it happening is difficult to discern and especially difficult to discern early when some of these degradations are at early stages that leads to high OPEX in fact we we have one carrier partner in China that estimated that this kind of troubleshooting occupied more than 1/10 of their total OPEX So the category of function we're trying to create here would take us toward the fact of being able to use software to detect early and then appropriately classify localized degradation of different types and sources and then be able to monitor them and look for trends to classify their severity at any point in time and to look to when they may become critical and to suggest the appropriate proactive response before criticality is reached and with this kind of effective function we estimate in partnership with carriers who we work with that the impact on operations costs let alone disturbances to the customer services could be very high indeed, you know this is really a true big data problem, where we're essentially performing an automated diagnostic function similar to what doctors do in their practices right so we have all of the sensory information from the network's feeding up into the into the cloud and we're using machine learning built models that pass all of this sensory data and have learned to discern the signatures of different fault types again very similar to what a doctor would do right you go to a doctor and you say I have a fever he'll say you know well do you also have nausea he'll take your blood pressure pulse check your breathing and he'll look across the severity and the order of presentation of these different symptoms to try to figure out exactly what's wrong with you it's the same idea here right so we've done work with the number of customers on live data from real networks where we also have the final classifications and localizations of degradation and faults to support learning to produce algorithms that in fact are capable at this kind of classification by type and severity of degradation or incipient fault and ultimately that will lead us to the ability to recommend pre-emptive or proactive maintenance steps. One of the things that we need to understand well both for provisioning automation and for health prediction where we may be interested in both what is the current state of health and why what ought it to be under better or more ideal circumstances is the ability to accurately model and predict the performance and services on the network this is a very complex problem it's something that we've made a lot of progress on in research a simple picture here if we want to understand not just bit error rate but margin on a service we need to understand first what is the baseline the back to back performance and the transmitter receiver pair right so in the absence of impairments on the connection what would be the native performance on that circuit of course in general a transmitter and receiver will never have been tested directly back to back together so we have to find ways to that point to manufacture measure specific data on each in a way that they can be brought together subsequently in a model to generate the back-to-back performance estimation and then we have basically five categories there's four categories of impairments that effect thresholding optical signal to noise so noise generated in amplifiers non-linearity or nonlinear noise PDL polarization dependent loss and penalty filtering and in all of these cases we need a couple of things first of all we want to put as much ability directly to measure these things into the physical Network as possible the more we can directly measure the less we have to model and the better our results are going to be but in all cases we need models as well such that with whatever available information we have we can get good estimations as an example as I hinted earlier we've done a lot of work on EDFA modeling we've figured out ways to construct fixed neural network based models for gain and noise figure performance that are customized for each in the in the final model results for each amplifier instance through the application of a per amplifier baseline measurement that's taken at the point of manufacture and recorded on the ROM for each device that's something we use for power estimation for commissioning but also for assessment as a second example filter penalty impacts here we need two things first of all we need to be able to measure in the field the net filter shape on any particular path and that's capability that we're developing and we need a model and here are two neural network models have proven to be a good vehicle to take us from a measured filter shape to bit error rate, margin impact of that particular shape and on the lower right just some examples showing across many measurements that we can in fact get very good correlation between the model results and the measured results what's another way we can use this beyond provisioning if we look to the planning problem we're trying to squeeze as much capacity utilization out of the network as we can one thing that we found is that of course if we go from fixed grid fixed transponders to flex grid we're able to get a capacity improvement on the network but we've also found that if we go beyond flex grid and two flexible transponders that is where we're able to tune the modulation rate format and coding we are in a sense to make that also a flexible settable parameter we're able to get even more capacity out of the network and what's shown in this graph is a measure of congestion with the optimal network solutions across all of these parameters for a particular case which is on the left so the farther from congestion 100% we are at a given capacity the the better the maximum are the most remaining capacity to use on the network so you know how is that possible well it's really of course the idea that with flexibility and transponder setting we're able to trade capacity or transmitted capacity for reach right the more we try to put through a particular bandwidth the less the reach we understand that and it's exploiting that that really gets us the better results but of course if all we have is a default table of reaches default meaning conservative that is in general you're always going to get at least this much reach it's not an accurate answer for any particular path on a particular network we're going to be leaving some capacity on the table now if we can substitute those reaches when we're doing these operations planning operations in real time with accurate reach answers on particular paths that are delivered by intelligence capability then we will get to if you like Network specific planning that will get us much closer to the actually available capacity on the network and this can be ongoing and acted something we call margin management or margin compression and of course the more capacity we can squeeze out of a given physical network the more efficient we're going to be on capex so just to show things in action a little bit a few views from the product interface here's a particular view showing statistics on fiber health optical channels and services for cases that are for example showing some criticality or incipient criticality of performance we can go to our embedded application table and look for for example provisioning options that will give us performance improvement in particular cases and here's a little bit of the view of the window there where we've looked to work on a particular optical channel with degraded performance the application identifies the optical channels that are similarly affected because they are on shared optical multiple at section portions recommends alternative provisioning parameters and runs those through the model to predict the result performance here's an example showing a topology capture the particular part of the network in a particular service and we can go in and click on the abstraction view to get a result physical view of the actual physical resources involved and here's the for example the amplifier chain involved on a particular service and we can go down and click on for example a particular amplifier and the simulation capability here is telling us what the OSN are on a particular channel is at the output of that particular amplifier and just in another case we're simulating here all of the paths on an entire network to get statistics on optical signal to noise ratio variation across the evolution across all of the service channels commissioned across the whole network and on the lower right this is some assessment of performance this is lab based results on oh and that's where I think ten amplifiers and a whole host of configuration changes with different channel states pre and post configuration comparing the modeled and resulting OS and our changes on observed channels you still hear me yes we can okay okay I apologize something came through there I just wanted to check he could still hear me, so as you see in general we're well within one db of model versus reality in our evaluation across these cases okay resource assurance this is the last one I want to touch on of course one of the issues we have broadly in optical network operation systems is we don't or it can be cumbersome we don't always have an accurate view of the resources in the network how they're connected and how service is mapped to the deployed topology and it gets further complicated as we try to add planning generated data into the picture in the emerging system that problem is taken away first of all the model based NBIs give us a clear automatic linkage from the network up of the resource and topology state I'll cover that at the moment but basically we move from the database of record being the network itself to also being in software right so we've got that real-time sync of resource and the resource visualization ability when we talk about resource assurance we're talking about moving from that to an ability to predict first the required accretions to the physical network to support predicted traffic and service growth right so we're relying on algorithms to take inputs on service planning but also to study the evolution of service and capacity trends to make these predictions basically to tell us which resources where and when ought to be placed into Depot for a rapid deployment in the field trying to get those answers is accurately part of it as possible and the third step is also going beyond that to recommending the provisioning operations that ought to happen once those particular resources are delivered to the field and turned out, right so we're looking at the policies that may be set to govern capacity expansion and service provisioning through two recommendations that are auto-generated and can be enabled simply by button push so I mentioned this idea of abstraction based NBI so let me touch on that quite quickly this has been an important aspect of the community development of SDN as it relates to the transport we're really what we're trying to do is move from a legacy state that is something like this I call it the closed solution but it really means it's it's just a fractured vendor and operator system environment where there is no if you like standard interface between a vendor system and the operator system environment there may be very little commonality among operator system environments themselves the result of which is first of all we've got a very cumbersome state for the development maintenance and change of provider operation systems swapping vendors is a very big deal the integration of new equipment or vendor capabilities is difficult and time-consuming but also we support very low levels of abstraction from the network up to the provider system so really the provider system the OSS is staring at the full native complexity of the network and that places the the burden of responsibility on that OSS at a very high level what we've been we're very hard as an industry to get to is a much more rationalized and standards-based open environment that looks something like this where we have standards defined model-based abstraction oriented northbound interfaces that we can use to connect vendor systems software systems to provider systems and those interfaces will look the same regardless of vendor that stabilizes the environment for the software environment for the provider but also because these interfaces are abstraction oriented that means that they can if requested present the raw total native topology or inventory to the OSS but they can also present whatever degree of control abstraction of view is required right so basically this allows the provider system to determine provisioning operations on a much more rationalized level it doesn't need to understand the full complexity in detail and it doesn't see any difference in appearance from one vendors network to another it's basically dealing with provisioning at a high level of abstraction and this is something that's been supported by a very successful development in standards and the IETF under the ACTN framework most of the key pieces are now published RFC's and it's something that's seen extensive testing and evaluation and proof in the field we've been involved in such work with other vendors and the number of customers this is just an example from 2018 that we did with China Mobile and one of our peer vendors using a CTN NBI and we evaluated a large number of transport use cases and resolved support for everything from multi-layer topology discovery to multi-domain service commissioning for example Ethernet services of various kinds looking at protection and restoration operations and dynamic service modification so this this aspect of software-ization if you like of systems is approaching quite a matured state it's something that in terms of the standards-based NBI is supported on our product and increasingly across vendors one last thing I'd like to say obviously you can think of network autonomy as a destination a place we'd like to get but it should also be thought of as a journey and it's one that is both controlled and won't happen overnight right so here's here's just one possible view of how this could evolve I think in every particular operator there will be somewhat different paths but basically we can separate the notions of information synthesis analysis decision-making and actuation we can separate these things and place manual steps at whatever point in the process we want until confidence is built and capabilities are developed and then further we can separate these across use cases right so things can go step by step and under operator control and following the operator priorities but of course with every step taken we get a capability that can be leveraged and we get one step closer to where we want to be in the end which is a fully autonomous set of network operations right so that's the end of my presentation I hope that was useful thank you thank you everybody for joining and thank you Chris for your great presentation 