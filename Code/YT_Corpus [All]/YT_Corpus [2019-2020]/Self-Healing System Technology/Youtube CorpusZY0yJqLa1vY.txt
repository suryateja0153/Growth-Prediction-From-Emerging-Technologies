 I am David Hay, a Cluster Engineer at LINBIT. LINBIT is a clustering and disaster recovery company. Today we will be talking about High Availability. So, what is High Availability or HA? High Availability is a system that strives to support applications in a way that will allow those applications to stay up for longer than they normally would be able to. Well what is normally would be able to mean? Alright, so in normal cases, or we're going to call normal cases, an application runs on a single node or a single computer. That computer is going to go down, so your application is going to go down along with it. So what we have is an application that is intrinsically bound to a single node - whatever happens to that node happens to that application. So, you generally need to make sure that that node really stays up in that case, right? And there is technologies for dealing with this.You have RAID, which is a Redundant Array of Disks that allows a single disk failure to not affect the system at all. You can do the same thing with power, you can do the same thing with memory - you can do all of these things to make components in the system more redundant. And adding redundancy to a system is a great way to deal with failures, to make failures not matter anymore - or at least make failures addressable and make them not take your application down, because that's what really matters is your application, your user experience. And what does "going down" mean? Well in a general sense, an application going down means that the user of that application, the end consumer, tries to access it or use it in a particular way and finds that they cannot. It is either frozen in a particular state, not accessible at all, or in some other way not as useful as it needs to be. Some applications have very very significant uptime requirements. Things in the medical field are a great example of this. There's all kinds of systems that really can't sustain any significant amount of downtime. To the point where we talk about things like 5 9s. 99.999% uptime - which is a lot of uptime. In this case, that's about five minutes of downtime a year. And, that's not really reasonable to expect out of a single node. You might get a reboot that takes five minutes, a single network disruption might take more than five minutes to address, and in the meantime your applications are down because the node is down. That's not really very desirable and no matter how much redundancy we add to that single node, that's still not going to matter if we have to reboot it for maintenance - and what if we have to reboot it more than once? So, the strategy that we can take here is to use a High Availability cluster of nodes. So, we are moving the failure domains from disks and memory to entire computers, so that you can lose an entire node and your application isn't necessarily affected. So, the goal is to increase availability. And how do we increase availability? We do that by eliminating single points of failure. Eliminating single points of failure is how we design a good cluster. You can do this at many layers - yes, you should probably still have redundant power on a node, you should still have redundant memory on a node, do whatever you want to a node. But in that case, really still need to be able to survive a node failure so a cluster of nodes is the way to go. And, that's how we are eliminating single point of failure of a node. But at the same time, there's also all kinds of other failure modes you need to account for. So, let's just talk about an application running on this node. We don't really have to refer to the fact that it's a cluster quite yet. These applications 0, 1 and 2 depend on each other to run. 1 can't run where 2 isn't, same with 0 and 1. And, they really should start in a particular order. Well, doing this manually is somewhat troublesome because what we have is a manual process that's required to bring an application up, And, you can automate this within a single node. Things like System D, or in a net system of some kind, usually have.. they have dependency chains where you can automatically start an application up. This isn't usually a big concern. But it is a big concern if we're talking about moving these pieces over to another machine. We don't necessarily have a way to automatically say to a computer - yeah start these applications up. That's a manual process if we don't have a real cluster, per say. So, those requirements that we have here for uptime, right, we need some kind of technology to satisfy that. So, say that these resources fail. An administrator has to find out about it for one, so we have to be monitoring them somehow. And there's a lot of different ways to do that, but let's say that an administrator gets an e-mail that an application is down. Well, that might have taken five minutes to get to in some cases, so it's already becoming pretty unacceptable. So, dealing with this automatically is a much better approach because it's always going to be the same process for bringing it up every time for the most part, maybe a little bit of extra logic here and there if we're dealing with EDGE cases, but if we have a cluster then it can automatically start those applications up over here, on Node 1. And it can do that not only automatically, but very quickly. And this is really important, because what if that failover takes less than a second. Well, your 5 9s are barely touched. There's all kinds of reasons why you'd want to do that. So not only to just account for failure, but what if you want to work on Node 0, what if it needs to be rebooted? Then you can just quickly move your applications over to Node 1 and then do whatever you want to Node 0. You can do your reboots, do your updates, do whatever you need to do - replace the RAM, that might take awhile - that sort of thing. So, a cluster allows you to not only avoid unscheduled unexpected downtime, but it also allows you to reduce the amount of scheduled downtime to a really small degree. So, we're doing this by adding redundancy - we've added redundancy by making another node - but at the same time, we need to add redundancy in other places to make it a really effective cluster. Take for example, we have one node here and here - well that's great, our applications can run on both. They're installed to both, we can tell them to run on both, we have clustering software that tells everything to run. We have a single link between them for replication and for applications in the cluster to talk to each other. And if that goes down, the cluster goes down. That's not great. So, we need to account for that. That's a single point of failure.The cluster has failed if that single link has failed. And maybe you can force your applications to run on a single node, maybe you can account for that, but this isn't really a great failure mode. So, what we can do to avoid that failure mode being a problem, is to make your network redundant - to make the links between machines redundant. So not only do we have two nodes, we now have two switches. So, if a single switch fails, if an entire network fails in that case, then it's okay. The cluster can account for this. The cluster will move cluster communication over to switch 1 and use that instead. Same goes with Node 0 - oh node 0 goes down, your applications will move to 1 - that sort of thing. But over and over again, we should have defense in depth, we should be able to account for failures of all kinds of different components in a system. Well, I mean, that accounts for networking. But, how do we actually get our application to run on the other side? We have some requirements there - sure, we need networking and that kind of thing, we need the applications to be installed and workable -- they also need to store data. This is a portability issue. The data over on Node 1 needs to be the same as Node 0, so those applications can behave in the same way. But how do we get there? One very common approach is to use a shared network storage system between nodes so that the view of storage from Node 1 and Node 0 are the same. And they continue to be the same. An update on one node is reflected on the other by virtue of writing it down to a shared system. However, what this creates is a shared infrastructure that the cluster uses. And, when designing clusters, we should always try to share nothing. We try to share nothing because sharing something usually results in a single point of failure. So what happens if this NAS goes down? Well the storage for the entire cluster is now failing, nothing can run. Well, that's not really acceptable. So then you would have to cluster the NAS. Well, clustering the NAS means making another high available cluster for your high availability cluster. So you see the problem is that in doing this, you've actually increased complexity maybe by two fold or more. So, a great approach is to not use shared storage if you can at all get away with it. And, a great way to get away with not using shared storage is to use DRBD. DRBD is a replication software. It will actually make a 1:1 continuous copy as writes are done, one disk to another. It works just like a RAID over a network. Because we have that, because we've actually got our replication in this cluster, this is a standalone cluster now that doesn't require shared infrastructure in order to provide data to both nodes. So, we've shared nothing. We've gone from a model that was much more complex to one that is much simpler. And, because we've done that, because we've reduced complexity, we now have a more reliable cluster. Reliability is obtained through a combination of simplicity and accounting for failures in this case. So, we want to monitor and enforce state as well. Right, all of this infrastructure is fine, but what it doesn't get us is our applications automatically moving from one node to another. It just provides the supporting infrastructure that will allow failures to be compensated for. But we still need the technology that automatically reforms things, moves your applications around - and that's Pacemaker in this case. Pacemaker is a clustering software, in this case a high availability clustering software that uses Corosync to communicate. Corosync is a messaging layer, so between Pacemaker and Corosync, you have a solution that is able to monitor and enforce state. So, if these applications on Node 0 are down, we immediately know about it and the application Pacemaker can communicate with Corosync and tell those resources to move over onto Node 1. Now that's a lot better than getting an e-mail about it, logging into a machine, and taking manual intervention whenever there's a failure. That's a lot better than stopping something on one node and going over and starting it on another node. That doesn't usually take less than a second. So even in unscheduled downtown and scheduled downtime, there's huge advantages. So, since we're using Pacemaker to do that. Since we have these multiple nodes now, multiple switches, multiple power supplies for those switches, and the ability to control the state of everything in a finite and predictable way, well now we have to follow a few tenants to keep this from getting out of control. We want to avoid complexity. We've done that several times here already, but in general a simpler cluster is going to be a more reliable cluster. Reducing the number of components reduces the number of components that can fail. So, we should genuinely strive to reduce complexity wherever we possibly can, except complexity almost as a last resort, and we should isolate failure domains. We don't want to necessarily depend on the cluster itself to account for every single failure. What if one disk in Node 0 goes down - we don't necessarily want Node 0 to die just because one disc went down - so we should still have a defense in depth there. We might still use RAID controllers, we might still use redundant power, and the same goes with the rest of the infrastructure. Oh, we lose a nick and a node, we don't necessarily want it to die. So we're isolating failure domains, we're deciding what components can fail and what can't, removing single points of failure, allowing the cluster to recover from failure automatically using Pacemaker and Corosync which can move our services and reorganize them as need to be, it keeps things running. And finally, we should account for human error. Once we have this perfect system set up, once everything here is automatically self-healing in a way, once everything is replicating - well, the real cause of failure in that case, most times, is human error, because the system takes care of itself so adequately that just an incorrect command or an inability to necessarily understand everything that's going on in a very complex system is generally going to be the number one cause of failure now. So you should reduce complexity, you should document what's going on, and you should use a product that is mature, stable and easy to understand. While these products are generally capable of expressing just about any kind of cluster that you want, reducing complexity allows administrators and other engineers to understand what is going on. And this is going to help reduce failures by increasing reliability - both with the technology and the people involved with the technology - and in general, that is going to increase availability. That's how we get there. We get high availability through increased reliability. And that's the magic of high availability clustering - this is how we get there. Thanks for watching. Don't forget to like and subscribe. Comment with any questions you have about any of this, we're always happy to answer. And, we have plenty of other videos for you to watch. 