 thanks everyone for coming so this is session is on software productivity and AI so I have to apologize for the confusion there was a session before this that was software software for AI developers and this is AI for software developers and I think that was really I think they got the wrong title and we got a different title so hopefully on the right class so I also want to introduce the three panelists they're here Margaret and story from University of Victoria and premkumar Devon book from UC Davis and also a Madison from Queens University in Canada so welcome everyone so I'm going to give a brief introduction to the area and also some of the stuff that we are doing at Microsoft that's related to this and then open it up to the panel to discuss further and take it from there and given it's a small crowd will kind of improvise as we go in terms of how we a lot a use of the time so first thing we want to talk about what does software developers look like today so you know you can predict the future of the AI workforce so there's been a ton of study on data scientists and data science and what data scientists should do and how do engineers become data scientists how do you data scientists become engineers etc and you know there was a LexisNexis did a study saying it there's going to be 20 exabytes of data by 2020 which means we need a lot of engineers to process the data and how are we going to hire those engineers which then IBM released some numbers saying that we are going to require 2.7 two million people so data scientists by the year 2020 and then McKinsey released some data is saying that the demand for data scientist is so much that we're going to be 50% short and we are going to be really short of data scientist by the year 2020 and then they released a later number which basically said AI technology and tools evolve and skill sets of data scientists will be rendered useless in 12 to 18 months and we see that you know people come knowing our and some standard programming models and by the time you know I have turned over probably three rounds of data scientists in the last four years because I pick up new skills are they don't right so so it is it is a challenge and then finally Katie nuggets did a survey and of ML engineers and that 51% of them feel that they will be out of a job by the year 2025 because they will be replaced by a software that can write code itself and they're not going to have jobs anymore so the whole data science engineering AI space is in a flux the definition is not very clear what kind of people we should hire you know what the future of that looks like etc and it's a challenge for people like us who hire engineers or scientists and the kind of problems we throw at them so hopefully through this discussion time we can get through to some of the interesting topics related to this area so I want to start with the story of Oh actually I wanted to say something about software 2.0 actually the fourth panelist was supposed to be on this karpati who came up with this notion of software 2.0 and his premise is that large portion of the code no real-world problems have the property that you know you cannot write algorithms for it you cannot write code for you they will have input and they will have output and develop patterns so you actually can build a model and replace code with models so his idea at least from the space that is working in works a Tesla and a lot of this is here do I make the car automatically part or render the windshield wipers go off automatically there's no logic for it or if you I try to large Loret logic for it it's very complicated so instead of that if I gather enough data on the input side and then I build a model and then produce output that actually works and learns from the output and goes back to the input and kind of a reinforcement kind of learning stuff then actually I don't have to write code all I have to do is replace what I would have done here with kind of a giant neural network which I need to train and deploy and all it is doing is finding the right kind of code in the program space this is directly borrowed from his blog which will basically you know find the right kind of algorithm but it is somewhere in this space rather than fighting that piece of code that I have to write which has very complex logic the nice thing about this idea is that you know it is self hailing and it's self correcting but it he say it's all certain kinds of problems very very well so if you talk to you know Anna's girl K was a fellow at Microsoft I think is a keynote later today you might talk about it they have replaced big chunks of code in Skype and teams would this kind of a model where their place thousands of lines of code with just a few lines of model where you know the the system is much more compact and performs more efficiently and my own journeyer says software engineers started when I when I graduated from college I was trying to figure out I did a thesis on compiler compilers and this is back in India and we had to type our dissertation and it actually had to give it a real typist that was a requirement of the University and the typist came back to me and said you know one too many compilers in your in your and they removed one compiler so wherever it said compiler compiler we just - I wrote the word compiler and completely described my dissertation and you know it was based upon this paper little later than 40 years ago by cattle basically on code generator generator so CMU was doing a ton of work on instead of you got em languages and n machines and you tried code write a code generator for a particular language to a particular machine you cannot be writing you to be writing em times and compilers instead of writing writing M times and compilers how do you reduce that to M plus n compilers so they have this idea of code generator generators and this was kind of the paper I read much later than it was published but you know that was before the internet and and I was in India and so you know how long it takes to get that paper out there but if this paper was published today you know what would it say a automatically generates code that run on any machine right because that's what you would say today but I mean there are some truth to it as well so when I when I started as an intern it was a research center back in India and you know we they hired top graduates from the top universities in India to do program translations a lot of the code that you write in computer center time was translating code in one language to another language so pro writing program translators and you did it in a human way so companies like hewlett-packard or IBM would have software written in one language like COBOL you need to translate it to pl one or you something that is written in module I need to translate that to C or C++ what would they do they give this transformation rules to these engineers who would have this card on their side and they will actually go edit the code and say oh I see and if left brace right brace I'll change it if condition begin end I mean that they physically did that so a lot of the problems we did was of this nature you've got a program in one language and you need to translate a program in another language and you know you need to write a code generator you wrote this code generator by hand but you know you don't really need to write this code generator my mentor at that time told me that you know programmers should get paid well but people who generate programs should get paid ten times as much so you don't write programs you generate programs so that is kind of the idea and so what you did was you got a grammar in one language a source language and your grammar in a target language and then you write a code generator generator that translates the code from that pertains one grammar and that satisfies the other grammar so you note a lot of rules and if you didn't have the grammar then you had examples so you take examples of code in one grammar and then you wrote rules to map them to examples of code in another another grammar and then you know you got your code generator generator which will generate a code generator which will translate programs from one language to another and you could further improve that by kind of saying here I don't want to do M by n code generator generator so I have an intermediate representation a standard kernel that goes between different grammars and so that way I have a retarget able code generator generator so I can reduce everything to an intermediate form and I can do it so all of my early programming went in writing three transformers so we were we would be writing three transformation rules we would be writing three algorithms efficient tree algorithms okay programs the tree you navigate that really tient lis and you write this make this transformer cellent really really fast so I spent a lot of time doing that and the nice thing about it was so I can I can quote one project where the company I was interning at where I got paid $200 for six months of course translated to Indian rupees they had quoted a hundred men year that they dare to tell you is to quote projects 100 men years to translate everything written in modular to UNIX NC because they're going to hire a hundred people to manually change the code millions of lines of code what I did was I actually wrote it a translate a code generator a generator system which will generate trees in one language and translated two trees in another language that was done in about two months time and the way they priced it was they charged eighty thousand dollars for writing the translator generator and charge 25 cents for each line that was translated and about three million lines of code was translated automatically right so this was way before AI but at the same time I had the satisfaction of being a part of that project of course I got paid as I said five hundred dollars for doing that but those are efficient system so if I were to do that today what would I do I cover all of this part I have a lot more data than before right so all of this stuff have examples of source code in one language examples of code in another language I'll build this giant AI model that'll learn from these examples and build a transformation model and automatically give my program translator so that's kind of the stuff that we do today right so in fact if you look at in a natural language processing there's a ton of progress that has been made in the program language community we are trying to learn from that and we're trying to apply that so the recent MIT Technology Review has a great article on how researchers have been able to translate between language barrier languages languages for which there's very little data and how they are mapped into other languages because maybe the past tense in the language are the verb in the language or the combination of urban now in the language are mapped a certain way across the languages and you can get an automatic translator and this is this is really exciting for us because we can learn from these things which is done for natural language and can apply to programming like which is so so it's really a good time to be an engineer writing compiler systems we don't find people writing compilers anymore but at the same time you can do much more than just writing standard compilers we're doing this kind of stuff we write I want to switch gears a little bit and say what do we do at Microsoft so I see some of my colleagues here some PMS and they're very familiar with these pictures so when we think about our developers right we think about what we call inner loop and outer loop so a typical developer has an inner loop edits code bill said debugs it that's kind of the inner loop experience and I'm not sure if that that terminology goes yes go ahead we're like it seemed it seems like this is Tom ball I mean compilers yay because that's how I cut my teeth too but it seems like that's the one area that we actually don't have we don't have a replacing compiler writers yet but you're saying but you're worrying me well let's do so so that it's not actually happened I mean I haven't seen it or maybe I'm unaware but I mean I sort of agree that you know it's within the scope it's a language it's got a grammar there are semantic rules but some like the correctness criteria for it's much more precise and unambiguous than say natural language so I sort of sort of wonder I'm a little scared by the talk so far but I'm also like we don't have evidence yet do we but at the same time you're not for the challenge so with you but I'm just saying like like that still remains like we have all these amazing examples like you said for natural language but it's been actually pretty tough going we haven't seen like huge progress in synthesis really I mean the synthesis has been mainly for small programs and in in translation between languages is still because of the semantic precision and as sort of small differences there's lots of corner cases and stuff so I'm not saying it won't happen but it I don't think it's happened yet so we're redoing small steps so today we are saying that's why I put up this picture if we take very very small steps yes the smallest step is to say hey you're writing code can I help you write code right that's where we start now the big picture I'll talk about it as we go we are not there yet and you write that if I'm translating from say as I have friends who are Chinese on Facebook they one of my friends has all his friends a Chinese except for me and they talk in Chinese I take their text with the Google Translate and then what I want to write in English I turned that to Chinese and translate it on Google and put it back pass it back on Facebook and they understand what I say I'm you know nine out of ten words might be correct there they laugh at me once in a while but at the same time they get the answer now if I feed that your computer it's going to just barf right it's it's not going to work because now you know if you're a program that's there is no program that's almost correct right it has to be character it's got to be wrong so you're right there but at the same time you know we can we can do stuff that actually improves the developer productivity helps helps the per a developer right eighty percent of the code and efficient ways and then put a human do the remaining hard twenty percent so a lot of this automation is about doing the easy ones really well through the machine and give the hard ones to the human being so that they they actually do it do it in the human way right so that's that's kind of the idea but we have to push the limits of it to see how far we can go right and come back coming back to this point so that's kind of the inner loop of development and I'll give you a couple of examples if Mark and Chang you are here they must you must have seen their demo on intellisense on how we help developers write a complete their code in an outer loop you have code reviews and testing in continuous integration CI CD stuff release management issue reporting documentation production analytics all of this come with a ton of data and we can use AI in every step of the way here to improve the productivity of the developer or the or the teams to do things automatically and when I say automatically I say do then mundane things automatically and have the human beings do the intelligent things so there is a ton of stuff that can be done automatically and today for example one of the one of my team members had done analysis of code review PR reviews we found that 60% of PR reviews are stylistic says hey you need a tab here you need a space here you need to capitalize your variable name you should have more meaningful variable names it's almost like somebody doing a paper review the night before the reviews are due due right so the the the conference tears expect you to write a page full of review and if you see that 80% of that is all typos and and spelling Corrections and stylistic stuff really doesn't talk about the details and that's kind of you know engineers are forced to do PR reviews and they're saying oh these are easy things I can say about the code and I will say it and that way I'm out of my you know I've done with our job so that can be easily automated and we are trying to automate that kind of stuff so then that has been that is hard code reviews and maybe there's a human expert that's required alright so for for coming back to the point of helping the user develop code so we have taken a very very systematic approach to helping developers write code so today when we started doing this we said well you know about 60% of what you write our API calls today especially people doing a lot of AI code a lot of systems code your you have a library you import and then you make an API call so when you when you call a class an object of a class or I make a class and a method call can I predict the method that you're going to call right that's the first step and if I can help you with that I significantly improve your productivity right and to do that now we have to make that a part of an editor so that's Visual Studio we have scored whatever that is and we were to make it efficient in the sense that we are not interfering with the developers experience we were to make sure that it delivers a model in a compact way so that gets it you know not take up enough memory on the client and also the model comes back the inference is so fast that you know it's not interfering with the user experience so today we have a very very simple model you must have seen the demo in the mornings I'm not going to show the demo it's basically understands all of the program for you the control flow you know things inside if conditions things inside while loops define before use etcetera etcetera takes advantage of that and builds a very nicely engineered machine learning model and works very well now it uses less than 30 megabytes of space and comes back with a recommendation within 15 milliseconds that's perfect but it if you want me to make it do more things so for example if you want to predict what argument you're going to use when you make a method call especially when you have overloaded methods right now you started start to push the limits of what this can do so you need something more than that and if you look at code code is kind of like text right so which means it's a sequence so you go left to right you know things that you say in the left it means things that um the right are kind of dependent on things that are on the left so it has kind of a sequence sequence model very much like human text so we have a model that is basically an lsdm based model with attention now this performs significantly better than there's a regular classification model that we have but at the same time it has other challenges number one it is not transparent I'm not able to explain when it makes a recommendation what recommendation it is making it is kind of an abstruse model because it's building stuff inside the thing that it's not able to explain so explained ability is not there but at the same time it gives me a lot more accuracy now if I throw a lot more data at it and lot more coded it then I can build a very very complex model I can use something like a GPT - or a Bert model or Excel net and I can understand a whole lot of things about your code not only I understand about code in one language like Python or Java like I can mix them up because almost all of the code all of the languages have you know a notion of a variable notion of assignment notion of you know definition use etcetera etcetera and I can just mix them all up and learn the structure of your code learn the syntax of record learn the semantics if you good so one of the things that happened when we built this model on the left was that when you built a model for c-sharp and Java right we have a type system so we could hang off of the type system when you're making a method call we could say oh this is of this type so only these method calls will apply but when you're working with languages like Python which are loosely typed or no type at all we have to figure out what the type might be we have to create approximations for the type so all of it had to be engineered but when you have a model that's as complex as this which uses a lot of data we don't have to worry about the types because we're learning from patterns of the court and today we have a model that works reasonably well and it can actually it doesn't care whether it's a function call or whether it's an argument it can finish lines of code for you it can do a lot more because and the nice thing about throwing everything into it is that it knows that the left parenthesis has to be matched by life right parentheses after four arguments just because it has so many examples of use of that so that's a beauty of it so going back to Tom's point Ken can we do that we are not there yet but I think we are getting there and there's an opportunity to get there so we can be you know these are these come back with about nineteen ninety two percent accuracy so given enough resources given enough cord enough given enough complexity of your algorithms I think you can do that so that's that's that's what I that's what I think we can do so what do we need to achieve all of this thing's so first thing is data right for us in terms of code we have tons of data if you look at github you know you got more than hundred million repos we've got you know thirty plus million contributors so you got 200 million full requests so there's a tons of data or that 50-plus languages of code and you can give access to all of this stuff and you can think you got English tags which French sexy German text you got code in 50 different languages and you can do it a lot of stuff with that similarly if your Stack Overflow you got six plus million users 12 plus million questions and answer so when when you somebody asks a question the answer comes with code related to its you know we can combine human text English text along with code right and you got corporate data so if I were Microsoft we got millions of lights the lines of code only in one piece of software like Windows so we can do something that's very specific to Microsoft inside Microsoft and to build this intelligence so basically we have tons of data and we can take advantage of it now to go with this data you know we have to have analytics so one thing we were very careful about was we didn't throw a model at it first we see is analyze the code deeply before we threw a model I did so for example there are some classes that are used all the time there's some classes or wherever use so if you want a high accuracy model you focus on things that are used more commonly patterns that occur more commonly and you can have a simpler model so learning code for example here back in 71 don't include look at 18 800 Fortran programs and said 95% of the loops are actually you have only one line inside of them right if you have this kind of information you can feed the Machine this information and build super efficient model and actually more recently some of my colleagues in MSR and they did analysis of 25 million lines of code and came up with almost the same answer which is amazing I think so some things have not changed over years right and we can use that to kind of do idiom mining and stuff like that right so anyway then we have a AI has made significant progress we we started with very simple models but today we have you know our own ends and LST ends we can use for Karuna you know to do the kind of stuff that we are doing and we were borrowing a ton of stuff I'm an LP and computer vision so and we have also tons of tools like auto ml and pie tarts in terms of flow and then we're also as we mind curve we are being careful that we are not violating people's privacy etc so we were a ton of tools for us to build good algorithms around right and in terms of system we have you know a lot of GPUs available in a GPS so by default in most of the computers today and you know be a much faster interconnects and we had before and we can actually take borrow ideas from HPC you know how do we overlap computation with communication right how do we do reduction in an efficient way how do we combine you know data parallelism with kind of model parallelism stuff like that so we we borrow techniques from there and we can combine all of these things and put them together and actually build systems that actually work right so and finally there are humans in the loop which we should not forget because one of the things we found when we deployed our models was offline the first model or offline my accuracy was say in the 70% and we deployed it our PMS went and did a survey of our customers and the customers were quite happy and usually I mean anybody who's done machine learning who who does an offline accurate 70% when they deploy it online it does just doesn't work so we did something right and one of the things we learned for us because we we did the user experience was perfect right we iterated a lot under user experience to make sure that the models don't interfere with the users flow users thinking flow or the code writing flow and we made sure that happened because the users don't care there's AI underneath all the users care is that you're helping them be more efficient and more productive right so basically my summary is you know there are four four things that you require if you want to build the air infused software so one thing is you know you know programming languages and compiler systems the second is algorithms I talked about good use of data in AI and you know methods from high-performance computing that we have to use to build large-scale models and finally you know user experience which is really really important a model that's good enough provides great user experience is much better than a model that's perfect kind of interferes with the users so this is our learning from some of the automation that we have done and to improve user productivity and I call upon the panelists to kind of touch upon some of these topics in their own experience and then we can have a discussion so we'll start with Peggy from you know Steve Victoria she has done a ton of work related to user interaction in software productivity and Peggy take it away and while she sets up if you have any questions comments please can you hear me ok ok so that was the PowerPoint AI at work there you see the way it was doomed in on my nose I think right and if the AI was really smart it would have known that Peggy is a synonym for Margaret and that into probably so you're finding my slides here so you remember you saw the four circles that he had there and human-computer interaction was the bottom one so that's the one that I'm gonna talk about just a little bit here thank you very much and I want to talk about why humans and AI need to join forces in software development so I am a professor at the University of Victoria which is a short plane ride if the plane is not canceled away up in Victoria just across the strait here and I'm also I work quite a bit I did some sabbatical and I'm still doing collaboration with the 1es team here at Microsoft and also working with guys over in Microsoft software research so a lot of the stuff that I've been doing is looking at productivity at Microsoft so I'll talk about that during my talk here today and first of all I want to talk about conceptualizing productivity and what do we mean by productivity so how do you even define what it is how do you if you can't really define it then how do you measure it how do you come with the metric how do you know that the AI system that you're building is actually going to make a difference if you can't really define what productivity is so a bunch of us from research and also some industry partners we met in dog store a couple of years ago now and we talked about productivity for a week and then we wrote a book and this is the book that came out of it rethinking productivity and software engineering it's written more with practitioners in mind and I was co-author on one of the chapters called conceptualizing or actually it was called a software development productivity framework and the goal of this chapter was to conceptualize productivity and we came up with this framework these three different ways that you can think about productivity so in the one hand you can think about the velocity of the work and on another hand you can think about three hands now sorry about that other way you can think about the quality so that if say the number of bugs in the system when you deploy it and then finally you can think about productivity in terms of developer satisfaction so developer satisfaction is often used as a proxy for perceived productivity and the part of the developer because developers do a lot of things during the day right they don't just write code right or test it they also review it they help other people they they write test cases they go to meetings they design they look at requirements and so on so looking at their perceived productivity is a good way to measure productivity as well and while I was at Microsoft two years ago and during the past year and a half as well we've been looking at trying to understand how to measure perceived productivity on the part of developers and out of that research so we did a lot of observations and we interviewed different people from different groups at Microsoft and that led to a big survey through which we have this initial kind of theory about how developer satisfaction and their perceived productivity match to each other and there is this bi-directional relationship between their satisfaction with their job and also their perceived productivity and we build on theories from the from management actually that looked at and saw that there was this bi-directional relationship so basically what that means is if you feel more satisfied at work you're going to feel more perceived product productive and if you feel more productive then you're gonna feel more satisfied so the purpose of this research though was to identify what are the other factors both social and technical factors that might influence how developers feel about their jobs and about their productivity this is still working and so what we did was we did the survey to identify these factors to also understand how satisfied developers were with these factors and then we built a regression model to try to understand which ones actually can explain their overall satisfaction and their productivity and let me look at the challenges first so the other thing that we found through this survey and the work that we did is that there's a lot of challenges that developers say influence our impact negatively their satisfaction and their productivity and and we expected that we would see sort of a one-to-one relationship with challenges and then the factors in our model that would sort of impact their overall satisfaction and perceived productivity but we didn't and so what you see is that some challenges just have this knock-on effect so for example if you report that your that your manager has a big impact on your ability to work that is going to then lead to lower satisfaction across 16 other measures and the same with team culture if you feel that the culture of your team is not good it's gonna have a knock-on effect with nine other factors which then in turn will impact the satisfaction and the perceived productivity of developers and so this diagram here shows that first theory that I showed you at the beginning which was kind of the high level theory relating the factors to overall job satisfaction and perceived productivity and after we ran the survey with I think we heard from about 470 developers we then created this this regression model which then allowed us to tease out which are the actual factors that helped explain their their satisfaction and which factors helped explain their perceived productivity and just a couple of things to note here one is that in the terms of the key factors that influence their perceived productivity you see that there are many different factors that play a role there sure engineering tools is one of them and that's where AI fits in right but there are also all of these other factors as well and then on the satisfaction side we see that work culture and under work culture we clustered team culture organization culture as well as their manager and how collaborative their team were we see that that actually this woman had the most explanatory power in terms of how they felt about how satisfied they felt about their jobs and so when we look at this and when we look at other research we do know that software development is a team sport right so a lot of the focus that I'm seeing so far coming out about AI and how AI can change software engineering and change software development is very much focused around the individual developer and how to help the individual developer and so what I'm trying to do through this short talk and maybe provoke some discussion afterwards is thinking about can we shift and think about how AI could help collaboration and help the team be more productive and and this is a picture to just kind of demonstrate this so on the one hand if we focus too much on the technical aspects so we may have this great AI that does a really good job of producing say more warnings I'll pick on Brendan's favorite example here maybe more security warnings right and so we have this AI that shows there are all these security warnings and and developers should address them right before shipping the code if you just do that and don't think about the developers and how they're gonna wreck and sort of deal with those then you may hit some problems because a lot of engineers will go I'm not gonna fix those bugs right I'm just gonna suppress the warnings or maybe I'm gonna you know hide them somehow from my manager and then carry on on the other hand then if you if you spend too much time just looking at the developer humans in the loop and you forget or you don't look at what are the the technical opportunities that you might have at hand then you also don't do as well as you could so it really is this sweet spot here in the middle that socio-technical intersection that if you pay attention to both just the technical side and the soul and the human and social side that you really will get those gains right in terms of the productivity so I'm really calling for this joint optimization of both so how can a I boost developer productivity so if we look just on the individual side we see a lot of examples of how AI can automate tasks particularly rote tasks and basically just kind of remove them right from the developers work that they have to do or we may see some AI that provides cognitive support so it doesn't completely automate the tasks but it amplifies their cognition and somehow someway maybe it gives them a list of recommendations maybe it's it you know removes the ability the need for the engineer to keep track of a list of things and it does that for them right or it does some calculations or maybe the AI will just provide some information about system attributes that are quality of the system or again recommendations or maybe it provides some feedback on personal productivity so the AI can be watching what the engineer the developer is doing and then give them feedback on terms of their own productivity with the team level in terms of have teams work now we have to start taking into consideration how developers and other stakeholders on the project how they communicate with each other how they coordinate their work how they collaborate on the projects that they have to collaborate on and also can the AI also give them awareness of how they're collaborating with others and even help them reflect on their collaborative aspects so how does AI do that so Neal actually gave a pretty good overview of some of the possibilities particularly in the inner and the outer loop and the different levels and the different ways that AI can be used and we're gonna hear more actually from Prem and Ahmed on that but how how is that AI that magical information how is that how is that communicated to the developers what does that look like well it may look like a prompt it may look like a pop-up box it may be a list of things that you might want to choose from a set of search results or you know or it may be just something that's done automatically and the developer and engineer doesn't even have to touch it but often AI has to be fed through through a user interface and often dashboards are used for doing that at the individual and team level so looking at telemetry data and running AI and telemetry data that is then brought to individual developer and stakeholders hands so that they can look at that information in the dashboards there's not actually a lot of research though on how these dashboards are used or how they're designed so there's a lack of research and how that's done and I'm not going to talk about that a lot today but we did write a short book chapter in that or the book that I told you from from the Dodge tool workshop and I'll touch on that a little bit later on and then another way that we're seeing AI sort of making its way into the hands of developers and in into the interplay between developers working together is through BOTS through these virtual team members and I do want to talk about these for a few minutes so what are BOTS so this is a definition that my student came up with for her thesis so software bots are interfaces that connect users with software services we've seen a lot of thoughts over the last few days depending on the breakout sessions that you went to and so basically a bot is this interface that may provide access to either integrated services you know or calculations or algorithms within the bot itself or it may actually connect to external services and access through an API and the bot brings those services to the user to use directly now of course they also bring additional value in terms of the AI or the intelligence that's in them but they may also bring value by the way that they are have a personality the way that the bolts work with the developers right and I'll talk about that next a little bit in development and software development we're seeing a lot of different BOTS propping up so we see BOTS wealth neil was talking about some of the ways of synthesizing code so we're seeing software BOTS as well providing a way to create that code we see test bots that are working together in the team of developers and detecting bugs and detecting code quality issues and then feeding information back to the developers perhaps in their team's channel or in a slot channel and telling them there are these issues or the boss may even automatically open up bugs and github and assign them to particular developers depending on which part of the code they find the code the problem in there's also DevOps bots that automate deployment and operation and run the things that were manually done before and then send messages or communicate with the developers again through these platforms these communication chat platforms there are code review bots so we were talking about those yesterday that might recommend changes often they're pretty simple changes based on the code sort of things like syntax is not syntax issues but naming issues code review bolts may also recommend reviewers they may wear they're also BOTS that support into interaction with product users so a lot of companies are using BOTS to actually directly talk to the users of this software so that they don't have to have engineer time doing that there are documentation bots that produce Doc's from developers artifacts and translate them from one language to another there are also interestingly a lot of entertainment bots so we heard in one of the keynotes the other day if you want to be productive it's good to also have fun and it's good to take a break so when we were studying how developers use BOTS we found a lot of these and we kind of chuckled but probably serves a good a good role right and whenever I teach and I use bots in the stuff that I'm teaching my students always ask can we have giffy please you know it's important that they have these fun things and this last one is sort of one that I'm I don't see a lot of this but we're starting to see it and I think we could in the future we can have sort of researcher BOTS that could study individual and team productivity so that's something that I think we'll start to see more of there's also this notion of chat ops instead of DevOps where the chat ops is kind of DevOps with this bot that also chats with the different people in the collaboration and I love this quote so chat ops is a collaboration model that connects people tools processes and automation into a transparent workflow and this is pretty important because the the bot is not just doing things in the background so it's more than just doing scripts but it's actually communicating through the the messaging platform with the other developers that they have issued this command and other developers will see that those commands have been issued and so that increases the level of transparency but also increases and the way that developers learn how to do DevOps themselves and this is just one I just wanted to share with you this was one study we did with a local startup company and Victoria and a lot of startups are relying on bots why do you think that is any guesses Fred in it's cheaper than humans they want to automate as much as they can so they're very very clever at figuring out which different human roles they can automate with BOTS to make you know make everything much more efficient and so talking to them they even had a bot I didn't put it on this list that answered the doorbell so when you rang the doorbell they would get a message in sack that somebody was at the door and they'd see a little picture whoever was there and that was cheaper than hiring somebody to open their door right so they had thoughts that basically connected all of the different things that they needed to do in their product including you know notifying team members when errors and exceptions occur they had thoughts that inter interacted with their end-users many they even had thoughts for linking their text and phone and so everything that they were doing was connected and came through the same kind of platform so so normally I'm sailing at this time of year and the summit is always in the middle of when I'm supposed to be sailing and I was complaining to a couple of people last weekend / that not you know I I'd rather be sailing and then preparing a talk for today and so I thought I know I'll prepare a talk about sailing instead of this other thing so I'm actually gonna talk about how bolts and how AI are actually how AI really and how data analytics help sailing and how maybe we could learn or take some lessons take some analogies from that and maybe how we could apply it to software development teams so how many people here are familiar with the America's Cup a few people okay so for those that are not the America's Cup is probably sort of the most prestigious sailing race in the world it's been around since about the 1850s and America the u.s. actually held on to the cup for 132 years and it's now this very contentious race where you know what there's only one winner there's a quote I think from the first race where Queen Victoria was watching the race and somebody came in first and the America came in first and she said well who's going to be second and they sand somebody said to her your highness there is no second there is only first and so this race is like really intense and countries and syndicates now put a lot of money into building these boats they used to be 12 meters for most of the race and lately they have become these catamarans that are like up to I think in 2017 there were 72 feet long and the boats cost 10 million or more upwards and the reason I'm talking about it not because I wish I was on a boat right now but is because we can learn a lot about how they use data analytics and how they use that in this team approach to winning the America's Cup so when I was looking at this I found some articles and I'm sharing just some of those with you here and I'm hoping we can kind of discuss some of these lessons and see how we could apply them so there are several of ways that America's Cup champions say sail like successful IT teams and I'm pulling out some of these ways or some of these analogies from this article here so the first one is management is important but building the complete team is mission mission-critical so we saw this a little bit on my survey right that the manager as a factor was very important in terms of proceeded productivity but the team was as well and so this we see this here in sailing and this article talks a lot about rethinking who is on the team so you might imagine that if you look at a team on an America's Cup boat that the team refers to the sailors right that are on the team but it's not just the sailors it's everybody else around it it's the engineers it's the training team and in fact you know you might even say America's cup can be sometimes won or lost before they even launch the boat that's going to raise because so much engineering goes into it but I also want to put out there that we can think about these BOTS that are really a user interface to the AI that we're trying to build right for developers to use that we can think of those also as virtual team members so this AI that we're building or could build can become like a virtual team member in the team so that's just one way the second way that they talked about is that winning teams embrace disruption so the America's Cup teams over the years that didn't sort of spy and what the other teams were doing and see the new technology that they were using that they would lose right they just would not be able to keep up and the engineering systems team needs to respond at the speed of opportunity and in the preparation for these races so the more recent America Cups boats and I don't know if you can see this in this picture but they actually they're more like they're flying than sailing because they have this technology the New Zealand team did this first you know they had this this this insight that if they built this what they call a foil between the two the two holes of the catamaran that they would get lift right from the water and as they reach as they go over 18 knots the boat lifts up and it literally flies over the water and they can get you know going like just crazy speeds like over 60 kilometers an hour or even maybe it's miles an hour and so the boats when as as soon as somebody comes with something you have they have to jump at the speed of opportunity to really be able to work that effectively so what disruptions have we seen in software development so Neal touched on a few what are the big disruptions do you think we've seen in software engineering and I look at the people who are have been around a bit longer I can help you out if you want yeah the whole continuous deployment and what about what what enabled that can ya automation the cloud right the internet right email containerization right so these disruptions are changing the game right they're changing the the speed at which we can deploy and you know another one that I wanted to just touch on a little bit is the use of social technologies and the use of the cloud and how developers communicate with each other so I'm referring here to some studies that I did that looked at how developers particularly in open-source no longer sort of you know sit in their room and write their own code but they actually are part of a big community and they learn and they help other people and so you this what's called a participatory culture of software development so a lot of my friends that are not software engineers will say you know software debt developers they're so antisocial they don't talk to other developers they just sit in the basement and I'm like no not today developers have to be very social and they have to know how to use these tools and how to use something like stack overflow so imagine not having Stack Overflow today so this is another example of a disruption and I don't think that we were really aware that it was happening while it was happening it was just something that caught a lot of people off-guard and we had to just kind of rush to keep up so I think with AI that we're gonna have what kind of culture is AI gonna lead to in teams right so I don't know I bet I think it's something we need to think about ok so going back to the sailing analogies this is this one is it's all about the data so in this America's Cup team so I'm referring back here I think this was from the 2013 team the their boat had a thousand IOT sensors on the boat and they were producing something like 10 gigabytes of data an hour when they were sailing pretty incredible that foil that I talked about had 300 sensors on it just on that and so they're using this data basically to fine tune not not just the engineering of the system or the boat that they're it's a system actually that they're sailing on but to fight but also to fine tune what the crew are doing right and to learn what's going to make the boat go faster learning from the different sea conditions learning from the different wind conditions and then putting that all together in models and then using that to help them win and one of the things they talked about is that within the team that every individual within that team needs their own unique dashboard so that they can sort of pull out from that dashboard an understanding of what it is that they are contributing to the project and I think in software development as well that we need to kind of think about what kind of unique dashboards do we need to support the different people on the team and of course the need for explainable AI comes up but it's not just explainable AI to one person but it's explainable AI from one kind of stakeholder to a different kind of stake right so having them somebody in the middle have to do that explanation and the other thing about data that they talk about is really important is supporting post-mortems by the entire team so one of the things that they do is they sit down after the race or after they've been practicing they sit down and they play through everything they did and they look at the data and the whole team again is there and they look at what could we have done differently what could we have done better and in particular they did a post-mortem after this race and actually this wasn't even a race this was a training session I don't know if anybody saw this in the paper and the time but this was a team Oracle for the USA and the the sale actually on this boat is not made of fabric it's it's stiff and you can see the foils underneath there so they decided to push or the the skipper actually decided to just push it just a little bit further to see you know what can this boat really do and pushed it just a little bit more and then the conditions changed which happens in life right and the boat sank and it's this amazing I've got some links here if you're there it's fascinating to read this boat thank God dragged out underneath the San Francisco Golden Gate and it was going to be blown out to sea anyway eventually they managed to get it back in but they they lost months they lost thousands and that not just millions but they lost the time before the race to be able to train for the race because of this but they do say if something doesn't break it's too heavy and so they do try to push right what they're doing so I'll come back to that again so another analogy that I like from this article is that great ideas come from the front lines so they don't design the technology for these boats and for these sailors that have to actually sail the boats in laboratories just right I mean obviously they do a lot of the engineering work in the laboratory and they do a lot of simulations but they also go out onto the boats with the sailors and they watch what they do in race time and also in training time so they learn a lot from doing that and I want to push and I know Neil is doing that with his teams so that it's important to observe how developers are using the AI and a day-to-day basis not just individually but as part of the team to understand how we can improve it more and in terms of sort of that frontline thing here's an example of something that you might you might learn so on my boat we call my Auto helm so my Auto helm is is just basically a mechanical thing i sat in a compass direction or I can connect it to the GPS and it steers the boat so we don't have to steer the boat and we call it auto and we personify auto and and actually this blog post talks about the same kind of thing and when Auto screws up we say well autos cranky today you know like why why did Auto screw it up screw us up right and but we call our GPS the GPS and I always wondered like why why do we you know give a name to the the Auto helm but everything else is just the GPS or the depth sounder and this blog post actually shed light on this for me so the the blog the writer of this blog post he explains that on their boat the Auto helm the automation that takes over the auto hem is like a crew member so it is doing something that decides where the boats gonna go and actually affects kind of their their life right it's taking over what what a crew member would do and so this is the kind of thing that you can learn when you go on the front lines that maybe you know some of the AI should be under the covers right but maybe some of the AI should be as part of a bought within the channel where there's awareness and transparency and conversation and feedback and so on happening and then the idea that it's all about just you know the Machine kind of you know or you know the data or engineering winning the race it's not at the end of the day it does come down to people but it comes down to people and how they're supported by the technology so it really is this effective sort of integration this is just you know the skipper saying that yeah he made the mistake you know that that led to them losing one of the races and I just want to also mention this this quote because I really like this one and I think some people have seen that cyclical graph right that shows the a I HDI every time there's an AI winter HCI labs go up and then then AI jumps again and that rather than doing this cycle that we need to really think about addressing both of these at the same time so that HCI doesn't have to come in and clean up the mess right that the AI researchers leave behind okay and I put some discussion points for the panel but we can maybe come back to these later but I do think that we need to think a lot more about how AI can enable but are an AI enabled BOTS can support or even potentially harm software team collaboration and communication I haven't talked about the risks in this talk I've done that in other talks but there's lots of risks and we've heard about lots of them at the other breakouts how can engineering system and development teams together embrace and evaluate disruptions from the AI so on the America's Cup team the engineering team and the sailors are one team and I think that in development we could think about that a lot more as well have a lot more closer collaboration between engineering team and the end the developers themselves and I didn't talk a lot about dashboards but I do think that this is an interesting future piece of work how these dashboards these AI infused dashboards could support tactical decision making support operations and also post mortems and how they could be personalized and how we could study them and by the way I do think we could use BOTS to study right some of these things as well so I pass it to you so while I set up prime square enix frame slides if anybody has questions for Peggy comment yeah that's that's very much a work in progress and it's a really interesting question yeah yeah I mean I think right now the way that BOTS are being used and being designed it's very ad hoc so it's kind of like here's an idea for a bot let's let's write one and let's deploy it and then see see how it sticks and see how it's being used and I think it could be useful to take more of an engineering or architecture or system perspective on the BOTS and understand where they play and really think about should this be a bot you know there is conversation enabled or should it just be you know and I think I'm kind of thinking in API terms it's when when would this thing want to interact like another human with this yeah that's the really interesting things yeah really super interesting yeah okay so thanks for the invitation it's my pleasure to be here and talk about this area so this is kind of taking over my life for the past eight years or so okay so let me just explain what this term naturalist means there's some confusion about this particular term so you know so the way I like to explain it is that like human speech and writing have evolved you know over thousands of years to to serve certain national human purposes and the kind of structure and use of these human languages you know I'll talk about where they come from but what we've discovered is that the same kinds of structures and patterns of usage also exist in code so now what does that mean right so when you think about natural language you know a good example is this guy here is about to have a very bad day and you know let's say his children are on the side of the pool and so how is he going to react to this right so is he going to sit up and recite some think of some glorious poem and recite it or is he just simply going to say you know get out of here right so so this is kind of the imperative under which natural languages evolve they've evolved to communicate very efficiently and quickly in noisy dangerous and distracted environments now what does this have to do with cord now this fella here let's assume that for a moment that he's actually a developer and that person is actually his manager right so is this is this chap now going to go off and think of glorious continuations and monads and recursive this is and that or is he just going to write the loop that he wants to write the simplest and most mundane way he can think of right so a lot of coding gets done under these circumstances and it's not just simply the coding as you'll see later it's when people think about what kind of code they want to write it's not just themselves they're thinking of right that also relates to natural language because when I'm speaking I'm not just simply thinking about myself and how I want to construct my utterances I'm also thinking about the listener and how they're going to react to my utterances so in that sense speaking is a very cooperative act because and a very conscious and mutually conscious act and so is coding so so essentially what natural means here is that human utterances in human speech are constructed in noisy dangerous and distracted environments and as a result the way we speak is highly repetitive very predictable and can be modeled statistically right and this is wonderful because this is what enables things like Google Translate speech recognition and the forms of tools that have made all these wonderful advances in natural language processing so code is a little bit different right so code is primarily intended to run on machines so when a programmer here is writing code they're sort of in some sense the end intent of it that is actually executing a machine machines don't really care how you write it it doesn't matter whether you write I less than 10 or 10 bigger than I or I plus 1 or 1 plus I it doesn't really matter and she can has a lot of flexibility in how she chooses to write her code so so far not much call for naturalness right but in fact the cords actually maintained by another developer and when the quarter writes code she is thinking about who's going to maintain the code and how the maintainer is going to read the cord and in fact when the maintainer reads the cord it's really a noisy Channel right so maintained errs are not computers maintainers don't do operational semantics or denotational semantics in their head they're actually genuinely reading in a noisy channel environment so by this I mean actually mean the Shannon noisy Channel right so when a developer reads cord she's thinking to herself this is probably the computation that their developer intended to write the maintainer hypothesizes and then the maintainer says if the developer were meaning to implement this computation what's the most likely way that she would have implemented it so I'm really sort of recapturing the Bayesian formulation of the 9c channel model right so she's gonna guess how would she have implemented this and she's gonna look for those bits in the cord and if those bits are not there she's gonna say well she must have done been doing something else he's gonna help but there's a different computation that the developer may have intended and she's gonna see how think about how that would have been implemented so it really is the noisy Channel right you're not computing semantics directly from the program using an operational Channel you're guessing meanings that's how people read code anyway and I can't prove this but this is just my hypothesis right so this formulation is called we call it the deu model because it's came out of a discussion between people at UC Davis and Edinburgh and UCL so Charles Sutton at Edinburgh and Albar at UCL so we don't have quite have a name for this formulation these times we call it by mortal comprehension sometimes we call it to channel comprehension we're not sure you the cool thing is that this the second channel here is really actually a noisy channel and there's a lot of interesting questions that come up out of this so right okay so because of this noisy channel that exists in code and because it's not formal operational semantics base software as it is used is a repetitive predictable and amenable to statistical modeling because the same imperatives that applied to the construction of natural language utterances apply to the construction of code okay all right so so what right so so what it's taken over the last eight years of my life right and there are two aspects to this the first aspect which I'm not going to say a lot about because I got the sense that really I should talk more about the engineering aspects of this in this forum but this is I spend more than half my life these days on the scientific aspects of this and we do a lot of human subjects studies we're doing a lot of corpus Studies and maybe even done some eye tracking studies so how does correlate to human preference and and performance right so you know people never say bread and butter for butter and bread for example they always say bread and butter right nobody ever writes I equals 1 plus I right so those things are actually related and relates to the noisy Channel right you expect certain computations to be written in certain ways and if you read it a different way it actually impedes comprehension and impedes impede smooth easy reading of course so we're doing a lot of human subjects studies in this area we just finished a mechanical turk study with 70 participants trying to figure out if we can predict which forms of writing code would be preferred by human beings and B and we can the and the next step is for us is to do to determine with what accuracy we can prefer how people would prefer the C code being written and then after that we're going to do some coded comprehension studies to see if we can predict which code is going to be easier for human beings to understand and which kind of code is going to be harder for human beings to read and this work is being done in collaboration with Emily Morgan who is a psycho linguist UC Davis and so this is very exciting for us because she's generally been studying forms of expression in natural language using a theory called rational speech act theory and we're now trying to apply that same theory to code but that's all I'm going to say about it I'm happy to discuss it more if anybody's interested in in the in the in the panel discussion all right so the other part of my life is trying to explore the engineering question how do you exploit code repetitiveness to help programmers so the first paper on this was written about eight years ago in our group and you know so we exploited some interesting properties to do some application so I'll talk more about the applications for the rest of this talk okay so you know the basic general scheme is you know as Neil mentioned is that you know this is sort of the way people used to build tools you think of a tool and the developer decides okay I need this tool so she goes off and writes a tool and then the tool can process source code and produce results so a lot of you know tools fit into this framework right so the twist here is that because now we have this property of naturalness you can take a large court corpus estimate various kinds of statistical models and these critical models can then improve the performance of the tool so this is sort of the framework for a lot of the tools and the you know the details depend upon the model you want to build the data you have and how the model improves the performance of the tool okay so you know there's lots of applications the first application that we did in back in 2011 was called suggestion I know and that's basically you estimate a model like that and there are various ways to do models we've kind of switched over to completely using you know neural network based models these days but you know there's lots of ways to estimate these models and you know obviously there's lots of data there's quite a few papers that that have described this we have something on github you can welcome to use it's called SLP core a lot of people are using it it's until very recently some very recent work from from from from Edinburgh this was our model was the best profiling model so so one very interesting problem in court that doesn't occur in natural languages vocabulary explosion so in natural language as you scan more and more tests that the Cadbury starts to grow slower and slower and eventually it saturates it's only like place names and people names that begin to grow but every new model of court introduces new vocabulary so there's a real problem in vocabulary and until very recently be planning models were terrible at it you have to cap the vocabulary are essentially the number of parameters he of the train is is becomes unmanageable the recent work from from Karen Paterson Sutton show how to deal with vocabulary explosion in D planning models using something called byte pattern coding which is a way of splitting up identifiers and and doing this quite efficiently so this is kind of I guess the the sort of the first application that has been was done and I was great to see it being used now and shipped and actually apparently being used by millions of users I saw a demo of intelligence and it was it was great to see that the big thing that has come up and a largest JavaScript the obfuscation it's I put the office Keshan in courts because it's not really the obfuscation is basically replacing you know dumb names with better names so you know people are people minify Java code and chips out and you can recover the names from it and so this is basically estimating a model of this sort you know estimating clear code using minified code by the way all these models you know this one and that one need data for estimation and you know and all these things you have huge amounts of data it's not really a problem you know once you have a tool the best minification you know you can produce as much data as you want to do this task and you know again there are various kinds of models the first one in the along these lines was a model based on conditional random fields from Rachel vitality th we've done some work along these lines using free space translation that works in a complimentary way to the CRF work from Revit also and actually if you put the two together you get much better performance the same technology is now being used for recovering identifier names from binaries in decompiled binaries so it's it's a pretty useful thing the other kind of exciting application is gradual typing so gradual typing is a framework where developers it's sort of a compromise between Java like languages where everything has to be declared in Python like languages where nothing has to be declared this is a way where you add declarations to suit yourself to sort of like look for errors in places where you think that you might make typing errors so this estimates a model of this type estimate the type this distribution the type given the name and the context so a lot of people have used context more recently this year there's some work on using the name of the variable to estimate this distribution and there's a bunch of work in this area we've done work on this so there's been work from ETH and Michael throttle has now a Schrute card has also done work in this area so these are some of the emerging early applications of this the first applications but there's a there's a lot of there's a lot of work to be done along these lines so so this is sort of like the simplest model you can think of and it turns out this is actually quite useful and that's for checking code right so so you can just simply estimate a model over a large corpus and if the model says the code is weird it turns out this actually you know quite quite accurate in finding finding defects so this was the paper in Dixie 2016 and in our lay enough I mean it's not a fair comparison but oddly enough this is about as good as many things like fine bugs of course fine box tells you what do you thinks is wrong this doesn't tell you anything except saying this looks weird right so so so this is kind of the simplest possible thing but it turns out you know already is arguably useful so this was the suggestion thing that I talked about earlier variable name recovery this can be used to you know again you can estimate this with any large corpus and so you can use this to either recover names as in the context of be efficient or you can use it to check whether using an improper valuable name so this is the the gradual typing problem and you know again I talked about this and it sort of again can be trained from large amounts of data and so so far these are sort of what you know what Neal was talking about is with this inner circle stuff there's also kind of more outer circle stuff that relates more to like not immediate coding but more sort of process oriented things so interestingly there's lots of data on these lines and there's been a number of papers coming out recently that allow you to to repair code so so there's been a few different experiments using some standard data sets to see how well machine learning models can pass code most of the existing work on automatic code patching has been using genetic approaches in other words you sort of have the hill-climbing search searching over a lot of possible patches trying to find something that can actually patch your defect the new approaches involve no search or very little search so they're much more efficient it's basically you train a translation model using large amounts of data from github that talked about how you know typically the datasets use one-line patches and they train a translator to translate from all quote in new code using sequence to sequence models or transfer transformer model so there's most of the recent work is now we have anyway anyway most of our work now has been using transformers and they're they are not as powerful and successful as the genetic approaches but they're much quicker so so this is kind of interesting stuff to do so the data can be trained using commit data you can also simply do you know things like denoising auto-encoder and so on and so forth and we've done some experiments with large datasets of student programs and denoising auto-encoders can correct about 50 to 60 percent of student errors so that's those those are syntax errors so this is another kind of interesting area of research then this is the so this is essentially producing English from code and there's some interesting issues here that there are that are going on right so there's there's useful comments and there's useless comments right so so there's for the kind of comments that are needed by somebody who is unfamiliar with the code is one thing the kind of comments that are needed by people who are familiar with the code is something else altogether so in other words you know if the code is written the way you expect it to be written right then this comment is sort of most useful to people who were unfamiliar with it but sometimes the court is not written the way you expect it and then you really need comments to explain what the court is doing so there's a summarization and there's commenting they're two different things and so Charles Sutton has some very interesting work where he tries to say if the English is predictable from the code that the comment is predictable from the court then it's actually probably useless comment for most people right so so there's some interesting stuff going on here and we don't quite know how to classify the two kinds of comments the comment that is really a little add one to I who cares right whereas something more complex like you know resync the lock or something like that which is not what you expect I equals I plus 1 to be doing right so so that's an interesting question then this code retrieval I mean I you know now we're really far away from sort of the singularity of all this this is mostly about like you know for example using Stack Overflow data so that you give some English description and you can find the code there are some very interesting problems here we've just finished a code snippet parser in typer so we can type and parse cord snippets with very high accuracy so that you can take fragments on stackoverflow and you know what type the bits are and you know what syntax it is so we can do it with well over 95% accuracy so and so you need that because if you're trying to paste some code from Stack Overflow to retrieve some cord and paste it you wouldn't need to be able to parse it and type it and then finally you know given some code recommend a person and this is for tasks various kinds of tasks assignment so this is very interesting we've now managed to train language models specifically individual developers in a project so you take it take a general language model trained over large code corpus you know and then specialize that model for each developer so given a code fragment we can score each developer so that we know you know how familiar that developer is with that code fragment and this you know we haven't you know we welcome collaboration on this so we're trying to apply to recommend code reviewers but I think there's lots of other potential applications let's say for example you get a warning on a piece of cord and you know you don't necessarily want to go ask the person who implemented the cord or the person last change the code to comment on it because that person may have committed to that module but they may not have touched that money in a long time so what you'd like to know is who's written code like this and language models are pretty good at supporting that so we welcome partners on this because it's it's kind of hard to do that particular experiment whether you're recommending the right person to fix a warning using open source code so you know if anybody's interested we'd be happy to work with you on this so so this is just some examples there's a lot more stuff we're doing some stuff would recommend to determine validity of invariants you know I think there's there's a lot of opportunities so anyway these are all different types of probabilistic probabilistic models now how exactly you approximate these probabilities from data sets you know there's a lot of ways to do that so there's many different models you know so there's you know discrete traditional models in Grand Aryans there's tree based models like pcfg and TS GS probabilistic context-free grammars and trees tree structure grammars there are faith-based translation models that there have been used this conditional random fields and of course you know now a lot of attention is being paid to these deep learning models lexical models sequence to sequence models sequence tagging models so as the difference between these two is a sequence tagging model the input and output length is identical so in part of speech tagging for instance or when you want to assign types to variables we only guess types of variables the inputs and output sizes are equivalent so these models generally tend to perform better because the the length is conserved anyway transformers are becoming very powerful and we've had a lot of success with them we hope to have some papers coming out soon with them they're very easy to train they're very efficient their enormous capacity so it's pretty promising and gated graphing a lot work so these models are very promising and powerful but they're very slow so there's some recent work at Google where they have managed to speed these think quite a bit using data structure layouts we're also doing some experiments trying to find ways to speed up the training of the aircraft networks so as there's lots of ways to approximate these functions that that I showed you in the previous slide and and you know it's all about finding enough data to train it so there are various issues that this I'm going to stop with this slide this is this is some of the issues that that are faced in court so one big problem in court is vocabulary proliferation and the DP seems very promising and we're getting good results with that as well we've been able to replicate the results from from certain competitors even in some cases improve on it so maybe that is the solution I don't know another big problem is explained ability so so I actually learned a few things that this at this faculty seminar that I'm really eager to go back and try so you know when you suggest a patch to a programmer or you suggest a type 4 in a gradual typing environment or you suggest a change or something along these lines or just you just give a normal course suggestion it'd be nice to have some explanations of why you're doing that especially with patching and also with like saying this code looks bad you should fix it it'd be nice to have some examples and I think there are ways to deal with this but I think this is really exciting and interesting open area and finally um so I think that sort of in some sense the most exciting thing about cord is the fact that it has both operational semantics and noisy channel going on right so you're writing court for the computer and writing code to actually to actually be read by a human being and so there's a probabilistic side to cord and then there's a deterministic formal side to code so how do you exploit these two things together I think some of the most exciting work in this area is gonna come out of that so stop there questions have they gone over or sorry the last speaker I said hopefully can stay for a few minutes longer so that we can get through the talk and I'm gonna had some really interesting things to say related to data and some of the work he did with the companies he wrote with okay okay yeah thanks so much giving a run in to break at least five minutes so hopefully can stay for that okay I'm gonna try to do this talk in the two minutes okay so basically so this is my lab we are the software analysis intelligence lab and what I want to do today is I want to talk about primarily our experience of actually having machine learning models that actually software developers have been using so some of these are systems actually not what I was thinking about I've been used for the last ten years or actually a decade of systems that was using machine learning on software engineering data and can our experience some more how developers have actually found them now quick overview so software developers produce quite a bit of data so things like code changes release notes bug reports email discussion code reviews we call these development repositories but as well users as well produce a lot of data so things like crashes locks or limit reads reviews okay we call these field repositories okay another type of data as well a people use so online code Stack Overflow github so all that information today unfortunately is only one way right the data doesn't call back in your next decision-making process the simple thing is think of Amazon how Amazon tells you when you buy a you buy B you could do that same thing for software developers when you change a people whenever people change your a they change B so now this is an area that kind of been going for many years it's called basically the mining software repository community so the mining software repository community now has been around 20 years it should actually consider one of the top 10 venues by a by Google Scholar for systems research ok so there's many many people that have been doing stuff on this I'm just giving you a flavor that's influencing about what I'm doing but if you're interested in this so we do check this community ok so I said so many companies are actually use this today in practice so these are some of them okay now really at the end of a this there's data there's a model and where we care about it's two things so we care about a decision and we care about insight now the decision is kind of like you want to say okay it's this piece of code buggy or not okay so that's binary but actually for developers and we talk to managers you don't care as much about this one they care about how to improve the process in the future right so that's actually where the insight comes in right for them the insight is more important for them actually that the decision for this bug because that will only fix this specific one right so now I'm gonna give you some flavors of that so ideally what you want is when a new coat change comes in you want to be able to flag it right so red yellow green so red means there is something really bad about this go change yellow you know this looks a bit kind of worrisome guys green is go ahead commit that okay now we were lucky actually that we work through his blackberry on this project and what they had is they had for their developers for every coat change so this was done on the device software for the BlackBerry's they would actually rank manually the change whether it's high risk or low risk okay now I want to emphasize something which is we learn is for them he didn't care about buggy or not buggy he actually cared about risks which means much bigger influence and just buggy or not buggy for example a change might be super simple you updated the UI but for some reason that change is touching a piece of code order required the whole codebase to be recertified by actually the the carriers so we'll push the whole release by a couple of months it's a very simple change but it's a high-risk change so now the idea is we have all these changes here we have risk classifications so wouldn't it be nice that actually we will learn from this build a model so now when a new change comes in we can actually predict that right so and that's exactly what we did so we took one year of changes okay and we actually would pass them through the human developers and we'll pass them for the model and then we actually get this is the risk that actually the developers did there's a system there this is the risk that actually the developers did and it's point eight for correlation correlations go from zero to one zero means it's random one is means it's perfect so actually we got a good correlation here now this was actually a study where we the student was there for a year so actually he looked at around 450 developers doing that across 60 teams now an interesting thing we learned out of this is actually what they did when they started using it didn't only they didn't only depend on the automated system they actually dependent on both of them so they treated both of them as different experts so if the developer says this is high risk and then the our system says low risk it will still be high risk and the other way around if we say it's low risk developer says is high-risk right so this is an example where it's not always about you know the system is right and you want to replace it it's more to support the human now this type of work has actually been kind of there's an open-source project called commit guru which you can actually go in and actually upload your it will analyze your codebase and I will give you that type of analysis now a lot of companies as well have adopted this so Ubisoft does that and many other companies now actually have developed this in-house that now this is the Ubisoft is done independent for my team and many other companies have done that okay now another so this is our call that gets the inner loop now when you talk about the outer loop is they're gonna look at actually at testing and very large-scale testing so think of Amazon do you don't only want to make sure that one user can buy a book you want to make sure that a million people could buy a book and nothing could go wrong now the problem today is a lot of these tests the way they're they're verified that the pass is did anything crash nothing crashed we're good right well that's probably not the best way to go about it so we wanted to actually leverage some of the data that's produced from these tests okay so simple thing is you have a sequence of events like let's say it's Oh logs in this case so we do a lot of log analytics and you know so we have event two so acquiring a log it's all about releasing a log so really what happens is we do this so we look at actually what's happening in the test and we because it's a test we expect to be very repetitive so if we see any deviations from the repetitiveness we can actually flag it now the beauty of this is we actually we and actually most developers don't exactly know how the system runs at scale but actually recovering that information with the logs we can actually flag look that this e six happen and it shouldn't happen okay so now what really happens is you actually the output out of this is something like this so says this log message here so this is the Dell DVD storage that is a application used by Dell to actually benchmark their servers so and so it's basically like a DVD store so you're entering a per se so 98% of the time this message is followed by this long message is followed by this long message one percent of the time this log messages fall by this message and your 358 times this sequence is followed and then what happens is actually all these sequences are sorted right so the highest one here like this one here the one I expanded actually shows you the most weirdest kind of sequence that we have seen so then what happens is the tests are now can copy and paste this this HTML kind of document and email to a developer and ask them okay what's happening in here good so this arrived you know ninety-eight ninety-nine point nine percent reduction the view log files okay and the procedure is quite high you know 56 to 100 percent now what was nice about this is we actually give them as well an example of exactly like that sequence of what went wrong in there now you could take that same log sequencing and you can start adding time on it okay so we know event what event will ABCD happen together now we can say you know it took this sequence like five seconds okay now we've seen that so many times so now we can create a distribution okay so the on the black here on the black s on the left side is one actually a run and on the right side is another run if you can see them because of the red here it is actually a bit slow or this run can put it other one okay and then here we'll actually lays it over time again send here what it's saying is you know we want to make sure maybe because what could happen is over time the system might be getting bad right or in this case it's always bad here okay okay so this was an example actually in my sequel there was a bug in my sequel so this actually eventually got fixed okay so these are some examples of actually how we have used this data some of these systems were actually being used as I said for last ten years okay so the question is what's the secret for long term and does your adoption so is it highly scalable top-performing models and the answer is actually not really the case and I'm gonna spend the next slide which is actually the last slide to explain at least my father but really what makes some of these things work okay one of the things that's special about we have is the hobbies we have a human in the loop a lot of the decisions we give them is now this person has to go to somebody up and manage to save example we're not gonna release the system we're not going to do this release now it's very hard to go I cannot do this release because this deep learner is saying no right so they really need to be have something that supports their decision because the manager will say get out of my room okay so this is kind of one of the big challenge here is how do you make that now there are two things so what I would say a domain challenges and some of them are actually machine learning challenges and I'm gonna go through them kind of one at a time so one of the tricks I think we found is it's really important that the decision or whatever the machine learning system gives you is assignable to somebody so an example for that is in this bug in the in the system I would actually say look this is a buggy change what was good about that is two things is we actually were able to tell we had a specific person which is this developer that did the change where we can say this is your problem deal with it which was very different and prior work in the area which barkaneers okay before the release analyze all your code changes and flag buggy files and non buggy files the question is okay so you know this file is super buggy so who do you give it to right where is this one it's your problem deal is your hopper there okay so that was one thing the other thing too is timely so again I'm gonna use that example just for time but the it applies for the other ones to you is we did it right there you have because it happened inside the IDE like you had that files open you could deal with it now right but when you say look this file that had 50 people that worked in it some of them work there are like a couple months ago it's troublesome it was much harder to do that now machine learning challenge so explainable is a key thing and the reason we want the models to be explainable is a lot of the developers and managers don't only care about this release they care about the long term you know if there's something we're doing bad we want to be able to flag it and improve our processes and this is what the explain ability was a key thing now another one that actually I've recently started relies is this idea of trust so let's say you have you have a lot of data raw data okay and over the years developers are smart and they've actually did their own scripts and their own I would say none a imodels just some warning signals that they know about so they have a script that runs over all the raw data and if that script says something is bad the deep trust that system right so now you have two ways you can go the deep learning idea which is just take all the raw data okay or you go to this simple way let's take there what I would say trustable dumb models which one would you go is right so intuitively you say well let's take all the raw data get the human out of the loop but instead actually it is much more easier to get the system adopt at least in our experience where you say look this is based on a combination of all your basic models you trusted all of these and we just built on top of your trust so that was one essential thing okay and the other thing is this maintainability aspect um and a lot of corporations the the machine learning team there's like one or two big teams and these teams have to go from different you know to help a group and then they need to go out like more of a consulting setup right so the idea is your models you want to be able to set them up leave not every two days get a phone call hey can you come and help us tweak the parameters right so this is an essential part as well okay so that's your like my big views on like really what I felt kind of worked and why some of these systems have been used for many years like you know this is systems we use to almost ten years now I want to think about it and we are never really called in to tweak the models on there right okay so is that in mind this is kind of basically what I talked about today so I talked I introduced about this idea of the mining software positive community and how there's so much data today and you can actually look at that data and produce it and reuse it to actually make your next decision I'll give you an example can how you could use the data about prior changes to actually detect whatever changes high risk or low risk I'll give you another example as well how to use logs so that outer loop aspect they actually attack performance problems and a system by actually mining these logs which are rarely ever used and then I walk through very rapidly of some of the reasons why I think is more essential to focus on these over just blindly the performance of the model itself okay so that's it and what is that ten minutes you're five minutes before today 