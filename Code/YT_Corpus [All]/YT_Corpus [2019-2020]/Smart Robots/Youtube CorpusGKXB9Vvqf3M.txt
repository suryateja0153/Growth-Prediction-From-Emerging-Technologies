 well it was certainly a great pleasure to introduce professor leslie cabling she's a professor of computer science at mit she did some amazing work on in robotics in reinforcement learning in planning in machine learning in fact she founded the journal of machine learning about what 20 years ago at this point so we are all looking uh very much looking forward to election leslie welcome to the brain machines summer course okay well thanks very much boris and uh welcome to everyone so okay so what's my research goal i come from the machine's end of this world roughly and what i really want to do is figure out how it is that we can make intelligent robots and i do this mostly because i'm interested in tele in intelligence more than i'm interested actually in robots but i think that trying to make a physical uh agent who out goes out and interacts in the world is a really good test bed for understanding um what kinds of reasoning and and perception and control we need in order to make it an intelligent system so the way i think about the problems this is kind of a definitely a computer scientist way to think about the problem um is to think about the robot as a as a transducer as some kind of a system that's connected up to the world and it makes observations of the world and it takes actions that change the state of the world and presumably it has there's some objective right we want to uh we want to take actions that change the state of the world in in some way that we think will be good the reason i want to start by backing all the way up to this like very basic control theory picture is that right now there's an enormous amount of argument about how one should make robots should they do planning and reasoning should they do reinforcement learning how should we do it so there's a huge kind of crisis almost in the field about what the best methods are and what i want to start out this talk by doing is actually thinking about how we can answer that question in a way that's not political or religious but technical so the way i want to think about this the job of this program so i'm going to make a robot i'm going to put a program in the head of the robot so let's say i'm not going to worry about hardware i'm just going to worry about the software and so the program that i'm going to put in the head of my robot it has to do this job that's written in the formula up here and what this is just shorthand for saying is that it has to represent some kind of mapping from observation and actions that it's had in the past so a oa star means the whole history of observations and actions that it's ever had based on that and has to pick the next action so that's not really saying much of anything at all that's just a description of every single robot control program basically that's been written you have to take your history of actions and observations compute the next action and so what we want to do is think about first of all what's the best what would be the best pie to put inside the robot how can we think about that and then we have to think about the problem of how is it that we in my case as me as an engineer i'm going to find that pie that i should put in my robot so one way to think about the whole problem setup then is that that i as the robotics engineer have to do for my robots the job that nature did for you that is to say i have to think about i'm i'm a robot factory i'm going to make these robots and the robots are going to go out in the world maybe they're going to go and work in people's kitchens or something and every kitchen is going to be different so there's going to be a lot that i don't know about the world but somehow i have to figure out the best program one program to put in the head of all my robots so that when they go out in the world to behave they can do a good job so that's the way that i think about the problem that i face and in order to think about what would be the best program uh i kind of think about it this way so i imagine that there's some distribution over possible environments that the robot could find itself in when it actually goes out into the world right so maybe it's going to go to houses and the houses are all somewhat different and once i put that program in the house maybe it's going to do some estimation or learning it's going to adapt to the circumstances it's in my job is to find a program that does a good job of adapting in all the environments it might find itself in so imagine that you have some kind of like probability distribution over the way over the worlds that the robot could actually end up operating in i want to find a program that's going to behave well let's say get a lot of reward in expectation on average over all the environments that it could possibly find itself in so that's that's i would say a kind of a reasonable formal objective for a robot um and one thing that's good about this as an objective is that um we don't have to argue about it right it doesn't it it doesn't say whether there should be learning in there or what kind of learning or should it be a genetic algorithm or should have planning in some sense you could say i just want to make the program that's going to be the best that can be on average over these environments but the problem is now i've written down an objective function i've said oh if you could tell me a distribution over possible worlds that you'd like this program to work well in then i know in a certain mathematical sense what the best program is but now my problem as the engineer as the person who is in the robot factory right which is again kind of maybe analogous to the problem of of nature is i have to figure out what is how do i how do i find this program that's going to be good in all these situations there's a bunch of ways you could think about the problem i mean one would be to say oh i'm really lazy i don't really want to think very much about working in the factory it seems awfully hard i will just make a robot that has roughly an empty head it doesn't really know very much at all and then it just has to interact in the world and learn everything by interacting but of course you don't really want a robot that comes to your kitchen and begins to learn about physics right that would be break a lot of dishes another strategy and this is like the classic engineering strategy is that no i'm like a serious engineer and i'm going to sit here and think really really hard and i'm going to write a program and it's going to be a great program and i'm just going to put it straight in the robot's head and it's going to go off and it's going to be awesome and do everything it needs to do and that strategy actually can work very well in certain kinds of problems it lets that you know the boston dynamics robots do parkour but as we try to address bigger and more complicated problems it becomes harder and harder for engineers to just straight up write the program we could just try to figure out how humans work because humans work pretty well in a variety of domains and so one program right would be to say well we figure out how humans work and then that's what we do we make robots that work like that so first of all that's a hard biology problem i think it's very important that people work on it but it's also not a general engineering methodology because for instance i might want robots that work in certain kinds of circumstances or problem domains that are really different from the niche that humans are well tuned for and so uh i you know i might want to make a robot that isn't really human-like in its intelligence um and then it seems like what we're left with and maybe we could just say well we'll somehow recapitulate evolution like we just search around in the space of programs and try to find ones that work well and then eventually get ones that are great for our environment but that seems slow and complicated so if i like enumerate my options and they all don't look very good i don't know what to do um so uh one thing to think about though is is this last thing so uh the the kind of evolution idea so let's just pursue this a little bit more so imagine that we want to try to find a program that works well in expectation over all environments one way to think about that is that like inside the factory we kind of simulate a bunch of environments we try a bunch of robot programs and we try to find one that works well in all those environments uh and that's um that's like a really interesting strategy we would have to think of us a space of possible programs for the robot some objective function we figure out well what are we trying to optimize a distribution over problems to test um in some sense uh this is a a thing that people have thought about for a long time right this would be like running some kind of evolutionary algorithm or some search or simulation um inside the factory and it's very attractive but i think generally speaking hard to make work well so the question is what should i do right should i just like i could maybe i could set up this whole evolutionary setup somehow and then i could just snooze for a really long time while some very complicated program tries to figure out the best robot program to put in the head of the robot um but i'm i don't know i am simultaneously i'm too impatient for that and so then the question is can i somehow take pieces and parts of all these ideas uh some human programming some robot learning in the wild some kind of search or evolution offline some inspiration from humans can i take all those things and put them together and see if i can find a way to engineer intelligent robots um so that's basically what i'm up to um i'm gonna well no okay let me say something about this right so so then the the one way to view the research agenda is to say that uh first of all i'd like to be inspired by what we know about humans and in particular i'm very interested in the this belky core knowledge type stuff because that tells me something about what evolution in some sense saw fit to engineer into natural intelligences and if i understand that natural systems seem to be born with a bias or some built-in structure uh to think in terms of of other agents uh to understand that they move through 3d space to talk about to think about objects as you know clumps of matter that cohere that's a very helpful engineering bias for building a system i also know just some physics and variants about the worlds that my robot is going to operate in and maybe humans don't have this built in but they explicitly but they almost really have it built in implicitly um and i also have some other constraints as an engineer who's trying to make intelligent robots which is that humans are the engineers right so if humans have to engineer a very complicated system then it has to be the engineering process has to have some modularity to it because humans are really bad at understanding one big messy system they're good at understanding pieces and parts that work together so it may be that we have to take a modular design approach in our engineering efforts for intelligence not because the intelligence needs to have that architecture but because we the human engineers need those tools for actually building a system so all these constraints need to somehow come together into a way of building intelligence systems okay actually i would stop here for a minute just because it's a convenient spot and see if there are questions i see some red q a button so maybe someone can ask oh yeah we have a question here from sasha furlik uh you said there's a big debate about which way to go in the quest for intelligent behavior but is there any established formalism that isn't reinforcement learning or essentially a derivation thereof yeah actually um for years there has been so a more typical formalization would be in terms of predictive models and planning or reasoning um so reinforcement learning and also it depends the the phrase unfortunately the phrase reinforcement learning has it grows and stretches to and sometimes for many people and in many discourses it's come to mean all of intelligent behavior in which case i would say well no it's all reinforcement learning but that's vacuous but um another for other formulations involve reasoning about objects and and their relationships and thinking about the long-term consequences of taking actions in the world and so on so there's certainly different ways of framing and formalizing the problem and they give you very different computational profiles and different learning strategies okay good so um i will just tell you some story because people usually like story and it's kind of the afternoon so and this is related to the question about reinforcement learning probably right so how did i get into this whole thing um when i just finished my undergraduate degree which actually was in philosophy weirdly enough i went to work at a research institute while i was starting my phd and they had this robot nobody really knew actually very much about robotics and it was my job as the brand new person to try to get the robot to drive down the hallway and so what happened was i programmed the robot and it would run into the wall and i would bring it back and i would fix the programming and it would run into the wall again hopefully for a slightly different reason and over the course of a couple weeks i managed to build write a program that would use these funny sonar sensors on the robot and make it drive down a hall without crashing into the walls and so that was good and i was happy in a way at the end of that that i had gotten it to work but i reflected on that a bit more and what i decided was that i had learned how to navigate down the hallway using the sonar sensors um and what i thought was that and it had taken a long time and it was kind of a hassle and really the system should have been doing the learning not me and so my view was that i should figure out a way to get out of the loop to build systems that could learn on their own to do stuff and then i could just wait for them to do that and that would be better so that that was flaky um then i kind of uh i sort of reinvented reinforcement learning in a not very good way really but um it was kind of entertaining and i this is a slide by the way for those young people in the audience you might know but back in the day we used to write with colored pens on pieces of clear plastic and that's what we used to give talks so i had this kind of pseudo-reinforcement learning thing and by 1990 i actually had this little robot called spanky they did actual reinforcement learning during my actual defense uh so it didn't learn anything too complicated but it didn't do it in real time so that was kind of fun um so okay so i finished my phd and i thought okay i know something about robot learning now but i really want to make robots that can do complicated things and i couldn't figure out how to get basic reinforcement learning methods to really scale up to problems that i cared about and so i this is one last slide i'll show you from some talk that i gave in 1995 and i kind of complained that the ideal that you could take just a big bunch of what i'd like to call neural goo now it's just a big bunch of generic neural network stuff and train it to be an intelligent agent all by itself but that wasn't going to be feasible and instead we needed some kind of compositional structure and that would give us more efficient learning and more robust behavior and so on so i'm still there okay so i'm still i'm still trying to figure out uh how we can design an architecture that can learn efficiently and so the research strategy that i have really adopted i worked closely with a colleague tomas lozano perez um our strategy has been the following which is to try to think of some very generic representation and inference mechanisms and build those in and then figure out how to learn the rest of this stuff and we're all used to i think by now the idea of some representation and inference mechanisms that we would want to build in for instance everyone's used to the idea of convolution now in image space right so uh but if you think about it and and i've had people tell me who work on convolutional neural networks that they don't build any structure into their system it's just a neural network but of course as soon as you build the convolutional structure into a neural network you are taking a position on some regularities that are in the the input signal and so on and you're taking advantage of that so that you don't have to learn a whole fully connected network but you just learn some convolutional kernels so just as convolution gives us a great leverage when you apply it to the right part of the problem then the intuition is well hopefully there there's a few mechanisms hopefully not like a hundred mechanisms but maybe ten and then if we figure out how to use those mechanisms to bias learning and to structure behavior that we could learn robust uh ways of behaving that that are efficient and so on so one set of possible kind of general ideas includes convolution in space also in time maybe understanding the kinematics of the system that it's connected together in joints and segments uh a notion of planning to move through space uh being able to do causal reasoning if i were to do this what would happen abstracting over individual objects various kinds of state and temporal abstraction and so on so our view i don't want to commit to a particular list but is that there's a list of structural principles that are pretty generic very broadly useful and we should build them in um so actually is any questions here yet no everything's been perfectly clear so no questions okay i'll keep going i'll surely be able to offend some people soon i'll work harder at that um okay so so if we kind of accept this idea that we're gonna build in some structure then what and the thing that that my colleague and i have done recently well now maybe not super recently but we said okay in order to test out the idea that this there's a set of mechanisms that that would work well what we did was we hand built the rest of the system so we hand built some transition models inference rules ways of doing search control and so on uh and connected them up to these general mechanisms and made a system okay and the the just again to kind of give you the motivation i really want a robot this isn't my kitchen by the way just in case you were worried not my kitchen um but imagine that you had to clean this kitchen or make breakfast in it or something uh it would be very hard and imagine programming a robot to do it that's extremely hard and so one thing that's useful to do is to think about what makes this problem hard um so one of the things that makes it hard is that like there are lots of objects right so the dimensionality of the space is kind of unthinkably high it's also not exactly clear what constitutes an object here if you were going to behave in this world it would be a very long sequence of primitive actions that you would take in order to say clean this kitchen um and also there's a just a fundamental amount of uncertainty in this problem right so you don't know what's in the blue bowl or what will happen if you try to pull out a certain thing you don't know when the people are coming home or what they want for dinner all sorts of stuff you don't know and so any approach that works effectively in a domain like this is going to have to handle very large spaces very long horizons and really lots of uncertainty so we have kind of a standard structural decomposition to this problem um we call this belief space hierarchical planning in the now i'll decode what that means a little bit fundamentally the way we think about it is that we decompose the computation that's in the robot's head now into two parts the first part is in charge of taking the sequence the history of actions and observations and trying to synthesize them into some representation of a belief or a probability distribution about the way the world might be and then another module that takes that belief and decides how to behave our belief state representation is a little bit complicated we want to be sure that we can deal with situations where we don't know in advance what all the objects are in the world oh i see a question i want to stop and take it sounds good we've got one from kwaja we saw you talked about evolution uh rl and creating robots that can really work at the same level as humans but as we now know the brain itself is quite complex to understand which itself has evolved over thousands of years to efficiently interact with this environment to extract maximum information to achieve optimal control of its random variables then creating an intelligent system bypassing the evolutionary mechanism according to you would involve what will it require finding those fundamental principles that govern the transformation of structures from one generation to the other i.e uh epistemy uh as focal talks about regarding evolution of system of thought or is it just a massive engineering problem which we can tackle with the current existing tools okay that was a question um so i i what i want to say is that i think the only way forward in a finite amount of time is some kind of combination of studying natural systems and trying to extract some general structural principles that seem to be useful and kind of broadly applicable uh engineering of the classic kind dividing the problem into pieces and trying to solve them as best we can and offline learning so i i sometimes talk about learning in the factory so some kinds of machine learning methods or evolutionary search methods or something deployed as locally and uh parsimoniously as we can so i'm hoping a combination of inspiration from natural systems classic engineering and offline learning can help us do for our robots the job that evolution did yeah great thanks uh the next one is from nate manuel uh could a robot use fast and frugal heuristics to make decisions oh absolutely absolutely so the the whole uh you know approximate reasoning uh a framework and the ideas from what conomon and zwerski and and other people about that are critical uh in the problems that we solve uh you know in computer science we like to maybe joke a little bit that like all the problems that ai people try to solve are like oh i don't know they're np hard or piece based hard or even undecidable and normally if that's true of a problem you're working on you try to work on a different problem because you know you're never going to find an efficient algorithm for it but we're kind of stuck with the problem we have and so we know for sure that we're going to have to to make lots and lots and lots of approximations all over the place everything we do is like ah this problem is too hard how can we approximate it and so heuristics like that are absolutely critical thanks and one more from uh laha alley how do you describe the complex observation like your kitchen for an rl agent okay we're not doing rl can i say that again let me say it very loudly we're not doing rl uh um uh the idea i think that we can view the problem we're taking as i mean the problem that we're faced with as taking a sequences of images in okay fundamentally the problem we're facing let me say this again in a different way the fundamentally the problem we're facing um is one in which we take in a sequence of images and we generate a sequence of motor torques let's say that is in fact what we're doing and it's also true that you could try to treat it as a reinforcement learning problem but there's a bunch of reasons why i think that that's not the right framework first of all are we doing this first of all you have to think about are we doing this learning in the factory so is this learning that i'm doing instead of engineering we there's a role for some of that learning or is this learning that the robot's doing in your kitchen if it's learning that the robot is doing in your kitchen i think it will have to do some learning in your kitchen but it should come to your kitchen very very well equipped already right if i came to your kitchen i might have to learn some things like where you keep stuff or what you like but i would not have to relearn physics and it had better be that if i sell you a robot for your kitchen it doesn't have to learn physics why it's in your kitchen so we engineers have to figure out a strategy for finding the program that goes in the robot's head and reinforcement learning in a simulator let's say is a conceivably a strategy for doing that but it is so desperately inefficient that i just don't believe that that can be the whole story i think it could be the right story for pieces and parts and i'll talk about that a little bit more as we go but i don't think that that's the that's the story so let me see if i can tell you my story okay i'm gonna go forward for a little bit here so so okay so we're gonna let's just look at this picture for a minute so now i'm talking about engineering so i'm gonna build a bunch of structure into this thing i'm first of all gonna build a structure into my system that there's a part that is in charge of remembering stuff from the past and synthesizing that into some representation of what's going on in the world and there's another part that's in charge of taking this belief and and generating action so now the question is how do i represent the belief uh and uh what we've done again is kind of build a fair amount of structure in here we assume that we don't know uh what all the objects in the world are going to be in advance in fact maybe we don't know any of them so we have an open world as the robot observes objects it adds them to a kind of mental you could think of it as a little kind of a database in its head um but it reasons all the time when it sees a new object it reasons about whether that is a new whether it's just seeing some object it already knew about before or not for each object maybe it keeps in its head something like a distribution over what what the type of the object is how much it weighs what its shape is all that kind of thing we also represent our uncertainty about the space that we're in what parts of the space have we observed or not yet and that's important we represent also other things like what kinds of objects tend to occur near what other kinds of objects let's say in a kitchen so it could find something efficiently if it opens a drawer and finds a fork it might not keep looking in there to try to find an apple so this is a complicated representation of what it believes in the world um we do planning to pick sequences of actions uh this is harder than typical planning because it involves integrating both planning in the continuous space of motions with planning at the higher level about which objects to pick up and what to do with them and we do a very major approximation for deciding how to behave so i said the state estimator is the thing that's what's keeping this distribution over objects and what space is occupied and so on and now my problem is to decide based on this belief how should i behave in the world to do that optimally is to solve something called a partially observable markov decision process upon dp that's known to be an undecidable problem in some settings and it when it's not undecidable it's just doubly exponential it's a terribly computationally difficult problem so what do we do we approximate right so back to the question about heuristics i'll say something about the approximation but we make a planner that basically pretends that things are deterministic kind of assumes that it's going to get the observations that it expects is going to get and it makes a plan which is oh which is not right it's a plan that doesn't cover all the possible eventualities but it makes a plan takes the first step executes that in the world gets an observation updates its belief and then makes a plan again so that if this first plan doesn't work out it's okay it tries again and what's interesting is that from the perspective of the planner it thinks about how the actions it takes are actually going to change its own belief about the world and so for instance it can make plans to change its own mental state it can make a plan to i don't know ask boris what we're going to have for dinner tonight and boris can tell me and answer blini and then uh we would update our belief and now is that right no that's breakfast okay good um but we so but i could i can take actions explicitly because of their information gathering properties and that lets me operate robustly in a complicated in an uncertain world because i can reason about doing things to get information just as well as i can reason about doing things to change the world state another aspect of again that we've designed into our system is hierarchy and it's a it's a it's not options it's a different framework so i'll tell you about that so we imagine that say the robot has a high level goal maybe this goal is a goal that the human gave it or maybe it has a very very high level goal which is to just try to make its human happy or something like that so it makes a plan at some high level of abstraction it used to be that when i was flying places and giving this talk i would always use the example of planning a trip so imagine that i was planning to go to california or something like that i might first plan at a very coarse level of abstraction like i am going to get from my house to it to the airport in boston and then i'm going to get to the airport in san francisco and then i'm going to get to where i'm going so i could plan at this high level of abstraction and this g is my final goal maybe it's that i'm at my hotel in san francisco and this is some operation i could do that would get me to the hotel in san francisco let's say from the san francisco airport so this is like a sub goal here this might be to be at the san francisco airport and this might be to be at the boston airport so i make this very abstract plan and then i'm optimistic i am optimistic when i travel i don't necessarily plan in detail how i'm going to walk through the san francisco airport before i get on the plane in boston i figure that i'm going to figure that out when i get there so what i do is i take this first sub goal uh this was getting to the boston airport and i planned for that same sub goal again but now i force myself to think about it in a bit more detail so i make a more refined plan maybe it's that i'm gonna get an uber and then i make an even more refined plan which involves i don't know getting out my phone and something um you know when i get to a primitive action i've just colored them green here for some reason maybe that's the thing i can actually do so this is actually to go get my phone because i'm gonna get an uber and uh maybe i go and get my phone but then and i do that and i get an observation and if everything goes well it will have achieved this i will be now in the set of states that's like okay this is what i expected but if it doesn't go well um i might make a new plan right so i might say my phone it's out of battery or the uber has crashed uh app has crashed or something and so i can't do this so rather that so what am i gonna do i have to reconsider i have to make a new plan and what's interesting to think about is how to manage that kind of reconsideration which is an interesting cognitive problem philosophers and cognitive scientists have thought about it and you know you might say well i can i should just plan again from the high level goal right but that's kind of terrifying right imagine that my head of a goal is like to be a successful academic um i wouldn't want to like reconsider all my career choices just because my phone was not charged so what the structure lets us do is for instance pop this whole plan off the stack right so think of this as a kind of a mental stack of plans or intentions if the bottom plan is not working out i can pop that plan and say okay i'm still trying to get to the boston airport and i really don't want to rethink that choice right you may know friends who rethink their choices too much it doesn't really work out all that well maybe i don't want to rethink that choice i just want to find a new way to get to the airport and maybe i'm going to drive instead so this kind of hierarchical structure gives us very flexible and robust behavior uh okay there's a question and then i'll show you a video great uh we've got a few uh first one's from colem uh is there an area any area of ai in which problems are uh tractable i mean an active area of research not solved problems no i mean uh let partly because in the history of computer science uh once a problem is quite clear well-formed there's an algorithm for it it's efficient it usually is considered not to be ai anymore so it's a moving target what constitutes ai in that sense but no pretty much all our problems are just kind of i think if you formulate them in the classic worst case algorithmic complexity way they're very very difficult if on the other hand you somehow look at the distribution of problems that you actually have to solve and you lower the bar in certain kinds of ways because humans are certainly not optimal in a kind of classic sense they might be there's a there's a nice notion of bounded rationality which i think is nice which says well of course you can't be expected to behave optimally you can't be expected to behave optimally because you lack information but even given the information you have you can't be expected to behave optimally because your computer's not that big so then we could try to ask the question could we make programs that are optimal subject to the computational limits that they have and that's a well-formed question but then it actually turns out to be hard to this is kind of the problem that i started with it's hard to find programs that are optimal subject to computational limitations um i'm going to charge ahead a little bit more actually even though there's a couple questions but and i'll stop again because i do want to get at least up to the learning stuff so that you don't think i'm just a completely old and boring person okay but first i want to show you what a robot can do without any learning um so what's interesting actually i want to stop this video i have to give you one small speech before i continue what's what's interesting i think about this robot doing this stuff is that any individual thing it does um uh several smart undergraduates in a couple weeks could program this robot to do maybe i don't know it's not that easy but but but yeah but what's interesting i think and important about the thing i'm going to show you here is that it's the same program that's controlling the robot roughly uh in all these cases and it's doing very very general purpose reasoning it knows about the objects in its world it knows that there are soup cans and boxes of a certain size so it has this prior knowledge which it shouldn't i would prefer that it didn't have to have and we're working on backing off of that but it's we don't tell it ever what to do it understands that it moves through space that objects move through space it understands what it means to grab something once it grabs something the thing is attached to its hand and now it's moving around with the thing in its hand and it understands what it means to put it back down again but it's reasoning about space about objects about moving things out of the way and so on is very general purpose um oops so uh here we told it to put the box in that bottom part of the shelf to do that it reasoned that it had to move so it can here we told it to put the green box on the corner of the table green box is too big to pick up so it has to push it and move the orange box out of the way it also knows that it's pushing is really unreliable so after it pushes it checks it reasons about its belief and checks to be sure it was good here we told it to go out of the lab it looked and saw that these chairs were in the way it's moving move the first chair out of the way the second chair it just took with it we didn't tell it to do that we didn't just insisted that here we're asking it to put a full oil bottle on the other table it's picking these oil bottles up to see if they're heavy um this is a random other demonstration that doesn't really matter and the next one will show you that it actually worked pretty nicely on another robot it didn't take us very long to get it to work on a different robot with weird cinematography okay but the thing about that is there was no learning in it whatsoever we had to do we we had the lower level algorithmic substrate that yellow layer which we're pretty happy about but we also had to hand build everything else which we're also not so happy about so now the question is and this is really what we've been working on i don't know for 10 years now almost the question is how can we learn a bunch of the things that we had to hand build how do we keep the general architectural stuff but learn the other stuff um and i think that one thing that's important to observe is that there's two really different kinds of learning uh there's i'll call these green boxes learning about the world right and right now in robotics most of the work is on learning perception right there's a ton of stuff in computer vision and so on which is learning to do object detection and pose estimation and segmentation and there's a ton of work i would say almost all the work in robot learning right now is in low-level motor control policies how do i manipulate something in my hand and so on and i think so somebody asked about reinforcement learning i think reinforcement learning is really good at learning low-level motor control policies things like bicycle writing and juggling and moving things in your hand and using a fork and all that stuff has to be closed loop the very the the dimensionality of the state space that matters for that problem is not super high the horizons are not very long you know when you're failing so i think reinforcement learning is awesome for for doing this job um but then we need to build higher level of abstractions we have to i think learn observation and transition models at a higher level so that's learning about how the world works then there's a bunch of other kind of learning which is also super important which here i've called learning to reason or sometimes people call it metacognitive learning or something like that things like learning what to attend to uh learning in that hierarchy of plans what to read when to reconsider what things learning search heuristics so this is like the kind of learning that happens in alpha zero when it learns to play go right it's not learning the model of how go works it absolutely knows that what it's learning is how to think more efficiently so these two things are both important um so let me say something about one chunk of learning that we've done and then i'll probably just skip to the end um so there's a story about building abstractions i'm actually going to skip this too it has beautiful figures uh okay good but sorry uh okay so let's just just one vignette of learning um so imagine we have a robot it's pretty competent you and you want to teach it a new thing or it wants to learn a new thing so it already knows how to pick things up and put them down let's say but now it wants to learn stirring or pouring or pushing or throwing or something like that and we'll assume that somebody already used reinforcement learning to learn a very basic low-level skill and the question is how can we add that skill into the general repertoire of this robot's existing ability right so you you brought this robot you're teaching it how to make a souffle it didn't know that it should learn how to make a souffle without too much trouble um the way we think about this is in the context of actually fairly classical planning problem formulations so we say well uh pouring let's say take pouring as an example somebody learned a motor skill for pouring and maybe it has a parameter like a gain parameter i have to learn now a formulation i have to learn the conditions under which if i do this pouring action the liquid will go where it's supposed to so what's the context in which this thing will work and so uh in this work we engineered this part in some other work we've we're learning it as well but in this case we observe that there's some set of state variables that really seems to be the the conditions that affect whether the pouring is going to be effective the sizes of the things we're pouring between and so on relative poses and so then what we try to do is learn a kind of a condition on these variables on the sizes of the of the vessels and on their relative poses on the grasp that the robot has of the vessel on the of the gain and the controller some relationship among those variables such that if i execute the pouring action under those circumstances then most of the liquid will go into the target the details of how we do that i'm not going to go into in detail we use gaussian process regression so we take a bunch of examples of pouring in a bunch of different circumstances and we try to learn when does it work out well and we use notions of learning that keep track of how certain we are about the hypothesis so that we can learn efficiently with relatively small amounts of data so i'm not going to talk about that i'm just going to show you a movie because it's fun oh no i'm going to say one more point one more point is we don't want to learn just one way to pour you might say oh pouring cool i'm going to learn a really good way to pour i'm awesome if i can pour like this it's good i'm perfect i pour reliably every single time i just this is what i need to do if you learn just that one way to pour you are completely at a loss if for some reason you can't do it that way right so imagine that the thing that you have to pour from today is like really big or uh you're a waiter if a wine waiter in a fancy restaurant and now your manager tells you you have to do like the backhanded fancy wine waiter pour or your right arm is broken you have to pour with your left arm or something so you'd like to know the whole space of like reliable ways to pour and that lets you meet a much more robust person okay i'm gonna show you a movie this is a fun movie so here's robot it already knew how to pick things up and put them down but we did this learning strategy for pouring and for pushing and so here we can put the objects on the table basically how we want to and we can give the robot different goals so we gave it the goal of putting stuff in the little white bowl that time this time i think the goal is to oh to serve the stuff on the tray uh here it's moving the green box out of the way so it can get a good grasp of the cup that it has to pour from it's coming over here to pour some stuff into the bowl um i will show you one uh no one more example after this one so here we told it to put the to at the end have stuff in the white bowl and the white bowl on the thing um in this next example um it pushes the red bowl over in front of it so that it can pour into it we never told it that that was a thing to do but it reasons that it it it's gonna pour with one hand and with that one hand it can't reach all the way over where the bowl was so it just knew that it had to get the bowl into a place that it could pour from and it figured out how to do that i think at this point i am going to stop and answer a couple questions and that will be time so questions great uh we have one from gm bautista a provoking question since you asked for one that's right sutton talks about the bitter lesson ah yeah i.e that methods that don't scale with data and compute eventually succumb to those that do despite being better in the short term hence the bitterness what do you think about it yeah awesome so so rich was on my thesis committee and i argue with him all the time um and in fact i wrote an answer to him that's on medium if you want to try to find it uh so i think in some way he's right and in some way he's wrong um it is it is absolutely true that anything we build into a system will be wrong uh and it will introduce bias in machine learning not when we're thinking about bias and fairness but if we're thinking about bias in the bias variance sense we think about bias bias has two properties one is that it keeps you from arriving at certain answers but the other one is that it makes you learning much more efficient and i think that in order to make progress in any reasonable time frame i need to build some stuff in um i think it's kind of it's rationally possible to imagine not building in anything but even rich doesn't want to do that right so i had to fight with him actually i said you were building stuff in even now he said no i'm not i said you absolutely are using convnets and he said yes but as soon as you're using convnets you're building something in so it's it's a matter of degree and it's a matter of deciding what are the general things the beautiful general things like convolution that we can build in and not do too much damage to the psyche of our robots so i agree and i disagree thanks uh the next one from kwaja approaching the problem of learning if we consider the fact that a robot sees an onion then it needs to figure out that it needs a knife then to think that it needs a knife it needs to know that it needs to cut the onion then why it needs to cut the onion would require it to need to know that it wants to make curry why it needs to make curry it needs to feed people and it goes on so the thing is very it is every problem that we as humans face in the world has a hierarchical recursive structure which basically is an indirect form of inference is planning at every level which relates one part in the hierarchy to other levels of hierarchy so even resolving it on the engineering front that you need ensemble densities which form new ensemble densities as the hierarchy goes but what is needed is the principle that defines the growth and direction of that hierarchy so isn't it largely a science and philosophical problem of finding those first principles that govern our need for survival and desire to live okay um you guys are good at coming up with long and complicated questions let's see there's so much packed into that question i will answer some projection of it um i mean so there's reasoning from first principles so i skipped this giant thing about models and that there was this quote about all models are wrong but some are useful so i think what an intelligent agent has to do is build models that are good enough right so cooking is is an example i like to think about a lot because on the one hand i feel like i can have a this robot has a somewhat general model of of a space and occlusion and moving things out of the way and so you can give it uh rearrange the objects and it'll do kind of something reasonable uh and the more that you have a good theoretical causal model of the domain the more robust you can be and the more you can deal with things that go uh not as planned cooking is an interesting domain because i think most people have like a partial causal model so we all know that things will burn if you cook them too hot and a bunch of stuff like that but we don't have perfect models of the applied chemistry that you would really need to infer exactly what will happen if you do something in the kitchen and the reason we sometimes follow recipes is because uh because we don't have the causal model that we need and we just do the steps because somebody told us these steps were going to work and we try to do it that way so i think we need we need models of all different kinds different fidelities different levels of abstraction and we have to figure out how to kind of deploy them all together in some way great thanks uh the next one from anonymous uh could this uh or could these abilities you are trying to build into robots be the analogs to executive functions in humans for example cognitive flexibility is well represented on the goal changing in the airport uh story when you didn't find your phone as expected i think so i don't know anything about humans i have only folk psychology not real psychology but so so i can't i can't say anything about how any of these mechanisms relate to humans i'm sorry sounds good next one's from ella have you repeated this experiment to see if the robot makes the same error i.e taking the chair out with it as it goes outside the room okay good good good good i love that question and i'm gonna give a little speech because it's a thing that i think is really important um i actually had a couple of military ethicists come to my office a couple years ago and they wanted they were interested in autonomous weapons they were smart people who had studied a lot about ethics and philosophy and all these things and their first question to me was has your robot ever done something you didn't expect and i just like burst out laughing because the question is does it ever do anything i do expect uh and the answer is roughly no so no of course if i repeated that experiment with the robot and the chairs and the door it would do something really different every single time we were like we were aghast when it went out the door with the chair in his hand we did not expect that um but i think it's important what's interesting and important actually now from a kind of a pragmatic and and moral view in a way is to understand the way we program robots right so we no longer program robots in the way that kids learn to program robots in lego mindstorms where you say move forward one meter look turn left take a picture move whatever we don't give a sequence of instructions that say do this followed by this if that's what we did then it would be it would be surprising if the robot did something we didn't expect instead we have a combination of machine learning and planning these uh they're basically optimization algorithms that search a big space of possible answers and try to find one that's good and the searching is non-deterministic uh the the the particular sequence of images and the timing that they arrive that's all non-deterministic so so so no we have no actual idea what's going to happen thanks next one's from lou uh why did the robot push the bow instead of taking the bow to a proper place how did the robot evaluate among the actions good so what it um it doesn't think you can pick up that bowl i think that that red bowl is too busy too big uh and it didn't learn that i think we probably coded that in there so that's why i pushed it rather than picking it up so it asks itself it says how can i move something okay so how did that reasoning go let's just let me work it backward for you the high level objective was to have stuff in the red bowl uh it's it it and it says well what are the things that i could do to cause stuff to be in some container that it's not in and it looks in his list of things it knows how to do and it says oh i have this general ability to pour and that is an ability that will cause stuff to be in a new vessel so i think i have to do pouring so it kind of figures out that the last step it's going to need to do is pour then it uses that learning that i glossed by very quickly the learned thing that said well if you're gonna pour it needs to be that you're holding the thing that's got the stuff in it and you're holding it in a position a relative position to the other thing that is suitable so that the pouring is going to work out so it said okay well that must mean i need to be holding the blue cup and i need the blue cup to be somewhere relative to the red bowl that satisfies this constraint and then it it it said it you know it probably thought about well could i do that by just with the red bull where it is and it figured out that it couldn't do it where the red bull is where it is because of its kinematics it couldn't reach all the way over there so it said well how can i get two objects to be in some relation to each other when one is you know and then it it realized that by moving the red ball into a a more accessible place it could it could do this so it does a kind of backward reasoning yeah great uh we have time for one or two more uh the next one from lahar is how to generate the sub goals and sub rewards according to a general goal and observation right so there aren't rewards really in this system so uh the i mean the way that we view it is that there's some high level objective and there are costs associated with the steps right so you could say that those are like negative rewards um so the way we think about making the hierarchy is that we have planning operations that have preconditions we say well if these things were true and i did this operation then it would have this result and the way that we make a hierarchy is by postponing thinking about some of those preconditions so we might say for instance i and and the the principle for postponing preconditions is to postpone the ones that are easy to fix up locally like if you were planning to drive across the country you might say oh well i know that i'm going to have to have gas but i also know that i can probably arrange to get gas when i need it so i'm going to ignore that when i plan at the high level of extraction or like i might need to have my arm in a certain location to actually do that final pouring but i know that it's pretty easy to move my arm around when i need to that's easier than like acquiring the cup in the first place so when i do my high level planning i'll ignore exactly where my arm is for a while so that's the kind of the idea thanks uh next one from quan uh how does the robot decide what to learn and what not to learn ah good right now i decide what to learn and what not to learn and so that's completely unsatisfying almost no one i think has like a nice integrated architectural view of how you integrate learning and reasoning in a system that has an ongoing interaction with the world and that's something that we're actively working toward but right now i don't really know how to do it great next one is from quan as well to what level are you using cognitive science or maybe neuroscience to install abstractions and learning algorithms not really at all um i the the things that i take inspiration from are this like the idea that it's sensible to build in an abstraction of the world in terms of objects and properties and relations that it's sensible to build in some 3d structural understanding of space um but no i don't i i unfortunately don't really take any direct advantage right now of particular neuroscience or really cognitive results thanks i think we can sneak one more in uh from judith uh could the robot learn something by itself i.e by reasoning that it has to learn that thing in order to achieve a certain goal for example if the goal is to have stuff in the red bull but there is only stuff in the blue bowl so the robot reasons that it needs to learn pouring ah awesome no i wish i mean i i like i'm burning to do that i think i know kind of how to get that set up but right now it cannot do that leslie that was a totally wonderful lecture thank you so much for doing it all right thank you for the good questions and have a good the rest of your summer school 