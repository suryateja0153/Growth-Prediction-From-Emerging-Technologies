 Hi, I'm Kyle Vogt, the CTO and Co-Founder at Cruise. Last week, I sat down withOpenAI CEO, Sam Altman, to get his reaction for the first time to an unreleased video of 75 minutes of autonomous driving in downtown San Francisco, let's take a look. (upbeat music) I think that what's in these videos is, this one that you're gonna see, I think is the most advanced, self-driving technology that's been shown so far just with the complexity of the maneuvers and the length of it and everything else. So I'm hoping that with your eye you'll find some interesting stuff or... you can talk about whatever you see that's interesting. okay, and how much of this is based off of LIDAR versus visual? We have a bunch of LIDARS on this car and I'd say we're still a LIDAR-heavy system but there are detections that happen purely in camera space in and radar space. Okay. But you get really good recall obviously with like a laser scanner versus a camera image where you might miss things. The challenge with LIDAR is classification because you see a blob of 3D points and it's sometimes hard to tell if they're a car or a box. Anything I can't talk about? Well, I won't answer the question if I don't wanna talk about it. Is there anything you don't wanna talk about? No, that works great. Why don't we go ahead and get started? This is 15 minutes of driving through San Francisco. No disengagements and this is 75 minutes of driving. It's sped up at 5X and so, Sam, when you're watching here if you see something cool, let's just hit pause and then we can talk about what's going on here. Okay, so I haven't seen this before so I'm very excited because I actually, my model would be that a car can not do this for 75 minutes in the city. Right, and this is starting right outside our office in SoMA. He drove a little bit and turned it on. Wow the map of cars, other cars seems quite accurate relative to ours. Yeah and so, I mean it's not a map, those are real-time detections. Right. And the tracking. Yeah. The video-game like maps right above here. That's wild. It definitely seems to react very quickly when a person or another car turns or the scene changes in some way. And one of the benefits of LIDAR—this type of LIDAR—is it can see pretty far out. And then we really put a lot of work into our tracking stack. So the boxes stay stable as your position changes relative to them. I want to go back and look at that. Yeah, sure. Can you talk about how this is? So this is an unprotected left, Yeah. But you saw a little choreographed maneuver where we had to predict that he was turning right, the other car, to know when we could start turning left. And if you get that wrong, obviously that's pretty bad. Yeah. That was something that I would not have expected it to get right. How is it doing that? Well, there's a bunch of ML models that, that do prediction. And then there's a, on the planning side, it plans a path for an AV, but as you're planning a path, you also have to like simulate forward in time, what you think the other agents are going to do. And so there's sort of the simulation that's unrolling for each other agent and the scene and multiple possible futures for each agent, depending on whether it's going to go straight or turn right. We might have to simulate two futures and pick the one that's more likely. Yeah, That's basically how humans do it. That lane changed before that turn was impressive. And then as that construction vehicle, which is probably not a super common thing backed out in front of it, and it had to sort of look at that somewhat unusual scene. Yeah, I think what we had there was, there was a pause there that was the remote assistance system kicked in. The AV for whatever reason, got a little confused there and remote assistance came in and it took like two clicks for that person to plot a path and then it carried on and then seamlessly transitions back into fully autonomous. How often does that happen? I don't know, off the top of my head, but it's, it's like rare enough that you could have one operator for many AVs. So maybe, maybe on the order of one, every five to 10 miles right now, And that's something that, you know, we optimize over time as we put more and more effort into, you know, automating those long-tail events. And the car basically just slows down or stops when something weird like that happens? Yeah, Sometimes it doesn't even come to a stop depending on how much advance notice the system gives the remote operator, because if it sees a bunch of construction stuff and it looks like the path is blocked, it's not going to wait until it comes to a stop to sort of phone a friend. Got it. The, the, the smoothness of how it sort of handles like minor deviations, a car sticking out from the side of the road a little bit too much, or like, you know, that truck moving around another one and stopping quickly is definitely more than I've seen for other city-based self-driving systems. Well, just goes to show you, we're not like reliant on the lane centers that a human labeled in a map, you know, there's, there's actually, in this case, we've got learned trajectories so we've driven a couple of million miles in San Francisco now, so the AV actually knows what most other vehicles have done at similar intersections, what trajectories they take and that's sort of the default plan. And then the AV can deviate from that pretty substantially, including crossing lane boundaries, if it needs to. What's happening now? It's pulled over for a virtual waypoint. So that was like a simulated pickup or drop off. and what happened when it came to a stop is the person in the car clicked advance waypoint, and then the car started driving towards the next one. How many, like how many miles of driving do you think a human needs to get good at sort of, oh, did it just let a car pass it? It was going slow because all those cars were parked at a 90 degree angle and that's a scary thing for a human and a robot cause one can back out at any time And I think the car behind it got a little impatient, which that happens, especially when you see an AV and you're not sure what it's going to do. How many miles do you think a human needs to learn to, to like plot routes through these sort of unusual situations? I mean like how many miles does a 16-year-old kid drive before they get their driver's license? Like a thousand, maybe? Maybe. One of, I think the most interesting challenges. I mean, the, the unfair part of that comparison is that the human got all of the learning of evolution, plus the first 16 years of their life of sitting in the car, watching everything else. They have higher-level reasoning that can sort of override their lack of reflexes and instincts. For sure. But I think one of the things that we're really interested in at OpenAI is let's say that, like, with all of your 16 years of life experience, plus everything evolution did to help you get a vision system, how long, how long, like then it only takes you a thousand miles of actually driving to do all the transfer learning or whatever, And I think one cool thing, like one interesting project for us is if we, if we start with a system of human level intelligence, can we get to a thousand miles of driving to do all of this? So, you want to work backwards. You want to build the human AI first and then teach the human AI to drive. When people talk about learning things as fast as a human learns them, I think it's only a fair comparison if the system got all the benefits that the human had when they started, we don't start from a clean slate. That's fair. So there's all these like parklets and things going on. That's that's like a new feature of San Francisco since COVID took hold. That, that adaption to that particular street where you had like, you know, someone who kind of like walked out on the side of the street, multiple parklets, a construction vehicle, two trucks like jamming you up, that was super impressive. Yeah. The interactions with cyclists are really challenging too. I'm not sure if we've seen it yet, but there's a cyclist going in the wrong direction of traffic and the default thing for an AV is to just slam on the brakes, 'cause it looks like their path is going to interact with yours, but when your prediction gets really good and you're in between two objects that are predicted to collide gets pretty good. You can figure out that no, the correct thing is to sort of nudge over to the right, rather than slam on the brakes. That, I've seen it do a few things like that, where it just sort of, yeah. It swerves within its lane or it moves within its lane a little bit. That was the Panhandle there. Oh yeah, Cool. That was impressive. Where it sort of noticed that there were like a bunch of slower cars and it just switched lanes to go around them. Is that what was happening? Yep, the system keeps track of traffic in each lane. And if there's a, you know, there's a higher-level routing algorithm running in the background and when it thinks it sees a more efficient route, including lane changes, it will, it will consider that. Will it be willing to do that even if it's like going to switch to the right. Oh, I guess it will. Cause now it's making a left turn and it switched to the right lane pretty, pretty recently. Yeah, it knows the cost, for example, of like missing a right turn. Like if you're human and you have to make a lane change to make a right turn, it knows that you know, that the cost is not that bad if you do it three blocks away from your turn. But if you wait till the very last block and you miss it, you might have to drive around in a circle, you know, for two miles. What does it do if it like accidentally turns into a one-way street? That's a good question. That hasn't happened because we have those mapped. If, if a street were to go from one way to not one way, I think it would probably, if we didn't have detection for the one-way street sign, it would probably try to treat, treat it as a two-lane road and proceed until, you know, but obviously if it comes into contact with an oncoming car, it's going to come to a stop. So it sort of has a safe fallback from that, that perspective. Picking the traffic in, you know, picking the lane with the least traffic here and sort of smoothly moving between them is that's, that's also working much better than I would've expected. And lane changes are hard. It, it's not just about like constructing a trajectory, but it's about finding a gap. And if there isn't a gap, sort of speeding up or slowing down to signal your intent to other drivers and to sort of coerce them into making a gap for you in traffic, there's a lot of like, you know, human psychology at play there. I noticed something here. Oh yeah, so like in a thing like that, how does it decide if it will go around that car or wait? Is it just, if the opposing lane is clear? Go, for like for oncoming traffic? It just sort of went into an oncoming traffic lane to get around this car right here. Oh yeah, So it's, it's looking, it's scanning the oncoming lane of traffic and looking to see basically the same thing. It's saying, If I predict that object forward—if there is one— if I predict it forward, I will either yield to it or simulate that I have just enough time to go around the car and cut back into my lane before the oncoming car. I've noticed that it seems to—oh, it just did it again It only seems to do that. If there's not a double yellow line, is that accurate? It will cross over double yellow lines, but it's a little more deliberate behavior. It'll actually like plot a new path, whereas minor nudges and things don't require any like sort of different behaviors. Am I noticing correctly that the, the route plan in the upper map seems to change the length of the route plan based off of the uncertainty or the difficulty, the situation in front of it? It's mostly plotting a fixed time horizon. So when it's going fast and straight, you'll see it shoot way out. That makes sense. And you, if you also pay attention, you might actually see it change routes dynamically. If it thinks that a road is closed, it'll, it'll say instead of going left, I'm now going to go straight. Cause it looks like there's a construction truck in the opposing lane of traffic. That was an impressive moment worth of stuff. Just trying to rewind it. I mean, there's a lot of things going on and it really kind of, I mean, it was a little bit aggressive about when it went there, but it did it at a time that seemed like a reasonable human could do it too. Well, we're, we're playing it 5X, so we're going to see some stuff that probably looks better than it actually is, and some things that are worse, as an artifact of that speed up. I also noticed that it's been good at not slowing down if a human will walk in, in front of it far enough away that the human is on a trajectory to clearly get out of its path by the time it gets there. Yeah, I mean, we've been doing this a long time and we've had to like gradually crank up I wouldn't say the aggressiveness, but more like the, the confidence of the AV as it better and better at predicting things. Because in the early days of an AV, you just treat any sort of uncertainty as a reason to slam on the brakes and you have to be pretty good, pretty correct to be able to drive this confidently and not have it be a safety risk. This is really quite remarkable. I'd say this is the first time I've seen any demo like this, where it looks like a human could be plausibly driving the car. Actually, I couldn't even tell if it's a human driving the car or not. It just seems like it's very human-like and it's kind of the fluidity that it's doing this stuff with, and it's not doing the AV default of just slam on the brakes every, every time something is even a little bit off. We're getting there. I mean, getting the steering right, even a few months ago, maybe four months ago, the, the steering would kind of overshoot in a very robotic way, like a controls, you know, underdamped oscillation. And then we really got that tuned in recently and like, it feels really smooth. Can you say how many parameters the neural network for this is? Well, it's not one giant neural network. like maybe some of the projects you guys work on. There's probably somewhere between one and two dozen different neural networks. And we're constantly reshuffling and re-racking those Sometimes like one network will have multiple tasks and, and outputs and depending on like which sensor or processes, or, you know, quite frankly, the velocity of the developers, some people are working on a brand new model and they don't have time to integrate it into a more massive model. And so we're pretty flexible and fluid on the number of models and their purpose. And that's sort of constantly evolving. There's like a specific one, for example, to see for each, each individual blue car that you see in the top image. we're trying to predict whether it's a traffic participant or if it's a car we should go around, you know, like something very basic, like that is actually pretty hard to do without a lot of context from the scene. Do you know how the network does that? In that particular case, that one relies mostly on human-engineered features, like for example, a feature that might say, is there a car in front of the car I'm concerned about? Or how far off is it from the center of the lane? And that works pretty well. We haven't really topped out in performance for that particular task, but there are other ones where we have to go to more of a deep learning approach where, you know, we can't necessarily figure out what the right human engineered features might be. The thing I keep coming back to that is I think the coolest about this, is the degree to which the decision-making seems human-like, in lane changes or going around an object or reacting to a unusual situation, or just overall fluidity and seeming to have pretty good situational awareness. That's kind of the fun part of this job is to reverse engineer, you know, human psychology or your decision-making framework. And sometimes you can come up with a heuristic that's pretty straight-forward. Like for example, do I try to make this lane change or do it on the next block? Mentally, you're calculating the cost of missing that lane change. And other things are more complicated, like trying to figure out whether the car in front of you is ever going to move again. There's a, there's a lot of things your brain is looking at to try to make that decision. That was the cyclist, there. One thing that I do, you know, probably driving around the city, like once a day is make some sort of hand gesture to some other driver to try to, or like a facial expression and try to figure out what they're going to do—if they're ever going to move again. How do you think self-driving is going to get around not being able to do that? So, we have a little bit. An example of that is a car with tinted windows. You can't see the driver, and humans can still figure out what to do and they signal in other ways by nudging and basically asserting right of way. And that's what the AV will do. If it thinks it has right away, it'll start moving. And then if someone else starts moving and sort of in a more aggressive fashion, it will yield to them. That makes sense. I was gonna say, you can get into some trouble if you have too much latency in the system. Then, you're playing this game where like one person asserts and then the other does, and it can kind of get stuck in this infinite loop. Given all of the ways cars can like move a little bit, that the systems will be able to figure out ways to communicate with each other, without having humans do stuff that makes total sense. The car itself is a social cue. Like how it's moving. Are there weather or lighting conditions that don't work at all? Or does kind of everything work? Did you say weather or lighting conditions? This version of hardware and software is designed to work in, I'd say, most of the time in San Francisco. And it has detectors built in to say like, okay, well there's too much fog or rain. And now my sensors aren't giving me enough data to drive safely, and it'll kick itself out and basically pull over. As we get closer and closer to a larger scale deployment, really going to dial down the number of situations that's in and sort of make sure that users are aware of those limitations. But at the end of the day, it's, you know, fog isn't really that much of an issue. We've mostly been able to drive in fog, but really heavy rain can be an issue because sometimes water droplets stick to the sensor lenses and we don't have quite enough sensor cleaning capability to do heavy rain, yet. We'll work it out when we move outside San Francisco. Why is it so far back here? This is another waypoint. This one is actually the very last one before the end of the trip. And so Luke, who's in the passenger seat, just typed in the address of our office. And so now it's going to go around the block and pull back into the parking lot of our office. One of the things that keeps coming to me, I'm like, oh, that was a great example. Like how you see that bike weave in between the lanes, which a human driver likely would've missed. Let me see if I can go back to it. One of the things I think is, so now on the little map on the upper, on the top part, you see this purple bike come through between lanes two and three. One of the things that I'm always excited about is not only like AI that will do things as well as humans—oh, it just changed routes that's cool—but do things much better. It occurs to me that like, we're not that far away from a driving system that is just going to be much better than what a human can do because it, this like constant perception that is evidently already superhuman from the upper map here. It's 360°, it's longer-range, it's better. I think that's the end of our video. It's better, you know, in many ways it's better at seeing objects that are far away or partially occluded. And so like the interesting thing about it for me is you look at the trajectory of improvement for this kind of software over time. And we're at a pretty consistent rate, you know, depending on the metric we're looking at, it can improve by 3 to 10X per year. And I don't think there's any reason that's going to top out at human performance. It's just going to keep going at 3 to 10X per year. That's a really interesting thing about the field in general is that these, the predictability by the year over year growth and performance, it is more than I would have guessed and, and sort of, it just seems to keep going. And so we can like make predictions at open AI, but when we'll have certain capabilities or when we'll get to a certain level and yeah, this like multiple, this multiple X per year is awesome. And that really gets powerful fast. And again, I say the most interesting takeaway I have from this video is it is already superhuman in some important senses from what it can understand about a world and then how I can act. Well, I want one that was really cool. Well good. So I'm curious though, when you say at OpenAI, you're seeing like, you know, multiple X per year or whatever it is, is that just a function of the models or is it also like the tooling and the infrastructure? I know you have like a cloud compute thing. It's all those things wrapped up, isn't it? All those things together. I mean, you definitely get bigger models every year. But this is the surprising thing is—the gains we make algorithmically year over year have been on a pretty consistently exponential curve themselves. There's no reason that should be in the same way that there was no like obvious reason that Moore's law should be a thing. But in some sense, like I've thought a lot about this. Maybe it's just kind of a self-fulfilling prophecy. And if you know, the team delivered a 3X gain last year, they expect they really want to get a 3X gain this year just from algorithms. I think there's also something to maybe the number of shots that like a team of engineers can take or new ideas they can try in like a calendar year. You know, if you're working on a one week cycle, whatever it is, and, you know, each one of those still delivers a 10 to 20% improvement. You compound that pretty quickly, but also, you know, like tooling and like dataset manipulation and labeling tools, training tools, inference tools, both the hardware and the software in the automation were pretty bad even a few years ago. And so I think there's a, there's a multiplier effect from the tooling. And I think we still have a pretty long runway of improvements to tooling before that multiplier effect taps out. And that's not even talking about advances in the silicon itself or the, you know, the performance that we're running on or, you know, better, better software tooling, I guess, or, or better models. It seems like there's a lot of runway still. Yeah. I think we're all badly underestimating, like even probably you and I, if you think about this a lot and are like pretty good at understanding exponentials. I think we're badly underestimating what it means to project out 10 years with this rate of compounding. Yeah. And I agree, like there's the chance of these systems, like getting exactly to human level and asymptoting there feels like zero and, and that's, I think that's like quite exciting. It is. I think another thing that might happen is sort of a fork where in things like cars and motorcycles and buses and semi-trucks, that are on the road, we'll reach a point of saturation where there's diminishing returns for making the AI that much better on those. And this won't be right away, but 5 or 10 years from now, maybe. And so then the goal there will be to like drive down the cost to make it run on super inexpensive, low-power hardware, like a single camera sensor or something like that. But you'll probably have a fork that lives in your world. That's, you know, like the most advanced driving AI ever and trying to figure out how to redesign roads and cities and all that other thing based on this, you know, super AI driver, that's like my imagination kind of stops pretty quickly getting into that realm. But I think that'll be pretty cool if it does continue at 3 to 10X, like, what would you do with that thing? What's it capable of? That's exactly how things go at OpenAI, which is first, we like make this system that we weren't sure if we could make, and then we can figure out how to make it like a 100 or a 1,000X cheaper. I'm sure that'll be the natural pressure self-driving faces, too. But I think that other thing you said is really important. It's important that we spend some of our effort and time and resources, not just like making it human-level performance or slightly superhuman-level performance but much cheaper, but really seeing like, just how good can we make this? And what happens when we do? Alan Kay had this thing like decades ago, which is that the software developers should have computers that were like at the absolute limit of what was possible, no matter the cost. And so like all the people in PARC have these $100,000 computers. And I think a lot of good things came out of that. And if you can, like, if you can buy your way a few years into the future, I generally think it's important to do that with some, with some, some set of the company's bandwidth and resources. So that's an ad for the best dev machines you can get and the best cloud compute and all that kind of stuff. I don't even know how to spend $100,000 on a dev machine anymore. I mean, maybe you can get some ridiculous Mac Pro that's like a terrible use of money for $40K or something. I think you can spend a $100K on a server, obviously that's not going to fit under your desk very easily. It's more, it's more an ad for saying like, you know, let's, once this gets pretty good, let's definitely figure out how to make it cheaper with like a single GPU on a single camera sensor. But let's also try to figure out just like, what happens if we push this ridiculously beyond what anyone thinks is reasonable? Yeah. It's exciting. I mean, we're not even talking about the multi-agent part of, of like a robot AI. The thing I'm really excited about is, you know, we're primarily thinking about each AV as its own independent entity, but when they start thinking collectively or, you know, doing things like hooking into little trains or chains and platooning, you get like, you get additional throughput on your existing roadway infrastructure. And so it's like traffic, you know, it could be dramatically reduced, but that can only really kick in when the robots or the AVs get so good that humans start saying, why do we, well, first of all, why do I bother driving? I'm like a menace or a hazard. Maybe I like it, but that, you know, the sort of hazard I introduce by driving versus the benefit of, of not having me on the road becomes, you know, severe at the point where maybe robots are a hundred times or a thousand times better than humans. When we start to kick the humans off the road, or they just naturally sort of stop driving because it doesn't make any sense anymore, whichever one happens, then you can do really interesting things where the robots drive like within six inches of each other at speed on existing roadways. And we can actually, you know, not have to build new roads to get to, to solve traffic, which I think is pretty compelling beyond the sort of basic argument arguments for AV around like safety and productivity. For sure, and that's the kind of thing where I think it's worth like pushing the capabilities way superhuman, because then eventually like once the whole world has switched to AVs you can, you, you know, you can put radio like direct, you know, car-to-car communication between them all and do stuff like drive a foot apart at a hundred miles an hour. Yeah, yeah. That, I think then we're bumping against like laws of physics and reaction times and all that, but that's, that's, that's a, that's a good, you know, yeah. That's a good bottleneck to reach—physics versus, you know, the unbounded rate of improvement for just these AI systems. This was really fun, Thank you for showing me that. Yeah. Thanks for taking the time. Awesome. All right. I'll see you, Sam. See you. 