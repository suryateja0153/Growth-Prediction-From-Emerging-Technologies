 DANIELA RUS: So today, I want to tell you about some of the extraordinary things we are experiencing these days in both academia, business, and industry. So think about it. Today, doctors can connect with patients and teachers can connect with students that are thousands of miles away. We have robots that help with packing in factories. We have 3D printing that creates customized goods. And we have network sensors that monitors facilities. We are surrounded by a world of opportunities. And these opportunities will only get bigger as we imagine the impact of the latest and greatest AI and robotics technologies. So picture this. Picture a world where routine tasks are taken off your plates. Now garbage bins take themselves out, and automated infrastructure ensures that they disappear. And food gets delivered to your doorsteps. Fresh fruit and produce gets delivered by drones. And intelligent assistance, whether embodied or not, help you optimize all aspects of your life to ensure that you live well and you work effectively. How are we going to live in this future? And what are the supports for taking us to this future? Now, AI provides support for cognitive tasks by providing autonomy at rest and support for physical tasks by providing autonomy in motion. And I would like to talk about these two topics for the rest of the talk. Together in the future, these technologies have the potential to eliminate traffic accidents, to better monitor, diagnose, and treat disease, to keep your information private and safe, to ensure that people connect and communicate instantaneously no matter what language they speak-- in general, to take care of the routine tasks and leave people to focus on critical thinking, strategic planning, the kind of thing all of you guys like to do. Now, progress in these areas is enabled by three interconnected fields. Robotics puts computation in motion and gives machines the ability to move. Artificial intelligence gives machines intelligence that enable machines to see, to hear, and even to communicate like humans. And machine learning cuts across AI and robotics and aims to learn from data and make predictions. Now, progress in AI is enabled by a convergence of three things. Algorithmic advances, very important-- people often talk about the explosion of data and the increase in the power of computing. But without the algorithms, we would not have anything. So these three pillars, algorithmic advances, data, and computing, are enabling this huge progress that we're seeing today in practically any field that has data. So all companies, all industries that have data can benefit. And machine-- and then benefits due to machine learning, which refers to a process that starts with a body of data and then aims to learn a rule or make a prediction about future use of the data, and medicine is a great example of a field that can benefit. Today, machines can read more radiology scans in a day than a doctor will see in a lifetime. Think about that. Now, in a recent study, doctors and machines were given scans of lymph node cells, and they were asked to label them cancer or not cancer. The humans made 3.5% error as compared to the machines' 7.5% error. But working together, the doctors and the machines achieved 0.5% error, and which, if you think about it, it's a significant reduction, a significant percentage reduction in the error. And it's extraordinary. Now, these techniques are currently employed by the most advanced treatment centers in the world. But imagine a day when all doctors have access to these techniques. Doctors in rural areas or doctors that are overwhelmed with work and don't have time to stay on top of the latest and greatest clinical trials can offer their patients the most advanced results by taking advantage of AI solutions that will, in principle, bring the most relevant information to the patient for the doctor to make the decision. Now, machines and people working together can do so much more in finance. I like to think about the machine as kind of an intern and running around and doing errands for you. And when you find a-- when the intern finds a good pattern or something interesting, that pattern is brought up to the human to act on to make a decision about. And so thinking about using these techniques to improve the conversation between people, to improve how you organize a portfolio, or to even make a prediction of what might happen to the markets are extraordinary possibilities. And if you bring blockchain into the whole system, then you ensure that all the processes done by the machine can be checked, can be trusted. One more example, machines and people working together form better lawyers. So where processing internet and email have already revolutionized how we draft documents, how we look up information, how we exchange information, and the next wave of technologies, which is getting really, really good, is natural language understanding. And with natural language understanding, we are able to get machines to read, and remember, and interpret entire libraries of documents. And so think about how useful that is for lawyers who need to know stuff about thousands and thousands of documents that are all very large. And so again, the idea is that the machine could bring the right information at the right time. And yet machines are not able to be lawyers because they cannot write compelling briefs. They cannot counsel clients. And they cannot persuade judges. But they might be able to support in predicting the decision that a lawyer might take. So my last example is about traffic. And this is actually from my research group. We built a new algorithm that matches supply and demand. And this can be applied to any field where this is a problem. We applied it to the traffic in New York City. And we have shown that with our algorithm we can reduce the number of taxis required to meet the 400,000-plus taxi ride requests a day with 3,000 vehicles. So think about getting rid of 11,000 taxis that are constantly roaming. So the taxis are not like my car. My car drives to my parking garage, stays there for 10, 11 hours, and drives back. But taxis provide constant movement in a city. And the catch is that these 3,000 cars, if you're taking a ride-- let's say I'm going from here to the airport, to LaGuardia. If somebody is at the street corner and is also going to LaGuardia, I have to make room for that person in the car. And I can, but I only get to share with up to four people-- three people, four people total. So think about the benefits to the city in terms of traffic, in terms of lower pollution, and in terms of noise. And all this introduces only less than three-minutes' delay in arriving at your destination. And this does not take into account the improved traffic you get by removing 11,000 cars from the roads. So all of these examples are extraordinary. And many of the advances you hear about in machine learning today are due to a technique called "deep neural networks." And in deep neural networks, you have large, very large computer networks, usually with millions of nodes. And millions of manually labeled data items are presented to the network to figure out the weights of the nodes inside the network. And so for instance, in the case of a network that processes images, a person might label this picture as "beach," "palm tree," and "sea." And this is so that when another similar image gets presented to the network, the network says, ah, this is a picture of a beach. OK, so this process with images works in two steps. Given a photograph, the network has to find out which pixels go with what objects. This is called "image segmentation." And they have very good algorithms to do that automatically. Now, once you have segmented the image, you have to rec-- you have to add labels to the objects recognized in the image. And this is actually very challenging. So for instance, in this case, you might want to say you have a building, sky, and car, and you can do the same thing with more images and even more images. And when it comes to labeling images, this is how it's done. So when it comes to labeling images, lots and lots of people sit across the world and manually say, "This is a car. This is a building. This is a road." So when we think about machine learning, we have to consider what it means for the machine to learn. So for instance, when we say that the network has learned that this is a picture of a beach, what this means is that the pixels that formed this picture look the same as the pictures-- I'm sorry-- as the pixels in other images that a human being said, "This is a beach." The system has no idea what the beach represents. It doesn't know what we do with it. Do we eat it? Do we drink it? Do we play on it? What do we do with a beach? How does the beach feel? What is the purpose of the beach? How much does it weigh? None of this is part of what the system learns. And it's important to remember this because we tend to anthropomorphize machines. And when we say, the machine has learned, we tend to imagine what a human might have learned at the same time. So it's good to keep in mind these issues. Furthermore, neural networks are not perfect in how they produce their results. So have a look at these two pictures, the two pictures of two dogs. Do these dogs look the same to you? Yeah, they look the same to me too. But in fact, they're slightly different. The second picture was obtained by injecting a little bit of error. And you can see the noise that was added to the picture. We can't detect that was a naked eye. And yet this little error is enough to trick the network from saying, this is a dog, to saying that the second picture is an ostrich. And if I play around with a shape of the error, I could get the second picture to be a car, or a chair, or anything I want. So while machine learning is making great progress, it's really important to keep perspective on what it can and cannot do for us. Furthermore, have a look at this video. So this is a video of a child, an 18-month-old child, that's watching the scene for the very first time. It's watching the scene for-- it has never seen this scene. And look at what this child does. The child has figured out that the adult needed help and has figured out an action to support the adult [INAUDIBLE]. Our machines are not able to do anything close to this. But this is an important and interesting grand challenge for the future of artificial intelligence and computation. Can we understand what is going on inside the brain of this 18-months-old and create machines that are engineered for similar levels of cognition, for similar cooperative behavior? So machine learning is changing the world, and it's enabling so many extraordinary applications. But there are significant challenges. Fields that have data, that have lots and lots of data, benefit much more than fields that don't. So obtaining massive data sets is a challenge. The image data sets contain tens of millions of images. In fact, image recognition took off about five years ago when ImageNet reached about 10 million images. Today, it's much higher. Data labeling is also a challenge because these tens of millions of data points have to be manually labeled. And then it's important to know that the results that come out of machine learning systems are not easily explainable nor generalizable. Finally, your answer is only as good as the data you have to train your network. And so if your data has bias in it, then your results, the results of the network will have bias. And we have to keep in mind what it means to learn. A few more challenges, most of today's machine learning solutions are one-off solutions. And machine learning is usually done by experts only. So the program that placed AlphaGo with you is not going to be able to play chess or poker with you whereas a human would very naturally switch from one to the other. Also, just crunching the data does not mean you have knowledge, and making complex calculations does not mean you have autonomy. And so we have to keep in mind, where are the benefits? And we have to think how-- what problems should we work on in order to expand the capabilities of these techniques? Still, we have a lot of opportunities with the technology we have today as we have seen in the area of medicine, of transportation, of finance, of law. And these opportunities are around personalization and customization, around using natural language to interpret knowledge, to interpret what is in our libraries, and to bring that information at our fingertips. And altogether with machine learning, we can really increase the quality and the efficiency of the time we spend doing various tasks, mostly low-level tasks, routine tasks. But this is all about machines and people working together. So let me switch gear and say a few words about autonomy in motion. And as I think about this, I observe that our world has been so changed by computation. Just try to imagine a day in your life without your smartphone, without the web, and everything they enable-- no social media, no online shopping, no email and texting, no digital media. It's incredible to think about what this might be like. But I will tell you that 20 years ago, which I remember, we didn't have any of this. OK, so in a world so changed by computation that's helping us with all these different tasks, what might it look with robots helping us with physical work? So I believe that autonomous driving is absolutely going to ensure that there will be no road fatalities. And it will give our parents and grandparents much higher quality of life in their retirements. And it will give all of us the ability to go anywhere anytime. It is not a matter of if. It is a matter of when, in my opinion. So how much work should we be able to have to offload to machines? So imagine driving home from work knowing that your car has the intelligence to keep you safe and the smarts to make that ride fun. Let's say you have to pick up supplies for dinner. Your car pulls over at the nearest grocery store where you hand the dinner menu to a robot at the door. This robot connects with your home, and the home figures out what items you're missing. And a few minutes later, a box gets presented to you by another robot. And when you get home, you hand the box to your kitchen robot. And you might even let your children help with cooking because, even though they make a mess, your home cleaning robot will clean up the mess. OK, now, I know you might-- I know what you might be thinking. You might be thinking that this sounds like one of those cartoons about the future that never comes to pass. But that future, in my opinion, is not that far off. Today, robots have become our partners in domestic and industrial settings. They work side by side with doctors in hospitals. They work side by side with workers on the factory floors. They mow our lawns. They vacuum our pools. They even milk our cows. And in the future, they will do so much more for us. But in order to get to this future, we really need to build the science of autonomy. We need to improve our robots. And that means making robots that are much more capable of figuring things out in the world, making the whole process of creating robots faster, and making the process of interaction between robots and people much more intuitive. Now, this, the first two, are very important because each machine is made of a body and a brain. And for any task, the machine has to have the body that is capable of that task and the brain that is capable of controlling the body to do the task. A robot that rolls on wheels is not going to be able to fly nor is it going to be able to climb stairs. So you see the body and the brain together are very important in thinking about machines. So let me spend the rest of the next segment talking about some examples of making better brains, better bodies, and better user interactions. Let me start with brains. And I will say that, today, robots have a limited ability to figure things out. Most of their interactions are fairly carefully specified. And in some cases, you have limited adaptation. But general, but robots do not have a general ability of figuring out what is happening around them. And I would like to use the example of self-driving cars to illustrate this point. This is how-- this is the recipe for making a self-driving car. It's the recipe that's used by all the companies and most research groups around the world. So autonomous driving usually works in a closed environment. In the first phase, the robot drives on every road in the environment, makes a map, and that map is then used to plan paths and execute the paths when ride requests happen. So here's how you might take your own car and turn it into a robot if you like. You start with your car. You add sensors, usually laser scanners and cameras. And then you'll write some code. You write some code to make maps, to identify obstacles in the maps, and to label what they are. And you also write code to figure out where the new obstacles that were not there when the map was made are located. And then these maps can also be used in real time to help the robot figure out where it is. Because, using sensors, the robot makes a profile of the road at the current moment and compares that with a map. This allows the robot to figure out where it is. And then planning and control enables the robot to figure out how to go from one location to another and how to execute that pass. OK, so this is the recipe So and by the way, this is one of our most recent cars. At MIT, we have a whole suite of vehicles for autonomous driving, ranging from wheelchairs, golf carts, and cars. And they were all done with the recipe I showed you. Now, here is the problem. So generally, the sensor pipeline for autonomous driving has a bunch of steps. And the steps in the middle are usually very carefully and manually fine-tuned. For every instance, we will usually put code, or we specify the parameters that are needed in order to cope with that instance. And this necessarily limits what the robot is able to do. And in particular, it is very hard to deal with nighttime driving. It is very hard to deal with situations where we do not have maps for the road. And it is very difficult to deal with rainy or snowy weather. Rain and snow is actually a deeper problem because the sensors we have today do not work well in rain and snow. So what can we do? Well, here comes machine learning where, instead of manually configuring everything that goes in the planning and reasoning system of a car then trying to anticipate all possible road situations that would come to the car, what we can do is we can try to learn how to drive by watching a human how to drive. And in our research, we are asking whether it is possible to learn how to steer the car by looking at single images of the road and watching what a human driver has done in those instances. So the answer is, yes, it is possible to make driving more flexible. And here is our car that's taking a first ride on a country road after being trained by human drivers driving in Cambridge, which is very urban as compared to this road. And so in red and blue, you see what the car should be doing and what the car thinks it should be doing. And you can see that the red and blue arrows are very close together for the shorter distance. But there, they get further apart as you go further out. But eventually, they converge. And to be honest, I'm really delighted by this first drive of our car. If I think about my own first drive, it was not this smooth. So I'm very encouraged by the possibility of using machine learning to move away from having to manually code all the parameters that go in the reasoning engine of a robot. Now we can do similar-- we can pose similar questions with respect to other robot tasks. So a similar approach was done to teach this robot how to do this task. And I might ask you, how many robots does it take to screw in a light bulb? And in this case, the answer is one. But it needs machine learning, and it needs soft hands. And I will tell you in a second what those soft hands are. If you look at these robot hands, they're very compliant. They look a lot like the human hands unlike today's industrial manipulators, which are very rigid and hard. So the other thing to notice is that the robot grasps the bottom using one type of grasp and the bulb using a different one. So how should the robot learn how to do that? This is a very important aspect of grasping and manipulation. And it turns out that a very similar approach to the one we have developed for the cars can be used to learn how to grasp objects. And this can be used to teach a robot the approach direction, so should I go this way, or this way, or this way, and also the pose of the gripper when the robot grasps. And what's kind of exciting about grasping, especially with soft fingers, is that soft fingers wrap around the objects they are grasping. So the soft fingers do not need to know accurate models of what it is that they're going for. And also, with soft fingers, we don't need to know exactly the location of the objects. So we can do much more complying, much more error-tolerant behaviors. Furthermore, what matters mostly is the aspect ratio of the objects we are trying to do. So it turns out that for this particular approach to grasping, we do not need millions of examples to train the robot. We just need the examples of the critical aspect ratios that result in different approach directions and in different poses. And if you specify that, then any object that fits in the enclosing box of that aspect ratio is in some sense considered trained for that particular task. So here it is. And you can see the robot apply using different approach directions and using different grasp poses. And this was all trained using not 10 million examples but using a very small data set. But again, the data set represented classes of different possibilities for grasping. So you see, it's very exciting to think about bringing machine learning into robot control systems. Because through machine learning, we are really advancing and moving away from manually coding all the parameters that go in the brain. And I should tell you that, in fact, Manuela has been quite the pioneer in this. She figured this out way before the rest of the world. So Manuela has been telling us to do this for many, many years. So I'm very excited and optimistic about the possibilities of making the robots much more capable of figuring things out in their surrounding environments. Now, let me say a few things about the robot bodies. Because as I said earlier, the robots really-- a robot really needs to have a body capable of the task that the robot has to do. And right now, I would say that designing new robots is kind of the way things were with programming before we invented the compiler. So every robot is designed from scratch. Every robot is designed in a very bottom-up way. And we have mostly spent the last 60 years of industrial robotics thinking about robot bodies that are either inspired by the human form, so humanoids, or robot arms, or robots on wheels. And occasionally, we had some inspiration from nature. But nature is so much broader. There are so many more things we can keep in mind in terms of robot bodies and shapes. And so a question is, can we speed up how we design and we fabricate new robots? Can we imagine a future where anybody can make a robot? So let's take Alice, for example. Alice-- and let's say we want to give Alice the ability to automate tasks in her home. And Alice works. So let's say Alice wants a robot to play with her cat while she's at work. Well, to do so in this near future, Alice heads to a new type of store called 24-hour Robot Manufacturing where, equipped with an intuitive design, Alice could figure out the shape of a machine she wants for the robot. And once she settles on the design, the store could make the robot overnight for a very low cost. And now the cat has a playmate. OK, so how crazy is this idea? Can you imagine a future where we can specify the function of the robot? Let's say I want a robot to play chess with me. And from this natural language specification, can we imagine using natural language understanding to parse the specification? To identify what behaviors are needed for playing chess, we have to be able to pick up a piece to move it from here to there, to not knock off other pieces, and then, using databases of available mechanisms, synthesize a device that is able to do exactly the behaviors that are needed for the task. And then we'd like to do something very simple, like printing, to create this robot. And so here is the robot that plays chess with you. Now, this robot was not developed starting with a natural language specification. But many parts of making this robot were, in fact, automated. And if you're interested in how it was done, I can tell you later in great detail. But here's the general idea. If you have a body shape and you want to create that body shape, you can use off-the-shelf technologies to turn a photo into a mesh and then to take that mesh and unfold it, either by slicing it or by an origami unfolding process, to the point where you end up with a flat representation of the face that can be folded into the 3D object. And with that, you can then create a compilation system that starts with a picture and creates an actual robot that turns that picture to life, like this bunny. And I believe that with this approach, we can stretch our minds to think about all sorts of objects becoming roboticized. Imagine if we can awaken many of the objects in our surrounding world and turning them into a type of robot, into a machine that can exert work for you. So for instance, we could ask ourselves, what might the Sydney Opera House look like if it were a robot? Can I have sound? Can we increase the sound? And this is what the Sydney Opera House would be if it were a robot. [OPERA PLAYING] [NON-ENGLISH SINGING] So we can get the Sydney Opera House to make itself. Well, we haven't quite gotten to that point. But here's the robots that starts as a piece of plastic that was designed like in the case of the robot bunny. And when exposed to heat, this robot can grow into a fully fledged, three-dimensional object that can move around and can do all sorts of interesting things. Now, we can make these robots at any scale. But it turns out that even at the small scale that you'll see in this picture, which is centimeter scale, these robots can do interesting things for us. We are using-- we're looking to use these kinds of robots, which we call "origami robots," to enable a better future for medicine, to enable the creation of mini-surgeons that will provide surgeries without incisions, without pain, without physical infections. And the particular task, the particular surgical task we looked at is whether we can use such robots to remove button batteries that people accidentally ingest. And the reason button batteries are dangerous is because they are-- the acid in the batteries pierces a hole in the stomach very quickly, like in a half an hour you'll get the battery to be fully submersed. So you want those out today. You have to have surgery for that. But now imagine taking that little origami robot, compressing it, and surrounding it in ice, so putting it inside the pill-shaped ice. The patient could swallow the pill. When the robot gets in the stomach, the ice could melt. The robot could deploy. And then with a little magnet embedded in the body, in its body, the robot could go and remove the foreign object using feedback from an fMRI-like machine. So here is the robot, and here's how it goes. And it pulls the battery, and now it's the whole thing can be eliminated through the digestive system. And later on, the robot could be sent back in the stomach. And now the robot could serve as a patch, or it could have medicine in its body. And it could be directed to the location of the wound to deliver the medicine in a very precise way. Think about doing that with cancers where right now, today, you have-- you expose your whole body to the cancer medicine. So it's kind of exciting to also think about the fact that these robots are actually made out of food. So we think of robots as being made out of plastic, or hard plastics, or metals. This robot is made out of sausage casing, and it's digestible. Nevertheless, it's still a robot. So there are so many possibilities for using these technologies to enable a future for medicine that does not require incisions for every [INAUDIBLE] that does not lead to the risk of infections and does not give physical pain. Now I want to spend the last part about robots looking at machine interactions. And when we talk about machine interactions, we usually talk about interactions between machines. And in fact, Manuela is quite an expert on this. She has been the reigning champion of RoboCup for many, many years where her machines works together to win against other machines. I'm going to let her present next time on this topic. Today, I will focus on intuitive interactions between machines and people. And I want to go back to my autonomous driving example. And during Q&A, we can talk about where we are really with autonomous driving because we are not ready for level 5 autonomy. But we are ready for some applications in level 4 autonomy. But another interesting application of all of that know-how, all of those algorithms is in aiding visually impaired people and blind people to experience the world in ways that is unprecedented. There are tens of millions, hundreds of millions of visually impaired people around the world. And today, the most they have is the white walking stick that gives them one point of information about the world. Close your eyes, and try to imagine what it would be like to enter into this auditorium, and find a seat, and determine whether it is empty or not using a white walking stick. Well, in today's age of technology where we count our steps and we send machines to other planets, we should be able to do better than the walking sticks. And here's my idea. So you take the technology of a self-driving car, and you map it onto wearables. So you take the laser scanners, and you make them into laser belts that are backed by vibrating motors. And then you take the cameras, and you make them into cool necklaces. And then you sew all the computers and everything else inside the clothes. And with this, you have essentially the hardware of a system that can look at the world, map the obstacles in the world, and give you vibration when you're close to an obstacle. So let's say I'm close to this wall. This side of my belt will vibrate. And also, through a Braille buckle, the system could also talk to me. It could tell me words. So with this technology, you could then walk down Fifth Avenue and describe the fabulous window displays. Or you could alert the person to obstacles. So you could say, hey, your friend Alice is walking by, or there's a cat next to you. And so this is actually very close. This kind of solution is really close to being used, to being deployed. And here is an example of an implementation on our user at MIT where the system aids the user to walk down hallways, to go down steps without bumping into any obstacles. The system is able to guide the user to a bench where he could sit there, and wait for the ducks, and feed the ducks. This is an incredible increase of quality of life. And so you'll see with AI and robotics, we really have the opportunity to make the world better for many people. With AI and robots, we can also begin to dream about the future of manufacturing where robots are no longer separated from people on the factory floor. And here, we show an example where our human is collaborating with a robot and using sensors that monitor muscle activity. The robot, essentially, adapts to what the human is doing. And so by stiffening and relaxing the muscles, the human can communicate to the robot to stiffen or relax its own grip. So in this case, the muscles are stiff. The robot is stiff. In the previous case, the robot was quite limp. But we can begin to think about interesting sensors that we could put on our bodies. And I suppose the ultimate question is, can we go directly from our heads to the machine? And the answer is, no, we cannot do that, in general, today. The sensors we have available for that are EEG caps that are quite sparse, 48 sensors distributed on a cap. And they measure electric activity in your brain. And most of the times, what you get from these sensors is really very complex and cannot be decoded. But it turns out that there is one signal that, with machine learning, we can detect quite accurately. And this is a signal we all make. It's not trained. It doesn't matter what language we think in. You know what the signal is? OK, this is the "you are wrong" signal. It is called the error-related potential. It turns out that "you are wrong" signal is something very strong and profound that we all feel. And it's a localized sensor. And it has a unique profile, and it can be detected. So with a "you are wrong" signal, we can watch machines, and we can tell whether they're performing well or not. And so to demonstrate that this is a possibility, we have-- could you put the sound down? So we can watch robots and correct their mistakes in real time. These signals can be detected in 100 milliseconds. And here's a user that's watching a robot. The task of the robot is to sort paint cans in a bin labeled "paint" and wire spooling in a bin labeled "wire." The robot is presented these objects in a random order and then randomly goes one way or another. And so here, the robot went to "wire" and then moved right back because the human said that is wrong. And here, you see? So it went, and the correction was done instantaneously. Here, it went directly to "paint," and that was correct. Now here's the "wire." It goes mistakenly to the paint box. And the robot gets directed by the person to shift over. So it is really exciting to think about a future where machines adapt to us rather than the other way around. Today's machines do not adapt to us. We actually have to code them. But imagine advancing our know-how, our technologies to the point where we reach that place. It's not quite there yet. OK, so we talked about AI and its possibilities. We talked about robots and its possibilities. I want to address an elephant that's always in the room when I talk about these technologies, which is jobs. And usually, when I tell people what I do, I get one of two reactions-- either people start cracking jokes about Skynet and ask me when the robots will take over their jobs or people ask when their cars will become self-driving. So I believe that, of course, our cars will be self-driving too. I'm very excited about the technology. But we have to understand the fears of the first group. We have to understand how to provide alternatives for how to see things differently. And this starts with understanding that AI and robotics are tools. They are tools that are not inherently good or bad. They are tools, and they do what we choose to do with them. And today, they mostly do-- they can mostly do routine, low-level things. So if you think about four classes of jobs in terms of how much cognition and how much manual, unskilled labor there is, I will tell you that the jobs at the top and at the bottom are too hard for machines today. It is much easier to send a robot to Mars than it is to get that robot to clear a tabletop. And likewise, robots are not going to make the decisions in finance, or in medicine, or in any other field. But even the jobs in the middle, like accountancy where there's a lot of routine activity, even those jobs have critical aspects that cannot be done by machines. So for instance, the accountant has to meet with you and discuss with you. And that the machine can do. So it is better to think about what we can automate in terms of the tasks that we do than in terms of professions that disappear. And here you'll see an excellent study from McKinsey that shows a variety of different jobs and the amount of time people spend managing others, applying expertise, doing stakeholder interactions, unpredictable physical or data collection, processing of data, and predictable, physical work. It turns out that the tasks that can be automated today with our level of technology are the data tasks and the predictable, physical work tasks. And I'm actually quite excited when I think about the future. Because we can spend a lot of time analyzing these graphs and thinking about what might go away, but it's very difficult for us to imagine what will come back. So for instance, in the 20th century, agricultural employment dropped from 40% to 2% percent. But nobody predicted the growths in the service jobs. And similarly, when the airline industry took off, the jobs in the airline industry increased, and the jobs in the train industry decreased. So I would like to say that as we advance technology, we create new jobs. And we have no idea what those jobs might be. In fact, I'm sure most of you can remember 10 years ago. I remember very well 10 years ago. There were no smartphones. There was no social media. There was no cloud. So all of these are sectors that are employing a lot of people today. These jobs did not exist 10 years ago. So getting there though requires that we think about whether we train our kids with the right skills. And we have an education problem, both short term and long term. Long term, if we start teaching computing-- in fact, I like computational thinking and computational making. If we do this from early on, then we will end up with a graduating class in 10 years' time where everyone can participate in the IT economy. In the meantime, we can take lessons from companies like Bit Source, which is a startup in Kentucky, a very successful startup. It's been training coal miners to become data miners. And it's one of the great successes in Kentucky. So I'd like to end by a few reflections on our future with robots and machines. This is the first industrial robot. It is called a Unimate. It was introduced in 1961. By 2020, we will get to 31 million industrial robots, so from 1 to 31 million. These industrial robots are masterpieces of engineering. They can do so much more than people do. And yet they remain isolated from people on the assembly line because they're large and dangerous to be around. But in fact, in nature, organisms in nature are soft, and compliant, and much more dexterous. And so look at this example of an octopus bending and twisting to escape through a narrow hole. Or look at this gentle elephant that is able to pick up the banana from the child. And yet the elephant can use the trunk to fend off a competitor. We can begin to think about new materials and new approaches for making robots in a way that enables them to be safer to be around, in a way that is much more inspired by how nature forms its organisms. And I wanted to show this example of one of my beloved robots, called Sophie. And here's Sophie swimming side by side with robots in the natural world. And if I didn't tell you that this was a robot, maybe you could indulge me and say, oh, yeah, it looks like a real fish. So with the development of soft materials, machines and materials are getting closer together with machines getting softer, more like materials, and materials getting much more intelligent, more like machines. And so this raises an interesting question. What is a robot? What do-- what kind of materials were used in order to make robots? And so I would like to propose that we should expand our view of what a robot is so that the next 60 years will usher adaptive, soft machines that could work side by side with humans and could give us pervasive support of machines and form diversity. And here is how this future might look like. So imagine waking up enabled by your personal assistant that figures out the optimal time when you wake up, and helps you organize the outfit you want to wear, and also what you might need for work. On your way to work, you walk by beautiful stores that display your own image with the latest and greatest fashions. And when you walk inside the store, your body gets scanned. And you get bespoke shoes and bespoke clothing done right away. And materials that have sensors and computation embedded in them, and have even the ability to reprogram what they look like, might even allow you to match your outfit to your friend's outfit you might come across in the stores. So here, they're matching the outfits. And at work, intelligent rooms might notice that you're stressing out over a meeting and adjust the temperature accordingly. And intuitive design interfaces might allow you to connect with your colleagues present virtually far away to design, let's say, the next flying car. And this flying car could be connected to the rest of the infrastructure and to your home to do the-- to become the ultimate assistance in managing your transportation and your chores. So for instance, in this case, Alice's mother gets the task to pick up some plants. And these plants are for Alice's grandmother, who's planting a garden. Your packages could arrive delivered by robots. The garbage bins could take themselves out. The bikes could have adaptive wheels that give you just the right level of support when you want to exercise. And while the robot digs in the garden, you can stop to have a nice conversation with your grandmother. And when the end of the day is near after a good day, it is time for a bedtime story, which allows you to enter the story and interact with a dragon. So these advances in science, in the science and engineering of autonomy can create an extraordinary future for all of us with machines taking on difficult-- I mean, with machines taking on increasingly more difficult tasks, machines allowing us to focus on what we find exciting, and interesting, and also, frankly, each other. Thank you very much. 